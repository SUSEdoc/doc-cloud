<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Managing Networking | Operations Guide CLM | SUSE OpenStack Cloud 9</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DAPS 3.0.0 (https://opensuse.github.io/daps) using SUSE XSL Stylesheets 2.0.17 (based on DocBook XSL Stylesheets 1.79.2) - chunked" /><meta name="product-name" content="SUSE OpenStack Cloud" /><meta name="product-number" content="9" /><meta name="book-title" content="Operations Guide CLM" /><meta name="chapter-title" content="Chapter 10. Managing Networking" /><meta name="description" content="Information about managing and configuring the Networking service." /><meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi" /><meta name="tracker-type" content="bsc" /><meta name="tracker-bsc-component" content="Documentation" /><meta name="tracker-bsc-product" content="SUSE OpenStack Cloud 9" /><link rel="home" href="index.html" title="Documentation" /><link rel="up" href="book-operations.html" title="Operations Guide CLM" /><link rel="prev" href="ops-managing-objectstorage.html" title="Chapter 9. Managing Object Storage" /><link rel="next" href="ops-managing-dashboards.html" title="Chapter 11. Managing the Dashboard" />
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css"></link>');
}
else {
  document.write('<link rel="stylesheet" type="text/css" href="static/css/fonts-onlylocal.css"></link>');
}

</script><noscript><link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-navigation">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><div id="_outer-wrap"><div id="_white-bg" style="background-color: #FABEBE;"><div id="_header"><div id="_logo"><img src="static/images/logo.png" alt="Logo" /></div><div class="crumbs"><a class="book-link" href="index.html" title="Documentation"><span class="book-icon">Documentation</span></a><span> › </span><a class="crumb" href="book-operations.html">Operations Guide CLM</a><span> › </span><a class="crumb" href="ops-managing-networking.html">Managing Networking</a></div><div class="clearme"></div></div></div><div id="_toolbar-wrap"><div id="_toolbar"><div id="_toc-area" class="inactive"><a id="_toc-area-button" class="tool" title="Contents" accesskey="c" href="index.html"><span class="tool-spacer"><span class="toc-icon">Contents</span><span class="clearme"></span></span><span class="tool-label">Contents</span></a><div class="active-contents bubble-corner"></div><div class="active-contents bubble"><div class="bubble-container"><h6>Operations Guide CLM</h6><div id="_bubble-toc"><ol><li class="inactive"><a href="gettingstarted-ops.html"><span class="number">1 </span><span class="name">Operations Overview</span></a></li><li class="inactive"><a href="tutorials.html"><span class="number">2 </span><span class="name">Tutorials</span></a></li><li class="inactive"><a href="clm-admin-ui.html"><span class="number">3 </span><span class="name">Cloud Lifecycle Manager Admin UI User Guide</span></a></li><li class="inactive"><a href="third-party-integrations.html"><span class="number">4 </span><span class="name">Third-Party Integrations</span></a></li><li class="inactive"><a href="ops-managing-identity.html"><span class="number">5 </span><span class="name">Managing Identity</span></a></li><li class="inactive"><a href="ops-managing-compute.html"><span class="number">6 </span><span class="name">Managing Compute</span></a></li><li class="inactive"><a href="ops-managing-esx.html"><span class="number">7 </span><span class="name">Managing ESX</span></a></li><li class="inactive"><a href="ops-managing-blockstorage.html"><span class="number">8 </span><span class="name">Managing Block Storage</span></a></li><li class="inactive"><a href="ops-managing-objectstorage.html"><span class="number">9 </span><span class="name">Managing Object Storage</span></a></li><li class="inactive"><a href="ops-managing-networking.html"><span class="number">10 </span><span class="name">Managing Networking</span></a></li><li class="inactive"><a href="ops-managing-dashboards.html"><span class="number">11 </span><span class="name">Managing the Dashboard</span></a></li><li class="inactive"><a href="ops-managing-orchestration.html"><span class="number">12 </span><span class="name">Managing Orchestration</span></a></li><li class="inactive"><a href="topic-ttn-5fg-4v.html"><span class="number">13 </span><span class="name">Managing Monitoring, Logging, and Usage Reporting</span></a></li><li class="inactive"><a href="using-container-as-a-service-overview.html"><span class="number">14 </span><span class="name">Managing Container as a Service (Magnum)</span></a></li><li class="inactive"><a href="system-maintenance.html"><span class="number">15 </span><span class="name">System Maintenance</span></a></li><li class="inactive"><a href="manage-ops-console.html"><span class="number">16 </span><span class="name">Operations Console</span></a></li><li class="inactive"><a href="bura-overview.html"><span class="number">17 </span><span class="name">Backup and Restore</span></a></li><li class="inactive"><a href="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1.html"><span class="number">18 </span><span class="name">Troubleshooting Issues</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_nav-area" class="inactive"><div class="tool"><span class="nav-inner"><span class="tool-label">Navigation</span><a accesskey="p" class="tool-spacer" title="Chapter 9. Managing Object Storage" href="ops-managing-objectstorage.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 11. Managing the Dashboard" href="ops-managing-dashboards.html"><span class="next-icon">→</span></a></span></div></div></div></div><div id="_fixed-header-wrap" style="background-color: #FABEBE;" class="inactive"><div id="_fixed-header"><div class="crumbs"><a class="book-link" href="index.html" title="Documentation"><span class="book-icon">Documentation</span></a><span> › </span><a class="crumb" href="book-operations.html">Operations Guide CLM</a><span> › </span><a class="crumb" href="ops-managing-networking.html">Managing Networking</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="button"><a accesskey="p" class="tool-spacer" title="Chapter 9. Managing Object Storage" href="ops-managing-objectstorage.html"><span class="prev-icon">←</span></a><a accesskey="n" class="tool-spacer" title="Chapter 11. Managing the Dashboard" href="ops-managing-dashboards.html"><span class="next-icon">→</span></a></div><div class="clearme"></div></div><div class="clearme"></div></div></div><div id="_content" class="draft "><div class="documentation"><div class="chapter " id="ops-managing-networking"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname "><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <span class="productnumber ">9</span></div><div><h1 class="title"><span class="number">10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Networking</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-managing_networking.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_networking.xml</li><li><span class="ds-label">ID: </span>ops-managing-networking</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="ops-managing-networking.html#topic-gll-nsn-15"><span class="number">10.1 </span><span class="name">SUSE <span class="productname">OpenStack</span> Cloud Firewall</span></a></span></dt><dt><span class="section"><a href="ops-managing-networking.html#UsingVPNaaS"><span class="number">10.2 </span><span class="name">Using VPN as a Service (VPNaaS)</span></a></span></dt><dt><span class="section"><a href="ops-managing-networking.html#designateOverview"><span class="number">10.3 </span><span class="name">DNS Service Overview</span></a></span></dt><dt><span class="section"><a href="ops-managing-networking.html#neutron-overview"><span class="number">10.4 </span><span class="name">Networking Service Overview</span></a></span></dt><dt><span class="section"><a href="ops-managing-networking.html#CreateHARouter"><span class="number">10.5 </span><span class="name">Creating a Highly Available Router</span></a></span></dt></dl></div></div><p>
  Information about managing and configuring the Networking service.
 </p><div class="sect1" id="topic-gll-nsn-15"><div class="titlepage"><div><div><h2 class="title"><span class="number">10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE <span class="productname">OpenStack</span> Cloud Firewall</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#topic-gll-nsn-15">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configure_firewall.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configure_firewall.xml</li><li><span class="ds-label">ID: </span>topic-gll-nsn-15</li></ul></div></div></div></div><p>
  Firewall as a Service (FWaaS) provides the ability to assign network-level,
  port security for all traffic entering an existing tenant network. More
  information on this service can be found in the public OpenStack
  documentation located at <a class="link" href="http://specs.openstack.org/openstack/neutron-specs/specs/api/firewall_as_a_service__fwaas_.html" target="_blank">http://specs.openstack.org/openstack/neutron-specs/specs/api/firewall_as_a_service__fwaas_.html</a>.
  The following documentation provides command-line interface example
  instructions for configuring and testing a SUSE <span class="productname">OpenStack</span> Cloud firewall. FWaaS can also
  be configured and managed by the horizon web interface.
 </p><p>
  With SUSE <span class="productname">OpenStack</span> Cloud, FWaaS is implemented directly in the L3 agent
  (<span class="emphasis"><em>neutron-l3-agent</em></span>). However if VPNaaS is enabled, FWaaS
  is implemented in the VPNaaS agent (<span class="emphasis"><em>neutron-vpn-agent</em></span>).
  Because FWaaS does not use a separate agent process or start a specific
  service, there currently are no monasca alarms for it.
 </p><p>
  If DVR is enabled, the firewall service currently does not filter traffic
  between OpenStack private networks, also known as <span class="emphasis"><em>east-west
  traffic</em></span> and will only filter traffic from external networks, also
  known as <span class="emphasis"><em>north-south traffic</em></span>.
 </p><div id="id-1.5.12.3.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   The L3 agent must be restarted on each compute node hosting a DVR router
   when removing the FWaaS or adding a new FWaaS. This condition only applies
   when updating existing instances connected to DVR routers. For more
   information, see the <a class="link" href="https://bugs.launchpad.net/neutron/+bug/1845557" target="_blank">upstream
   bug</a>.
  </p></div><div class="sect2" id="id-1.5.12.3.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Overview of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Firewall configuration</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.3.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configure_firewall.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configure_firewall.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
  The following instructions provide information about how to identify and
  modify the overall <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> firewall that is configured in front of the
  control services. This firewall is administered only by a cloud admin and is
  not available for tenant use for private network firewall services.
 </p><p>
  During the installation process, the configuration processor will
  automatically generate "allow" firewall rules for each server based on the
  services deployed and block all other ports. These are populated in
  <code class="literal">~/openstack/my_cloud/info/firewall_info.yml</code>, which includes
  a list of all the ports by network, including the addresses on which the
  ports will be opened. This is described in more detail in
  <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 5 “Input Model”, Section 5.2 “Concepts”, Section 5.2.10 “Networking”, Section 5.2.10.5 “Firewall Configuration”</span>.
 </p><p>
  The <code class="literal">firewall_rules.yml</code> file in the input model allows you
  to define additional rules for each network group. You can read more about
  this in <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.15 “Firewall Rules”</span>.
 </p><p>
  The purpose of this document is to show you how to make post-installation
  changes to the firewall rules if the need arises.
 </p><div id="id-1.5.12.3.6.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   This process is not to be confused with Firewall-as-a-Service,
   which is a separate service that enables the ability for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> tenants
   to create north-south, network-level firewalls to provide stateful
   protection to all instances in a private, tenant network. This service is
   optional and is tenant-configured.
  </p></div></div><div class="sect2" id="idg-all-networking-fwaas-xml-9"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 FWaaS Configuration</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#idg-all-networking-fwaas-xml-9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configure_firewall.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configure_firewall.xml</li><li><span class="ds-label">ID: </span>idg-all-networking-fwaas-xml-9</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Check for an enabled firewall.</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     You should check to determine if the firewall is enabled. The output of
     the <span class="emphasis"><em>openstack extension list</em></span> should contain a
     firewall entry.
    </p><div class="verbatim-wrap"><pre class="screen">openstack extension list</pre></div></li><li class="listitem "><p>
     Assuming the external network is already created by the admin, this
     command will show the external network.
    </p><div class="verbatim-wrap"><pre class="screen">openstack network list</pre></div></li></ol></div><p>
   <span class="bold"><strong>Create required assets.</strong></span>
  </p><p>
   Before creating firewalls, you will need to create a network, subnet,
   router, security group rules, start an instance and assign it a floating IP
   address.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Create the network, subnet and router.
    </p><div class="verbatim-wrap"><pre class="screen">openstack network create private
openstack subnet create --name sub private 10.0.0.0/24 --gateway 10.0.0.1
openstack router create router
openstack router add subnet router sub
openstack router set router ext-net</pre></div></li><li class="listitem "><p>
     Create security group rules. Security group rules filter traffic at VM
     level.
    </p><div class="verbatim-wrap"><pre class="screen">openstack security group rule create default --protocol icmp
openstack security group rule create default --protocol tcp --port-range-min 22 --port-range-max 22
openstack security group rule create default --protocol tcp --port-range-min 80 --port-range-max 80</pre></div></li><li class="listitem "><p>
     Boot a VM.
    </p><div class="verbatim-wrap"><pre class="screen">NET=$(openstack network list | awk '/private/ {print $2}')
openstack server create --flavor 1 --image &lt;image&gt; --nic net-id=$NET vm1 --poll</pre></div></li><li class="listitem "><p>
     Verify if the instance is ACTIVE and is assigned an IP address.
    </p><div class="verbatim-wrap"><pre class="screen">openstack server list</pre></div></li><li class="listitem "><p>
     Get the port id of the vm1 instance.
    </p><div class="verbatim-wrap"><pre class="screen">fixedip=$(openstack server list | awk '/vm1/ {print $12}' | awk -F '=' '{print $2}' | awk -F ',' '{print $1}')
vmportuuid=$(openstack port list | grep $fixedip | awk '{print $2}')</pre></div></li><li class="listitem "><p>
     Create and associate a floating IP address to the vm1 instance.
    </p><div class="verbatim-wrap"><pre class="screen">openstack floating ip create ext-net --port-id $vmportuuid</pre></div></li><li class="listitem "><p>
     Verify if the floating IP is assigned to the instance. The following
     command should show an assigned floating IP address from the external
     network range.
    </p><div class="verbatim-wrap"><pre class="screen">openstack server show vm1</pre></div></li><li class="listitem "><p>
     Verify if the instance is reachable from the external network. SSH into
     the instance from a node in (or has route to) the external network.
    </p><div class="verbatim-wrap"><pre class="screen">ssh cirros@FIP-VM1
password: &lt;password&gt;</pre></div></li></ol></div><p>
   <span class="bold"><strong>Create and attach the firewall.</strong></span>
  </p><div id="id-1.5.12.3.7.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    By default, an internal "drop all" rule is enabled in IP tables if none of
    the defined rules match the real-time data packets.
   </p></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Create new firewall rules using <code class="literal">firewall-rule-create</code>
     command and providing the protocol, action (allow, deny, reject) and name
     for the new rule.
    </p><p>
     Firewall actions provide rules in which data traffic can be handled. An
     <span class="bold"><strong>allow</strong></span> rule will allow traffic to pass
     through the firewall,
     <span class="bold"><strong>deny</strong></span>
     will stop and prevent data traffic from passing through the firewall and
     <span class="bold"><strong>reject</strong></span>
     will reject the data traffic and return a
     <span class="emphasis"><em>destination-unreachable</em></span> response. Using
     <span class="bold"><strong>reject</strong></span> will speed up failure detection
     time dramatically for legitimate users, since they will not be required to
     wait for retransmission timeouts or submit retries. Some customers should
     stick with <span class="bold"><strong>deny</strong></span> where prevention of port
     scanners and similar methods may be attempted by hostile attackers. Using
     <span class="bold"><strong>deny</strong></span>
     will drop all of the packets, making it more difficult for malicious
     intent. The firewall action, <span class="bold"><strong>deny</strong></span> is the
     default behavior.
    </p><p>
     The example below demonstrates how to allow icmp and ssh while denying
     access to http. See the <code class="literal">OpenStackClient</code> command-line reference
     at <a class="link" href="https://docs.openstack.org/python-openstackclient/rocky/" target="_blank">https://docs.openstack.org/python-openstackclient/rocky/</a>
     on additional options such as source IP, destination IP, source port and
     destination port.
    </p><div id="id-1.5.12.3.7.9.1.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      You can create a firewall rule with an identical name and each instance
      will have a unique id associated with the created rule, however for
      clarity purposes this is not recommended.
     </p></div><div class="verbatim-wrap"><pre class="screen">neutron firewall-rule-create --protocol icmp --action allow --name allow-icmp
neutron firewall-rule-create --protocol tcp --destination-port 80 --action deny --name deny-http
neutron firewall-rule-create --protocol tcp --destination-port 22 --action allow --name allow-ssh</pre></div></li><li class="listitem "><p>
     Once the rules are created, create the firewall policy by using the
     <code class="literal">firewall-policy-create</code> command with the
     <code class="literal">--firewall-rules</code> option and rules to include in quotes,
     followed by the name of the new policy. The order of the rules is
     important.
    </p><div class="verbatim-wrap"><pre class="screen">neutron firewall-policy-create --firewall-rules "allow-icmp deny-http allow-ssh" policy-fw</pre></div></li><li class="listitem "><p>
     Finish the firewall creation by using the
     <code class="literal">firewall-create</code> command, the policy name and the new
     name you want to give to your new firewall.
    </p><div class="verbatim-wrap"><pre class="screen">neutron firewall-create policy-fw --name user-fw</pre></div></li><li class="listitem "><p>
     You can view the details of your new firewall by using the
     <code class="literal">firewall-show</code> command and the name of your firewall.
     This will verify that the status of the firewall is ACTIVE.
    </p><div class="verbatim-wrap"><pre class="screen">neutron firewall-show user-fw</pre></div></li></ol></div><p>
   <span class="bold"><strong>Verify the FWaaS is functional.</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Since allow-icmp firewall rule is set you can ping the floating IP address
     of the instance from the external network.
    </p><div class="verbatim-wrap"><pre class="screen">ping &lt;FIP-VM1&gt;</pre></div></li><li class="listitem "><p>
     Similarly, you can connect via ssh to the instance due to the allow-ssh
     firewall rule.
    </p><div class="verbatim-wrap"><pre class="screen">ssh cirros@&lt;FIP-VM1&gt;
password: &lt;password&gt;</pre></div></li><li class="listitem "><p>
     Run a web server on vm1 instance that listens over port 80, accepts
     requests and sends a WELCOME response.
    </p><div class="verbatim-wrap"><pre class="screen">$ vi webserv.sh

#!/bin/bash

MYIP=$(/sbin/ifconfig eth0|grep 'inet addr'|awk -F: '{print $2}'| awk '{print $1}');
while true; do
  echo -e "HTTP/1.0 200 OK

Welcome to $MYIP" | sudo nc -l -p 80
done

# Give it Exec rights
$ chmod 755 webserv.sh

# Execute the script
$ ./webserv.sh</pre></div></li><li class="listitem "><p>
     You should expect to see curl fail over port 80 because of the deny-http
     firewall rule. If curl succeeds, the firewall is not blocking incoming
     http requests.
    </p><div class="verbatim-wrap"><pre class="screen">curl -vvv &lt;FIP-VM1&gt;</pre></div></li></ol></div><div id="id-1.5.12.3.7.12" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
    When using reference implementation, new networks, FIPs and routers created
    after the Firewall creation will not be automatically updated with firewall
    rules. Thus, execute the firewall-update command by passing the current and
    new router Ids such that the rules are reconfigured across all the routers
    (both current and new).
   </p><p>
    For example if router-1 is created before and router-2 is created after the
    firewall creation
   </p><div class="verbatim-wrap"><pre class="screen">$ neutron firewall-update —router &lt;router-1-id&gt; —router &lt;router-2-id&gt; &lt;firewall-name&gt;</pre></div></div></div><div class="sect2" id="idg-all-operations-configure-firewall-xml-7"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Making Changes to the Firewall Rules</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#idg-all-operations-configure-firewall-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configure_firewall.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configure_firewall.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-configure-firewall-xml-7</li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit your
     <code class="literal">~/openstack/my_cloud/definition/data/firewall_rules.yml</code>
     file and add the lines necessary to allow the port(s) needed through the
     firewall.
    </p><p>
     In this example we are going to open up port range 5900-5905 to allow VNC
     traffic through the firewall:
    </p><div class="verbatim-wrap"><pre class="screen">  - name: VNC
    network-groups:
  - MANAGEMENT
    rules:
     - type: allow
       remote-ip-prefix:  0.0.0.0/0
       port-range-min: 5900
       port-range-max: 5905
       protocol: tcp</pre></div><div id="id-1.5.12.3.8.2.2.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The example above shows a <code class="literal">remote-ip-prefix</code> of
      <code class="literal">0.0.0.0/0</code> which opens the ports up to all IP ranges.
      To be more secure you can specify your local IP address CIDR you will
      be running the VNC connect from.
     </p></div></li><li class="listitem "><p>
     Commit those changes to your local git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "firewall rule update"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Create the deployment directory structure:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Change to the deployment directory and run the
     <code class="literal">osconfig-iptables-deploy.yml</code> playbook to update your
     iptable rules to allow VNC:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-iptables-deploy.yml</pre></div></li></ol></div><p>
   You can repeat these steps as needed to add, remove, or edit any of these
   firewall rules.
  </p></div><div class="sect2" id="sec-hp20fwaas-more"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">More Information</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#sec-hp20fwaas-more">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configure_firewall.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configure_firewall.xml</li><li><span class="ds-label">ID: </span>sec-hp20fwaas-more</li></ul></div></div></div></div><p>
   Firewalls are based in IPtable settings.
  </p><p>
   Each firewall that is created is known as an instance.
  </p><p>
   A firewall instance can be deployed on selected project routers. If no
   specific project router is selected, a firewall instance is automatically
   applied to all project routers.
  </p><p>
   Only 1 firewall instance can be applied to a project router.
  </p><p>
   Only 1 firewall policy can be applied to a firewall instance.
  </p><p>
   Multiple firewall rules can be added and applied to a firewall policy.
  </p><p>
   Firewall rules can be shared across different projects via the Share API
   flag.
  </p><p>
   Firewall rules supersede the Security Group rules that are applied at the
   Instance level for all traffic entering or leaving a private, project
   network.
  </p><p>
   For more information on the command-line interface (CLI) and
   firewalls, see the OpenStack networking command-line client reference:
   <a class="link" href="https://docs.openstack.org/python-openstackclient/rocky/" target="_blank">https://docs.openstack.org/python-openstackclient/rocky/</a>
  </p></div></div><div class="sect1" id="UsingVPNaaS"><div class="titlepage"><div><div><h2 class="title"><span class="number">10.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using VPN as a Service (VPNaaS)</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#UsingVPNaaS">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-networking-vpnaas.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-networking-vpnaas.xml</li><li><span class="ds-label">ID: </span>UsingVPNaaS</li></ul></div></div></div></div><p>
  <span class="bold"><strong><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 VPNaaS Configuration</strong></span>
 </p><p>
  This document describes the configuration process and requirements for the
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 Virtual Private Network (VPN) as a Service
  (VPNaaS) module.
 </p><div class="sect2" id="idg-all-networking-vpnaas-xml-7"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#idg-all-networking-vpnaas-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-networking-vpnaas.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-networking-vpnaas.xml</li><li><span class="ds-label">ID: </span>idg-all-networking-vpnaas-xml-7</li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> must be installed.
    </p></li><li class="listitem "><p>
     Before setting up VPNaaS, you will need to have created an external
     network and a subnet with access to the internet. Information on how to
     create the external network and subnet can be found in
     <a class="xref" href="ops-managing-networking.html#sec-vpnaas-more" title="10.2.4. More Information">Section 10.2.4, “More Information”</a>.
    </p></li><li class="listitem "><p>
     You should assume 172.16.0.0/16 as the ext-net CIDR in this document.
    </p></li></ol></div></div><div class="sect2" id="Considerations"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Considerations</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#Considerations">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-networking-vpnaas.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-networking-vpnaas.xml</li><li><span class="ds-label">ID: </span>Considerations</li></ul></div></div></div></div><p>
   Using the neutron plugin-based VPNaaS causes additional processes to be run
   on the Network Service Nodes. One of these processes, the ipsec charon
   process from StrongSwan, runs as root and listens on an external network. A
   vulnerability in that process can lead to remote root compromise of the
   Network Service Nodes. If this is a concern customers should consider using
   a VPN solution other than the neutron plugin-based VPNaaS and/or deploying
   additional protection mechanisms.
  </p></div><div class="sect2" id="idg-all-networking-vpnaas-xml-9"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuration</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#idg-all-networking-vpnaas-xml-9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-networking-vpnaas.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-networking-vpnaas.xml</li><li><span class="ds-label">ID: </span>idg-all-networking-vpnaas-xml-9</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Setup Networks</strong></span> You can setup VPN as a
   Service (VPNaaS) by first creating networks, subnets and routers using the
   <code class="literal">neutron</code> command line. The VPNaaS module enables the
   ability to extend access between private networks across two different
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> clouds or between a <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud and a non-cloud network. VPNaaS
   is based on the open source software application called StrongSwan.
   StrongSwan (more information available
   at <a class="link" href="http://www.strongswan.org/" target="_blank">http://www.strongswan.org/</a>)
   is an IPsec implementation and provides basic VPN gateway functionality.
  </p><div id="id-1.5.12.4.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    You can execute the included commands from any shell with access to the
    service APIs. In the included examples, the commands are executed from the
    lifecycle manager, however you could execute the commands from the
    controller node or any other shell with aforementioned service API access.
   </p></div><div id="id-1.5.12.4.6.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    The use of floating IP's is not possible with the current version of
    VPNaaS when DVR is enabled. Ensure that no floating IP is associated to
    instances that will be using VPNaaS when using a DVR router. Floating IP
    associated to instances are ok when using CVR router.
   </p></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     From the Cloud Lifecycle Manager, create first private network, subnet and
     router assuming that <span class="emphasis"><em>ext-net</em></span> is created by admin.
    </p><div class="verbatim-wrap"><pre class="screen">openstack network create privateA
openstack subnet create --name subA privateA 10.1.0.0/24 --gateway 10.1.0.1
openstack router create router1
openstack router add subnet router1 subA
openstack router set router1 ext-net</pre></div></li><li class="step "><p>
     Create second private network, subnet and router.
    </p><div class="verbatim-wrap"><pre class="screen">openstack network create privateB
openstack subnet create --name subB privateB 10.2.0.0/24 --gateway 10.2.0.1
openstack router create router2
openstack router add subnet router2 subB
openstack router set router2 ext-net</pre></div></li></ol></div></div><div class="procedure " id="pro-vpnaas-start"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 10.1: </span><span class="name">Starting Virtual Machines </span><a title="Permalink" class="permalink" href="ops-managing-networking.html#pro-vpnaas-start">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     From the Cloud Lifecycle Manager run the following to start the virtual
     machines. Begin with adding secgroup rules for SSH and ICMP.
    </p><div class="verbatim-wrap"><pre class="screen">openstack security group rule create default --protocol icmp
openstack security group rule create default --protocol tcp --port-range-min 22 --port-range-max 22</pre></div></li><li class="step "><p>
     Start the virtual machine in the privateA subnet. Using <span class="emphasis"><em>nova
     images-list</em></span>, use the image id to boot image instead of the
     image name. After executing this step, it is recommended that you wait
     approximately 10 seconds to allow the virtual machine to become active.
    </p><div class="verbatim-wrap"><pre class="screen">NETA=$(openstack network list | awk '/privateA/ {print $2}')
openstack server create --flavor 1 --image &lt;id&gt; --nic net-id=$NETA vm1</pre></div></li><li class="step "><p>
     Start the virtual machine in the privateB subnet.
    </p><div class="verbatim-wrap"><pre class="screen">NETB=$(openstack network list | awk '/privateB/ {print $2}')
openstack server create --flavor 1 --image &lt;id&gt; --nic net-id=$NETB vm2</pre></div></li><li class="step " id="st-vpnaas-obtain-ip"><p>
     Verify private IP's are allocated to the respective vms. Take note of IP's
     for later use.
    </p><div class="verbatim-wrap"><pre class="screen">openstack server show vm1
openstack server show vm2</pre></div></li></ol></div></div><div class="procedure " id="id-1.5.12.4.6.7"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 10.2: </span><span class="name">Create VPN </span><a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.4.6.7">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     You can set up the VPN by executing the below commands from the lifecycle
     manager or any shell with access to the service APIs. Begin with creating
     the policies with <code class="literal">vpn-ikepolicy-create</code> and
     <code class="literal">vpn-ipsecpolicy-create </code>.
    </p><div class="verbatim-wrap"><pre class="screen">neutron vpn-ikepolicy-create ikepolicy
neutron vpn-ipsecpolicy-create ipsecpolicy</pre></div></li><li class="step "><p>
     Create the VPN service at router1.
    </p><div class="verbatim-wrap"><pre class="screen">neutron vpn-service-create --name myvpnA --description "My vpn service" router1 subA</pre></div></li><li class="step "><p>
     Wait at least 5 seconds and then run
     <code class="literal">ipsec-site-connection-create</code> to create a ipsec-site
     connection. Note that <code class="literal">--peer-address</code> is the assign
     ext-net IP from router2 and <code class="literal">--peer-cidr</code> is subB cidr.
    </p><div class="verbatim-wrap"><pre class="screen">neutron ipsec-site-connection-create --name vpnconnection1 --vpnservice-id myvpnA \
--ikepolicy-id ikepolicy --ipsecpolicy-id ipsecpolicy --peer-address 172.16.0.3 \
--peer-id 172.16.0.3 --peer-cidr 10.2.0.0/24 --psk secret</pre></div></li><li class="step "><p>
     Create the VPN service at router2.
    </p><div class="verbatim-wrap"><pre class="screen">neutron vpn-service-create --name myvpnB --description "My vpn serviceB" router2 subB</pre></div></li><li class="step "><p>
     Wait at least 5 seconds and then run
     <code class="literal">ipsec-site-connection-create</code> to create a ipsec-site
     connection. Note that <code class="literal">--peer-address</code> is the assigned
     ext-net IP from router1 and <code class="literal">--peer-cidr</code> is subA cidr.
    </p><div class="verbatim-wrap"><pre class="screen">neutron ipsec-site-connection-create --name vpnconnection2 --vpnservice-id myvpnB \
--ikepolicy-id ikepolicy --ipsecpolicy-id ipsecpolicy --peer-address 172.16.0.2 \
--peer-id 172.16.0.2 --peer-cidr 10.1.0.0/24 --psk secret</pre></div></li><li class="step "><p>
     On the Cloud Lifecycle Manager, run the
     <code class="literal">ipsec-site-connection-list</code> command to see the active
     connections. Be sure to check that the vpn_services are ACTIVE. You can
     check this by running <code class="literal">vpn-service-list</code> and then
     checking ipsec-site-connections status. You should expect that the time
     for both vpn-services and ipsec-site-connections to become ACTIVE could
     take as long as 1 to 3 minutes.
    </p><div class="verbatim-wrap"><pre class="screen">neutron ipsec-site-connection-list
+--------------------------------------+----------------+--------------+---------------+------------+-----------+--------+
| id                                   | name           | peer_address | peer_cidrs    | route_mode | auth_mode | status |
+--------------------------------------+----------------+--------------+---------------+------------+-----------+--------+
| 1e8763e3-fc6a-444c-a00e-426a4e5b737c | vpnconnection2 | 172.16.0.2   | "10.1.0.0/24" | static     | psk       | ACTIVE |
| 4a97118e-6d1d-4d8c-b449-b63b41e1eb23 | vpnconnection1 | 172.16.0.3   | "10.2.0.0/24" | static     | psk       | ACTIVE |
+--------------------------------------+----------------+--------------+---------------+------------+-----------+--------+</pre></div></li></ol></div></div><p>
   <span class="bold"><strong>Verify VPN</strong></span> In the case of non-admin users,
   you can verify the VPN connection by pinging the virtual machines.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Check the VPN connections.
    </p><div id="id-1.5.12.4.6.9.1.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      vm1-ip and vm2-ip denotes private IP's for vm1 and vm2 respectively.
      The private IPs are obtained, as described in of
      <a class="xref" href="ops-managing-networking.html#st-vpnaas-obtain-ip" title="Step 4">Step 4</a>. If you are unable to SSH to the
      private network due to a lack
      of direct access, the VM console can be accessed through horizon.
     </p></div><div class="verbatim-wrap"><pre class="screen">ssh cirros@vm1-ip
password: &lt;password&gt;

# ping the private IP address of vm2
ping ###.###.###.###</pre></div></li><li class="step "><p>
     In another terminal.
    </p><div class="verbatim-wrap"><pre class="screen">ssh cirros@vm2-ip
password: &lt;password&gt;

# ping the private IP address of vm1
ping ###.###.###.###</pre></div></li><li class="step "><p>
     You should see ping responses from both virtual machines.
    </p></li></ol></div></div><p>
   As the admin user, you should check to make sure that a route exists between
   the router gateways. Once the gateways have been checked, packet encryption
   can be verified by using traffic analyzer (tcpdump) by tapping on the
   respective namespace (qrouter-* in case of non-DVR and snat-* in case of
   DVR) and tapping the right interface (qg-***).
  </p><div id="id-1.5.12.4.6.11" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    When using DVR namespaces, all the occurrences of qrouter-xxxxxx in the
    following commands should be replaced with respective snat-xxxxxx.
   </p></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Check the if the route exists between two router gateways. You can get the
     right qrouter namespace id by executing <span class="emphasis"><em>sudo ip
     netns</em></span>. Once you have the qrouter namespace id, you can get the
     interface by executing <span class="emphasis"><em>sudo ip netns qrouter-xxxxxxxx ip
     addr</em></span> and from the result the interface can be found.
    </p><div class="verbatim-wrap"><pre class="screen">sudo ip netns
sudo ip netns exec qrouter-&lt;router1 UUID&gt; ping &lt;router2 gateway&gt;
sudo ip netns exec qrouter-&lt;router2 UUID&gt; ping &lt;router1 gateway&gt;</pre></div></li><li class="step "><p>
     Initiate a tcpdump on the interface.
    </p><div class="verbatim-wrap"><pre class="screen">sudo ip netns exec qrouter-xxxxxxxx tcpdump -i qg-xxxxxx</pre></div></li><li class="step "><p>
     Check the VPN connection.
    </p><div class="verbatim-wrap"><pre class="screen">ssh cirros@vm1-ip
password: &lt;password&gt;

# ping the private IP address of vm2
ping ###.###.###.###</pre></div></li><li class="step "><p>
     Repeat for other namespace and right tap interface.
    </p><div class="verbatim-wrap"><pre class="screen">sudo ip netns exec qrouter-xxxxxxxx tcpdump -i qg-xxxxxx</pre></div></li><li class="step "><p>
     In another terminal.
    </p><div class="verbatim-wrap"><pre class="screen">ssh cirros@vm2-ip
password: &lt;password&gt;

# ping the private IP address of vm1
ping ###.###.###.###</pre></div></li><li class="step "><p>
     You will find encrypted packets containing ‘ESP’ in the tcpdump trace.
    </p></li></ol></div></div></div><div class="sect2" id="sec-vpnaas-more"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">More Information</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#sec-vpnaas-more">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-networking-vpnaas.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-networking-vpnaas.xml</li><li><span class="ds-label">ID: </span>sec-vpnaas-more</li></ul></div></div></div></div><p>
   VPNaaS currently only supports Pre-shared Keys (PSK) security between VPN
   gateways. A different VPN gateway solution should be considered if stronger,
   certificate-based security is required.
  </p><p>
   For more information on the neutron command-line interface (CLI) and VPN as
   a Service (VPNaaS), see the OpenStack networking command-line client
   reference:
   <a class="link" href="https://docs.openstack.org/python-openstackclient/rocky/" target="_blank">https://docs.openstack.org/python-openstackclient/rocky/</a>
  </p><p>
   For information on how to create an external network and subnet, see the
   OpenStack manual:
   <a class="link" href="http://docs.openstack.org/user-guide/dashboard_create_networks.html" target="_blank">http://docs.openstack.org/user-guide/dashboard_create_networks.html</a>
  </p></div></div><div class="sect1" id="designateOverview"><div class="titlepage"><div><div><h2 class="title"><span class="number">10.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DNS Service Overview</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#designateOverview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-dns-designate_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_overview.xml</li><li><span class="ds-label">ID: </span>designateOverview</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> DNS service provides multi-tenant Domain Name Service with REST
  API management for domain and records.
 </p><div id="id-1.5.12.5.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
   The DNS Service is not intended to be used as an
   <span class="emphasis"><em>internal</em></span> or
   <span class="emphasis"><em>private</em></span> DNS service. The name records in
   DNSaaS should be treated as public information that anyone could query.
   There are controls to prevent tenants from creating records for domains they
   do not own. TSIG provides a <span class="bold"><strong>T</strong></span>ransaction
   <span class="bold"><strong>SIG</strong></span> nature to ensure integrity during zone
   transfer to other DNS servers.
  </p></div><div class="sect2" id="id-1.5.12.5.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.5.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-dns-designate_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_overview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     For more information about designate REST APIs, see the <span class="productname">OpenStack</span> REST
     API Documentation at
     <a class="link" href="http://docs.openstack.org/developer/designate/rest.html" target="_blank">http://docs.openstack.org/developer/designate/rest.html</a>.
    </p></li><li class="listitem "><p>
     For a glossary of terms for designate, see the <span class="productname">OpenStack</span> glossary at
     <a class="link" href="http://docs.openstack.org/developer/designate/glossary.html" target="_blank">http://docs.openstack.org/developer/designate/glossary.html</a>.
    </p></li></ul></div></div><div class="sect2" id="designateInitialConfig"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">designate Initial Configuration</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#designateInitialConfig">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-dns-designate_initialconfig.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_initialconfig.xml</li><li><span class="ds-label">ID: </span>designateInitialConfig</li></ul></div></div></div></div><p>
  After the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installation
  has been completed, designate requires initial configuration to operate.
 </p><div class="sect3" id="sec-designate-identify"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.3.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Identifying Name Server Public IPs</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#sec-designate-identify">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-dns-designate_initialconfig.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_initialconfig.xml</li><li><span class="ds-label">ID: </span>sec-designate-identify</li></ul></div></div></div></div><p>
   Depending on the back-end, the method used to identify the name servers'
   public IPs will differ.
  </p><div class="sect4" id="sec-designate-infoblox"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.3.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">InfoBlox</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#sec-designate-infoblox">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-dns-designate_initialconfig.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_initialconfig.xml</li><li><span class="ds-label">ID: </span>sec-designate-infoblox</li></ul></div></div></div></div><p>
    InfoBlox will act as your public name servers, consult the InfoBlox
    management UI to identify the IPs.
   </p></div><div class="sect4" id="sec-designate-bind"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.3.2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">BIND Back-end</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#sec-designate-bind">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-dns-designate_initialconfig.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_initialconfig.xml</li><li><span class="ds-label">ID: </span>sec-designate-bind</li></ul></div></div></div></div><p>
    You can find the name server IPs in <code class="filename">/etc/hosts</code> by
    looking for the <code class="literal">ext-api</code> addresses, which are the
    addresses of the controllers. For example:
   </p><div class="verbatim-wrap"><pre class="screen">192.168.10.1 example-cp1-c1-m1-extapi
192.168.10.2 example-cp1-c1-m2-extapi
192.168.10.3 example-cp1-c1-m3-extapi</pre></div></div><div class="sect4" id="sec-designate-a-record"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.3.2.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating Name Server A Records</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#sec-designate-a-record">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-dns-designate_initialconfig.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_initialconfig.xml</li><li><span class="ds-label">ID: </span>sec-designate-a-record</li></ul></div></div></div></div><p>
    Each name server requires a public name, for example
    <code class="literal">ns1.example.com.</code>, to which designate-managed domains will
    be delegated. There are two common locations where these may be registered,
    either within a zone hosted on designate itself, or within a zone hosted on a
    external DNS service.
   </p><p>
    <span class="bold"><strong>If you are using an externally managed zone for these
    names:</strong></span>
   </p><div class="procedure "><div class="procedure-contents"><ul class="procedure"><li class="step "><p>
      For each name server public IP, create the necessary A records in the
      external system.
     </p></li></ul></div></div><p>
    <span class="bold"><strong>If you are using a designate-managed zone for these
    names:</strong></span>
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Create the zone in designate which will contain the records:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack zone create --email hostmaster@example.com example.com.
+----------------+--------------------------------------+
| Field          | Value                                |
+----------------+--------------------------------------+
| action         | CREATE                               |
| created_at     | 2016-03-09T13:16:41.000000           |
| description    | None                                 |
| email          | hostmaster@example.com               |
| id             | 23501581-7e34-4b88-94f4-ad8cec1f4387 |
| masters        |                                      |
| name           | example.com.                         |
| pool_id        | 794ccc2c-d751-44fe-b57f-8894c9f5c842 |
| project_id     | a194d740818942a8bea6f3674e0a3d71     |
| serial         | 1457529400                           |
| status         | PENDING                              |
| transferred_at | None                                 |
| ttl            | 3600                                 |
| type           | PRIMARY                              |
| updated_at     | None                                 |
| version        | 1                                    |
+----------------+--------------------------------------+</pre></div></li><li class="step "><p>
      For each name server public IP, create an A record. For example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack recordset create --records 192.168.10.1 --type A example.com. ns1.example.com.
+-------------+--------------------------------------+
| Field       | Value                                |
+-------------+--------------------------------------+
| action      | CREATE                               |
| created_at  | 2016-03-09T13:18:36.000000           |
| description | None                                 |
| id          | 09e962ed-6915-441a-a5a1-e8d93c3239b6 |
| name        | ns1.example.com.                     |
| records     | 192.168.10.1                         |
| status      | PENDING                              |
| ttl         | None                                 |
| type        | A                                    |
| updated_at  | None                                 |
| version     | 1                                    |
| zone_id     | 23501581-7e34-4b88-94f4-ad8cec1f4387 |
+-------------+--------------------------------------+</pre></div></li><li class="step "><p>
      When records have been added, list the record sets in the zone to
      validate:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack recordset list example.com.
+--------------+------------------+------+---------------------------------------------------+
| id           | name             | type | records                                           |
+--------------+------------------+------+---------------------------------------------------+
| 2d6cf...655b | example.com.     | SOA  | ns1.example.com. hostmaster.example.com 145...600 |
| 33466...bd9c | example.com.     | NS   | ns1.example.com.                                  |
| da98c...bc2f | example.com.     | NS   | ns2.example.com.                                  |
| 672ee...74dd | example.com.     | NS   | ns3.example.com.                                  |
| 09e96...39b6 | ns1.example.com. | A    | 192.168.10.1                                      |
| bca4f...a752 | ns2.example.com. | A    | 192.168.10.2                                      |
| 0f123...2117 | ns3.example.com. | A    | 192.168.10.3                                      |
+--------------+------------------+------+---------------------------------------------------+</pre></div></li><li class="step "><p>
      Contact your domain registrar requesting <span class="emphasis"><em>Glue
      Records</em></span> to be registered in the
      <code class="literal">com.</code> zone for the nameserver and public
      IP address pairs above. If you are using a sub-zone of an existing
      company zone (for example, <code class="literal">ns1.cloud.mycompany.com.</code>),
      the Glue must be placed in the <code class="literal">mycompany.com.</code> zone.
     </p></li></ol></div></div></div><div class="sect4" id="sec-designate-more"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.3.2.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#sec-designate-more">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-dns-designate_initialconfig.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_initialconfig.xml</li><li><span class="ds-label">ID: </span>sec-designate-more</li></ul></div></div></div></div><p>
    For additional DNS integration and configuration information, see the
    <span class="productname">OpenStack</span> designate documentation at
    <a class="link" href="https://docs.openstack.org/designate/rocky/" target="_blank">https://docs.openstack.org/designate/rocky/</a>.
   </p><p>
    For more information on creating servers, domains and examples, see the
    <span class="productname">OpenStack</span> REST API documentation at
    <a class="link" href="https://developer.openstack.org/api-ref/dns/" target="_blank">https://developer.openstack.org/api-ref/dns/</a>.
   </p></div></div></div><div class="sect2" id="designateMonitoringSupport"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DNS Service Monitoring Support</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#designateMonitoringSupport">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-dns-designate_monitor_support.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_monitor_support.xml</li><li><span class="ds-label">ID: </span>designateMonitoringSupport</li></ul></div></div></div></div><div class="sect3" id="MonitoringSupport"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.3.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DNS Service Monitoring Support</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#MonitoringSupport">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-dns-designate_monitor_support.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_monitor_support.xml</li><li><span class="ds-label">ID: </span>MonitoringSupport</li></ul></div></div></div></div><p>
   Additional monitoring support for the DNS Service (designate) has been added
   to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  </p><p>
   In the Networking section of the Operations Console, you can see alarms for all of
   the DNS Services (designate), such as designate-zone-manager, designate-api,
   designate-pool-manager, designate-mdns, and designate-central after running
   <code class="literal">designate-stop.yml</code>.
  </p><p>
   You can run <code class="literal">designate-start.yml</code> to start the DNS Services
   back up and the alarms will change from a red status to green and be removed
   from the <span class="bold"><strong>New Alarms</strong></span> panel of the
   Operations Console.
  </p><p>
   An example of the generated alarms from the Operations Console is provided below
   after running <code class="literal">designate-stop.yml</code>:
  </p><div class="verbatim-wrap"><pre class="screen">ALARM:  STATE:  ALARM ID:  LAST CHECK:  DIMENSION:
Process Check
0f221056-1b0e-4507-9a28-2e42561fac3e 2016-10-03T10:06:32.106Z hostname=ardana-cp1-c1-m1-mgmt,
service=dns,
cluster=cluster1,
process_name=designate-zone-manager,
component=designate-zone-manager,
control_plane=control-plane-1,
cloud_name=entry-scale-kvm

Process Check
50dc4c7b-6fae-416c-9388-6194d2cfc837 2016-10-03T10:04:32.086Z hostname=ardana-cp1-c1-m1-mgmt,
service=dns,
cluster=cluster1,
process_name=designate-api,
component=designate-api,
control_plane=control-plane-1,
cloud_name=entry-scale-kvm

Process Check
55cf49cd-1189-4d07-aaf4-09ed08463044 2016-10-03T10:05:32.109Z hostname=ardana-cp1-c1-m1-mgmt,
service=dns,
cluster=cluster1,
process_name=designate-pool-manager,
component=designate-pool-manager,
control_plane=control-plane-1,
cloud_name=entry-scale-kvm

Process Check
c4ab7a2e-19d7-4eb2-a9e9-26d3b14465ea 2016-10-03T10:06:32.105Z hostname=ardana-cp1-c1-m1-mgmt,
service=dns,
cluster=cluster1,
process_name=designate-mdns,
component=designate-mdns,
control_plane=control-plane-1,
cloud_name=entry-scale-kvm
HTTP Status
c6349bbf-4fd1-461a-9932-434169b86ce5 2016-10-03T10:05:01.731Z service=dns,
cluster=cluster1,
url=http://100.60.90.3:9001/,
hostname=ardana-cp1-c1-m3-mgmt,
component=designate-api,
control_plane=control-plane-1,
api_endpoint=internal,
cloud_name=entry-scale-kvm,
monitored_host_type=instance

Process Check
ec2c32c8-3b91-4656-be70-27ff0c271c89 2016-10-03T10:04:32.082Z hostname=ardana-cp1-c1-m1-mgmt,
service=dns,
cluster=cluster1,
process_name=designate-central,
component=designate-central,
control_plane=control-plane-1,
cloud_name=entry-scale-kvm</pre></div></div></div></div><div class="sect1" id="neutron-overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">10.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networking Service Overview</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#neutron-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-networking_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-networking_overview.xml</li><li><span class="ds-label">ID: </span>neutron-overview</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Networking is a virtual Networking service that leverages the
  OpenStack neutron service to provide network connectivity and addressing to
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Compute service devices.
 </p><p>
  The Networking service also provides an API to configure and manage a variety
  of network services.
 </p><p>
  You can use the Networking service to connect guest servers or you can define
  and configure your own virtual network topology.
 </p><div class="sect2" id="installing-the-networking-service"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing the Networking Service</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#installing-the-networking-service">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-networking_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-networking_overview.xml</li><li><span class="ds-label">ID: </span>installing-the-networking-service</li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Network Administrators are responsible for planning for the neutron
   Networking service, and once installed, to configure the service to meet the
   needs of their cloud network users.
  </p></div><div class="sect2" id="working-with-the-networking-service"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Working with the Networking service</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#working-with-the-networking-service">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-networking_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-networking_overview.xml</li><li><span class="ds-label">ID: </span>working-with-the-networking-service</li></ul></div></div></div></div><p>
   To perform tasks using the Networking service, you can use the dashboard,
   API or CLI.
  </p></div><div class="sect2" id="restarting"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reconfiguring the Networking service</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#restarting">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-networking_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-networking_overview.xml</li><li><span class="ds-label">ID: </span>restarting</li></ul></div></div></div></div><p>
   If you change any of the network configuration after installation, it is
   recommended that you reconfigure the Networking service by running the
   neutron-reconfigure playbook.
  </p><p>
   On the Cloud Lifecycle Manager:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></div><div class="sect2" id="for-more-information"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For more information</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#for-more-information">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-networking_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-networking_overview.xml</li><li><span class="ds-label">ID: </span>for-more-information</li></ul></div></div></div></div><p>
   For information on how to operate your cloud we suggest you read the
   <a class="link" href="http://docs.openstack.org/ops/" target="_blank">OpenStack Operations
   Guide</a>. The <span class="emphasis"><em>Architecture</em></span> section contains useful
   information about how an OpenStack Cloud is put together. However, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   takes care of these details for you. The <span class="emphasis"><em>Operations</em></span>
   section contains information on how to manage the system.
  </p></div><div class="sect2" id="neutron-external-networks"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Neutron External Networks</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#neutron-external-networks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_external_networks.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_external_networks.xml</li><li><span class="ds-label">ID: </span>neutron-external-networks</li></ul></div></div></div></div><div class="sect3" id="id-1.5.12.6.9.2"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">External networks overview</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.9.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_external_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_external_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   This topic explains how to create a neutron external network.
  </p><p>
   External networks provide access to the internet.
  </p><p>
   The typical use is to provide an IP address that can be used to reach a VM
   from an external network which can be a public network like the internet or
   a network that is private to an organization.
  </p></div><div class="sect3" id="idg-all-networking-neutron-external-networks-xml-4"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Ansible Playbook</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#idg-all-networking-neutron-external-networks-xml-4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_external_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_external_networks.xml</li><li><span class="ds-label">ID: </span>idg-all-networking-neutron-external-networks-xml-4</li></ul></div></div></div></div><p>
   This playbook will query the Networking service for an existing external
   network, and then create a new one if you do not already have one. The
   resulting external network will have the name <code class="literal">ext-net</code>
   with a subnet matching the CIDR you specify in the command below.
  </p><p>
   If you need to specify more granularity, for example specifying an
   allocation pool for the subnet, use the
   <a class="xref" href="ops-managing-networking.html#idg-all-networking-neutron-external-networks-xml-6" title="10.4.5.3. Using the python-neutronclient CLI">Section 10.4.5.3, “Using the python-neutronclient CLI”</a>.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-cloud-configure.yml -e EXT_NET_CIDR=&lt;CIDR&gt;</pre></div><p>
   The table below shows the optional switch that you can use as part of this
   playbook to specify environment-specific information:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Switch</th><th>Description</th></tr></thead><tbody><tr><td>
       <p>
        <code class="literal">-e EXT_NET_CIDR=&lt;CIDR&gt;</code>
       </p>
      </td><td>
       <p>
        Optional. You can use this switch to specify the external network CIDR.
        If you choose not to use this switch, or use a wrong value, the VMs
        will not be accessible over the network.
       </p>
       <p>
        This CIDR will be from the <code class="literal">EXTERNAL VM</code> network.
       </p>
      </td></tr></tbody></table></div></div><div class="sect3" id="idg-all-networking-neutron-external-networks-xml-6"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the python-neutronclient CLI</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#idg-all-networking-neutron-external-networks-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_external_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_external_networks.xml</li><li><span class="ds-label">ID: </span>idg-all-networking-neutron-external-networks-xml-6</li></ul></div></div></div></div><p>
   For more granularity you can utilize the OpenStackClient tool to create your
   external network.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Source the Admin creds:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc</pre></div></li><li class="listitem "><p>
     Create the external network and then the subnet using these commands
     below.
    </p><p>
     Creating the network:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network create --router:external &lt;external-network-name&gt;</pre></div><p>
     Creating the subnet:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack subnet create <em class="replaceable ">EXTERNAL-NETWORK-NAME</em> <em class="replaceable ">CIDR</em> --gateway <em class="replaceable ">GATEWAY</em> --allocation-pool start=<em class="replaceable ">IP_START</em>,end=<em class="replaceable ">IP_END</em> [--disable-dhcp]</pre></div><p>
     Where:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Value</th><th>Description</th></tr></thead><tbody><tr><td>external-network-name</td><td>
         <p>
          This is the name given to your external network. This is a unique
          value that you will choose. The value <code class="literal">ext-net</code> is
          usually used.
         </p>
        </td></tr><tr><td>CIDR</td><td>
         <p>
          Use this switch to specify the external network CIDR. If you do not
          use this switch or use a wrong value, the VMs will not be accessible
          over the network.
         </p>
         <p>
          This CIDR will be from the EXTERNAL VM network.
         </p>
        </td></tr><tr><td>--gateway</td><td>
         <p>
          Optional switch to specify the gateway IP for your subnet. If this is
          not included, it will choose the first available IP.
         </p>
        </td></tr><tr><td>
         <p>
          --allocation-pool start end
         </p>
        </td><td>
         <p>
          Optional switch to specify start and end IP addresses to use as the
          allocation pool for this subnet.
         </p>
        </td></tr><tr><td>--disable-dhcp</td><td>
         <p>
          Optional switch if you want to disable DHCP on this subnet. If this
          is not specified, DHCP will be enabled.
         </p>
        </td></tr></tbody></table></div></li></ol></div></div><div class="sect3" id="MultipleExternalNetworks"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Multiple External Networks</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#MultipleExternalNetworks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_external_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_external_networks.xml</li><li><span class="ds-label">ID: </span>MultipleExternalNetworks</li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> provides the ability to have multiple external networks, by using
   the Network Service (neutron) provider networks for external networks. You
   can configure <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to allow the use of provider VLANs as external
   networks by following these steps.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Do NOT include the
     <code class="literal">neutron.l3_agent.external_network_bridge</code> tag in the
     network_groups definition for your cloud. This results in the
     <code class="literal">l3_agent.ini external_network_bridge</code> being set to an
     empty value (rather than the traditional br-ex).
    </p></li><li class="listitem "><p>
     Configure your cloud to use provider VLANs, by specifying the
     <code class="literal">provider_physical_network</code> tag on one of the
     network_groups defined for your cloud.
    </p><p>
     For example, to run provider VLANS over the EXAMPLE network group: (some
     attributes omitted for brevity)
    </p><div class="verbatim-wrap"><pre class="screen">network-groups:

  - name: EXAMPLE
    tags:
      - neutron.networks.vlan:
          provider-physical-network: physnet1</pre></div></li><li class="listitem "><p>
     After the cloud has been deployed, you can create external networks using
     provider VLANs.
    </p><p>
     For example, using the OpenStackClient:
    </p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       Create external network 1 on vlan101
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network create --provider-network-type vlan
--provider-physical-network physnet1 --provider-segment 101 --external ext-net1</pre></div></li><li class="listitem "><p>
       Create external network 2 on vlan102
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network create --provider-network-type vlan
--provider-physical-network physnet1 --provider-segment 102 --external ext-net2</pre></div></li></ol></div></li></ol></div></div></div><div class="sect2" id="neutron-provider-networks"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Neutron Provider Networks</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#neutron-provider-networks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span>neutron-provider-networks</li></ul></div></div></div></div><p>
  This topic explains how to create a neutron provider network.
 </p><p>
  A provider network is a virtual network created in the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud that
  is consumed by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services. The distinctive element of a provider
  network is that it does not create a virtual router; rather, it depends on
  L3 routing that is provided by the infrastructure.
 </p><p>
  A provider network is created by adding the specification to the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  input model. It consists of at least one network and one or more subnets.
 </p><div class="sect3" id="id-1.5.12.6.10.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> input model</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.10.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The input model is the primary mechanism a cloud admin uses in defining a
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installation. It exists as a directory with a data subdirectory that
   contains YAML files. By convention, any service that creates a neutron
   provider network will create a subdirectory under the data directory and the
   name of the subdirectory shall be the project name. For example, the Octavia
   project will use neutron provider networks so it will have a subdirectory
   named 'octavia' and the config file that specifies the neutron network will
   exist in that subdirectory.
  </p><div class="verbatim-wrap"><pre class="screen">├── cloudConfig.yml
    ├── data
    │   ├── control_plane.yml
    │   ├── disks_compute.yml
    │   ├── disks_controller_1TB.yml
    │   ├── disks_controller.yml
    │   ├── firewall_rules.yml
    │   ├── net_interfaces.yml
    │   ├── network_groups.yml
    │   ├── networks.yml
    │   ├── neutron
    │   │   └── neutron_config.yml
    │   ├── nic_mappings.yml
    │   ├── server_groups.yml
    │   ├── server_roles.yml
    │   ├── servers.yml
    │   ├── swift
    │   │   └── swift_config.yml
    │   └── octavia
    │       └── octavia_config.yml
    ├── README.html
    └── README.md</pre></div></div><div class="sect3" id="id-1.5.12.6.10.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network/Subnet specification</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.10.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The elements required in the input model for you to define a network are:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     name
    </p></li><li class="listitem "><p>
     network_type
    </p></li><li class="listitem "><p>
     physical_network
    </p></li></ul></div><p>
   Elements that are optional when defining a network are:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     segmentation_id
    </p></li><li class="listitem "><p>
     shared
    </p></li></ul></div><p>
   Required elements for the subnet definition are:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     cidr
    </p></li></ul></div><p>
   Optional elements for the subnet definition are:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     allocation_pools which will require start and end addresses
    </p></li><li class="listitem "><p>
     host_routes which will require a destination and nexthop
    </p></li><li class="listitem "><p>
     gateway_ip
    </p></li><li class="listitem "><p>
     no_gateway
    </p></li><li class="listitem "><p>
     enable-dhcp
    </p></li></ul></div><p>
   NOTE: Only IPv4 is supported at the present time.
  </p></div><div class="sect3" id="id-1.5.12.6.10.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network details</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.10.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following table outlines the network values to be set, and what they
   represent.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /></colgroup><thead><tr><th>Attribute</th><th>Required/optional</th><th>Allowed Values</th><th>Usage</th></tr></thead><tbody><tr><td>name</td><td>Required</td><td> </td><td> </td></tr><tr><td>network_type</td><td>Required</td><td>flat, vlan, vxlan</td><td>The type of desired network</td></tr><tr><td>physical_network</td><td>Required</td><td>Valid</td><td>Name of physical network that is overlayed with the virtual network</td></tr><tr><td>segmentation_id</td><td>Optional</td><td>vlan or vxlan ranges</td><td>VLAN id for vlan or tunnel id for vxlan</td></tr><tr><td>shared</td><td>Optional</td><td>True</td><td>Shared by all projects or private to a single project</td></tr></tbody></table></div></div><div class="sect3" id="id-1.5.12.6.10.8"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Subnet details</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.10.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following table outlines the subnet values to be set, and what they
   represent.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /></colgroup><thead><tr><th>Attribute</th><th>Req/Opt</th><th>Allowed Values</th><th>Usage</th></tr></thead><tbody><tr><td>cidr</td><td>Required</td><td>Valid CIDR range</td><td>for example, 172.30.0.0/24</td></tr><tr><td>allocation_pools</td><td>Optional</td><td>See allocation_pools table below</td><td> </td></tr><tr><td>host_routes</td><td>Optional</td><td>See host_routes table below</td><td> </td></tr><tr><td>gateway_ip</td><td>Optional</td><td>Valid IP addr</td><td>Subnet gateway to other nets</td></tr><tr><td>no_gateway</td><td>Optional</td><td>True</td><td>No distribution of gateway</td></tr><tr><td>enable-dhcp</td><td>Optional</td><td>True</td><td>Enable dhcp for this subnet</td></tr></tbody></table></div></div><div class="sect3" id="id-1.5.12.6.10.9"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.6.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">ALLOCATION_POOLS details</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.10.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following table explains allocation pool settings.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /></colgroup><thead><tr><th>Attribute</th><th>Req/Opt</th><th>Allowed Values</th><th>Usage</th></tr></thead><tbody><tr><td>start</td><td>Required</td><td>Valid IP addr</td><td>First ip address in pool</td></tr><tr><td>end</td><td>Required</td><td>Valid IP addr</td><td>Last ip address in pool</td></tr></tbody></table></div></div><div class="sect3" id="id-1.5.12.6.10.10"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.6.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HOST_ROUTES details</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.10.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following table explains host route settings.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /></colgroup><thead><tr><th>Attribute</th><th>Req/Opt</th><th>Allowed Values</th><th>Usage</th></tr></thead><tbody><tr><td>destination</td><td>Required</td><td>Valid CIDR</td><td>Destination subnet</td></tr><tr><td>nexthop</td><td>Required</td><td>Valid IP addr</td><td>Hop to take to destination subnet</td></tr></tbody></table></div><div id="id-1.5.12.6.10.10.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    Multiple destination/nexthop values can be used.
   </p></div></div><div class="sect3" id="id-1.5.12.6.10.11"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.6.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Examples</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.10.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following examples show the configuration file settings for neutron and
   Octavia.
  </p><p>
   <span class="bold"><strong>Octavia configuration</strong></span>
  </p><p>
   This file defines the mapping. It does not need to be edited unless you want
   to change the name of your VLAN.
  </p><p>
   Path:
   <code class="literal">~/openstack/my_cloud/definition/data/octavia/octavia_config.yml</code>
  </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  configuration-data:
    - name: OCTAVIA-CONFIG-CP1
      services:
        - octavia
      data:
        amp_network_name: OCTAVIA-MGMT-NET</pre></div><p>
   <span class="bold"><strong>neutron configuration</strong></span>
  </p><p>
   Input your network configuration information for your provider VLANs in
   <code class="literal">neutron_config.yml</code> found here:
  </p><p>
   <code class="literal">~/openstack/my_cloud/definition/data/neutron/</code>.
  </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  configuration-data:
    - name:  NEUTRON-CONFIG-CP1
      services:
        - neutron
      data:
        neutron_provider_networks:
        - name: OCTAVIA-MGMT-NET
          provider:
            - network_type: vlan
              physical_network: physnet1
              segmentation_id: 2754
          cidr: 10.13.189.0/24
          no_gateway:  True
          enable_dhcp: True
          allocation_pools:
            - start: 10.13.189.4
              end: 10.13.189.252
          host_routes:
            # route to MANAGEMENT-NET
            - destination: 10.13.111.128/26
              nexthop:  10.13.189.5</pre></div></div><div class="sect3" id="id-1.5.12.6.10.12"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.6.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Implementing your changes</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.10.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "configuring provider network"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Then continue with your clean cloud installation.
    </p></li><li class="listitem "><p>
     If you are only adding a neutron Provider network to an existing model,
     then run the neutron-deploy.yml playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-deploy.yml</pre></div></li></ol></div></div><div class="sect3" id="MultipleProviderNetworks"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.6.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Multiple Provider Networks</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#MultipleProviderNetworks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span>MultipleProviderNetworks</li></ul></div></div></div></div><p>
   The physical network infrastructure must be configured to convey the
   provider VLAN traffic as tagged VLANs to the cloud compute nodes and network
   service network nodes. Configuration of the physical network infrastructure
   is outside the scope of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 software.
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 automates the server networking configuration and the
   Network Service configuration based on information in the cloud definition.
   To configure the system for provider VLANs, specify the<span class="emphasis"><em>
   neutron.networks.vlan</em></span> tag with a
   <span class="emphasis"><em>provider-physical-network</em></span> attribute on one or more
   network groups. For example (some attributes omitted for brevity):
  </p><div class="verbatim-wrap"><pre class="screen">network-groups:

        - name: NET_GROUP_A
        tags:
        - neutron.networks.vlan:
        provider-physical-network: physnet1

        - name: NET_GROUP_B
        tags:
        - neutron.networks.vlan:
        provider-physical-network: physnet2</pre></div><p>
   A network group is associated with a server network interface via an
   interface model. For example (some attributes omitted for brevity):
  </p><div class="verbatim-wrap"><pre class="screen">interface-models:
        - name: INTERFACE_SET_X
        network-interfaces:
        - device:
        name: bond0
        network-groups:
        - NET_GROUP_A
        - device:
        name: eth3
        network-groups:
        - NET_GROUP_B</pre></div><p>
   A network group used for provider VLANs may contain only a single <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   network, because that VLAN must span all compute nodes and any Network
   Service network nodes/controllers (that is, it is a single L2 segment). The
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> network must be defined with tagged-vlan false, otherwise a Linux
   VLAN network interface will be created. For example:
  </p><div class="verbatim-wrap"><pre class="screen">networks:

        - name: NET_A
        tagged-vlan: false
        network-group: NET_GROUP_A

        - name: NET_B
        tagged-vlan: false
        network-group: NET_GROUP_B</pre></div><p>
   When the cloud is deployed, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 will create the appropriate
   bridges on the servers, and set the appropriate attributes in the neutron
   configuration files (for example, bridge_mappings).
  </p><p>
   After the cloud has been deployed, create Network Service network objects
   for each provider VLAN. For example, using the Network Service CLI:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network create --provider:network_type vlan --provider:physical_network physnet1 --provider-segment 101 mynet101
<code class="prompt user">ardana &gt; </code>openstack network create --provider:network_type vlan --provider:physical_network physnet2 --provider-segment 234 mynet234</pre></div></div><div class="sect3" id="idg-all-networking-neutron-provider-networks-xml-10"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.6.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">More Information</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#idg-all-networking-neutron-provider-networks-xml-10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span>idg-all-networking-neutron-provider-networks-xml-10</li></ul></div></div></div></div><p>
   For more information on the Network Service command-line interface (CLI),
   see the OpenStack networking command-line client reference:
   <a class="link" href="http://docs.openstack.org/cli-reference/content/neutronclient_commands.html" target="_blank">http://docs.openstack.org/cli-reference/content/neutronclient_commands.html</a>
  </p></div></div><div class="sect2" id="ipam"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using IPAM Drivers in the Networking Service</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#ipam">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-using_ipam.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-using_ipam.xml</li><li><span class="ds-label">ID: </span>ipam</li></ul></div></div></div></div><p>
  This topic describes how to choose and implement an IPAM driver.
 </p><div class="sect3" id="id-1.5.12.6.11.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Selecting and implementing an IPAM driver</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.11.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-using_ipam.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-using_ipam.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Beginning with the Liberty release, OpenStack networking includes a
   pluggable interface for the IP Address Management (IPAM) function. This
   interface creates a driver framework for the allocation and de-allocation of
   subnets and IP addresses, enabling the integration of alternate IPAM
   implementations or third-party IP Address Management systems.
  </p><p>
   There are three possible IPAM driver options:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Non-pluggable driver. This option is the default when the ipam_driver
     parameter is not specified in neutron.conf.
    </p></li><li class="listitem "><p>
     Pluggable reference IPAM driver. The pluggable IPAM driver interface was
     introduced in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 (OpenStack Liberty). It is a refactoring of
     the Kilo non-pluggable driver to use the new pluggable interface. The
     setting in neutron.conf to specify this driver is <code class="literal">ipam_driver =
     internal</code>.
    </p></li><li class="listitem "><p>
     Pluggable Infoblox IPAM driver. The pluggable Infoblox IPAM driver is a
     third-party implementation of the pluggable IPAM interface. the
     corresponding setting in neutron.conf to specify this driver is
     <code class="literal">ipam_driver =
     networking_infoblox.ipam.driver.InfobloxPool</code>.
    </p><div id="id-1.5.12.6.11.3.4.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      You can use either the non-pluggable IPAM driver or a pluggable one.
      However, you cannot use both.
     </p></div></li></ul></div></div><div class="sect3" id="id-1.5.12.6.11.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Pluggable reference IPAM driver</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.11.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-using_ipam.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-using_ipam.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To indicate that you want to use the Pluggable reference IPAM driver, the
   only parameter needed is "ipam_driver." You can set it by looking for the
   following commented line in the
   <code class="literal">neutron.conf.j2</code> template (ipam_driver = internal)
   uncommenting it, and committing the file. After following the standard
   steps to deploy neutron, neutron will be configured to run using the
   Pluggable reference IPAM driver.
  </p><p>
   As stated, the file you must edit is <code class="literal">neutron.conf.j2</code> on
   the Cloud Lifecycle Manager in the directory
   <code class="literal">~/openstack/my_cloud/config/neutron</code>. Here is the relevant
   section where you can see the <code class="literal">ipam_driver</code> parameter
   commented out:
  </p><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
  ...
  l3_ha_net_cidr = 169.254.192.0/18

  # Uncomment the line below if the Reference Pluggable IPAM driver is to be used
  # ipam_driver = internal
  ...</pre></div><p>
   After uncommenting the line <code class="literal">ipam_driver = internal</code>,
   commit the file using git commit from the <code class="literal">openstack/my_cloud</code>
   directory:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git commit -a -m 'My config for enabling the internal IPAM Driver'</pre></div><p>
   Then follow the steps to deploy <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> in the
   <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 13 “Overview”</span> appropriate to your cloud configuration.
  </p><div id="id-1.5.12.6.11.4.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    Currently there is no migration path from the non-pluggable driver to a
    pluggable IPAM driver because changes are needed to database tables and
    neutron currently cannot make those changes.
   </p></div></div><div class="sect3" id="id-1.5.12.6.11.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.7.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Infoblox IPAM driver</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.11.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-using_ipam.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-using_ipam.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   As suggested above, using the Infoblox IPAM driver requires changes to
   existing parameters in <code class="literal">nova.conf</code> and
   <code class="literal">neutron.conf</code>. If you want to use the infoblox appliance,
   you will need to add the "infoblox service-component" to the service-role
   containing the neutron API server. To use the infoblox appliance for IPAM,
   both the agent <span class="emphasis"><em>and</em></span> the Infoblox IPAM driver are
   required. The <code class="literal">infoblox-ipam-agent</code> should be deployed on
   the same node where the neutron-server component is running. Usually this is
   a Controller node.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Have the Infoblox appliance running on the management network (the
     Infoblox appliance admin or the datacenter administrator should know how
     to perform this step).
    </p></li><li class="listitem "><p>
     Change the control plane definition to add
     i<code class="literal">nfoblox-ipam-agent</code> as a service in the controller node
     cluster (see change in bold). Make the changes in
     <code class="literal">control_plane.yml</code> found here:
     <code class="literal">~/openstack/my_cloud/definition/data/control_plane.yml</code>
    </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  control-planes:
    - name: ccp
      control-plane-prefix: ccp
 ...
      clusters:
        - name: cluster0
          cluster-prefix: c0
          server-role: ARDANA-ROLE
          member-count: 1
          allocation-policy: strict
          service-components:
            - lifecycle-manager
        - name: cluster1
          cluster-prefix: c1
          server-role: CONTROLLER-ROLE
          member-count: 3
          allocation-policy: strict
          service-components:
            - ntp-server
...
            - neutron-server
            <span class="bold"><strong>- infoblox-ipam-agent</strong></span>
...
            - designate-client
            - bind
      resources:
        - name: compute
          resource-prefix: comp
          server-role: COMPUTE-ROLE
          allocation-policy: any</pre></div></li><li class="listitem "><p>
     Modify the
     <code class="literal">~/openstack/my_cloud/config/neutron/neutron.conf.j2</code> file
     on the controller node to comment and uncomment the lines noted below to
     enable use with the Infoblox appliance:
    </p><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
            ...
            l3_ha_net_cidr = 169.254.192.0/18


            # Uncomment the line below if the Reference Pluggable IPAM driver is to be used
            # ipam_driver = internal


            # Comment out the line below if the Infoblox IPAM Driver is to be used
            # notification_driver = messaging

            # Uncomment the lines below if the Infoblox IPAM driver is to be used
            ipam_driver = networking_infoblox.ipam.driver.InfobloxPool
            notification_driver = messagingv2


            # Modify the infoblox sections below to suit your cloud environment

            [infoblox]
            cloud_data_center_id = 1
            # This name of this section is formed by "infoblox-dc:&lt;infoblox.cloud_data_center_id&gt;"
            # If cloud_data_center_id is 1, then the section name is "infoblox-dc:1"

            [infoblox-dc:0]
            http_request_timeout = 120
            http_pool_maxsize = 100
            http_pool_connections = 100
            ssl_verify = False
            wapi_version = 2.2
            admin_user_name = admin
            admin_password = infoblox
            grid_master_name = infoblox.localdomain
            grid_master_host = 1.2.3.4


            [QUOTAS]
            ...</pre></div></li><li class="listitem "><p>
     Change <code class="literal">nova.conf.j2</code> to replace the notification driver
     "messaging" to "messagingv2"
    </p><div class="verbatim-wrap"><pre class="screen"> ...

 # Oslo messaging
 notification_driver = log

 #  Note:
 #  If the infoblox-ipam-agent is to be deployed in the cloud, change the
 #  notification_driver setting from "messaging" to "messagingv2".
 notification_driver = messagingv2
 notification_topics = notifications

 # Policy
 ...</pre></div></li><li class="listitem "><p>
     Commit the changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud
<code class="prompt user">ardana &gt; </code>git commit –a –m 'My config for enabling the Infoblox IPAM driver'</pre></div></li><li class="listitem "><p>
     Deploy the cloud with the changes. Due to changes to the
     control_plane.yml, you will need to rerun the config-processor-run.yml
     playbook if you have run it already during the install process.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml</pre></div></li></ol></div></div><div class="sect3" id="id-1.5.12.6.11.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.7.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuration parameters for using the Infoblox IPAM driver</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.11.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-using_ipam.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-using_ipam.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Changes required in the notification parameters in nova.conf:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /></colgroup><thead><tr><th>Parameter Name</th><th>Section in nova.conf</th><th>Default Value</th><th>Current Value </th><th>Description</th></tr></thead><tbody><tr><td>notify_on_state_change</td><td>DEFAULT</td><td>None</td><td>vm_and_task_state</td><td>
       <p>
        Send compute.instance.update notifications on instance state changes.
       </p>
       <p>
        Vm_and_task_state means notify on vm and task state changes.
       </p>
       <p>
        Infoblox requires the value to be vm_state (notify on vm state change).
       </p>
       <p>
        <span class="bold"><strong> Thus NO CHANGE is needed for infoblox</strong></span>
       </p>
      </td></tr><tr><td>notification_topics</td><td>DEFAULT</td><td>empty list</td><td>notifications</td><td>
       <p>
        <span class="bold"><strong>NO CHANGE is needed for infoblox.</strong></span>
       </p>
       <p>
        The infoblox installation guide requires the notifications to be
        "notifications"
       </p>
      </td></tr><tr><td>notification_driver</td><td>DEFAULT</td><td>None</td><td>messaging</td><td>
       <p>
        <span class="bold"><strong>Change needed.</strong></span>
       </p>
       <p>
        The infoblox installation guide requires the notification driver to be
        "messagingv2".
       </p>
      </td></tr></tbody></table></div><p>
   Changes to existing parameters in neutron.conf
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /></colgroup><thead><tr><th>Parameter Name</th><th>Section in neutron.conf</th><th>Default Value</th><th>Current Value </th><th>Description</th></tr></thead><tbody><tr><td>ipam_driver</td><td>DEFAULT</td><td>None</td><td>
       <p>
        None
       </p>
       <p>
        (param is undeclared in neutron.conf)
       </p>
      </td><td>
       <p>
        Pluggable IPAM driver to be used by neutron API server.
       </p>
       <p>
        For infoblox, the value is
        "networking_infoblox.ipam.driver.InfobloxPool"
       </p>
      </td></tr><tr><td>notification_driver</td><td>DEFAULT</td><td>empty list</td><td>messaging</td><td>
       <p>
        The driver used to send notifications from the neutron API server to
        the neutron agents.
       </p>
       <p>
        The installation guide for networking-infoblox calls for the
        notification_driver to be "messagingv2"
       </p>
      </td></tr><tr><td>notification_topics</td><td>DEFAULT</td><td>None</td><td>notifications</td><td>
       <p>
        <span class="bold"><strong>No change needed</strong></span>.
       </p>
       <p>
        The row is here show the changes in the neutron parameters described in
        the installation guide for networking-infoblox
       </p>
      </td></tr></tbody></table></div><p>
   Parameters specific to the Networking Infoblox Driver. All the parameters
   for the Infoblox IPAM driver must be defined in neutron.conf.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /></colgroup><thead><tr><th>Parameter Name</th><th>Section in neutron.conf</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td>cloud_data_center_id</td><td>infoblox</td><td>0</td><td>ID for selecting a particular grid from one or more grids to serve networks in
                the Infoblox back end</td></tr><tr><td>ipam_agent_workers</td><td>infoblox</td><td>1</td><td>Number of Infoblox IPAM agent works to run</td></tr><tr><td>grid_master_host</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>empty string</td><td>IP address of the grid master. WAPI requests are sent to the
                grid_master_host</td></tr><tr><td>ssl_verify</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>False</td><td>Ensure whether WAPI requests sent over HTTPS require SSL verification</td></tr><tr><td>WAPI Version</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>1.4</td><td>The WAPI version. Value should be 2.2.</td></tr><tr><td>admin_user_name</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>empty string</td><td>Admin user name to access the grid master or cloud platform appliance</td></tr><tr><td>admin_password</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>empty string</td><td>Admin user password</td></tr><tr><td>http_pool_connections</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>100</td><td> </td></tr><tr><td>http_pool_maxsize</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>100</td><td> </td></tr><tr><td>http_request_timeout</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>120</td><td> </td></tr></tbody></table></div><p>
  The diagram below shows nova compute sending notification to the
  infoblox-ipam-agent
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-networking-ipam.png" target="_blank"><img src="images/media-networking-ipam.png" width="" /></a></div></div></div><div class="sect3" id="id-1.5.12.6.11.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.7.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.11.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-using_ipam.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-using_ipam.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     There is no IPAM migration path from non-pluggable to pluggable IPAM
     driver
     (<a class="link" href="https://bugs.launchpad.net/neutron/+bug/1516156" target="_blank">https://bugs.launchpad.net/neutron/+bug/1516156</a>).
     This means there is no way to reconfigure the neutron database if you
     wanted to change neutron to use a pluggable IPAM driver. Unless you change
     the default of non-pluggable IPAM configuration to a pluggable driver at
     install time, you will have no other opportunity to make that change
     because reconfiguration of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9from using the default
     non-pluggable IPAM configuration to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 using a pluggable IPAM
     driver is not supported.
    </p></li><li class="listitem "><p>
     Upgrade from previous versions of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 to use a
     pluggable IPAM driver is not supported.
    </p></li><li class="listitem "><p>
     The Infoblox appliance does not allow for overlapping IPs. For example,
     only one tenant can have a CIDR of 10.0.0.0/24.
    </p></li><li class="listitem "><p>
     The infoblox IPAM driver fails the creation of a subnet when a there is no
     gateway-ip supplied. For example, the command <code class="command">openstack subnet create ...
     --no-gateway ...</code> will fail.
    </p></li></ul></div></div></div><div class="sect2" id="HP2-0LBaaSAdmin"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Load Balancing as a Service (LBaaS)</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#HP2-0LBaaSAdmin">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-lbaas_admin.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-lbaas_admin.xml</li><li><span class="ds-label">ID: </span>HP2-0LBaaSAdmin</li></ul></div></div></div></div><p>
  <span class="bold"><strong><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 LBaaS Configuration</strong></span>
 </p><p>
  Load Balancing as a Service (LBaaS) is an advanced networking service that
  allows load balancing of multi-node environments. It provides the ability to
  spread requests across multiple servers thereby reducing the load on any
  single server. This document describes the installation steps and the
  configuration for LBaaS v2.
 </p><div id="id-1.5.12.6.12.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
   The LBaaS architecture is based on a driver model to support different load
   balancers. LBaaS-compatible drivers are provided by load balancer vendors
   including F5 and Citrix. A new software load balancer driver was introduced
   in the OpenStack Liberty release called "<span class="emphasis"><em>Octavia</em></span>". The
   Octavia driver deploys a software load balancer called HAProxy. Octavia is
   the default load balancing provider in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 for LBaaS V2.
   Until Octavia is configured the creation of load balancers will fail with an
   error. Refer to <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 43 “Configuring Load Balancer as a Service”</span> document for
   information on installing Octavia.
  </p></div><div id="id-1.5.12.6.12.5" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
   Before upgrading to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9, contact F5 and
   SUSE to determine which F5 drivers have been certified for use with
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. Loading drivers not certified by SUSE may result in
   failure of your cloud deployment.
  </p></div><p>
  LBaaS V2 offers with <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 43 “Configuring Load Balancer as a Service”</span> a software load
  balancing solution that supports both a highly available control plane and
  data plane. However, should an external hardware load balancer be selected
  the cloud operation can achieve additional performance and availability.
 </p><p>
  <span class="bold"><strong>LBaaS v2</strong></span>
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Your vendor already has a driver that supports LBaaS v2. Many hardware load
    balancer vendors already support LBaaS v2 and this list is growing all the
    time.
   </p></li><li class="listitem "><p>
    You intend to script your load balancer creation and management so a UI is
    not important right now (horizon support will be added in a future
    release).
   </p></li><li class="listitem "><p>
    You intend to support TLS termination at the load balancer.
   </p></li><li class="listitem "><p>
    You intend to use the Octavia software load balancer (adding HA and
    scalability).
   </p></li><li class="listitem "><p>
    You do not want to take your load balancers offline to perform
    subsequent LBaaS upgrades.
   </p></li><li class="listitem "><p>
    You intend in future releases to need L7 load balancing.
   </p></li></ol></div><p>
  Reasons not to select this version.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Your LBaaS vendor does not have a v2 driver.
   </p></li><li class="listitem "><p>
    You must be able to manage your load balancers from horizon.
   </p></li><li class="listitem "><p>
    You have legacy software which utilizes the LBaaS v1 API.
   </p></li></ol></div><p>
  LBaaS v2 is installed by default with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> and requires
  minimal configuration to start the service.
 </p><div id="id-1.5.12.6.12.12" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   LBaaS V2 API currently supports load balancer failover with Octavia.
   LBaaS v2 API includes automatic failover of a deployed load balancer with
   Octavia. More information about this driver can be found in
   <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 43 “Configuring Load Balancer as a Service”</span>.
  </p></div><div class="sect3" id="idg-all-networking-lbaas-admin-xml-7"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#idg-all-networking-lbaas-admin-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-lbaas_admin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-lbaas_admin.xml</li><li><span class="ds-label">ID: </span>idg-all-networking-lbaas-admin-xml-7</li></ul></div></div></div></div><p>
   <span class="bold"><strong><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> LBaaS v2</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> must be installed for LBaaS v2.
    </p></li><li class="listitem "><p>
     Follow the instructions to install <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 43 “Configuring Load Balancer as a Service”</span>
    </p></li></ol></div></div></div><div class="sect2" id="OctaviaAdmin"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Load Balancer: Octavia Driver Administration</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#OctaviaAdmin">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-octavia_admin.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-octavia_admin.xml</li><li><span class="ds-label">ID: </span>OctaviaAdmin</li></ul></div></div></div></div><p>
  This document provides the instructions on how to enable and manage various
  components of the Load Balancer Octavia driver if that driver is enabled.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="ops-managing-networking.html#Alerts" title="10.4.9.1. Monasca Alerts">Section 10.4.9.1, “Monasca Alerts”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="ops-managing-networking.html#Tuning" title="10.4.9.2. Tuning Octavia Installation">Section 10.4.9.2, “Tuning Octavia Installation”</a>
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Homogeneous Compute Configuration
     </p></li><li class="listitem "><p>
      Octavia and Floating IP's
     </p></li><li class="listitem "><p>
      Configuration Files
     </p></li><li class="listitem "><p>
      Spare Pools
     </p></li></ul></div></li><li class="listitem "><p>
    <a class="xref" href="ops-managing-networking.html#Amphora" title="10.4.9.3. Managing Amphora">Section 10.4.9.3, “Managing Amphora”</a>
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Updating the Cryptographic Certificates
     </p></li><li class="listitem "><p>
      Accessing VM information in nova
     </p></li><li class="listitem "><p>
      Initiating Failover of an Amphora VM
     </p></li></ul></div></li></ul></div><div class="sect3" id="Alerts"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monasca Alerts</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#Alerts">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-octavia_admin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-octavia_admin.xml</li><li><span class="ds-label">ID: </span>Alerts</li></ul></div></div></div></div><p>
   The monasca-agent has the following Octavia-related plugins:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Process checks – checks if octavia processes are running. When it
     starts, it detects which processes are running and then monitors them.
    </p></li><li class="listitem "><p>
     http_connect check – checks if it can connect to octavia api servers.
    </p></li></ul></div><p>
   Alerts are displayed in the Operations Console.
  </p></div><div class="sect3" id="Tuning"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Tuning Octavia Installation</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#Tuning">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-octavia_admin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-octavia_admin.xml</li><li><span class="ds-label">ID: </span>Tuning</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Homogeneous Compute Configuration</strong></span>
  </p><p>
   Octavia works only with homogeneous compute node configurations. Currently,
   Octavia does not support multiple nova flavors. If Octavia needs to be
   supported on multiple compute nodes, then all the compute nodes should carry
   same set of physnets (which will be used for Octavia).
  </p><p>
   <span class="bold"><strong>Octavia and Floating IPs</strong></span>
  </p><p>
   Due to a neutron limitation Octavia will only work with CVR routers. Another
   option is to use VLAN provider networks which do not require a router.
  </p><p>
   You cannot currently assign a floating IP address as the VIP (user facing)
   address for a load balancer created by the Octavia driver if the underlying
   neutron network is configured to support Distributed Virtual Router (DVR).
   The Octavia driver uses a neutron function known as
   <span class="emphasis"><em>allowed address pairs</em></span>
   to support load balancer fail over.
  </p><p>
   There is currently a neutron bug that does not support this function in a
   DVR configuration
  </p><p>
   <span class="bold"><strong>Octavia Configuration Files</strong></span>
  </p><p>
   The system comes pre-tuned and should not need any adjustments for most
   customers. If in rare instances manual tuning is needed, follow these steps:
  </p><div id="id-1.5.12.6.13.5.10" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
    Changes might be lost during <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> upgrades.
   </p></div><p>
   Edit the Octavia configuration files in
   <code class="literal">my_cloud/config/octavia</code>. It is recommended that any
   changes be made in all of the Octavia configuration files.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     octavia-api.conf.j2
    </p></li><li class="listitem "><p>
     octavia-health-manager.conf.j2
    </p></li><li class="listitem "><p>
     octavia-housekeeping.conf.j2
    </p></li><li class="listitem "><p>
     octavia-worker.conf.j2
    </p></li></ul></div><p>
   After the changes are made to the configuration files, redeploy the service.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Commit changes to git.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My Octavia Config"</pre></div></li><li class="listitem "><p>
     Run the configuration processor and ready deployment.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the Octavia reconfigure.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts octavia-reconfigure.yml</pre></div></li></ol></div><p>
   <span class="bold"><strong>Spare Pools</strong></span>
  </p><p>
    The Octavia driver provides support for creating spare pools of
    the HAProxy software installed in VMs. This means instead of
    creating a new load balancer when loads increase, create new load
    balancer calls will pull a load balancer from the spare pool. The
    spare pools feature consumes resources, therefore the load
    balancers in the spares pool has been set to 0, which is the
    default and also disables the feature.
  </p><p>
    Reasons to enable a load balancing spare pool in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
        You expect a large number of load balancers to be provisioned all at once
        (puppet scripts, or ansible scripts) and you want them to come up quickly.
      </p></li><li class="listitem "><p>
        You want to reduce the wait time a customer has while requesting a new
        load balancer.
      </p></li></ol></div><p>
    To increase the number of load balancers in your spares pool, edit
    the Octavia configuration files by uncommenting the
    <code class="literal">spare_amphora_pool_size</code> and adding the number of load
    balancers you would like to include in your spares pool.
  </p><div class="verbatim-wrap"><pre class="screen"># Pool size for the spare pool
# spare_amphora_pool_size = 0</pre></div></div><div class="sect3" id="Amphora"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.9.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Amphora</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#Amphora">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-octavia_admin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-octavia_admin.xml</li><li><span class="ds-label">ID: </span>Amphora</li></ul></div></div></div></div><p>
   Octavia starts a separate VM for each load balancing function. These VMs are
   called amphora.
  </p><p>
   <span class="bold"><strong>Updating the Cryptographic Certificates</strong></span>
  </p><p>
   Octavia uses two-way SSL encryption for communication between amphora and
   the control plane. Octavia keeps track of the certificates on the amphora
   and will automatically recycle them. The certificates on the control plane
   are valid for one year after installation of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  </p><p>
   You can check on the status of the certificate by logging into the
   controller node as root and running:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd /opt/stack/service/octavia-<em class="replaceable ">SOME UUID</em>/etc/certs/
openssl x509 -in client.pem  -text –noout</pre></div><p>
   This prints the certificate out where you can check on the expiration dates.
  </p><p>
   To renew the certificates, reconfigure Octavia. Reconfiguring causes Octavia
   to automatically generate new certificates and deploy them to the controller
   hosts.
  </p><p>
   On the Cloud Lifecycle Manager execute octavia-reconfigure:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts octavia-reconfigure.yml</pre></div><p>
   <span class="bold"><strong>Accessing VM information in nova</strong></span>
  </p><p>
   You can use <code class="literal">openstack project list</code> as an administrative
   user to obtain information about the tenant or project-id of the Octavia
   project. In the example below, the Octavia project has a project-id of
   <code class="literal">37fd6e4feac14741b6e75aba14aea833</code>.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack project list
+----------------------------------+------------------+
| ID                               | Name             |
+----------------------------------+------------------+
| 055071d8f25d450ea0b981ca67f7ccee | glance-swift     |
| 37fd6e4feac14741b6e75aba14aea833 | octavia          |
| 4b431ae087ef4bd285bc887da6405b12 | swift-monitor    |
| 8ecf2bb5754646ae97989ba6cba08607 | swift-dispersion |
| b6bd581f8d9a48e18c86008301d40b26 | services         |
| bfcada17189e4bc7b22a9072d663b52d | cinderinternal   |
| c410223059354dd19964063ef7d63eca | monitor          |
| d43bc229f513494189422d88709b7b73 | admin            |
| d5a80541ba324c54aeae58ac3de95f77 | demo             |
| ea6e039d973e4a58bbe42ee08eaf6a7a | backup           |
+----------------------------------+------------------+</pre></div><p>
   You can then use <code class="literal">openstack server list --tenant &lt;project-id&gt;</code> to
   list the VMs for the Octavia tenant. Take particular note of the IP address
   on the OCTAVIA-MGMT-NET; in the example below it is
   <code class="literal">172.30.1.11</code>. For additional nova command-line options see
   <a class="xref" href="ops-managing-networking.html#idg-all-networking-octavia-admin-xml-10" title="10.4.9.5. For More Information">Section 10.4.9.5, “For More Information”</a>.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list --tenant 37fd6e4feac14741b6e75aba14aea833
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+
| ID                                   | Name                                         | Tenant ID                        | Status | Task State | Power State | Networks                                       |
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+
| 1ed8f651-de31-4208-81c5-817363818596 | amphora-1c3a4598-5489-48ea-8b9c-60c821269e4c | 37fd6e4feac14741b6e75aba14aea833 | ACTIVE | -          | Running     | private=10.0.0.4; OCTAVIA-MGMT-NET=172.30.1.11 |
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+</pre></div><div id="id-1.5.12.6.13.6.16" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    The Amphora VMs do not have SSH or any other access. In the rare case that
    there is a problem with the underlying load balancer the whole amphora will
    need to be replaced.
   </p></div><p>
   <span class="bold"><strong>Initiating Failover of an Amphora VM</strong></span>
  </p><p>
   Under normal operations Octavia will monitor the health of the amphora
   constantly and automatically fail them over if there are any issues. This
   helps to minimize any potential downtime for load balancer users. There are,
   however, a few cases a failover needs to be initiated manually:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     The Loadbalancer has become unresponsive and Octavia has not detected an
     error.
    </p></li><li class="listitem "><p>
     A new image has become available and existing load balancers need to start
     using the new image.
    </p></li><li class="listitem "><p>
     The cryptographic certificates to control and/or the HMAC password to
     verify Health information of the amphora have been compromised.
    </p></li></ol></div><p>
   To minimize the impact for end users we will keep the existing load balancer
   working until shortly before the new one has been provisioned. There will be
   a short interruption for the load balancing service so keep that in mind
   when scheduling the failovers. To achieve that follow these steps (assuming
   the management ip from the previous step):
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Assign the IP to a SHELL variable for better readability.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>export MGM_IP=172.30.1.11</pre></div></li><li class="listitem "><p>
     Identify the port of the vm on the management network.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port list | grep $MGM_IP
| 0b0301b9-4ee8-4fb6-a47c-2690594173f4 |                                                   | fa:16:3e:d7:50:92 |
{"subnet_id": "3e0de487-e255-4fc3-84b8-60e08564c5b7", "ip_address": "172.30.1.11"} |</pre></div></li><li class="listitem "><p>
     Disable the port to initiate a failover. Note the load balancer will still
     function but cannot be controlled any longer by Octavia.
    </p><div id="id-1.5.12.6.13.6.21.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      Changes after disabling the port will result in errors.
     </p></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port set --admin-state-up False 0b0301b9-4ee8-4fb6-a47c-2690594173f4
Updated port: 0b0301b9-4ee8-4fb6-a47c-2690594173f4</pre></div></li><li class="listitem "><p>
     You can check to see if the amphora failed over with <code class="literal">openstack
     server list --tenant &lt;project-id&gt;</code>. This may take some time
     and in some cases may need to be repeated several times. You can tell that
     the failover has been successful by the changed IP on the management
     network.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list --tenant 37fd6e4feac14741b6e75aba14aea833
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+
| ID                                   | Name                                         | Tenant ID                        | Status | Task State | Power State | Networks                                       |
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+
| 1ed8f651-de31-4208-81c5-817363818596 | amphora-1c3a4598-5489-48ea-8b9c-60c821269e4c | 37fd6e4feac14741b6e75aba14aea833 | ACTIVE | -          | Running     | private=10.0.0.4; OCTAVIA-MGMT-NET=172.30.1.12 |
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+</pre></div></li></ol></div><div id="id-1.5.12.6.13.6.22" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
    Do not issue too many failovers at once. In a big installation you might be
    tempted to initiate several failovers in parallel for instance to speed up
    an update of amphora images. This will put a strain on the nova service and
    depending on the size of your installation you might need to throttle the
    failover rate.
   </p></div></div><div class="sect3" id="OctaviaMaintenance"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.9.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Load Balancer: Octavia Administration</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#OctaviaMaintenance">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-octavia-maintenance.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-octavia-maintenance.xml</li><li><span class="ds-label">ID: </span>OctaviaMaintenance</li></ul></div></div></div></div><div class="sect4" id="octavia-admin-delete"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.9.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Removing load balancers</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#octavia-admin-delete">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-octavia-maintenance.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-octavia-maintenance.xml</li><li><span class="ds-label">ID: </span>octavia-admin-delete</li></ul></div></div></div></div><p>
      The following procedures demonstrate how to delete a load
      balancer that is in the <code class="literal">ERROR</code>,
      <code class="literal">PENDING_CREATE</code>, or
      <code class="literal">PENDING_DELETE</code> state.
    </p><div class="procedure " id="id-1.5.12.6.13.7.2.3"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 10.3: </span><span class="name">
        Manually deleting load balancers created with neutron lbaasv2
        (in an upgrade/migration scenario)
       </span><a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.13.7.2.3">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Query the Neutron service for the loadbalancer ID:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron lbaas-loadbalancer-list
neutron CLI is deprecated and will be removed in the future. Use openstack CLI instead.
+--------------------------------------+---------+----------------------------------+--------------+---------------------+----------+
| id                                   | name    | tenant_id                        | vip_address  | provisioning_status | provider |
+--------------------------------------+---------+----------------------------------+--------------+---------------------+----------+
| 7be4e4ab-e9c6-4a57-b767-da9af5ba7405 | test-lb | d62a1510b0f54b5693566fb8afeb5e33 | 192.168.1.10 | ERROR               | haproxy  |
+--------------------------------------+---------+----------------------------------+--------------+---------------------+----------+</pre></div></li><li class="step "><p>
          Connect to the neutron database:
        </p><div id="id-1.5.12.6.13.7.2.3.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
            The default database name depends on the life cycle
            manager. Ardana uses <code class="literal">ovs_neutron</code> while
            Crowbar uses <code class="literal">neutron</code>.
          </p></div><p>Ardana:</p><div class="verbatim-wrap"><pre class="screen">mysql&gt; use ovs_neutron</pre></div><p>Crowbar:</p><div class="verbatim-wrap"><pre class="screen">mysql&gt; use neutron</pre></div></li><li class="step "><p>
          Get the pools and healthmonitors associated with the loadbalancer:
        </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; select id, healthmonitor_id, loadbalancer_id from lbaas_pools where loadbalancer_id = '7be4e4ab-e9c6-4a57-b767-da9af5ba7405';
+--------------------------------------+--------------------------------------+--------------------------------------+
| id                                   | healthmonitor_id                     | loadbalancer_id                      |
+--------------------------------------+--------------------------------------+--------------------------------------+
| 26c0384b-fc76-4943-83e5-9de40dd1c78c | 323a3c4b-8083-41e1-b1d9-04e1fef1a331 | 7be4e4ab-e9c6-4a57-b767-da9af5ba7405 |
+--------------------------------------+--------------------------------------+--------------------------------------+</pre></div></li><li class="step "><p>
          Get the members associated with the pool:
        </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; select id, pool_id from lbaas_members where pool_id = '26c0384b-fc76-4943-83e5-9de40dd1c78c';
+--------------------------------------+--------------------------------------+
| id                                   | pool_id                              |
+--------------------------------------+--------------------------------------+
| 6730f6c1-634c-4371-9df5-1a880662acc9 | 26c0384b-fc76-4943-83e5-9de40dd1c78c |
| 06f0cfc9-379a-4e3d-ab31-cdba1580afc2 | 26c0384b-fc76-4943-83e5-9de40dd1c78c |
+--------------------------------------+--------------------------------------+</pre></div></li><li class="step "><p>
          Delete the pool members:
        </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; delete from lbaas_members where id = '6730f6c1-634c-4371-9df5-1a880662acc9';
mysql&gt; delete from lbaas_members where id = '06f0cfc9-379a-4e3d-ab31-cdba1580afc2';</pre></div></li><li class="step "><p>
          Find and delete the listener associated with the loadbalancer:
        </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; select id, loadbalancer_id, default_pool_id from lbaas_listeners where loadbalancer_id = '7be4e4ab-e9c6-4a57-b767-da9af5ba7405';
+--------------------------------------+--------------------------------------+--------------------------------------+
| id                                   | loadbalancer_id                      | default_pool_id                      |
+--------------------------------------+--------------------------------------+--------------------------------------+
| 3283f589-8464-43b3-96e0-399377642e0a | 7be4e4ab-e9c6-4a57-b767-da9af5ba7405 | 26c0384b-fc76-4943-83e5-9de40dd1c78c |
+--------------------------------------+--------------------------------------+--------------------------------------+
mysql&gt; delete from lbaas_listeners where id = '3283f589-8464-43b3-96e0-399377642e0a';</pre></div></li><li class="step "><p>
          Delete the pool associated with the loadbalancer:
        </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; delete from lbaas_pools where id = '26c0384b-fc76-4943-83e5-9de40dd1c78c';</pre></div></li><li class="step "><p>
          Delete the healthmonitor associated with the pool:
        </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; delete from lbaas_healthmonitors where id = '323a3c4b-8083-41e1-b1d9-04e1fef1a331';</pre></div></li><li class="step "><p>
          Delete the loadbalancer:
        </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; delete from lbaas_loadbalancer_statistics where loadbalancer_id = '7be4e4ab-e9c6-4a57-b767-da9af5ba7405';
mysql&gt; delete from lbaas_loadbalancers where id = '7be4e4ab-e9c6-4a57-b767-da9af5ba7405';</pre></div></li></ol></div></div><div class="procedure " id="id-1.5.12.6.13.7.2.4"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 10.4: </span><span class="name">Manually Deleting Load Balancers Created With Octavia </span><a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.13.7.2.4">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Query the Octavia service for the loadbalancer ID:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack loadbalancer list --column id --column name --column provisioning_status
+--------------------------------------+---------+---------------------+
| id                                   | name    | provisioning_status |
+--------------------------------------+---------+---------------------+
| d8ac085d-e077-4af2-b47a-bdec0c162928 | test-lb | ERROR               |
+--------------------------------------+---------+---------------------+</pre></div></li><li class="step "><p>
          Query the Octavia service for the amphora IDs (in this
          example we use <code class="literal">ACTIVE/STANDBY</code> topology with 1 spare Amphora):
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack loadbalancer amphora list
+--------------------------------------+--------------------------------------+-----------+--------+---------------+-------------+
| id                                   | loadbalancer_id                      | status    | role   | lb_network_ip | ha_ip       |
+--------------------------------------+--------------------------------------+-----------+--------+---------------+-------------+
| 6dc66d41-e4b6-4c33-945d-563f8b26e675 | d8ac085d-e077-4af2-b47a-bdec0c162928 | ALLOCATED | BACKUP | 172.30.1.7    | 192.168.1.8 |
| 1b195602-3b14-4352-b355-5c4a70e200cf | d8ac085d-e077-4af2-b47a-bdec0c162928 | ALLOCATED | MASTER | 172.30.1.6    | 192.168.1.8 |
| b2ee14df-8ac6-4bb0-a8d3-3f378dbc2509 | None                                 | READY     | None   | 172.30.1.20   | None        |
+--------------------------------------+--------------------------------------+-----------+--------+---------------+-------------+</pre></div></li><li class="step "><p>
          Query the Octavia service for the loadbalancer pools:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack loadbalancer pool list
+--------------------------------------+-----------+----------------------------------+---------------------+----------+--------------+----------------+
| id                                   | name      | project_id                       | provisioning_status | protocol | lb_algorithm | admin_state_up |
+--------------------------------------+-----------+----------------------------------+---------------------+----------+--------------+----------------+
| 39c4c791-6e66-4dd5-9b80-14ea11152bb5 | test-pool | 86fba765e67f430b83437f2f25225b65 | ACTIVE              | TCP      | ROUND_ROBIN  | True           |
+--------------------------------------+-----------+----------------------------------+---------------------+----------+--------------+----------------+</pre></div></li><li class="step "><p>
          Connect to the octavia database:
        </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; use octavia</pre></div></li><li class="step "><p>
          Delete any listeners, pools, health monitors, and members
          from the load balancer:
        </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; delete from listener where load_balancer_id = 'd8ac085d-e077-4af2-b47a-bdec0c162928';
mysql&gt; delete from health_monitor where pool_id = '39c4c791-6e66-4dd5-9b80-14ea11152bb5';
mysql&gt; delete from member where pool_id = '39c4c791-6e66-4dd5-9b80-14ea11152bb5';
mysql&gt; delete from pool where load_balancer_id = 'd8ac085d-e077-4af2-b47a-bdec0c162928';</pre></div></li><li class="step "><p>
          Delete the amphora entries in the database:
        </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; delete from amphora_health where amphora_id = '6dc66d41-e4b6-4c33-945d-563f8b26e675';
mysql&gt; update amphora set status = 'DELETED' where id = '6dc66d41-e4b6-4c33-945d-563f8b26e675';
mysql&gt; delete from amphora_health where amphora_id = '1b195602-3b14-4352-b355-5c4a70e200cf';
mysql&gt; update amphora set status = 'DELETED' where id = '1b195602-3b14-4352-b355-5c4a70e200cf';</pre></div></li><li class="step "><p>
          Delete the load balancer instance:
        </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; update load_balancer set provisioning_status = 'DELETED' where id = 'd8ac085d-e077-4af2-b47a-bdec0c162928';</pre></div></li><li class="step "><p>
          The following script automates the above steps:
        </p><div class="verbatim-wrap"><pre class="screen">#!/bin/bash

if (( $# != 1 )); then
echo "Please specify a loadbalancer ID"
exit 1
fi

LB_ID=$1

set -u -e -x

readarray -t AMPHORAE &lt; &lt;(openstack loadbalancer amphora list \
--format value \
--column id \
--column loadbalancer_id \
| grep ${LB_ID} \
| cut -d ' ' -f 1)

readarray -t POOLS &lt; &lt;(openstack loadbalancer show ${LB_ID} \
--format value \
--column pools)

mysql octavia --execute "delete from listener where load_balancer_id = '${LB_ID}';"
for p in "${POOLS[@]}"; do
mysql octavia --execute "delete from health_monitor where pool_id = '${p}';"
mysql octavia --execute "delete from member where pool_id = '${p}';"
done
mysql octavia --execute "delete from pool where load_balancer_id = '${LB_ID}';"
for a in "${AMPHORAE[@]}"; do
mysql octavia --execute "delete from amphora_health where amphora_id = '${a}';"
mysql octavia --execute "update amphora set status = 'DELETED' where id = '${a}';"
done
mysql octavia --execute "update load_balancer set provisioning_status = 'DELETED' where id = '${LB_ID}';"</pre></div></li></ol></div></div></div></div><div class="sect3" id="idg-all-networking-octavia-admin-xml-10"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.9.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#idg-all-networking-octavia-admin-xml-10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-octavia_admin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-octavia_admin.xml</li><li><span class="ds-label">ID: </span>idg-all-networking-octavia-admin-xml-10</li></ul></div></div></div></div><p>
   For more information on the OpenStackClient and Octavia terminology, see the
   <a class="link" href="https://docs.openstack.org/python-openstackclient/latest/" target="_blank">OpenStackClient</a>
   guide.
  </p></div></div><div class="sect2" id="cha-network-rbac"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Role-based Access Control in neutron</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#cha-network-rbac">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span>cha-network-rbac</li></ul></div></div></div></div><p>
   This topic explains how to achieve more granular access control for your
   neutron networks.
  </p><p>
   Previously in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, a network object was either private to a project or
   could be used by all projects. If the network's shared attribute was True,
   then the network could be used by every project in the cloud. If false, only
   the members of the owning project could use it. There was no way for the
   network to be shared by only a subset of the projects.
  </p><p>
  <span class="phrase">neutron Role Based Access Control (RBAC) solves this problem for
   networks. Now the network owner can create RBAC policies that give network
   access to target projects. Members of a targeted project can use the
   network named in the RBAC policy the same way as if the network was owned
   by the project. Constraints are described in the
   section</span>
  <a class="xref" href="ops-managing-networking.html#sec-network-rbac-limitation" title="10.4.10.10. Limitations">Section 10.4.10.10, “Limitations”</a>.
 </p><p>
   With RBAC you are able to let another tenant use a network that you created,
   but as the owner of the network, you need to create the subnet and the
   router for the network.
  </p><div class="sect3" id="id-1.5.12.6.14.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a Network</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.14.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network create demo-net
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | UP                                   |
| availability_zone_hints   |                                      |
| availability_zones        |                                      |
| created_at                | 2018-07-25T17:43:59Z                 |
| description               |                                      |
| dns_domain                |                                      |
| id                        | 9c801954-ec7f-4a65-82f8-e313120aabc4 |
| ipv4_address_scope        | None                                 |
| ipv6_address_scope        | None                                 |
| is_default                | False                                |
| is_vlan_transparent       | None                                 |
| mtu                       | 1450                                 |
| name                      | demo-net                             |
| port_security_enabled     | False                                |
| project_id                | cb67c79e25a84e328326d186bf703e1b     |
| provider:network_type     | vxlan                                |
| provider:physical_network | None                                 |
| provider:segmentation_id  | 1009                                 |
| qos_policy_id             | None                                 |
| revision_number           | 2                                    |
| router:external           | Internal                             |
| segments                  | None                                 |
| shared                    | False                                |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tags                      |                                      |
| updated_at                | 2018-07-25T17:43:59Z                 |
+---------------------------+--------------------------------------+</pre></div></div><div class="sect3" id="id-1.5.12.6.14.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.10.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating an RBAC Policy</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.14.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Here we will create an RBAC policy where a member of the project called
   'demo' will share the network with members of project 'demo2'
  </p><p>
   To create the RBAC policy, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac create  --target-project <em class="replaceable ">DEMO2-PROJECT-ID</em> --type network --action access_as_shared demo-net</pre></div><p>
   Here is an example where the <em class="replaceable ">DEMO2-PROJECT-ID</em> is
   5a582af8b44b422fafcd4545bd2b7eb5
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac create --target-tenant 5a582af8b44b422fafcd4545bd2b7eb5 \
  --type network --action access_as_shared demo-net</pre></div></div><div class="sect3" id="id-1.5.12.6.14.8"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.10.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Listing RBACs</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.14.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To list all the RBAC rules/policies, execute:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac list
+--------------------------------------+-------------+--------------------------------------+
| ID                                   | Object Type | Object ID                            |
+--------------------------------------+-------------+--------------------------------------+
| 0fdec7f0-9b94-42b4-a4cd-b291d04282c1 | network     | 7cd94877-4276-488d-b682-7328fc85d721 |
+--------------------------------------+-------------+--------------------------------------+</pre></div></div><div class="sect3" id="id-1.5.12.6.14.9"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.10.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Listing the Attributes of an RBAC</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.14.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To see the attributes of a specific RBAC policy, run
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac show <em class="replaceable ">POLICY-ID</em></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac show 0fd89dcb-9809-4a5e-adc1-39dd676cb386</pre></div><p>
   Here is the output:
  </p><div class="verbatim-wrap"><pre class="screen">+---------------+--------------------------------------+
| Field         | Value                                |
+---------------+--------------------------------------+
| action        | access_as_shared                     |
| id            | 0fd89dcb-9809-4a5e-adc1-39dd676cb386 |
| object_id     | c3d55c21-d8c9-4ee5-944b-560b7e0ea33b |
| object_type   | network                              |
| target_tenant | 5a582af8b44b422fafcd4545bd2b7eb5     |
| tenant_id     | 75eb5efae5764682bca2fede6f4d8c6f     |
+---------------+--------------------------------------+</pre></div></div><div class="sect3" id="id-1.5.12.6.14.10"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.10.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deleting an RBAC Policy</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.14.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To delete an RBAC policy, run <code class="literal">openstack network rbac delete</code> passing the policy id:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac delete <em class="replaceable ">POLICY-ID</em></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac delete 0fd89dcb-9809-4a5e-adc1-39dd676cb386</pre></div><p>
   Here is the output:
  </p><div class="verbatim-wrap"><pre class="screen">Deleted rbac_policy: 0fd89dcb-9809-4a5e-adc1-39dd676cb386</pre></div></div><div class="sect3" id="id-1.5.12.6.14.11"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.10.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Sharing a Network with All Tenants</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.14.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Either the administrator or the network owner can make a network shareable
   by all tenants.
  </p><p>
   The administrator can make a tenant's network shareable by all tenants.
   To make the network <code class="literal">demo-shareall-net</code> accessible by all
   tenants in the cloud:
  </p><p>
   To share a network with all tenants:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Get a list of all projects
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>~/service.osrc
<code class="prompt user">ardana &gt; </code>openstack project list</pre></div><p>
      which produces the list:
    </p><div class="verbatim-wrap"><pre class="screen">+----------------------------------+------------------+
| ID                               | Name             |
+----------------------------------+------------------+
| 1be57778b61645a7a1c07ca0ac488f9e | demo             |
| 5346676226274cd2b3e3862c2d5ceadd | admin            |
| 749a557b2b9c482ca047e8f4abf348cd | swift-monitor    |
| 8284a83df4df429fb04996c59f9a314b | swift-dispersion |
| c7a74026ed8d4345a48a3860048dcb39 | demo-sharee      |
| e771266d937440828372090c4f99a995 | glance-swift     |
| f43fb69f107b4b109d22431766b85f20 | services         |
+----------------------------------+------------------+</pre></div></li><li class="step "><p>
     Get a list of networks:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network list</pre></div><p>
     This produces the following list:
    </p><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+-------------------+----------------------------------------------------+
| id                                   | name              | subnets                                            |
+--------------------------------------+-------------------+----------------------------------------------------+
| f50f9a63-c048-444d-939d-370cb0af1387 | ext-net           | ef3873db-fc7a-4085-8454-5566fb5578ea 172.31.0.0/16 |
| 9fb676f5-137e-4646-ac6e-db675a885fd3 | demo-net          | 18fb0b77-fc8b-4f8d-9172-ee47869f92cc 10.0.1.0/24   |
| 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e | demo-shareall-net | 2bbc85a9-3ffe-464c-944b-2476c7804877 10.0.250.0/24 |
| 73f946ee-bd2b-42e9-87e4-87f19edd0682 | demo-share-subset | c088b0ef-f541-42a7-b4b9-6ef3c9921e44 10.0.2.0/24   |
+--------------------------------------+-------------------+----------------------------------------------------+</pre></div></li><li class="step "><p>
     Set the network you want to share to a shared value of True:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network set --share 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e</pre></div><p>
     You should see the following output:
    </p><div class="verbatim-wrap"><pre class="screen">Updated network: 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e</pre></div></li><li class="step "><p>
     Check the attributes of that network by running the following command
     using the ID of the network in question:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network show 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e</pre></div><p>
     The output will look like this:
    </p><div class="verbatim-wrap"><pre class="screen">+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | UP                                   |
| availability_zone_hints   |                                      |
| availability_zones        |                                      |
| created_at                | 2018-07-25T17:43:59Z                 |
| description               |                                      |
| dns_domain                |                                      |
| id                        | 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e |
| ipv4_address_scope        | None                                 |
| ipv6_address_scope        | None                                 |
| is_default                | None                                 |
| is_vlan_transparent       | None                                 |
| mtu                       | 1450                                 |
| name                      | demo-net                             |
| port_security_enabled     | False                                |
| project_id                | cb67c79e25a84e328326d186bf703e1b     |
| provider:network_type     | vxlan                                |
| provider:physical_network | None                                 |
| provider:segmentation_id  | 1009                                 |
| qos_policy_id             | None                                 |
| revision_number           | 2                                    |
| router:external           | Internal                             |
| segments                  | None                                 |
| shared                    | False                                |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tags                      |                                      |
| updated_at                | 2018-07-25T17:43:59Z                 |
+---------------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     As the owner of the <code class="literal">demo-shareall-net</code> network, view
     the RBAC attributes for
     <code class="literal">demo-shareall-net</code>
     (<code class="literal">id=8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e</code>) by first
     getting an RBAC list:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>echo $OS_USERNAME ; echo $OS_PROJECT_NAME
demo
demo
<code class="prompt user">ardana &gt; </code>openstack network rbac list</pre></div><p>
     This produces the list:
    </p><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+--------------------------------------+
| id                                   | object_id                            |
+--------------------------------------+--------------------------------------+
| ...                                                                         |
| 3e078293-f55d-461c-9a0b-67b5dae321e8 | 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e |
+--------------------------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     View the RBAC information:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac show 3e078293-f55d-461c-9a0b-67b5dae321e8

+---------------+--------------------------------------+
| Field         | Value                                |
+---------------+--------------------------------------+
| action        | access_as_shared                     |
| id            | 3e078293-f55d-461c-9a0b-67b5dae321e8 |
| object_id     | 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e |
| object_type   | network                              |
| target_tenant | *                                    |
| tenant_id     | 1be57778b61645a7a1c07ca0ac488f9e     |
+---------------+--------------------------------------+</pre></div></li><li class="step "><p>
     With network RBAC, the owner of the network can also make the network
     shareable by all tenants. First create the network:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>echo $OS_PROJECT_NAME ; echo $OS_USERNAME
demo
demo
<code class="prompt user">ardana &gt; </code>openstack network create test-net</pre></div><p>
     The network is created:
    </p><div class="verbatim-wrap"><pre class="screen">+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | UP                                   |
| availability_zone_hints   |                                      |
| availability_zones        |                                      |
| created_at                | 2018-07-25T18:04:25Z                 |
| description               |                                      |
| dns_domain                |                                      |
| id                        | a4bd7c3a-818f-4431-8cdb-fedf7ff40f73 |
| ipv4_address_scope        | None                                 |
| ipv6_address_scope        | None                                 |
| is_default                | False                                |
| is_vlan_transparent       | None                                 |
| mtu                       | 1450                                 |
| name                      | test-net                             |
| port_security_enabled     | False                                |
| project_id                | cb67c79e25a84e328326d186bf703e1b     |
| provider:network_type     | vxlan                                |
| provider:physical_network | None                                 |
| provider:segmentation_id  | 1073                                 |
| qos_policy_id             | None                                 |
| revision_number           | 2                                    |
| router:external           | Internal                             |
| segments                  | None                                 |
| shared                    | False                                |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tags                      |                                      |
| updated_at                | 2018-07-25T18:04:25Z                 |
+---------------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     Create the RBAC. It is important that the asterisk is surrounded by
     single-quotes to prevent the shell from expanding it to all files in the
     current directory.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac create --type network \
  --action access_as_shared --target-project '*' test-net</pre></div><p>
     Here are the resulting RBAC attributes:
    </p><div class="verbatim-wrap"><pre class="screen">+---------------+--------------------------------------+
| Field         | Value                                |
+---------------+--------------------------------------+
| action        | access_as_shared                     |
| id            | 0b797cc6-debc-48a1-bf9d-d294b077d0d9 |
| object_id     | a4bd7c3a-818f-4431-8cdb-fedf7ff40f73 |
| object_type   | network                              |
| target_tenant | *                                    |
| tenant_id     | 1be57778b61645a7a1c07ca0ac488f9e     |
+---------------+--------------------------------------+</pre></div></li></ol></div></div></div><div class="sect3" id="id-1.5.12.6.14.12"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.10.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Target Project (<code class="literal">demo2</code>) View of Networks and Subnets</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.14.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Note that the owner of the network and subnet is not the tenant named
   <code class="literal">demo2</code>. Both the network and subnet are owned by tenant <code class="literal">demo</code>.
   <code class="literal">Demo2</code>members cannot create subnets of the network. They also cannot
   modify or delete subnets owned by <code class="literal">demo</code>.
  </p><p>
   As the tenant <code class="literal">demo2</code>, you can get a list of neutron networks:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network list</pre></div><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+-----------+--------------------------------------------------+
| id                                   | name      | subnets                                          |
+--------------------------------------+-----------+--------------------------------------------------+
| f60f3896-2854-4f20-b03f-584a0dcce7a6 | ext-net   | 50e39973-b2e3-466b-81c9-31f4d83d990b             |
| c3d55c21-d8c9-4ee5-944b-560b7e0ea33b | demo-net  | d9b765da-45eb-4543-be96-1b69a00a2556 10.0.1.0/24 |
   ...
+--------------------------------------+-----------+--------------------------------------------------+</pre></div><p>
   And get a list of subnets:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack subnet list --network c3d55c21-d8c9-4ee5-944b-560b7e0ea33b</pre></div><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+---------+--------------------------------------+---------------+
| ID                                   | Name    | Network                              | Subnet        |
+--------------------------------------+---------+--------------------------------------+---------------+
| a806f28b-ad66-47f1-b280-a1caa9beb832 | ext-net | c3d55c21-d8c9-4ee5-944b-560b7e0ea33b | 10.0.1.0/24   |
+--------------------------------------+---------+--------------------------------------+---------------+</pre></div><p>
To show details of the subnet:
</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack subnet show d9b765da-45eb-4543-be96-1b69a00a2556</pre></div><div class="verbatim-wrap"><pre class="screen">+-------------------+--------------------------------------------+
| Field             | Value                                      |
+-------------------+--------------------------------------------+
| allocation_pools  | {"start": "10.0.1.2", "end": "10.0.1.254"} |
| cidr              | 10.0.1.0/24                                |
| dns_nameservers   |                                            |
| enable_dhcp       | True                                       |
| gateway_ip        | 10.0.1.1                                   |
| host_routes       |                                            |
| id                | d9b765da-45eb-4543-be96-1b69a00a2556       |
| ip_version        | 4                                          |
| ipv6_address_mode |                                            |
| ipv6_ra_mode      |                                            |
| name              | sb-demo-net                                |
| network_id        | c3d55c21-d8c9-4ee5-944b-560b7e0ea33b       |
| subnetpool_id     |                                            |
| tenant_id         | 75eb5efae5764682bca2fede6f4d8c6f           |
+-------------------+--------------------------------------------+</pre></div></div><div class="sect3" id="id-1.5.12.6.14.13"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.10.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Target Project: Creating a Port Using demo-net</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.14.13">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The owner of the port is <code class="literal">demo2</code>. Members of the network owner project
   (<code class="literal">demo</code>) will not see this port.
  </p><p>
   Running the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port create c3d55c21-d8c9-4ee5-944b-560b7e0ea33b</pre></div><p>
   Creates a new port:
  </p><div class="verbatim-wrap"><pre class="screen">+-----------------------+-----------------------------------------------------------------------------------------------------+
| Field                 | Value                                                                                               |
+-----------------------+-----------------------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                                |
| allowed_address_pairs |                                                                                                     |
| binding:vnic_type     | normal                                                                                              |
| device_id             |                                                                                                     |
| device_owner          |                                                                                                     |
| dns_assignment        | {"hostname": "host-10-0-1-10", "ip_address": "10.0.1.10", "fqdn": "host-10-0-1-10.openstacklocal."} |
| dns_name              |                                                                                                     |
| fixed_ips             | {"subnet_id": "d9b765da-45eb-4543-be96-1b69a00a2556", "ip_address": "10.0.1.10"}                    |
| id                    | 03ef2dce-20dc-47e5-9160-942320b4e503                                                                |
| mac_address           | fa:16:3e:27:8d:ca                                                                                   |
| name                  |                                                                                                     |
| network_id            | c3d55c21-d8c9-4ee5-944b-560b7e0ea33b                                                                |
| security_groups       | 275802d0-33cb-4796-9e57-03d8ddd29b94                                                                |
| status                | DOWN                                                                                                |
| tenant_id             | 5a582af8b44b422fafcd4545bd2b7eb5                                                                    |
+-----------------------+-----------------------------------------------------------------------------------------------------+</pre></div></div><div class="sect3" id="id-1.5.12.6.14.14"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.10.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Target Project Booting a VM Using Demo-Net</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.14.14">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Here the tenant <code class="literal">demo2</code> boots a VM that uses the <code class="literal">demo-net</code> shared network:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server create --flavor 1 --image $OS_IMAGE --nic net-id=c3d55c21-d8c9-4ee5-944b-560b7e0ea33b demo2-vm-using-demo-net-nic</pre></div><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+------------------------------------------------+
| Property                             | Value                                          |
+--------------------------------------+------------------------------------------------+
| OS-EXT-AZ:availability_zone          |                                                |
| OS-EXT-STS:power_state               | 0                                              |
| OS-EXT-STS:task_state                | scheduling                                     |
| OS-EXT-STS:vm_state                  | building                                       |
| OS-SRV-USG:launched_at               | -                                              |
| OS-SRV-USG:terminated_at             | -                                              |
| accessIPv4                           |                                                |
| accessIPv6                           |                                                |
| adminPass                            | sS9uSv9PT79F                                   |
| config_drive                         |                                                |
| created                              | 2016-01-04T19:23:24Z                           |
| flavor                               | m1.tiny (1)                                    |
| hostId                               |                                                |
| id                                   | 3a4dc44a-027b-45e9-acf8-054a7c2dca2a           |
| image                                | cirros-0.3.3-x86_64 (6ae23432-8636-4e...1efc5) |
| key_name                             | -                                              |
| metadata                             | {}                                             |
| name                                 | demo2-vm-using-demo-net-nic                    |
| os-extended-volumes:volumes_attached | []                                             |
| progress                             | 0                                              |
| security_groups                      | default                                        |
| status                               | BUILD                                          |
| tenant_id                            | 5a582af8b44b422fafcd4545bd2b7eb5               |
| updated                              | 2016-01-04T19:23:24Z                           |
| user_id                              | a0e6427b036344fdb47162987cb0cee5               |
+--------------------------------------+------------------------------------------------+</pre></div><p>
   Run openstack server list:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list</pre></div><p>
   See the VM running:
  </p><div class="verbatim-wrap"><pre class="screen">+-------------------+-----------------------------+--------+------------+-------------+--------------------+
| ID                | Name                        | Status | Task State | Power State | Networks           |
+-------------------+-----------------------------+--------+------------+-------------+--------------------+
| 3a4dc...a7c2dca2a | demo2-vm-using-demo-net-nic | ACTIVE | -          | Running     | demo-net=10.0.1.11 |
+-------------------+-----------------------------+--------+------------+-------------+--------------------+</pre></div><p>
   Run openstack port list:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstask port list --device-id 3a4dc44a-027b-45e9-acf8-054a7c2dca2a</pre></div><p>
   View the subnet:
  </p><div class="verbatim-wrap"><pre class="screen">+---------------------+------+-------------------+-------------------------------------------------------------------+
| id                  | name | mac_address       | fixed_ips                                                         |
+---------------------+------+-------------------+-------------------------------------------------------------------+
| 7d14ef8b-9...80348f |      | fa:16:3e:75:32:8e | {"subnet_id": "d9b765da-45...00a2556", "ip_address": "10.0.1.11"} |
+---------------------+------+-------------------+-------------------------------------------------------------------+</pre></div><p>
   Run openstack port show:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port show 7d14ef8b-9d48-4310-8c02-00c74d80348f</pre></div><div class="verbatim-wrap"><pre class="screen">+-----------------------+-----------------------------------------------------------------------------------------------------+
| Field                 | Value                                                                                               |
+-----------------------+-----------------------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                                |
| allowed_address_pairs |                                                                                                     |
| binding:vnic_type     | normal                                                                                              |
| device_id             | 3a4dc44a-027b-45e9-acf8-054a7c2dca2a                                                                |
| device_owner          | compute:None                                                                                        |
| dns_assignment        | {"hostname": "host-10-0-1-11", "ip_address": "10.0.1.11", "fqdn": "host-10-0-1-11.openstacklocal."} |
| dns_name              |                                                                                                     |
| extra_dhcp_opts       |                                                                                                     |
| fixed_ips             | {"subnet_id": "d9b765da-45eb-4543-be96-1b69a00a2556", "ip_address": "10.0.1.11"}                    |
| id                    | 7d14ef8b-9d48-4310-8c02-00c74d80348f                                                                |
| mac_address           | fa:16:3e:75:32:8e                                                                                   |
| name                  |                                                                                                     |
| network_id            | c3d55c21-d8c9-4ee5-944b-560b7e0ea33b                                                                |
| security_groups       | 275802d0-33cb-4796-9e57-03d8ddd29b94                                                                |
| status                | ACTIVE                                                                                              |
| tenant_id             | 5a582af8b44b422fafcd4545bd2b7eb5                                                                    |
+-----------------------+-----------------------------------------------------------------------------------------------------+</pre></div></div><div class="sect3" id="sec-network-rbac-limitation"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.10.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#sec-network-rbac-limitation">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span>sec-network-rbac-limitation</li></ul></div></div></div></div><p>
   Note the following limitations of RBAC in neutron.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     neutron network is the only supported RBAC neutron object type.
    </p></li><li class="listitem "><p>
     The "access_as_external" action is not supported – even though it is
     listed as a valid action by python-neutronclient.
    </p></li><li class="listitem "><p>
     The neutron-api server will not accept action value of
     'access_as_external'. The <code class="literal">access_as_external</code> definition
     is not found in the specs.
    </p></li><li class="listitem "><p>
     The target project users cannot create, modify, or delete subnets on
     networks that have RBAC policies.
    </p></li><li class="listitem "><p>
     The subnet of a network that has an RBAC policy cannot be added as an
     interface of a target tenant's router. For example, the command
     <code class="literal">openstack router add subnet tgt-tenant-router &lt;sb-demo-net
     uuid&gt;</code> will error out.
    </p></li><li class="listitem "><p>
     The security group rules on the network owner do not apply to other
     projects that can use the network.
    </p></li><li class="listitem "><p>
     A user in target project can boot up VMs using a VNIC using the shared
     network. The user of the target project can assign a floating IP (FIP) to
     the VM. The target project must have SG rules that allows SSH and/or ICMP
     for VM connectivity.
    </p></li><li class="listitem "><p>
     neutron RBAC creation and management are currently not supported in
     horizon. For now, the neutron CLI has to be used to manage RBAC rules.
    </p></li><li class="listitem "><p>
     A RBAC rule tells neutron whether a tenant can access a network (Allow).
     Currently there is no DENY action.
    </p></li><li class="listitem "><p>
     Port creation on a shared network fails if <code class="literal">--fixed-ip</code>
     is specified in the <code class="literal">openstack port create</code> command.
     
     
    </p></li></ul></div></div></div><div class="sect2" id="configureMTU"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Maximum Transmission Units in neutron</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#configureMTU">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-configure_mtu.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-configure_mtu.xml</li><li><span class="ds-label">ID: </span>configureMTU</li></ul></div></div></div></div><p>
  This topic explains how you can configure MTUs, what to look out for, and
  the results and implications of changing the default MTU settings. It is
  important to note that every network within a network group will have the
  same MTU.
 </p><div id="id-1.5.12.6.15.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
   An MTU change will not affect existing networks that have had VMs created
   on them. It will only take effect on new networks created after the
   reconfiguration process.
  </p></div><div class="sect3" id="id-1.5.12.6.15.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.11.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Overview</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.15.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-configure_mtu.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-configure_mtu.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   A Maximum Transmission Unit, or MTU is the maximum packet size (in bytes)
   that a network device can or is configured to handle. There are a number of
   places in your cloud where MTU configuration is relevant: the physical
   interfaces managed and configured by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the virtual
   interfaces created by neutron and nova for neutron networking, and the
   interfaces inside the VMs.
  </p><p>
   <span class="bold"><strong><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>-managed physical interfaces </strong></span>
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>-managed physical interfaces include the physical interfaces
   and the bonds, bridges, and VLANs created on top of them. The MTU for these
   interfaces is configured via the 'mtu' property of a network group. Because
   multiple network groups can be mapped to one physical interface, there may
   have to be some resolution of differing MTUs between the untagged and tagged
   VLANs on the same physical interface. For instance, if one untagged VLAN,
   vlan101 (with an MTU of 1500) and a tagged VLAN vlan201 (with an MTU of
   9000) are both on one interface (eth0), this means that eth0 can handle
   1500, but the VLAN interface which is created on top of eth0 (that is,
   <code class="literal">vlan201@eth0</code>) wants 9000. However, vlan201 cannot have a
   higher MTU than eth0, so vlan201 will be limited to 1500 when it is brought
   up, and fragmentation will result.
  </p><p>
   In general, a VLAN interface MTU must be lower than or equal to the base
   device MTU. If they are different, as in the case above, the MTU of eth0 can
   be overridden and raised to 9000, but in any case the discrepancy will have
   to be reconciled.
  </p><p>
   <span class="bold"><strong>neutron/nova interfaces </strong></span>
  </p><p>
   neutron/nova interfaces include the virtual devices created by neutron
   and nova during the normal process of realizing a neutron
   network/router and booting a VM on it (qr-*, qg-*, tap-*, qvo-*, qvb-*,
   etc.). There is currently no support in neutron/nova for per-network
   MTUs in which every interface along the path for a particular neutron
   network has the correct MTU for that network. There is, however, support for
   globally changing the MTU of devices created by neutron/nova (see
   network_device_mtu below). This means that if you want to enable jumbo
   frames for any set of VMs, you will have to enable it for all your VMs. You
   cannot just enable them for a particular neutron network.
  </p><p>
   <span class="bold"><strong>VM interfaces</strong></span>
  </p><p>
   VMs typically get their MTU via DHCP advertisement, which means that the
   dnsmasq processes spawned by the neutron-dhcp-agent actually advertise a
   particular MTU to the VMs. In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9, the DHCP server advertises to
   all VMS a 1400 MTU via a forced setting in dnsmasq-neutron.conf. This is
   suboptimal for every network type (vxlan, flat, vlan, etc) but it does
   prevent fragmentation of a VM's packets due to encapsulation.
  </p><p>
   For instance, if you set the new *-mtu configuration options to a default of
   1500 and create a VXLAN network, it will be given an MTU of 1450 (with the
   remaining 50 bytes used by the VXLAN encapsulation header) and will
   advertise a 1450 MTU to any VM booted on that network. If you create a
   provider VLAN network, it will have an MTU of 1500 and will advertise 1500
   to booted VMs on the network. It should be noted that this default starting
   point for MTU calculation and advertisement is also global, meaning you
   cannot have an MTU of 8950 on one VXLAN network and 1450 on
   another. However, you can have provider physical networks with different
   MTUs by using the physical_network_mtus config option, but nova still
   requires a global MTU option for the interfaces it creates, thus you cannot
   really take advantage of that configuration option.
  </p></div><div class="sect3" id="id-1.5.12.6.15.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.11.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network settings in the input model</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.15.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-configure_mtu.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-configure_mtu.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   MTU can be set as an attribute of a network group in network_groups.yml.
   Note that this applies only to KVM. That setting means that every network in
   the network group will be assigned the specified MTU. The MTU value must be
   set individually for each network group. For example:
  </p><div class="verbatim-wrap"><pre class="screen">network-groups:
        - name: GUEST
        mtu: 9000
        ...

        - name: EXTERNAL-API
        mtu: 9000
        ...

        - name: EXTERNAL-VM
        mtu: 9000
        ...</pre></div></div><div class="sect3" id="id-1.5.12.6.15.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.11.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Infrastructure support for jumbo frames</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.15.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-configure_mtu.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-configure_mtu.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If you want to use jumbo frames, or frames with an MTU of 9000 or more, the
   physical switches and routers that make up the infrastructure of the
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installation must be configured to support them. To realize
   the advantages, all devices in the same broadcast domain must have
   the same MTU.
  </p><p>
   If you want to configure jumbo frames on compute and controller nodes, then
   all switches joining the compute and controller nodes must have jumbo frames
   enabled. Similarly, the "infrastructure gateway" through which the external
   VM network flows, commonly known as the default route for the external VM
   VLAN, must also have the same MTU configured.
  </p><p>
   You can also consider anything in the same broadcast domain to be anything
   in the same VLAN or anything in the same IP subnet.
  </p></div><div class="sect3" id="id-1.5.12.6.15.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.11.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling end-to-end jumbo frames for a VM</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.15.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-configure_mtu.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-configure_mtu.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Add an <code class="literal">mtu</code> attribute to all the network groups in your
     model. Note that adding the MTU for the network groups will only affect
     the configuration for physical network interfaces.
    </p><p>
     To add the mtu attribute, find the YAML file that contains your
     network-groups entry. We will assume it is network_groups.yml, unless you
     have changed it. Whatever the file is named, it will be found in
     ~/openstack/my_cloud/definition/data/.
    </p><p>
     To edit these files, begin by checking out the
     <span class="bold"><strong>site</strong></span> branch on the Cloud Lifecycle Manager
     node. You may already be on that branch. If so, you will remain there.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git checkout site</pre></div><p>
     Then begin editing the files. In <code class="filename">network_groups.yml</code>,
     add <code class="literal">mtu: 9000</code>.
    </p><div class="verbatim-wrap"><pre class="screen">network-groups:
            - name: GUEST
            hostname-suffix: guest
            mtu: 9000
            tags:
            - neutron.networks.vxlan</pre></div><p>
     This sets the physical interface managed by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 that has the GUEST
     network group tag assigned to it. This can be found in the
     <code class="filename">interfaces_set.yml</code> file under the
     <code class="literal">interface-models</code> section.
    </p></li><li class="step "><p>
     Edit <code class="filename">neutron.conf.j2</code> found in
     <code class="filename">~/openstack/my_cloud/config/neutron/</code> to set
     <code class="literal">global_physnet_mtu</code> to <code class="literal">9000</code> under
     <code class="literal">[DEFAULT]</code>:
    </p><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
...
global_physnet_mtu = 9000</pre></div><p>
     This allows neutron to advertise the optimal MTU to instances (based on
     <code class="literal">global_physnet_mtu</code> minus the encapsulation size).
    </p></li><li class="step "><p>
     Remove the <code class="literal">dhcp-option-force=26,1400</code> line from
     <code class="filename">~/openstack/my_cloud/config/neutron/dnsmasq-neutron.conf.j2</code>.
    </p></li><li class="step "><p>
     OvS will set <code class="literal">br-int</code> to the value of the lowest physical
     interface. If you are using Jumbo frames on some of your networks,
     <code class="literal">br-int</code> on the controllers may be set to 1500 instead of
     9000. Work around this condition by running:
    </p><div class="verbatim-wrap"><pre class="screen">ovs-vsctl set int br-int mtu_request=9000</pre></div></li><li class="step "><p>
     Commit your changes
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "your commit message goes here in quotes"</pre></div></li><li class="step "><p>
     If <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> has not been deployed yet, do normal deployment and skip to
     <a class="xref" href="ops-managing-networking.html#enable-jumbo-normal" title="Step 8">Step 8</a>.
    </p></li><li class="step "><p>
     Assuming it has been deployed already, continue here:
    </p><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
     and ready the deployment:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div><p>
     Then run the network_interface-reconfigure.yml playbook, changing
     directories first:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts network_interface-reconfigure.yml</pre></div><p>
     Then run neutron-reconfigure.yml:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div><p>
     Then nova-reconfigure.yml:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</pre></div><p>
     Note: adding/changing network-group mtu settings will likely require a
     network restart when
     <code class="filename">network_interface-reconfigure.yml</code> is run.
    </p></li><li class="step " id="enable-jumbo-normal"><p>
     Follow the normal process for creating a neutron network and booting a VM
     or two. In this example, if a VXLAN network is created and a VM is booted
     on it, the VM will have an MTU of 8950, with the remaining 50 bytes used
     by the VXLAN encapsulation header.
    </p></li><li class="step "><p>
     Test and verify that the VM can send and receive jumbo frames without
     fragmentation. You can use <code class="command">ping</code>. For example, to test
     an MTU of 9000 using VXLAN:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ping –M do –s 8950 <em class="replaceable ">YOUR_VM_FLOATING_IP</em></pre></div><p>
     Substitute your actual floating IP address for the
     <em class="replaceable ">YOUR_VM_FLOATING_IP</em>.
    </p></li></ol></div></div></div><div class="sect3" id="optimal-mtu"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.11.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling Optimal MTU Advertisement Feature</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#optimal-mtu">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-configure_mtu.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-configure_mtu.xml</li><li><span class="ds-label">ID: </span>optimal-mtu</li></ul></div></div></div></div><p>
   To enable the optimal MTU feature, follow these steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Edit
     <code class="literal">~/openstack/my_cloud/config/neutron/neutron.conf.j2</code> to
     <span class="bold"><strong>remove</strong></span> <code class="literal">advertise_mtu</code>
     variable under [DEFAULT]
    </p><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
...
advertise_mtu = False #remove this</pre></div></li><li class="step "><p>
     Remove the <code class="literal">dhcp-option-force=26,1400</code> line from
     <code class="filename">~/openstack/my_cloud/config/neutron/dnsmasq-neutron.conf.j2</code>.
    </p></li><li class="step "><p>
     If <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> has already been deployed, follow the remaining steps,
     otherwise follow the normal deployment procedures.
    </p></li><li class="step "><p>
     Commit your changes
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "your commit message goes here in quotes"</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Run ready deployment:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the <code class="literal">network_interface-reconfigure.yml</code> playbook,
     changing directories first:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts network_interface-reconfigure.yml</pre></div></li><li class="step "><p>
     Run the <code class="literal">neutron-reconfigure.yml</code> playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></li></ol></div></div><div id="id-1.5.12.6.15.8.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    If you are upgrading an existing deployment, avoid creating MTU mismatch
    between network interfaces in preexisting VMs and that of VMs created after
    upgrade. If you do have an MTU mismatch, then the new VMs (having interface
    with 1500 minus the underlay protocol overhead) will not be able to have L2
    connectivity with preexisting VMs (with 1400 MTU due to
    <code class="literal">dhcp-option-force</code>).
   </p></div></div></div><div class="sect2" id="topic-shy-ksv-jw"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Improve Network Peformance with Isolated Metadata Settings</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#topic-shy-ksv-jw">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-isolated_metadata.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-isolated_metadata.xml</li><li><span class="ds-label">ID: </span>topic-shy-ksv-jw</li></ul></div></div></div></div><p>
  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, neutron currently sets <code class="literal">enable_isolated_metadata =
  True</code> by default in <code class="literal">dhcp_agent.ini</code> because
  several services require isolated networks (neutron networks without a
  router). It also sets <code class="literal">force_metadata = True</code> if DVR is 
  enabled to improve the scalability on large environments with a high churn 
  rate. However, this has the effect of spawning a <code class="literal">neutron-ns-metadata-proxy</code>
  process on one of the controller nodes for every active neutron network.
 </p><p>
  In environments that create many neutron networks, these extra
  <code class="literal">neutron-ns-metadata-proxy</code> processes can quickly eat up a
  lot of memory on the controllers, which does not scale up well.
 </p><p>
  For deployments that do not require isolated metadata (that is, they do not
  require the Platform Services and will always create networks with an
  attached router) and do not have a high churn rate, you can set 
  <code class="literal">enable_isolated_metadata = False</code> and <code class="literal">force_metadata = False</code>
  in <code class="literal">dhcp_agent.ini</code> to reduce neutron memory usage on controllers,
  allowing a greater number of active neutron networks.
 </p><p>
  Note that the <code class="literal">dhcp_agent.ini.j2</code> template is found in
  <code class="literal">~/openstack/my_cloud/config/neutron</code> on the Cloud Lifecycle Manager
  node. The edit can be made there and the standard deployment can be run if
  this is install time. In a deployed cloud, run the neutron reconfiguration
  procedure outlined here:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    First check out the site branch:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/config/neutron
<code class="prompt user">ardana &gt; </code>git checkout site</pre></div></li><li class="step "><p>
    Edit the <code class="literal">dhcp_agent.ini.j2</code> file to change the
    <code class="literal">enable_isolated_metadata = {{ neutron_enable_isolated_metadata }}</code>
    <code class="literal">force_metadata = {{ router_distributed }}</code>
    line in the <code class="literal">[DEFAULT]</code> section to read:
   </p><div class="verbatim-wrap"><pre class="screen">enable_isolated_metadata = False</pre></div><div class="verbatim-wrap"><pre class="screen">force_metadata = False</pre></div></li><li class="step "><p>
    Commit the file:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "your commit message goes here in quotes"</pre></div></li><li class="step "><p>
    Run the <code class="literal">ready-deployment.yml</code> playbook from
    <code class="literal">~/openstack/ardana/ansible</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
    Then run the <code class="literal">neutron-reconfigure.yml</code> playbook, changing
    directories first:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect2" id="moving-from-dvr-deployments"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Moving from DVR deployments to non_DVR</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#moving-from-dvr-deployments">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-moving_from_dvr_deployments.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-moving_from_dvr_deployments.xml</li><li><span class="ds-label">ID: </span>moving-from-dvr-deployments</li></ul></div></div></div></div><p>
  If you have an older deployment of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> which is using DVR as a default
  and you are attempting to move to non_DVR, follow these steps:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Remove all your existing DVR routers and their workloads. Make sure to
    remove interfaces, floating ips and gateways, if applicable.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack router remove subnet <em class="replaceable ">ROUTER-NAME</em> <em class="replaceable ">SUBNET-NAME</em>/<em class="replaceable ">SUBNET-ID</em>
<code class="prompt user">ardana &gt; </code>openstack floating ip unset –port <em class="replaceable ">FLOATINGIP-ID</em> <em class="replaceable ">PRIVATE-PORT-ID</em>
<code class="prompt user">ardana &gt; </code>openstack router unset <em class="replaceable ">ROUTER-NAME</em> <em class="replaceable ">-NET-NAME</em>/<em class="replaceable ">EXT-NET-ID</em></pre></div></li><li class="step "><p>
    Then delete the router.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack router delete <em class="replaceable ">ROUTER-NAME</em></pre></div></li><li class="step "><p>
    Before you create any non_DVR router make sure that l3-agents and
    metadata-agents are not running in any compute host. You can run the
    command <code class="literal">openstack network agent list</code> to see if there are
    any neutron-l3-agent running in any compute-host in your deployment.
   </p><p>
    You must disable <code class="literal">neutron-l3-agent</code> and
    <code class="literal">neutron-metadata-agent</code> on every compute host by running
    the following commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network agent list
+--------------------------------------+----------------------+--------------------------+-------------------+-------+----------------+---------------------------+
| id                                   | agent_type           | host                     | availability_zone | alive | admin_state_up | binary                    |
+--------------------------------------+----------------------+--------------------------+-------------------+-------+----------------+---------------------------+
| 810f0ae7-63aa-4ee3-952d-69837b4b2fe4 | L3 agent             | ardana-cp1-comp0001-mgmt | nova              | :-)   | True           | neutron-l3-agent          |
| 89ac17ba-2f43-428a-98fa-b3698646543d | Metadata agent       | ardana-cp1-comp0001-mgmt |                   | :-)   | True           | neutron-metadata-agent    |
| f602edce-1d2a-4c8a-ba56-fa41103d4e17 | Open vSwitch agent   | ardana-cp1-comp0001-mgmt |                   | :-)   | True           | neutron-openvswitch-agent |
...
+--------------------------------------+----------------------+--------------------------+-------------------+-------+----------------+---------------------------+

$ openstack network agent set --disable 810f0ae7-63aa-4ee3-952d-69837b4b2fe4
Updated agent: 810f0ae7-63aa-4ee3-952d-69837b4b2fe4

$ openstack network agent set --disable 89ac17ba-2f43-428a-98fa-b3698646543d
Updated agent: 89ac17ba-2f43-428a-98fa-b3698646543d</pre></div><div id="id-1.5.12.6.17.3.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      Only L3 and Metadata agents were disabled.
     </p></div></li><li class="step "><p>
    Once L3 and metadata neutron agents are stopped, follow steps 1 through 7
    in the document <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 12 “Alternative Configurations”, Section 12.2 “Configuring <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> without DVR”</span> and then run the
    <code class="literal">neutron-reconfigure.yml</code> playbook:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect2" id="dpdk"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">OVS-DPDK Support</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#dpdk">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-dpdk_ovs.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_ovs.xml</li><li><span class="ds-label">ID: </span>dpdk</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses a version of Open vSwitch (OVS) that is built with the
  Data Plane Development Kit (DPDK) and includes a QEMU hypervisor which
  supports vhost-user.
 </p><p>
  The OVS-DPDK package modifes the OVS fast path, which is normally performed
  in kernel space, and allows it to run in userspace so there is no context
  switch to the kernel for processing network packets.
 </p><p>
  The EAL component of DPDK supports mapping the Network Interface Card (NIC)
  registers directly into userspace. The DPDK provides a Poll Mode Driver
  (PMD) that can access the NIC hardware from userspace and uses polling
  instead of interrupts to avoid the user to kernel transition.
 </p><p>
  The PMD maps the shared address space of the VM that is provided by the
  vhost-user capability of QEMU. The vhost-user mode causes neutron to create
  a Unix domain socket that allows communication between the PMD and QEMU. The
  PMD uses this in order to acquire the file descriptors to the pre-allocated
  VM memory. This allows the PMD to directly access the VM memory space and
  perform a fast zero-copy of network packets directly into and out of the VMs
  virtio_net vring.
 </p><p>
  This yields performance improvements in the time it takes to process network
  packets.
 </p><div class="sect3" id="id-1.5.12.6.18.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.14.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Usage considerations</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.18.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-dpdk_ovs.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_ovs.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The target for a DPDK Open vSwitch is VM performance and VMs only run on
   compute nodes so the following considerations are compute node specific.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     In order to use DPDK with VMs, <code class="literal">hugepages</code> must be
     enabled; please see <a class="xref" href="ops-managing-networking.html#hugepages" title="10.4.14.3. Configuring Hugepages for DPDK in Networks">Section 10.4.14.3, “Configuring Hugepages for DPDK in Networks”</a>. The memory to be used
     must be allocated at boot time so you must know beforehand how many VMs
     will be scheduled on a node. Also, for NUMA considerations, you want those
     hugepages on the same NUMA node as the NIC.  A VM maps its entire address
     space into a hugepage.
    </p></li><li class="listitem "><p>
     For maximum performance you must reserve logical cores for DPDK poll mode
     driver (PMD) usage and for hypervisor (QEMU) usage. This keeps the Linux
     kernel from scheduling processes on those cores. The PMD threads will go
     to 100% cpu utilization since it uses polling of the hardware instead of
     interrupts. There will be at least 2 cores dedicated to PMD threads. Each
     VM will have a core dedicated to it although for less performance VMs can
     share cores.
    </p></li><li class="listitem "><p>
     VMs can use the virtio_net or the virtio_pmd drivers. There is also a PMD
     for an emulated e1000.
    </p></li><li class="listitem "><p>
     Only VMs that use hugepages can be sucessfully launched on a DPDK-enabled
     NIC. If there is a need to support both DPDK and non-DPDK-based VMs, an
     additional port managed by the Linux kernel must exist.
    </p></li></ol></div></div><div class="sect3" id="id-1.5.12.6.18.8"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.14.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For more information</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.18.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-dpdk_ovs.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_ovs.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   See the following topics for more information:
  </p></div><div class="sect3" id="hugepages"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.14.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Hugepages for DPDK in Networks</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#hugepages">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-hugepages.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-hugepages.xml</li><li><span class="ds-label">ID: </span>hugepages</li></ul></div></div></div></div><p>
  To take advantage of DPDK and its network
  performance enhancements, enable hugepages first.
 </p><p>
  With hugepages, physical RAM is reserved at boot time and dedicated to a
  virtual machine. Only that virtual machine and Open vSwitch can use this
  specifically allocated RAM. The host OS cannot access it. This memory is
  contiguous, and because of its larger size, reduces the number of entries in
  the memory map and number of times it must be read.
 </p><p>
  The hugepage reservation is made in <code class="literal">/etc/default/grub</code>,
  but this is handled by the Cloud Lifecycle Manager.
 </p><p>
  In addition to hugepages, to use DPDK, CPU isolation is required. This is
  achieved with the 'isolcups' command in
  <code class="literal">/etc/default/grub</code>, but this is also managed by the
  Cloud Lifecycle Manager using a new input model file.
 </p><p>
  The two new input model files introduced with this release to help you
  configure the necessary settings and persist them are:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    memory_models.yml (for hugepages)
   </p></li><li class="listitem "><p>
    cpu_models.yml (for CPU isolation)
   </p></li></ul></div><div class="sect4" id="id-1.5.12.6.18.9.8"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.14.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">memory_models.yml</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.18.9.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-hugepages.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-hugepages.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   In this file you set your huge page size along with the number of such
   huge-page allocations.
  </p><div class="verbatim-wrap"><pre class="screen"> ---
  product:
    version: 2

  memory-models:
    - name: COMPUTE-MEMORY-NUMA
      default-huge-page-size: 1G
      huge-pages:
        - size: 1G
          count: 24
          numa-node: 0
        - size: 1G
          count: 24
          numa-node: 1
        - size: 1G
          count: 48</pre></div></div><div class="sect4" id="id-1.5.12.6.18.9.9"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.14.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">cpu_models.yml</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.18.9.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-hugepages.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-hugepages.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  cpu-models:

    - name: COMPUTE-CPU
      assignments:
       - components:
           - nova-compute-kvm
         cpu:
           - processor-ids: 3-5,12-17
             role: vm

       - components:
           - openvswitch
         cpu:
           - processor-ids: 0
             role: eal
           - processor-ids: 1-2
             role: pmd</pre></div></div><div class="sect4" id="id-1.5.12.6.18.9.10"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.14.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">NUMA memory allocation</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.18.9.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-hugepages.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-hugepages.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   As mentioned above, the memory used for hugepages is locked down at boot
   time by an entry in <code class="literal">/etc/default/grub</code>. As an admin, you
   can specify in the input model how to arrange this memory on NUMA nodes. It
   can be spread across NUMA nodes or you can specify where you want it. For
   example, if you have only one NIC, you would probably want all the hugepages
   memory to be on the NUMA node closest to that NIC.


  </p><p>
   If you do not specify the <code class="literal">numa-node</code> settings in the
   <code class="literal">memory_models.yml</code> input model file and use only the last
   entry indicating "size: 1G" and "count: 48" then this memory is spread
   evenly across all NUMA nodes.
  </p><p>
   Also note that the hugepage service runs once at boot time and then goes to
   an inactive state so you should not expect to see it running. If you decide
   to make changes to the NUMA memory allocation, you will need to reboot the
   compute node for the changes to take effect.
  </p></div></div><div class="sect3" id="dpdk-setup"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.14.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DPDK Setup for Networking</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#dpdk-setup">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-dpdk_setup.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_setup.xml</li><li><span class="ds-label">ID: </span>dpdk-setup</li></ul></div></div></div></div><div class="sect4" id="id-1.5.12.6.18.10.2"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.14.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Hardware requirements</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.18.10.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-dpdk_setup.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_setup.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Intel-based compute node. DPDK is not available on AMD-based systems.
    </p></li><li class="listitem "><p>
     The following BIOS settings must be enabled for DL360 Gen9:
    </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
       Virtualization Technology
      </p></li><li class="listitem "><p>
       Intel(R) VT-d
      </p></li><li class="listitem "><p>
       PCI-PT (Also see <a class="xref" href="ops-managing-networking.html#pcipt-gen9" title="10.4.15.14. Enabling PCI-PT on HPE DL360 Gen 9 Servers">Section 10.4.15.14, “Enabling PCI-PT on HPE DL360 Gen 9 Servers”</a>)
      </p></li></ol></div></li><li class="listitem "><p>
     Need adequate host memory to allow for hugepages. The examples below use
     1G hugepages for the VMs
    </p></li></ul></div></div><div class="sect4" id="id-1.5.12.6.18.10.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.14.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.18.10.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-dpdk_setup.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_setup.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     DPDK is supported on SLES only.
    </p></li><li class="listitem "><p>
     Applies to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 only.
    </p></li><li class="listitem "><p>
     Tenant network can be untagged vlan or untagged vxlan
    </p></li><li class="listitem "><p>
     DPDK port names must be of the form 'dpdk&lt;portid&gt;' where port id is
     sequential and starts at 0
    </p></li><li class="listitem "><p>
     No support for converting DPDK ports to non DPDK ports without rebooting
     compute node.
    </p></li><li class="listitem "><p>
     No security group support, need userspace conntrack.
    </p></li><li class="listitem "><p>
     No jumbo frame support.
    </p></li></ul></div></div><div class="sect4" id="id-1.5.12.6.18.10.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.14.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setup instructions</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.18.10.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-dpdk_setup.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_setup.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   These setup instructions and example model are for a three-host system.
   There is one controller with Cloud Lifecycle Manager in cloud control plane and
   two compute hosts.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     After initial run of site.yml all compute nodes must be rebooted to pick
     up changes in grub for hugepages and isolcpus
    </p></li><li class="listitem "><p>
     Changes to non-uniform memory access (NUMA) memory, isolcpu, or network
     devices must be followed by a reboot of compute nodes
    </p></li><li class="listitem "><p>
     Run sudo reboot to pick up libvirt change and hugepage/isocpus grub
     changes
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo reboot</pre></div></li><li class="listitem "><p>
     Use the bash script below to configure nova aggregates, neutron networks,
     a new flavor, etc. And then it will spin up two VMs.
    </p></li></ol></div><p>
   <span class="bold"><strong>VM spin-up instructions</strong></span>
  </p><p>
   Before running the spin up script you need to get a copy of the cirros image
   to your Cloud Lifecycle Manager node. You can manually scp a copy of the cirros
   image to the system. You can copy it locallly with wget like so
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img</pre></div><p>
   Save the following shell script in the home directory and run it. This
   should spin up two VMs, one on each compute node.
  </p><div id="id-1.5.12.6.18.10.4.8" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
    Make sure to change all network-specific information in the script to match
    your environment.
   </p></div><div class="verbatim-wrap"><pre class="screen">#!/usr/bin/env bash

source service.osrc

######## register glance image
openstack image create --name='cirros' --container-format=bare --disk-format=qcow2 &lt; ~/cirros-0.3.4-x86_64-disk.img

####### create nova aggregate and flavor for dpdk

MI_NAME=dpdk

openstack aggregate create $MI_NAME nova
openstack aggregate add host $MI_NAME openstack-cp-comp0001-mgmt
openstack aggregate add host $MI_NAME openstack-cp-comp0002-mgmt
openstack aggregate set $MI_NAME pinned=true

openstack flavor create $MI_NAME 6 1024 20 1
openstack flavor set $MI_NAME set hw:cpu_policy=dedicated
openstack flavor set $MI_NAME set aggregate_instance_extra_specs:pinned=true
openstack flavor set $MI_NAME set hw:mem_page_size=1048576

######## sec groups NOTE: no sec groups supported on DPDK.  This is in case we do non-DPDK compute hosts.
nova secgroup-add-rule default tcp 22 22 0.0.0.0/0
nova secgroup-add-rule default icmp -1 -1 0.0.0.0/0

########  nova keys
openstack keypair create mykey &gt;mykey.pem
chmod 400 mykey.pem

######## create neutron external network
openstack network create ext-net --router:external --os-endpoint-type internalURL
openstack subnet create ext-net 10.231.0.0/19 --gateway_ip=10.231.0.1  --ip-version=4 --disable-dhcp  --allocation-pool start=10.231.17.0,end=10.231.17.255

########  neutron network
openstack network create mynet1
openstack subnet create mynet1 10.1.1.0/24 --name mysubnet1
openstack router create myrouter1
openstack router add subnet myrouter1 mysubnet1
openstack router set myrouter1 ext-net
export MYNET=$(openstack network list | grep mynet | awk '{print $2}')

######## spin up 2 VMs, 1 on each compute
openstack server create --image cirros --nic net-id=${MYNET} --key-name mykey --flavor dpdk --availability-zone nova:openstack-cp-comp0001-mgmt vm1
openstack server create --image cirros --nic net-id=${MYNET} --key-name mykey --flavor dpdk --availability-zone nova:openstack-cp-comp0002-mgmt vm2

######## create floating ip and attach to instance
export MYFIP1=$(nova floating-ip-create|grep ext-net|awk '{print $4}')
nova add-floating-ip vm1 ${MYFIP1}

export MYFIP2=$(nova floating-ip-create|grep ext-net|awk '{print $4}')
nova add-floating-ip vm2 ${MYFIP2}

openstack server list</pre></div></div></div><div class="sect3" id="dpdk-config"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.14.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DPDK Configurations</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#dpdk-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-dpdk_configurations.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_configurations.xml</li><li><span class="ds-label">ID: </span>dpdk-config</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="ops-managing-networking.html#base-config" title="10.4.14.5.1. Base configuration">Section 10.4.14.5.1, “Base configuration”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="ops-managing-networking.html#common-perf" title="10.4.14.5.2. Performance considerations common to all NIC types">Section 10.4.14.5.2, “Performance considerations common to all NIC types”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="ops-managing-networking.html#multiqueue-config" title="10.4.14.5.3. Multiqueue configuration">Section 10.4.14.5.3, “Multiqueue configuration”</a>
   </p></li></ul></div><div class="sect4" id="base-config"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.14.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Base configuration</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#base-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-dpdk_configurations.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_configurations.xml</li><li><span class="ds-label">ID: </span>base-config</li></ul></div></div></div></div><p>
   The following is specific to DL360 Gen9 and BIOS configuration as detailed
   in <a class="xref" href="ops-managing-networking.html#dpdk-setup" title="10.4.14.4. DPDK Setup for Networking">Section 10.4.14.4, “DPDK Setup for Networking”</a>.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     EAL cores - 1, isolate: False in cpu-models
    </p></li><li class="listitem "><p>
     PMD cores - 1 per NIC port
    </p></li><li class="listitem "><p>
     Hugepages - 1G per PMD thread
    </p></li><li class="listitem "><p>
     Memory channels - 4
    </p></li><li class="listitem "><p>
     Global rx queues - based on needs
    </p></li></ul></div></div><div class="sect4" id="common-perf"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.14.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Performance considerations common to all NIC types</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#common-perf">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-dpdk_configurations.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_configurations.xml</li><li><span class="ds-label">ID: </span>common-perf</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Compute host core frequency</strong></span>
  </p><p>
   Host CPUs should be running at maximum performance. The following is a
   script to set that. Note that in this case there are 24 cores. This needs to
   be modified to fit your environment. For a HP DL360 Gen9, the BIOS should be
   configured to use "OS Control Mode" which can be found on the iLO Power
   Settings page.
  </p><div class="verbatim-wrap"><pre class="screen">for i in `seq 0 23`; do echo "performance" &gt; /sys/devices/system/cpu/cpu$i/cpufreq/scaling_governor; done</pre></div><p>
   <span class="bold"><strong>IO non-posted prefetch</strong></span>
  </p><p>
   The DL360 Gen9 should have the IO non-posted prefetch disabled. Experimental
   evidence shows this yields an additional 6-8% performance boost.
  </p></div><div class="sect4" id="multiqueue-config"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.14.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Multiqueue configuration</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#multiqueue-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-dpdk_configurations.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_configurations.xml</li><li><span class="ds-label">ID: </span>multiqueue-config</li></ul></div></div></div></div><p>
   In order to use multiqueue, a property must be applied to the glance image
   and a setting inside the resulting VM must be applied. In this example we
   create a 4 vCPU flavor for DPDK using 1G hugepages.
  </p><div class="verbatim-wrap"><pre class="screen">MI_NAME=dpdk

openstack aggregate create $MI_NAME nova
openstack aggregate add host $MI_NAME openstack-cp-comp0001-mgmt
openstack aggregate add host $MI_NAME openstack-cp-comp0002-mgmt
openstack aggregate set $MI_NAME pinned=true

openstack flavor create $MI_NAME 6 1024 20 4
openstack flavor set $MI_NAME set hw:cpu_policy=dedicated
openstack flavor set $MI_NAME set aggregate_instance_extra_specs:pinned=true
openstack flavor set $MI_NAME set hw:mem_page_size=1048576</pre></div><p>
   And set the hw_vif_multiqueue_enabled property on the glance image
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack image set --property hw_vif_multiqueue_enabled=true <em class="replaceable ">IMAGE UUID</em></pre></div><p>
   Once the VM is booted using the flavor above, inside the VM, choose the
   number of combined rx and tx queues to be equal to the number of vCPUs
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo ethtool -L eth0 combined 4</pre></div><p>
   On the hypervisor you can verify that multiqueue has been properly set by
   looking at the qemu process
  </p><div class="verbatim-wrap"><pre class="screen">-netdev type=vhost-user,id=hostnet0,chardev=charnet0,queues=4 -device virtio-net-pci,mq=on,vectors=10,</pre></div><p>
   Here you can see that 'mq=on' and vectors=10. The formula for vectors is
   2*num_queues+2
  </p></div></div><div class="sect3" id="dpdk-troubleshooting"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.14.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting DPDK</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#dpdk-troubleshooting">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-ts_dpdk.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-ts_dpdk.xml</li><li><span class="ds-label">ID: </span>dpdk-troubleshooting</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="ops-managing-networking.html#hardware" title="10.4.14.6.1. Hardware configuration">Section 10.4.14.6.1, “Hardware configuration”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="ops-managing-networking.html#system" title="10.4.14.6.2. System configuration">Section 10.4.14.6.2, “System configuration”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="ops-managing-networking.html#inputModel" title="10.4.14.6.3. Input model configuration">Section 10.4.14.6.3, “Input model configuration”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="ops-managing-networking.html#reboot" title="10.4.14.6.4. Reboot requirements">Section 10.4.14.6.4, “Reboot requirements”</a>
   </p></li></ul></div><div class="sect4" id="hardware"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.14.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Hardware configuration</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#hardware">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-ts_dpdk.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-ts_dpdk.xml</li><li><span class="ds-label">ID: </span>hardware</li></ul></div></div></div></div><p>
   Because there are several variations of hardware, it is up to you to verify
   that the hardware is configured properly.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Only Intel based compute nodes are supported. There is no DPDK available
     for AMD-based CPUs.
    </p></li><li class="listitem "><p>
     PCI-PT must be enabled for the NIC that will be used with DPDK.
    </p></li><li class="listitem "><p>
     When using Intel Niantic and the igb_uio driver, the VT-d must be enabled
     in the BIOS.
    </p></li><li class="listitem "><p>
     For DL360 Gen9 systems, the BIOS shared-memory
     <a class="xref" href="ops-managing-networking.html#pcipt-gen9" title="10.4.15.14. Enabling PCI-PT on HPE DL360 Gen 9 Servers">Section 10.4.15.14, “Enabling PCI-PT on HPE DL360 Gen 9 Servers”</a>.
    </p></li><li class="listitem "><p>
     Adequate memory must be available for <a class="xref" href="ops-managing-networking.html#hugepages" title="10.4.14.3. Configuring Hugepages for DPDK in Networks">Section 10.4.14.3, “Configuring Hugepages for DPDK in Networks”</a> usage.
    </p></li><li class="listitem "><p>
     Hyper-threading can be enabled but is not required for base functionality.
    </p></li><li class="listitem "><p>
     Determine the PCI slot that the DPDK NIC(s) are installed in to
     determine the associated NUMA node.
    </p></li><li class="listitem "><p>
     Only the Intel Haswell, Broadwell, and Skylake microarchitectures are
     supported.
     Intel Sandy Bridge is not supported.
    </p></li></ul></div></div><div class="sect4" id="system"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.14.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">System configuration</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#system">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-ts_dpdk.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-ts_dpdk.xml</li><li><span class="ds-label">ID: </span>system</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Only SLES12-SP4 compute nodes are supported.
    </p></li><li class="listitem "><p>
     If a NIC port is used with PCI-PT, SRIOV-only, or PCI-PT+SRIOV, then it
     cannot be used with DPDK. They are mutually exclusive. This is because DPDK
     depends on an OvS bridge which does not exist if you use any combination of
     PCI-PT and SRIOV. You can use DPDK, SRIOV-only, and PCI-PT on difference
     interfaces of the same server.
    </p></li><li class="listitem "><p>
     There is an association between the PCI slot for the NIC and a NUMA node.
     Make sure to use logical CPU cores that are on the NUMA node associated to
     the NIC. Use the following to determine which CPUs are on which NUMA node.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>lscpu

Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                48
On-line CPU(s) list:   0-47
Thread(s) per core:    2
Core(s) per socket:    12
Socket(s):             2
NUMA node(s):          2
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 63
Model name:            Intel(R) Xeon(R) CPU E5-2650L v3 @ 1.80GHz
Stepping:              2
CPU MHz:               1200.000
CPU max MHz:           1800.0000
CPU min MHz:           1200.0000
BogoMIPS:              3597.06
Virtualization:        VT-x
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              30720K
NUMA node0 CPU(s):     0-11,24-35
NUMA node1 CPU(s):     12-23,36-47</pre></div></li></ul></div></div><div class="sect4" id="inputModel"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.14.6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Input model configuration</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#inputModel">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-ts_dpdk.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-ts_dpdk.xml</li><li><span class="ds-label">ID: </span>inputModel</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     If you do not specify a driver for a DPDK device, the igb_uio will be
     selected as default.
    </p></li><li class="listitem "><p>
     DPDK devices must be named <code class="literal">dpdk&lt;port-id&gt;</code> where
     the port-id starts at 0 and increments sequentially.
    </p></li><li class="listitem "><p>
     Tenant networks supported are untagged VXLAN and VLAN.
    </p></li><li class="listitem "><p>
     Jumbo Frames MTU is not supported with DPDK.
    </p></li><li class="listitem "><p>
     Sample VXLAN model
    </p></li><li class="listitem "><p>
     Sample VLAN model
    </p></li></ul></div></div><div class="sect4" id="reboot"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.14.6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reboot requirements</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#reboot">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-ts_dpdk.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-ts_dpdk.xml</li><li><span class="ds-label">ID: </span>reboot</li></ul></div></div></div></div><p>
   A reboot of a compute node must be performed when an input model change
   causes the following:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     After the initial <code class="filename">site.yml</code> play on a new <span class="productname">OpenStack</span>
     environment
    </p></li><li class="listitem "><p>
     Changes to an existing <span class="productname">OpenStack</span> environment that modify the
     <code class="literal">/etc/default/grub</code> file, such as
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       hugepage allocations
      </p></li><li class="listitem "><p>
       CPU isolation
      </p></li><li class="listitem "><p>
       iommu changes
      </p></li></ul></div></li><li class="listitem "><p>
     Changes to a NIC port usage type, such as
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       moving from DPDK to any combination of PCI-PT and SRIOV
      </p></li><li class="listitem "><p>
       moving from DPDK to kernel based eth driver
      </p></li></ul></div></li></ol></div></div></div></div><div class="sect2" id="sr-iov"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.15 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SR-IOV and PCI Passthrough Support</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#sr-iov">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span>sr-iov</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> supports both single-root I/O virtualization (SR-IOV) and PCI
  passthrough (PCIPT). Both technologies provide for better network
  performance.
 </p><p>
  This improves network I/O, decreases latency, and reduces processor overhead.
 </p><div class="sect3" id="id-1.5.12.6.19.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SR-IOV</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.19.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   A PCI-SIG Single Root I/O Virtualization and Sharing (SR-IOV) Ethernet
   interface is a physical PCI Ethernet NIC that implements hardware-based
   virtualization mechanisms to expose multiple virtual network interfaces that
   can be used by one or more virtual machines simultaneously. With SR-IOV
   based NICs, the traditional virtual bridge is no longer required. Each
   SR-IOV port is associated with a virtual function (VF).
  </p><p>
   When compared with a PCI Passthtrough Ethernet interface, an SR-IOV Ethernet
   interface:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Provides benefits similar to those of a PCI Passthtrough Ethernet
     interface, including lower latency packet processing.
    </p></li><li class="listitem "><p>
     Scales up more easily in a virtualized environment by providing multiple
     VFs that can be attached to multiple virtual machine interfaces.
    </p></li><li class="listitem "><p>
     Shares the same limitations, including the lack of support for LAG, QoS,
     ACL, and live migration.
    </p></li><li class="listitem "><p>
     Has the same requirements regarding the VLAN configuration of the access
     switches.
    </p></li></ul></div><p>
   The process for configuring SR-IOV includes creating a VLAN provider network
   and subnet, then attaching VMs to that network.
  </p><p>
   With SR-IOV based NICs, the traditional virtual bridge is no longer
   required. Each SR-IOV port is associated with a virtual function (VF)
  </p></div><div class="sect3" id="id-1.5.12.6.19.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">PCI passthrough Ethernet interfaces</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.19.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   A passthrough Ethernet interface is a physical PCI Ethernet NIC on a compute
   node to which a virtual machine is granted direct access. PCI passthrough
   allows a VM to have direct access to the hardware without being brokered by
   the hypervisor. This minimizes packet processing delays but at the same time
   demands special operational considerations. For all purposes, a PCI
   passthrough interface behaves as if it were physically attached to the
   virtual machine. Therefore any potential throughput limitations coming from
   the virtualized environment, such as the ones introduced by internal copying
   of data buffers, are eliminated. However, by bypassing the virtualized
   environment, the use of PCI passthrough Ethernet devices introduces several
   restrictions that must be taken into consideration. They include:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     no support for LAG, QoS, ACL, or host interface monitoring
    </p></li><li class="listitem "><p>
     no support for live migration
    </p></li><li class="listitem "><p>
     no access to the compute node's OVS switch
    </p></li></ul></div><p>
   A passthrough interface bypasses the compute node's OVS switch completely,
   and is attached instead directly to the provider network's access switch.
   Therefore, proper routing of traffic to connect the passthrough interface to
   a particular tenant network depends entirely on the VLAN tagging options
   configured on both the passthrough interface and the access port on the
   switch (TOR).
  </p><p>
   The access switch routes incoming traffic based on a VLAN ID, which
   ultimately determines the tenant network to which the traffic belongs. The
   VLAN ID is either explicit, as found in incoming tagged packets, or
   implicit, as defined by the access port's default VLAN ID when the incoming
   packets are untagged. In both cases the access switch must be configured to
   process the proper VLAN ID, which therefore has to be known in advance
  </p></div><div class="sect3" id="pci-passthrough"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Leveraging PCI Passthrough</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#pci-passthrough">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span>pci-passthrough</li></ul></div></div></div></div><p>
   Two parts are necessary to leverage PCI passthrough on a SUSE <span class="productname">OpenStack</span> Cloud
   9 Compute Node: preparing the Compute Node, preparing nova and
   glance.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     <span class="bold"><strong>Preparing the Compute Node</strong></span>
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       There should be no kernel drivers or binaries with direct access to the
       PCI device. If there are kernel modules, they should be blacklisted.
      </p><p>
       For example, it is common to have a <code class="literal">nouveau</code> driver
       from when the node was installed. This driver is a graphics driver for
       Nvidia-based GPUs. It must be blacklisted as shown in this example.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>echo 'blacklist nouveau' &gt;&gt; /etc/modprobe.d/nouveau-default.conf</pre></div><p>
       The file location and its contents are important; the name of the file
       is your choice. Other drivers can be blacklisted in the same manner,
       possibly including Nvidia drivers.
      </p></li><li class="step "><p>
       On the host, <code class="literal">iommu_groups</code> is necessary and may
       already be enabled. To check if IOMMU is enabled:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>virt-host-validate
.....
QEMU: Checking if IOMMU is enabled by kernel
: WARN (IOMMU appears to be disabled in kernel. Add intel_iommu=on to kernel cmdline arguments)
.....</pre></div><p>
       To modify the kernel cmdline as suggested in the warning, edit the file
       <code class="filename">/etc/default/grub</code> and append
       <code class="literal">intel_iommu=on</code> to the
       <code class="literal">GRUB_CMDLINE_LINUX_DEFAULT</code> variable. Then run
       <code class="literal">update-bootloader</code>.
      </p><p>
       A reboot will be required for <code class="literal">iommu_groups</code> to be
       enabled.
      </p></li><li class="step "><p>
       After the reboot, check that IOMMU is enabled:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>virt-host-validate
.....
QEMU: Checking if IOMMU is enabled by kernel
: PASS
.....</pre></div></li><li class="step "><p>
       Confirm IOMMU groups are available by finding the group associated with
       your PCI device (for example Nvidia GPU):
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>lspci -nn | grep -i nvidia
08:00.0 VGA compatible controller [0300]: NVIDIA Corporation GT218 [NVS 300] [10de:10d8] (rev a2)
08:00.1 Audio device [0403]: NVIDIA Corporation High Definition Audio Controller [10de:0be3] (rev a1)</pre></div><p>
       In this example, <code class="literal">08:00.0</code> and
       <code class="literal">08:00.1</code> are addresses of the PCI device. The vendorID
       is <code class="literal">10de</code>. The productIDs are <code class="literal">10d8</code>
       and <code class="literal">0be3</code>.
      </p></li><li class="step "><p>
       Confirm that the devices are available for passthrough:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls -ld /sys/kernel/iommu_groups/*/devices/*08:00.?/
drwxr-xr-x 3 root root 0 Feb 14 13:05 /sys/kernel/iommu_groups/20/devices/0000:08:00.0/
drwxr-xr-x 3 root root 0 Feb 19 16:09 /sys/kernel/iommu_groups/20/devices/0000:08:00.1/</pre></div><div id="id-1.5.12.6.19.6.3.1.2.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
        With PCI passthrough, only an entire IOMMU group can be passed. Parts
        of the group cannot be passed. In this example, the IOMMU group is
        <code class="literal">20</code>.
       </p></div></li></ol></div></div></li><li class="listitem "><p>
     <span class="bold"><strong>Preparing nova and glance for
     passthrough</strong></span>
    </p><p>
     Information about configuring nova and glance is available in the
     documentation at
     <a class="link" href="https://docs.openstack.org/nova/rocky/admin/pci-passthrough.html" target="_blank">https://docs.openstack.org/nova/rocky/admin/pci-passthrough.html</a>.
     Both <code class="literal">nova-compute</code> and <code class="literal">nova-scheduler</code>
     must be configured.
    </p></li></ol></div></div><div class="sect3" id="id-1.5.12.6.19.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Supported Intel 82599 Devices</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.19.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="table" id="intel-82599-table"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 10.1: </span><span class="name">Intel 82599 devices supported with SRIOV and PCIPT </span><a title="Permalink" class="permalink" href="ops-managing-networking.html#intel-82599-table">#</a></h6></div><div class="table-contents"><table class="table" summary="Intel 82599 devices supported with SRIOV and PCIPT" border="1"><colgroup><col align="center" class="c1" /><col align="center" class="c2" /><col align="center" class="c3" /></colgroup><thead><tr><th align="center">Vendor</th><th align="center">Device</th><th align="center">Title</th></tr></thead><tbody><tr><td align="center">Intel Corporation</td><td align="center">10f8</td><td align="center">82599 10 Gigabit Dual Port Backplane Connection</td></tr><tr><td align="center">Intel Corporation</td><td align="center">10f9</td><td align="center">82599 10 Gigabit Dual Port Network Connection</td></tr><tr><td align="center">Intel Corporation</td><td align="center">10fb</td><td align="center">82599ES 10-Gigabit SFI/SFP+ Network Connection</td></tr><tr><td align="center">Intel Corporation</td><td align="center">10fc</td><td align="center">82599 10 Gigabit Dual Port Network Connection</td></tr></tbody></table></div></div></div><div class="sect3" id="id-1.5.12.6.19.8"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SRIOV PCIPT configuration</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.19.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If you plan to take advantage of SR-IOV support in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, plan in
   advance to meet the following requirements:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Use one of the supported NIC cards:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       HP Ethernet 10Gb 2-port 560FLR-SFP+ Adapter (Intel Niantic). Product
       part number: 665243-B21 -- Same part number for the following card
       options:
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         FlexLOM card
        </p></li><li class="listitem "><p>
         PCI slot adapter card
        </p></li></ul></div></li></ul></div></li><li class="listitem "><p>
     Identify the NIC ports to be used for PCI Passthrough devices and SRIOV
     devices from each compute node
    </p></li><li class="listitem "><p>
     Ensure that:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       SRIOV is enabled in the BIOS
      </p></li><li class="listitem "><p>
       HP Shared memory is disabled in the BIOS on the compute nodes.
      </p></li><li class="listitem "><p>
       The Intel boot agent is disabled on the compute
       (<a class="xref" href="ops-managing-networking.html#bootutil" title="10.4.15.11. Intel bootutils">Section 10.4.15.11, “Intel bootutils”</a> can be used to perform this)
      </p></li></ul></div><div id="id-1.5.12.6.19.8.3.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      Because of Intel driver limitations, you cannot use a NIC port as an
      SRIOV NIC as well as a physical NIC. Using the physical function to carry
      the normal tenant traffic through the OVS bridge at the same time as
      assigning the VFs from the same NIC device as passthrough to the guest VM
      is not supported.
     </p></div></li></ol></div><p>
   If the above prerequisites are met, then SR-IOV or PCIPT can be reconfigured
   at any time. There is no need to do it at install time.
  </p></div><div class="sect3" id="id-1.5.12.6.19.9"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment use cases</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.19.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following are typical use cases that should cover your particular needs:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     A device on the host needs to be enabled for both PCI-passthrough and
     PCI-SRIOV during deployment. At run time nova decides whether to use
     physical functions or virtual function depending on vnic_type of the port
     used for booting the VM.
    </p></li><li class="listitem "><p>
     A device on the host needs to be configured only for PCI-passthrough.
    </p></li><li class="listitem "><p>
     A device on the host needs to be configured only for PCI-SRIOV virtual
     functions.
    </p></li></ol></div></div><div class="sect3" id="id-1.5.12.6.19.10"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Input model updates</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.19.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 provides various options for the user to configure the
   network for tenant VMs. These options have been enhanced to support SRIOV
   and PCIPT.
  </p><p>
   the Cloud Lifecycle Manager input model changes to support SRIOV and PCIPT are as follows. If
   you were familiar with the configuration settings previously, you will
   notice these changes.
  </p><p>
   <span class="bold"><strong>net_interfaces.yml:</strong></span> This file defines the
   interface details of the nodes. In it, the following fields have been added
   under the compute node interface section:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /></colgroup><thead><tr><th>Key</th><th>Value</th></tr></thead><tbody><tr><td>sriov_only: </td><td>
       <p>
        Indicates that only SR-IOV be enabled on the interface. This should be
        set to true if you want to dedicate the NIC interface to support only
        SR-IOV functionality.
       </p>
      </td></tr><tr><td>pci-pt: </td><td>
       <p>
        When this value is set to true, it indicates that PCIPT should be
        enabled on the interface.
       </p>
      </td></tr><tr><td>vf-count: </td><td>
       <p>
        Indicates the number of VFs to be configured on a given interface.
       </p>
      </td></tr></tbody></table></div><p>
   In <code class="filename">control_plane.yml</code> under <code class="literal">Compute
   resource</code>, <code class="literal">neutron-sriov-nic-agent</code> has been
   added as a service component.
  </p><p>
   under resources:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /></colgroup><thead><tr><th>Key</th><th>Value</th></tr></thead><tbody><tr><td>name:</td><td> Compute</td></tr><tr><td>resource-prefix:</td><td> Comp</td></tr><tr><td>server-role:</td><td>COMPUTE-ROLE</td></tr><tr><td>allocation-policy:</td><td> Any</td></tr><tr><td>min-count:</td><td> 0</td></tr><tr><td>service-components:</td><td>ntp-client</td></tr><tr><td> </td><td>nova-compute</td></tr><tr><td> </td><td>nova-compute-kvm</td></tr><tr><td> </td><td>neutron-l3-agent</td></tr><tr><td> </td><td>neutron-metadata-agent</td></tr><tr><td> </td><td>neutron-openvswitch-agent</td></tr><tr><td> </td><td>- neutron-sriov-nic-agent*</td></tr></tbody></table></div><p>
   <span class="bold"><strong>nic_device_data.yml:</strong></span> This is the new file
   added with this release to support SRIOV and PCIPT configuration details. It
   contains information about the specifics of a nic, and is found at
   <code class="literal">/usr/share/ardana/input-model/2.0/services/osconfig/nic_device_data.yml</code>.
   The fields in this file are as follows.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     <span class="bold"><strong>nic-device-types:</strong></span> The nic-device-types
     section contains the following key-value pairs:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value</th></tr></thead><tbody><tr><td>name:</td><td>
         <p>
          The name of the nic-device-types that will be referenced in
          nic_mappings.yml
         </p>
        </td></tr><tr><td>family:</td><td>
         <p>
          The name of the nic-device-families to be used with this
          nic_device_type
         </p>
        </td></tr><tr><td>device_id:</td><td>
         <p>
          Device ID as specified by the vendor for the particular NIC
         </p>
        </td></tr><tr><td>type:</td><td>
         <p>
          The value of this field can be <code class="literal">simple-port</code> or
          <code class="literal">multi-port</code>. If a single bus address is assigned to
          more than one nic, The value will be
          <code class="literal">multi-port</code>. If there is a one-to-one mapping
          between bus address and the nic, it will be
          <code class="literal">simple-port</code>.
         </p>
        </td></tr></tbody></table></div></li><li class="listitem "><p>
     <span class="bold"><strong>nic-device-families:</strong></span> The
     nic-device-families section contains the following key-value pairs:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value</th></tr></thead><tbody><tr><td>name:</td><td>
         <p>
          The name of the device family that can be used for reference in
          nic-device-types.
         </p>
        </td></tr><tr><td>vendor-id: </td><td>
         <p>
          Vendor ID of the NIC
         </p>
        </td></tr><tr><td>config-script:</td><td>
         <p>
          A script file used to create the virtual functions (VF) on the
          Compute node.
         </p>
        </td></tr><tr><td>driver:</td><td>
         <p>
          Indicates the NIC driver that needs to be used.
         </p>
        </td></tr><tr><td>vf-count-type:</td><td>
         <p>
          This value can be either <code class="literal">port</code> or
          <code class="literal">driver</code>.
         </p>
        </td></tr><tr><td>“port”:</td><td>
         <p>
          Indicates that the device supports per-port virtual function (VF)
          counts.
         </p>
        </td></tr><tr><td>“driver:”</td><td>
         <p>
          Indicates that all ports using the same driver will be configured
          with the same number of VFs, whether or not the interface model
          specifies a vf-count attribute for the port. If two or more ports
          specify different vf-count values, the config processor errors out.
         </p>
        </td></tr><tr><td>Max-vf-count:</td><td>
         <p>
          This field indicates the maximum VFs that can be configured on an
          interface as defined by the vendor.
         </p>
        </td></tr></tbody></table></div></li></ol></div><p>
   <span class="bold"><strong>control_plane.yml:</strong></span> This file provides the
   information about the services to be run on a particular node. To support
   SR-IOV on a particular compute node, you must run
   <code class="literal">neutron-sriov-nic-agent</code> on that node.
  </p><p>
   <span class="bold"><strong>Mapping the use cases with various fields in input
   model</strong></span>
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /><col class="col6" /><col class="col7" /></colgroup><thead><tr><th> </th><th>Vf-count</th><th>SR-IOV</th><th>PCIPT</th><th>OVS bridge</th><th>Can be NIC bonded</th><th>Use case</th></tr></thead><tbody><tr><td>sriov-only: true</td><td>Mandatory</td><td>Yes</td><td>No</td><td>No</td><td>No</td><td>Dedicated to SRIOV</td></tr><tr><td>pci-pt : true</td><td>Not Specified</td><td>No</td><td>Yes</td><td>No</td><td>No</td><td>Dedicated to PCI-PT</td></tr><tr><td>pci-pt : true</td><td>Specified</td><td>Yes</td><td>Yes</td><td>No</td><td>No</td><td>PCI-PT or SRIOV</td></tr><tr><td>pci-pt and sriov-only keywords are not specified</td><td>Specified</td><td>Yes</td><td>No</td><td>Yes</td><td>No</td><td>SRIOV with PF used by host</td></tr><tr><td>pci-pt and sriov-only keywords are not specified</td><td>Not Specified</td><td>No</td><td>No</td><td>Yes</td><td>Yes</td><td>Traditional/Usual use case</td></tr></tbody></table></div></div><div class="sect3" id="id-1.5.12.6.19.11"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Mappings between <code class="filename">nic_mappings.yml</code> and <code class="filename">net_interfaces.yml</code></span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.19.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following diagram shows which fields in
   <code class="filename">nic_mappings.yml</code> map to corresponding fields in
   <code class="filename">net_interfaces.yml</code>:
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-sriov_pcpit.png" target="_blank"><img src="images/media-sriov_pcpit.png" width="" /></a></div></div></div><div class="sect3" id="id-1.5.12.6.19.12"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Example Use Cases for Intel</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.19.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     <span class="bold"><strong>Nic-device-types and nic-device-families</strong></span>
     with Intel 82559 with ixgbe as the driver.
    </p><div class="verbatim-wrap"><pre class="screen">nic-device-types:
    - name: ''8086:10fb
      family: INTEL-82599
      device-id: '10fb'
      type: simple-port
nic-device-families:
    # Niantic
    - name: INTEL-82599
      vendor-id: '8086'
      config-script: intel-82599.sh
      driver: ixgbe
      vf-count-type: port
      max-vf-count: 63</pre></div></li><li class="listitem "><p>
     <span class="bold"><strong>net_interfaces.yml</strong></span> for the SRIOV-only use
     case:
    </p><div class="verbatim-wrap"><pre class="screen">- name: COMPUTE-INTERFACES
   - name: hed1
     device:
       name: hed1
       sriov-only: true
       vf-count: 6
     network-groups:
      - GUEST1</pre></div></li><li class="listitem "><p>
     <span class="bold"><strong>net_interfaces.yml</strong></span> for the PCIPT-only use
     case:
    </p><div class="verbatim-wrap"><pre class="screen">- name: COMPUTE-INTERFACES
   - name: hed1
     device:
       name: hed1
       pci-pt: true
    network-groups:
     - GUEST1</pre></div></li><li class="listitem "><p>
     <span class="bold"><strong>net_interfaces.yml</strong></span> for the SRIOV and
     PCIPT use case
    </p><div class="verbatim-wrap"><pre class="screen"> - name: COMPUTE-INTERFACES
    - name: hed1
      device:
        name: hed1
        pci-pt: true
        vf-count: 6
      network-groups:
      - GUEST1</pre></div></li><li class="listitem "><p>
     <span class="bold"><strong>net_interfaces.yml</strong></span> for SRIOV and Normal
     Virtio use case
    </p><div class="verbatim-wrap"><pre class="screen">- name: COMPUTE-INTERFACES
   - name: hed1
     device:
        name: hed1
        vf-count: 6
      network-groups:
      - GUEST1</pre></div></li><li class="listitem "><p>
     <span class="bold"><strong>net_interfaces.yml</strong></span> for PCI-PT
     (<code class="literal">hed1</code> and <code class="literal">hed4</code> refer to the DUAL
     ports of the PCI-PT NIC)
    </p><div class="verbatim-wrap"><pre class="screen">    - name: COMPUTE-PCI-INTERFACES
      network-interfaces:
      - name: hed3
        device:
          name: hed3
        network-groups:
          - MANAGEMENT
          - EXTERNAL-VM
        forced-network-groups:
          - EXTERNAL-API
      - name: hed1
        device:
          name: hed1
          pci-pt: true
        network-groups:
          - GUEST
      - name: hed4
        device:
          name: hed4
          pci-pt: true
        network-groups:
          - GUEST</pre></div></li></ol></div></div><div class="sect3" id="id-1.5.12.6.19.13"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Launching Virtual Machines</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.19.13">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Provisioning a VM with SR-IOV NIC is a two-step process.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Create a neutron port with <code class="literal">vnic_type = direct</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port create --network $net_id --vnic-type direct sriov_port</pre></div></li><li class="step "><p>
     Boot a VM with the created <code class="literal">port-id</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server create --flavor m1.large --image opensuse --nic port-id=$port_id test-sriov</pre></div></li></ol></div></div><p>
   Provisioning a VM with PCI-PT NIC is a two-step process.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Create two neutron ports with <code class="literal">vnic_type =
     direct-physical</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port create --network net1 --vnic-type direct-physical pci-port1
<code class="prompt user">ardana &gt; </code>openstack port create --network net1 --vnic-type direct-physical pci-port2</pre></div></li><li class="step "><p>
     Boot a VM with the created ports.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server create --flavor 4 --image opensuse --nic port-id pci-port1-port-id \
--nic port-id pci-port2-port-id vm1-pci-passthrough</pre></div></li></ol></div></div><p>
   If PCI-PT VM gets stuck (hangs) at boot time when using an Intel NIC, the
   boot agent should be disabled.
  </p></div><div class="sect3" id="bootutil"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Intel bootutils</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#bootutil">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span>bootutil</li></ul></div></div></div></div><p>
   When Intel cards are used for PCI-PT, a tenant VM can get stuck at boot
   time. When this happens, you should download Intel bootutils and use it to
   should disable bootagent.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Download <code class="filename">Preboot.tar.gz</code> from
     <a class="link" href="https://downloadcenter.intel.com/download/19186/Intel-Ethernet-Connections-Boot-Utility-Preboot-Images-and-EFI-Drivers" target="_blank">https://downloadcenter.intel.com/download/19186/Intel-Ethernet-Connections-Boot-Utility-Preboot-Images-and-EFI-Drivers</a>
    </p></li><li class="listitem "><p>
     Untar the <code class="filename">Preboot.tar.gz</code> on the compute node where the
     PCI-PT VM is to be hosted.
    </p></li><li class="listitem "><p>
     Go to <code class="filename">~/APPS/BootUtil/Linux_x64</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/APPS/BootUtil/Linux_x64</pre></div><p>
     and run following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>./bootutil64e -BOOTENABLE disable -all</pre></div></li><li class="listitem "><p>
     Boot the PCI-PT VM; it should boot without getting stuck.
    </p><div id="id-1.5.12.6.19.14.3.4.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      Even though VM console shows VM getting stuck at PXE boot, it is not
      related to BIOS PXE settings.
     </p></div></li></ol></div></div><div class="sect3" id="id-1.5.12.6.19.15"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Making input model changes and implementing PCI PT and SR-IOV</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.19.15">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To implement the configuration you require, log into the Cloud Lifecycle Manager node and update
   the Cloud Lifecycle Manager model files to enable SR-IOV or PCIPT following the relevant use
   case explained above. You will need to edit the following:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <code class="filename">net_interfaces.yml</code>
    </p></li><li class="listitem "><p>
     <code class="filename">nic_device_data.yml</code>
    </p></li><li class="listitem "><p>
     <code class="filename">control_plane.yml</code>
    </p></li></ul></div><p>
   To make the edits,
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Check out the site branch of the local git repository and change to the
     correct directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git checkout site
<code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition/data/</pre></div></li><li class="step "><p>
     Open each file in vim or another editor and make the necessary changes.
     Save each file, then commit to the local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "your commit message goes here in quotes"</pre></div></li><li class="step "><p>
     Have the Cloud Lifecycle Manager enable your changes by running the necessary
     playbooks:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml</pre></div></li></ol></div></div><div id="id-1.5.12.6.19.15.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    After running the <code class="filename">site.yml</code> playbook above, you must
    reboot the compute nodes that are configured with Intel PCI devices.
   </p></div><div id="id-1.5.12.6.19.15.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    When a VM is running on an SRIOV port on a given compute node,
    reconfiguration is not supported.
   </p></div><p>
   You can set the number of virtual functions that must be enabled on a
   compute node at install time. You can update the number of virtual functions
   after deployment. If any VMs have been spawned before you change the number
   of virtual functions, those VMs may lose connectivity. Therefore, it is
   always recommended that if any virtual function is used by any tenant VM,
   you should not reconfigure the virtual functions. Instead, you should
   delete/migrate all the VMs on that NIC before reconfiguring the number of
   virtual functions.
  </p></div><div class="sect3" id="id-1.5.12.6.19.16"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.19.16">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Security groups are not applicable for PCI-PT and SRIOV ports.
    </p></li><li class="listitem "><p>
     Live migration is not supported for VMs with PCI-PT and SRIOV ports.
    </p></li><li class="listitem "><p>
     Rate limiting (QoS) is not applicable on SRIOV and PCI-PT ports.
    </p></li><li class="listitem "><p>
     SRIOV/PCIPT is not supported for VxLAN network.
    </p></li><li class="listitem "><p>
     DVR is not supported with SRIOV/PCIPT.
    </p></li><li class="listitem "><p>
     For Intel cards, the same NIC cannot be used for both SRIOV and normal VM
     boot.
    </p></li><li class="listitem "><p>
     Current upstream OpenStack code does not support this hot plugin of
     SRIOV/PCIPT interface using the nova <code class="literal">attach_interface</code>
     command. See <a class="link" href="https://review.openstack.org/#/c/139910/" target="_blank">https://review.openstack.org/#/c/139910/</a>
     for more information.
    </p></li><li class="listitem "><p>
     The <code class="command">openstack port update</code> command will not work when
     admin state is down.
    </p></li><li class="listitem "><p>
     SLES Compute Nodes with dual-port PCI-PT NICs, both ports should always be
     passed in the VM. It is not possible to split the dual port and pass
     through just a single port.
    </p></li></ul></div></div><div class="sect3" id="pcipt-gen9"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling PCI-PT on HPE DL360 Gen 9 Servers</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#pcipt-gen9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-enabling_pcipt_on_gen9.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-enabling_pcipt_on_gen9.xml</li><li><span class="ds-label">ID: </span>pcipt-gen9</li></ul></div></div></div></div><p>
  The HPE DL360 Gen 9 and HPE ProLiant systems with Intel processors use a
  region of system memory for sideband communication of management
  information. The BIOS sets up Reserved Memory Region Reporting (RMRR) to
  report these memory regions and devices to the operating system. There is a
  conflict between the Linux kernel and RMRR which causes problems with PCI
  pass-through (PCI-PT). This is needed for IOMMU use by DPDK. Note that this
  does not affect SR-IOV.
 </p><p>
  In order to enable PCI-PT on the HPE DL360 Gen 9 you must have a version of
  firmware that supports setting this and you must change a BIOS setting.
 </p><p>
  To begin, get the latest firmware and install it on your compute nodes.
 </p><p>
  Once the firmware has been updated:
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Reboot the server and press <span class="keycap">F9</span> (system utilities) during POST (power on
    self test)
   </p></li><li class="listitem "><p>
    Choose <span class="guimenu ">System Configuration</span>
   </p></li><li class="listitem "><p>
    Select the NIC for which you want to enable PCI-PT
   </p></li><li class="listitem "><p>
    Choose <span class="guimenu ">Device Level Configuration</span>
   </p></li><li class="listitem "><p>
    Disable the shared memory feature in the BIOS.
   </p></li><li class="listitem "><p>
    Save the changes and reboot server
   </p></li></ol></div></div></div><div class="sect2" id="vlan-aware"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.16 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting up VLAN-Aware VMs</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#vlan-aware">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span>vlan-aware</li></ul></div></div></div></div><p>
  Creating a VM with a trunk port will allow a VM to gain connectivity to one
  or more networks over the same virtual NIC (vNIC) through the use VLAN
  interfaces in the guest VM. Connectivity to different networks can be added
  and removed dynamically through the use of subports. The network of the
  parent port will be presented to the VM as the untagged VLAN, and the
  networks of the child ports will be presented to the VM as the tagged VLANs
  (the VIDs of which can be chosen arbitrarily as long as they are unique to
  that trunk). The VM will send/receive VLAN-tagged traffic over the subports,
  and neutron will mux/demux the traffic onto the subport's corresponding
  network. This is not to be confused with VLAN transparency where a VM can
  pass VLAN-tagged traffic transparently across the network without
  interference from neutron. VLAN transparency is not supported.
 </p><div class="sect3" id="id-1.5.12.6.20.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.16.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Terminology</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.20.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>Trunk</strong></span>: a resource that logically
     represents a trunked vNIC and references a parent port.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Parent port</strong></span>: a neutron port that a Trunk
     is referenced to. Its network is presented as the untagged VLAN.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Subport</strong></span>: a resource that logically
     represents a tagged VLAN port on a Trunk. A Subport references a child
     port and consists of the
     &lt;port&gt;,&lt;segmentation-type&gt;,&lt;segmentation-id&gt; tuple.
     Currently only the <code class="literal">vlan</code> segmentation type is supported.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Child port</strong></span>: a neutron port that a Subport
     is referenced to. Its network is presented as a tagged VLAN based upon the
     segmentation-id used when creating/adding a Subport.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Legacy VM</strong></span>: a VM that does not use a trunk
     port.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Legacy port</strong></span>: a neutron port that is not
     used in a Trunk.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>VLAN-aware VM</strong></span>: a VM that uses at least
     one trunk port.
    </p></li></ul></div></div><div class="sect3" id="id-1.5.12.6.20.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.16.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Trunk CLI reference</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.20.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th> Command</th><th>Action</th></tr></thead><tbody><tr><td>network trunk create </td><td>Create a trunk.</td></tr><tr><td>network trunk delete </td><td>Delete a given trunk.</td></tr><tr><td>network trunk list </td><td>List all trunks.</td></tr><tr><td>network trunk show </td><td>Show information of a given trunk.</td></tr><tr><td>network trunk set </td><td>Add subports to a given trunk.</td></tr><tr><td>network subport list </td><td>List all subports for a given trunk.</td></tr><tr><td>network trunk unset</td><td>Remove subports from a given trunk.</td></tr><tr><td>network trunk set</td><td>Update trunk properties.</td></tr></tbody></table></div></div><div class="sect3" id="id-1.5.12.6.20.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.16.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling VLAN-aware VM capability</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.20.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Edit
     <code class="filename">~/openstack/my_cloud/config/neutron/neutron.conf.j2</code>
     to add the <code class="literal">trunk</code> service_plugin:
    </p><div class="verbatim-wrap"><pre class="screen">service_plugins = {{ neutron_service_plugins }},trunk</pre></div></li><li class="step "><p>
     Edit
     <code class="filename">~/openstack/my_cloud/config/neutron/ml2_conf.ini.j2</code>
     to enable the noop firewall driver:
    </p><div class="verbatim-wrap"><pre class="screen">[securitygroup]
firewall_driver = neutron.agent.firewall.NoopFirewallDriver</pre></div><div id="id-1.5.12.6.20.5.2.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      This is a manual configuration step because it must be made apparent that
      this step disables neutron security groups completely. The default
      <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> firewall_driver is
      <code class="literal">neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewall
      Driver</code> which does not implement security groups for trunk
      ports. Optionally, the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> default firewall_driver may still be used
      (this step can be skipped), which would provide security groups for legacy
      VMs but not for VLAN-aware VMs. However, this mixed environment is not
      recommended. For more information, see <a class="xref" href="ops-managing-networking.html#firewall" title="10.4.16.6. Firewall issues">Section 10.4.16.6, “Firewall issues”</a>.
     </p></div></li><li class="step "><p>
     Commit the configuration changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Enable vlan-aware VMs"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/</pre></div></li><li class="step "><p>
     If this is an initial deployment, continue the rest of normal deployment
     process:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml</pre></div></li><li class="step "><p>
     If the cloud has already been deployed and this is a reconfiguration:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="id-1.5.12.6.20.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.16.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Use Cases</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.20.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="bold"><strong>Creating a trunk port</strong></span>
  </p><p>
   Assume that a number of neutron networks/subnets already exist: private,
   foo-net, and bar-net. This will create a trunk with two subports allocated
   to it. The parent port will be on the "private" network, while the two child
   ports will be on "foo-net" and "bar-net", respectively:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Create a port that will function as the trunk's parent port:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port create --name trunkparent private</pre></div></li><li class="step "><p>
     Create ports that will function as the child ports to be used in subports:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port create --name subport1 foo-net
<code class="prompt user">ardana &gt; </code>openstack port create --name subport2 bar-net</pre></div></li><li class="step "><p>
     Create a trunk port using the <code class="literal">openstack network trunk
     create</code> command, passing the parent port created in step 1 and
     child ports created in step 2:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network trunk create --parent-port trunkparent --subport port=subport1,segmentation-type=vlan,segmentation-id=1 --subport port=subport2,segmentation-type=vlan,segmentation-id=2 mytrunk
+-----------------+-----------------------------------------------------------------------------------------------+
| Field           | Value                                                                                         |
+-----------------+-----------------------------------------------------------------------------------------------+
| admin_state_up  | UP                                                                                            |
| created_at      | 2017-06-02T21:49:59Z                                                                          |
| description     |                                                                                               |
| id              | bd822ebd-33d5-423e-8731-dfe16dcebac2                                                          |
| name            | mytrunk                                                                                       |
| port_id         | 239f8807-be2e-4732-9de6-c64519f46358                                                          |
| project_id      | f51610e1ac8941a9a0d08940f11ed9b9                                                              |
| revision_number | 1                                                                                             |
| status          | DOWN                                                                                          |
| sub_ports       | port_id='9d25abcf-d8a4-4272-9436-75735d2d39dc', segmentation_id='1', segmentation_type='vlan' |
|                 | port_id='e3c38cb2-0567-4501-9602-c7a78300461e', segmentation_id='2', segmentation_type='vlan' |
| tenant_id       | f51610e1ac8941a9a0d08940f11ed9b9                                                              |
| updated_at      | 2017-06-02T21:49:59Z                                                                          |
+-----------------+-----------------------------------------------------------------------------------------------+

$ openstack network subport list --trunk mytrunk
+--------------------------------------+-------------------+-----------------+
| Port                                 | Segmentation Type | Segmentation ID |
+--------------------------------------+-------------------+-----------------+
| 9d25abcf-d8a4-4272-9436-75735d2d39dc | vlan              |               1 |
| e3c38cb2-0567-4501-9602-c7a78300461e | vlan              |               2 |
+--------------------------------------+-------------------+-----------------+</pre></div><p>
     Optionally, a trunk may be created without subports (they can be added
     later):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network trunk create --parent-port trunkparent mytrunk
+-----------------+--------------------------------------+
| Field           | Value                                |
+-----------------+--------------------------------------+
| admin_state_up  | UP                                   |
| created_at      | 2017-06-02T21:45:35Z                 |
| description     |                                      |
| id              | eb8a3c7d-9f0a-42db-b26a-ca15c2b38e6e |
| name            | mytrunk                              |
| port_id         | 239f8807-be2e-4732-9de6-c64519f46358 |
| project_id      | f51610e1ac8941a9a0d08940f11ed9b9     |
| revision_number | 1                                    |
| status          | DOWN                                 |
| sub_ports       |                                      |
| tenant_id       | f51610e1ac8941a9a0d08940f11ed9b9     |
| updated_at      | 2017-06-02T21:45:35Z                 |
+-----------------+--------------------------------------+</pre></div><p>
     A port that is already bound (that is, already in use by a VM) cannot be
     upgraded to a trunk port. The port must be unbound to be eligible for
     use as a trunk's parent port. When adding subports to a trunk, the child
     ports must be unbound as well.
    </p></li></ol></div></div><p>
   <span class="bold"><strong>Checking a port's trunk details</strong></span>
  </p><p>
   Once a trunk has been created, its parent port will show the
   <code class="literal">trunk_details</code> attribute, which consists of the
   <code class="literal">trunk_id</code> and list of subport dictionaries:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port show -F trunk_details trunkparent
+---------------+-------------------------------------------------------------------------------------+
| Field         | Value                                                                               |
+---------------+-------------------------------------------------------------------------------------+
| trunk_details | {"trunk_id": "bd822ebd-33d5-423e-8731-dfe16dcebac2", "sub_ports":                   |
|               | [{"segmentation_id": 2, "port_id": "e3c38cb2-0567-4501-9602-c7a78300461e",          |
|               | "segmentation_type": "vlan", "mac_address": "fa:16:3e:11:90:d2"},                   |
|               | {"segmentation_id": 1, "port_id": "9d25abcf-d8a4-4272-9436-75735d2d39dc",           |
|               | "segmentation_type": "vlan", "mac_address": "fa:16:3e:ff:de:73"}]}                  |
+---------------+-------------------------------------------------------------------------------------+</pre></div><p>
   Ports that are not trunk parent ports will not have a
   <code class="literal">trunk_details</code> field:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port show -F trunk_details subport1
need more than 0 values to unpack</pre></div><p>
   <span class="bold"><strong>Adding subports to a trunk</strong></span>
  </p><p>
   Assuming a trunk and new child port have been created already, the
   <code class="literal">trunk-subport-add</code> command will add one or more subports
   to the trunk.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Run <code class="literal">openstack network trunk set</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network trunk set --subport port=subport3,segmentation-type=vlan,segmentation-id=3 mytrunk</pre></div></li><li class="step "><p>
     Run <code class="literal">openstack network subport list</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network subport list --trunk mytrunk
+--------------------------------------+-------------------+-----------------+
| Port                                 | Segmentation Type | Segmentation ID |
+--------------------------------------+-------------------+-----------------+
| 9d25abcf-d8a4-4272-9436-75735d2d39dc | vlan              |               1 |
| e3c38cb2-0567-4501-9602-c7a78300461e | vlan              |               2 |
| bf958742-dbf9-467f-b889-9f8f2d6414ad | vlan              |               3 |
+--------------------------------------+-------------------+-----------------+</pre></div></li></ol></div></div><div id="id-1.5.12.6.20.6.13" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    The <code class="literal">--subport</code> option may be repeated multiple times in
    order to add multiple subports at a time.
   </p></div><p>
   <span class="bold"><strong>Removing subports from a trunk</strong></span>
  </p><p>
   To remove a subport from a trunk, use <code class="literal">openstack network trunk
   unset</code> command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network trunk unset --subport subport3 mytrunk</pre></div><p>
   <span class="bold"><strong>Deleting a trunk port</strong></span>
  </p><p>
   To delete a trunk port, use the <code class="literal">openstack network trunk
   delete</code> command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network trunk delete mytrunk</pre></div><p>
   Once a trunk has been created successfully, its parent port may be passed to
   the <code class="literal">openstack server create</code> command, which will make the VM VLAN-aware:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server create --image ubuntu-server --flavor 1 --nic port-id=239f8807-be2e-4732-9de6-c64519f46358 vlan-aware-vm</pre></div><div id="id-1.5.12.6.20.6.22" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    A trunk cannot be deleted until its parent port is unbound. This means you
    must delete the VM using the trunk port before you are allowed to delete
    the trunk.
   </p></div></div><div class="sect3" id="id-1.5.12.6.20.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.16.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">VLAN-aware VM network configuration</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#id-1.5.12.6.20.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   This section illustrates how to configure the VLAN interfaces inside a
   VLAN-aware VM based upon the subports allocated to the trunk port being
   used.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Run <code class="literal">openstack network trunk subport list</code> to see the
     VLAN IDs in use on the trunk port:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network subport list --trunk mytrunk
+--------------------------------------+-------------------+-----------------+
| Port                                 | Segmentation Type | Segmentation ID |
+--------------------------------------+-------------------+-----------------+
| e3c38cb2-0567-4501-9602-c7a78300461e | vlan              |               2 |
+--------------------------------------+-------------------+-----------------+</pre></div></li><li class="step "><p>
     Run <code class="command">openstack port show</code> on the child port to get its
     mac_address:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port show -F mac_address 08848e38-50e6-4d22-900c-b21b07886fb7
+-------------+-------------------+
| Field       | Value             |
+-------------+-------------------+
| mac_address | fa:16:3e:08:24:61 |
+-------------+-------------------+</pre></div></li><li class="step "><p>
     Log into the VLAN-aware VM and run the following commands to set up the
     VLAN interface:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo ip link add link ens3 ens3.2 address fa:16:3e:11:90:d2 broadcast ff:ff:ff:ff:ff:ff type vlan id 2
$ sudo ip link set dev ens3.2 up</pre></div></li><li class="step "><p>
     Note the usage of the mac_address from step 2 and VLAN ID from step 1 in
     configuring the VLAN interface:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo ip link add link ens3 ens3.2 address fa:16:3e:11:90:d2 broadcast ff:ff:ff:ff:ff:ff type vlan id 2</pre></div></li><li class="step "><p>
     Trigger a DHCP request for the new vlan interface to verify connectivity
     and retrieve its IP address. On an Ubuntu VM, this might be:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo dhclient ens3.2
<code class="prompt user">ardana &gt; </code>sudo ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: ens3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc pfifo_fast state UP group default qlen 1000
    link/ether fa:16:3e:8d:77:39 brd ff:ff:ff:ff:ff:ff
    inet 10.10.10.5/24 brd 10.10.10.255 scope global ens3
       valid_lft forever preferred_lft forever
    inet6 fe80::f816:3eff:fe8d:7739/64 scope link
       valid_lft forever preferred_lft forever
3: ens3.2@ens3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc noqueue state UP group default qlen 1000
    link/ether fa:16:3e:11:90:d2 brd ff:ff:ff:ff:ff:ff
    inet 10.10.12.7/24 brd 10.10.12.255 scope global ens3.2
       valid_lft forever preferred_lft forever
    inet6 fe80::f816:3eff:fe11:90d2/64 scope link
       valid_lft forever preferred_lft forever</pre></div></li></ol></div></div></div><div class="sect3" id="firewall"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.16.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Firewall issues</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#firewall">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span>firewall</li></ul></div></div></div></div><p>
   The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> default firewall_driver is
   <code class="literal">neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver</code>.
   This default does not implement security groups for VLAN-aware VMs, but it
   does implement security groups for legacy VMs. For this reason, it is
   recommended to disable neutron security groups altogether when using
   VLAN-aware VMs. To do so, set:
  </p><div class="verbatim-wrap"><pre class="screen">firewall_driver = neutron.agent.firewall.NoopFirewallDriver</pre></div><p>
   Doing this will prevent having a mix of firewalled and non-firewalled VMs in
   the same environment, but it should be done with caution because all VMs
   would be non-firewalled.
  </p></div></div></div><div class="sect1" id="CreateHARouter"><div class="titlepage"><div><div><h2 class="title"><span class="number">10.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a Highly Available Router</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#CreateHARouter">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-create-ha-router.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-create-ha-router.xml</li><li><span class="ds-label">ID: </span>CreateHARouter</li></ul></div></div></div></div><div class="sect2" id="CVRDVR"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">CVR and DVR High Available Routers</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#CVRDVR">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-create-ha-router.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-create-ha-router.xml</li><li><span class="ds-label">ID: </span>CVRDVR</li></ul></div></div></div></div><p>
   CVR (Centralized Virtual Routing) and DVR (Distributed Virtual Routing) are
   two types of technologies which can be used to provide routing processes in
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9. You can create Highly Available (HA) versions
   of CVR and DVR routers by using the options in the table below when creating
   your router.
  </p><p>
   The neutron command for creating a router <code class="literal">openstack router create
   router_name --distributed=True|False --ha=True|False</code> requires
   administrative permissions. See the example in the next section, <a class="xref" href="ops-managing-networking.html#CreateRouter" title="10.5.2. Creating a High Availability Router">Section 10.5.2, “Creating a High Availability Router”</a>.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col align="center" class="c1" /><col align="center" class="c2" /><col align="center" class="c3" /><col align="left" class="c4" /></colgroup><thead><tr><th align="center">--distributed</th><th align="center">--ha</th><th align="center">Router Type</th><th align="left">Description</th></tr></thead><tbody><tr><td align="center">False</td><td align="center">False</td><td align="center">CVR</td><td align="left">Centralized Virtual Router</td></tr><tr><td align="center">False</td><td align="center">True</td><td align="center">CVRHA</td><td align="left">Centralized Virtual Router with L3 High Availablity</td></tr><tr><td align="center">True</td><td align="center">False</td><td align="center">DVR</td><td align="left">Distributed Virtual Router without SNAT High Availability</td></tr><tr><td align="center">True</td><td align="center">True</td><td align="center">DVRHA</td><td align="left">Distributed Virtual Router with SNAT High Availability</td></tr></tbody></table></div></div><div class="sect2" id="CreateRouter"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a High Availability Router</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#CreateRouter">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-create-ha-router.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-create-ha-router.xml</li><li><span class="ds-label">ID: </span>CreateRouter</li></ul></div></div></div></div><p>
   You can create a highly available router using the OpenStackClient.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To create the HA router, add <code class="literal">--ha=True</code> to the
     <code class="command">openstack router create</code> command. If you also want to
     make the router distributed, add <code class="literal">--distributed=True</code>. In
     this example, a DVR SNAT HA router is created with the name
     <code class="literal">routerHA</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack router create routerHA --distributed=True --ha=True</pre></div></li><li class="listitem "><p>
     Set the gateway for the external network and add interface
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack router set routerHA &lt;ext-net-id&gt;
<code class="prompt user">ardana &gt; </code>openstack router add subnet routerHA &lt;private_subnet_id&gt;</pre></div></li><li class="listitem "><p>
     When the router is created, the gateway is set, and the interface
     attached, you have a router with high availability.
    </p></li></ol></div></div><div class="sect2" id="TestRouter"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Test Router for High Availability</span> <a title="Permalink" class="permalink" href="ops-managing-networking.html#TestRouter">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-create-ha-router.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-create-ha-router.xml</li><li><span class="ds-label">ID: </span>TestRouter</li></ul></div></div></div></div><p>
   You can demonstrate that the router is HA by running a continuous ping from
   a VM instance that is running on the private network to an external server
   such as a public DNS. As the ping is running, list the l3 agents hosting the
   router and identify the agent that is responsible for hosting the active
   router. Induce the failover mechanism by creating a catastrophic event such as
   shutting down node hosting the l3 agent. Once the node is shut down, you
   will see that the ping from the VM to the external network continues to run
   as the backup l3 agent takes over. To verify the agent hosting the primary
   router has changed, list the agents hosting the router. You will see a
   different agent is now hosting the active router.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Boot an instance on the private network
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server create --image &lt;image_id&gt; --flavor &lt;flavor_id&gt; --nic net_id=&lt;private_net_id&gt; --key_name &lt;key&gt; VM1</pre></div></li><li class="step "><p>
     Log into the VM using the SSH keys
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ssh -i &lt;key&gt; &lt;ipaddress of VM1&gt;</pre></div></li><li class="step "><p>
     Start a <code class="command">ping</code> to X.X.X.X. While pinging, make sure there
     is no packet loss and leave the ping running.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ping X.X.X.X</pre></div></li><li class="step "><p>
     Check which agent is hosting the active router.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network agent list –routers &lt;router_id&gt;</pre></div></li><li class="step "><p>
     Shutdown the node hosting the agent.
    </p></li><li class="step "><p>
     Within 10 seconds, check again to see which L3 agent is hosting the active
     router.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network agent list –routers &lt;router_id&gt;</pre></div></li><li class="step "><p>
     You will see a different agent.
    </p></li></ol></div></div></div></div></div></div><div class="page-bottom"><div id="_bottom-navigation"><a class="nav-link" href="ops-managing-dashboards.html"><span class="next-icon">→</span><span class="nav-label"><span class="number">Chapter 11 </span>Managing the Dashboard</span></a><a class="nav-link" href="ops-managing-objectstorage.html"><span class="prev-icon">←</span><span class="nav-label"><span class="number">Chapter 9 </span>Managing Object Storage</span></a></div><div class="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span class="_share-fb bottom-button">Facebook</span><span class="spacer"> • </span><span class="_share-in bottom-button">LinkedIn</span><span class="spacer"> • </span><span class="_share-tw bottom-button">Twitter</span><span class="spacer"> • </span><span class="_share-mail bottom-button">E-Mail</span></span></div><div class="print"><span class="_print-button bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2021 
        SUSE</p><ul><li><a href="https://jobs.suse.com/" target="_top">Careers</a></li><li><a href="https://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="https://www.suse.com/company/about/" target="_top">About</a></li><li><a href="https://www.suse.com/contact/" target="_top">Contact Us</a></li></ul></div></div></body></html>