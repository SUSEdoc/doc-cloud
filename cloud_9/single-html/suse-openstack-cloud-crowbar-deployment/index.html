<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>SUSE OpenStack Cloud Crowbar 9 | Deployment Guide using Crowbar</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="Deployment Guide using Crowbar | SUSE OpenStack Cloud …"/>
<meta name="description" content="SUSE® OpenStack Cloud Crowbar is an open source software solution that provides the fundamental capabilities to deploy and manage a cloud infrastruct…"/>
<meta name="product-name" content="SUSE OpenStack Cloud Crowbar"/>
<meta name="product-number" content="9"/>
<meta name="book-title" content="Deployment Guide using Crowbar"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-assignee" content="dpopov@suse.com"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="SUSE OpenStack Cloud 9"/>
<meta property="og:title" content="Deployment Guide using Crowbar | SUSE OpenStack Cloud …"/>
<meta property="og:description" content="SUSE® OpenStack Cloud Crowbar is an open source software solution that provides the fundamental capabilities to deploy and manage a cloud infrastruct…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Deployment Guide using Crowbar | SUSE OpenStack Cloud …"/>
<meta name="twitter:description" content="SUSE® OpenStack Cloud Crowbar is an open source software solution that provides the fundamental capabilities to deploy and manage a cloud infrastruct…"/>

<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.12.4.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft single normal offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="#book-crowbar-deployment">Deployment Guide using Crowbar</a></div></div><main id="_content"><nav class="side-toc placebo" id="_side-toc-overall"> </nav><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section xml:lang="en" class="book" id="book-crowbar-deployment" data-id-title="Deployment Guide using Crowbar"><div class="titlepage"><div><div class="big-version-info"><span class="productname"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span></span> <span class="productnumber">9</span></div><div class="title-container"><h1 class="title"><em class="citetitle">Deployment Guide using Crowbar</em></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="date"><span class="imprint-label">Publication Date: </span>
November 02, 2022
</div></div></div><div class="toc"><ul><li><span class="preface"><a href="#pre-cloud-deploy"><span class="title-name">About This Guide</span></a></span><ul><li><span class="sect1"><a href="#id-1.4.2.9"><span class="title-name">Available Documentation</span></a></span></li><li><span class="sect1"><a href="#id-1.4.2.10"><span class="title-name">Feedback</span></a></span></li><li><span class="sect1"><a href="#id-1.4.2.11"><span class="title-name">Documentation Conventions</span></a></span></li><li><span class="sect1"><a href="#id-1.4.2.12"><span class="title-name">About the Making of This Manual</span></a></span></li></ul></li><li><span class="part"><a href="#part-depl-intro"><span class="title-number">I </span><span class="title-name">Architecture and Requirements</span></a></span><ul><li><span class="chapter"><a href="#cha-depl-arch"><span class="title-number">1 </span><span class="title-name">The SUSE <span class="productname">OpenStack</span> Cloud Architecture</span></a></span><ul><li><span class="sect1"><a href="#sec-depl-arch-components-admin"><span class="title-number">1.1 </span><span class="title-name">The Administration Server</span></a></span></li><li><span class="sect1"><a href="#sec-depl-arch-components-control"><span class="title-number">1.2 </span><span class="title-name">The Control Node(s)</span></a></span></li><li><span class="sect1"><a href="#sec-depl-arch-components-compute"><span class="title-number">1.3 </span><span class="title-name">The Compute Nodes</span></a></span></li><li><span class="sect1"><a href="#sec-depl-arch-components-storage"><span class="title-number">1.4 </span><span class="title-name">The Storage Nodes</span></a></span></li><li><span class="sect1"><a href="#sec-depl-arch-components-monitoring"><span class="title-number">1.5 </span><span class="title-name">The Monitoring Node</span></a></span></li><li><span class="sect1"><a href="#sec-depl-arch-components-ha"><span class="title-number">1.6 </span><span class="title-name">HA Setup</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-depl-req"><span class="title-number">2 </span><span class="title-name">Considerations and Requirements</span></a></span><ul><li><span class="sect1"><a href="#sec-depl-req-network"><span class="title-number">2.1 </span><span class="title-name">Network</span></a></span></li><li><span class="sect1"><a href="#sec-depl-req-storage"><span class="title-number">2.2 </span><span class="title-name">Persistent Storage</span></a></span></li><li><span class="sect1"><a href="#sec-depl-req-ssl"><span class="title-number">2.3 </span><span class="title-name">SSL Encryption</span></a></span></li><li><span class="sect1"><a href="#sec-depl-req-hardware"><span class="title-number">2.4 </span><span class="title-name">Hardware Requirements</span></a></span></li><li><span class="sect1"><a href="#sec-depl-req-software"><span class="title-number">2.5 </span><span class="title-name">Software Requirements</span></a></span></li><li><span class="sect1"><a href="#sec-depl-req-ha"><span class="title-number">2.6 </span><span class="title-name">High Availability</span></a></span></li><li><span class="sect1"><a href="#sec-depl-req-summary"><span class="title-number">2.7 </span><span class="title-name">Summary: Considerations and Requirements</span></a></span></li><li><span class="sect1"><a href="#sec-depl-req-installation"><span class="title-number">2.8 </span><span class="title-name">Overview of the SUSE <span class="productname">OpenStack</span> Cloud Installation</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-depl-admserv"><span class="title-number">II </span><span class="title-name">Setting Up the Administration Server</span></a></span><ul><li><span class="chapter"><a href="#cha-depl-adm-inst"><span class="title-number">3 </span><span class="title-name">Installing the Administration Server</span></a></span><ul><li><span class="sect1"><a href="#sec-depl-adm-inst-os"><span class="title-number">3.1 </span><span class="title-name">Starting the Operating System Installation</span></a></span></li><li><span class="sect1"><a href="#sec-depl-adm-inst-online-update"><span class="title-number">3.2 </span><span class="title-name">Registration and Online Updates</span></a></span></li><li><span class="sect1"><a href="#sec-depl-adm-inst-add-on"><span class="title-number">3.3 </span><span class="title-name">Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Extension</span></a></span></li><li><span class="sect1"><a href="#sec-depl-adm-inst-partition"><span class="title-number">3.4 </span><span class="title-name">Partitioning</span></a></span></li><li><span class="sect1"><a href="#sec-depl-adm-inst-settings"><span class="title-number">3.5 </span><span class="title-name">Installation Settings</span></a></span></li></ul></li><li><span class="chapter"><a href="#app-deploy-smt"><span class="title-number">4 </span><span class="title-name">Installing and Setting Up an SMT Server on the Administration Server (Optional)</span></a></span><ul><li><span class="sect1"><a href="#app-deploy-smt-install"><span class="title-number">4.1 </span><span class="title-name">SMT Installation</span></a></span></li><li><span class="sect1"><a href="#app-deploy-smt-config"><span class="title-number">4.2 </span><span class="title-name">SMT Configuration</span></a></span></li><li><span class="sect1"><a href="#app-deploy-smt-repos"><span class="title-number">4.3 </span><span class="title-name">Setting up Repository Mirroring on the SMT Server</span></a></span></li><li><span class="sect1"><a href="#app-deploy-smt-info"><span class="title-number">4.4 </span><span class="title-name">For More Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-depl-repo-conf"><span class="title-number">5 </span><span class="title-name">Software Repository Setup</span></a></span><ul><li><span class="sect1"><a href="#sec-depl-adm-conf-repos-product"><span class="title-number">5.1 </span><span class="title-name">Copying the Product Media Repositories</span></a></span></li><li><span class="sect1"><a href="#sec-depl-adm-conf-repos-scc"><span class="title-number">5.2 </span><span class="title-name">Update and Pool Repositories</span></a></span></li><li><span class="sect1"><a href="#sec-depl-inst-admserv-post-adm-repos"><span class="title-number">5.3 </span><span class="title-name">
    Software Repository Sources for the Administration Server Operating System
   </span></a></span></li><li><span class="sect1"><a href="#sec-deploy-repo-locations"><span class="title-number">5.4 </span><span class="title-name">Repository Locations</span></a></span></li></ul></li><li><span class="chapter"><a href="#sec-depl-adm-inst-network"><span class="title-number">6 </span><span class="title-name">Service Configuration:  Administration Server Network Configuration</span></a></span></li><li><span class="chapter"><a href="#sec-depl-adm-inst-crowbar"><span class="title-number">7 </span><span class="title-name">Crowbar Setup</span></a></span><ul><li><span class="sect1"><a href="#sec-depl-adm-inst-crowbar-user"><span class="title-number">7.1 </span><span class="title-name"><span class="guimenu">User Settings</span></span></a></span></li><li><span class="sect1"><a href="#sec-depl-adm-inst-crowbar-network"><span class="title-number">7.2 </span><span class="title-name"><span class="guimenu">Networks</span></span></a></span></li><li><span class="sect1"><a href="#sec-depl-adm-inst-crowbar-mode"><span class="title-number">7.3 </span><span class="title-name"><span class="guimenu">Network Mode</span></span></a></span></li><li><span class="sect1"><a href="#sec-depl-adm-inst-crowbar-repos"><span class="title-number">7.4 </span><span class="title-name"><span class="guimenu">Repositories</span></span></a></span></li><li><span class="sect1"><a href="#sec-depl-inst-admserv-post-network"><span class="title-number">7.5 </span><span class="title-name">Custom Network Configuration</span></a></span></li></ul></li><li><span class="chapter"><a href="#sec-depl-adm-start-crowbar"><span class="title-number">8 </span><span class="title-name">Starting the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation</span></a></span></li><li><span class="chapter"><a href="#sec-depl-adm-crowbar-extra-features"><span class="title-number">9 </span><span class="title-name">Customizing Crowbar</span></a></span><ul><li><span class="section"><a href="#id-1.4.4.8.2"><span class="title-number">9.1 </span><span class="title-name">Skip Unready Nodes</span></a></span></li><li><span class="section"><a href="#id-1.4.4.8.3"><span class="title-number">9.2 </span><span class="title-name">Skip Unchanged Nodes</span></a></span></li><li><span class="section"><a href="#id-1.4.4.8.4"><span class="title-number">9.3 </span><span class="title-name">Controlling Chef Restarts Manually</span></a></span></li><li><span class="section"><a href="#id-1.4.4.8.5"><span class="title-number">9.4 </span><span class="title-name">Prevent Automatic Restart</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-depl-ostack"><span class="title-number">III </span><span class="title-name">Setting Up <span class="productname">OpenStack</span> Nodes and Services</span></a></span><ul><li><span class="chapter"><a href="#cha-depl-crowbar"><span class="title-number">10 </span><span class="title-name">The Crowbar Web Interface</span></a></span><ul><li><span class="sect1"><a href="#sec-depl-crow-login"><span class="title-number">10.1 </span><span class="title-name">Logging In</span></a></span></li><li><span class="sect1"><a href="#sec-depl-crow-overview"><span class="title-number">10.2 </span><span class="title-name">Overview: Main Elements</span></a></span></li><li><span class="sect1"><a href="#sec-depl-ostack-barclamps"><span class="title-number">10.3 </span><span class="title-name">Deploying Barclamp Proposals</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-depl-inst-nodes"><span class="title-number">11 </span><span class="title-name">Installing the <span class="productname">OpenStack</span> Nodes</span></a></span><ul><li><span class="sect1"><a href="#sec-depl-inst-nodes-prep"><span class="title-number">11.1 </span><span class="title-name">Preparations</span></a></span></li><li><span class="sect1"><a href="#sec-depl-inst-nodes-install"><span class="title-number">11.2 </span><span class="title-name">Node Installation</span></a></span></li><li><span class="sect1"><a href="#sec-depl-inst-nodes-install-external"><span class="title-number">11.3 </span><span class="title-name">Converting Existing SUSE Linux Enterprise Server 12 SP4 Machines Into SUSE <span class="productname">OpenStack</span> Cloud Nodes</span></a></span></li><li><span class="sect1"><a href="#sec-depl-inst-nodes-post"><span class="title-number">11.4 </span><span class="title-name">Post-Installation Configuration</span></a></span></li><li><span class="sect1"><a href="#sec-depl-inst-nodes-edit"><span class="title-number">11.5 </span><span class="title-name">Editing Allocated Nodes</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-depl-ostack"><span class="title-number">12 </span><span class="title-name">Deploying the <span class="productname">OpenStack</span> Services</span></a></span><ul><li><span class="sect1"><a href="#sec-depl-ostack-designate"><span class="title-number">12.1 </span><span class="title-name">Deploying designate</span></a></span></li><li><span class="sect1"><a href="#sec-depl-ostack-pacemaker"><span class="title-number">12.2 </span><span class="title-name">Deploying Pacemaker (Optional, HA Setup Only)</span></a></span></li><li><span class="sect1"><a href="#sec-depl-ostack-db"><span class="title-number">12.3 </span><span class="title-name">Deploying the Database</span></a></span></li><li><span class="sect1"><a href="#sec-depl-ostack-rabbit"><span class="title-number">12.4 </span><span class="title-name">Deploying RabbitMQ</span></a></span></li><li><span class="sect1"><a href="#sec-depl-ostack-keystone"><span class="title-number">12.5 </span><span class="title-name">Deploying keystone</span></a></span></li><li><span class="sect1"><a href="#sec-depl-ostack-monasca"><span class="title-number">12.6 </span><span class="title-name">Deploying monasca (Optional)</span></a></span></li><li><span class="sect1"><a href="#sec-depl-ostack-swift"><span class="title-number">12.7 </span><span class="title-name">Deploying swift (optional)</span></a></span></li><li><span class="sect1"><a href="#sec-depl-ostack-glance"><span class="title-number">12.8 </span><span class="title-name">Deploying glance</span></a></span></li><li><span class="sect1"><a href="#sec-depl-ostack-cinder"><span class="title-number">12.9 </span><span class="title-name">Deploying cinder</span></a></span></li><li><span class="sect1"><a href="#sec-depl-ostack-neutron"><span class="title-number">12.10 </span><span class="title-name">Deploying neutron</span></a></span></li><li><span class="sect1"><a href="#sec-depl-ostack-nova"><span class="title-number">12.11 </span><span class="title-name">Deploying nova</span></a></span></li><li><span class="sect1"><a href="#sec-depl-ostack-dash"><span class="title-number">12.12 </span><span class="title-name">Deploying horizon (<span class="productname">OpenStack</span> Dashboard)</span></a></span></li><li><span class="sect1"><a href="#sec-depl-ostack-heat"><span class="title-number">12.13 </span><span class="title-name">Deploying heat (Optional)</span></a></span></li><li><span class="sect1"><a href="#sec-depl-ostack-ceilometer"><span class="title-number">12.14 </span><span class="title-name">Deploying ceilometer (Optional)</span></a></span></li><li><span class="sect1"><a href="#sec-depl-ostack-manila"><span class="title-number">12.15 </span><span class="title-name">Deploying manila</span></a></span></li><li><span class="sect1"><a href="#sec-depl-ostack-tempest"><span class="title-number">12.16 </span><span class="title-name">Deploying Tempest (Optional)</span></a></span></li><li><span class="sect1"><a href="#sec-depl-ostack-magnum"><span class="title-number">12.17 </span><span class="title-name">Deploying Magnum (Optional)</span></a></span></li><li><span class="sect1"><a href="#sec-depl-ostack-barbican"><span class="title-number">12.18 </span><span class="title-name">Deploying barbican (Optional)</span></a></span></li><li><span class="sect1"><a href="#sec-depl-ostack-sahara"><span class="title-number">12.19 </span><span class="title-name">Deploying sahara</span></a></span></li><li><span class="sect1"><a href="#sec-depl-ostack-octavia"><span class="title-number">12.20 </span><span class="title-name">Deploying Octavia</span></a></span></li><li><span class="sect1"><a href="#sec-depl-ostack-ironic"><span class="title-number">12.21 </span><span class="title-name">Deploying ironic (optional)</span></a></span></li><li><span class="sect1"><a href="#sec-depl-ostack-final"><span class="title-number">12.22 </span><span class="title-name">How to Proceed</span></a></span></li><li><span class="sect1"><a href="#crow-ses-integration"><span class="title-number">12.23 </span><span class="title-name">SUSE Enterprise Storage integration</span></a></span></li><li><span class="sect1"><a href="#sec-depl-services"><span class="title-number">12.24 </span><span class="title-name">Roles and Services in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span></span></a></span></li><li><span class="sect1"><a href="#sec-deploy-crowbatch-description"><span class="title-number">12.25 </span><span class="title-name">Crowbar Batch Command</span></a></span></li></ul></li><li><span class="chapter"><a href="#sec-deploy-policy-json"><span class="title-number">13 </span><span class="title-name">Limiting Users' Access Rights</span></a></span><ul><li><span class="sect1"><a href="#sec-deploy-policy-json-edit"><span class="title-number">13.1 </span><span class="title-name">Editing <code class="filename">policy.json</code></span></a></span></li><li><span class="sect1"><a href="#sec-deploy-keystone-policy-json-edit"><span class="title-number">13.2 </span><span class="title-name">Editing <code class="filename">keystone_policy.json</code></span></a></span></li><li><span class="sect1"><a href="#sec-deploy-policy-json-keystone"><span class="title-number">13.3 </span><span class="title-name">Adjusting the <span class="guimenu">keystone</span> Barclamp
   Proposal</span></a></span></li><li><span class="sect1"><a href="#sec-deploy-policy-json-horizon"><span class="title-number">13.4 </span><span class="title-name">Adjusting the <span class="guimenu">horizon</span> Barclamp
   Proposal</span></a></span></li><li><span class="sect1"><a href="#sec-deploy-policy-json-admin"><span class="title-number">13.5 </span><span class="title-name">Pre-Installed Service Admin Role Components</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-depl-ostack-configs"><span class="title-number">14 </span><span class="title-name">Configuration Files for <span class="productname">OpenStack</span> Services</span></a></span><ul><li><span class="sect1"><a href="#id-1.4.5.6.2"><span class="title-number">14.1 </span><span class="title-name">Default Configuration Files</span></a></span></li><li><span class="sect1"><a href="#id-1.4.5.6.3"><span class="title-number">14.2 </span><span class="title-name">Custom Configuration Files</span></a></span></li><li><span class="sect1"><a href="#sec-depl-ostack-configs-custom-naming"><span class="title-number">14.3 </span><span class="title-name">Naming Conventions for Custom Configuration Files</span></a></span></li><li><span class="sect1"><a href="#sec-depl-ostack-configs-custom-order"><span class="title-number">14.4 </span><span class="title-name">Processing Order of Configuration Files</span></a></span></li><li><span class="sect1"><a href="#sec-depl-ostack-configs-custom-restart"><span class="title-number">14.5 </span><span class="title-name">Restarting with New or Changed Configuration Files</span></a></span></li><li><span class="sect1"><a href="#sec-depl-ostack-configs-custom-more"><span class="title-number">14.6 </span><span class="title-name">For More Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#install-heat-templates"><span class="title-number">15 </span><span class="title-name">Installing SUSE CaaS Platform heat Templates</span></a></span><ul><li><span class="section"><a href="#sec-heat-templates-install"><span class="title-number">15.1 </span><span class="title-name">SUSE CaaS Platform heat Installation Procedure</span></a></span></li><li><span class="section"><a href="#id-1.4.5.7.4"><span class="title-number">15.2 </span><span class="title-name">Installing SUSE CaaS Platform with Multiple Masters</span></a></span></li><li><span class="section"><a href="#id-1.4.5.7.5"><span class="title-number">15.3 </span><span class="title-name">Enabling the Cloud Provider Integration (CPI) Feature</span></a></span></li><li><span class="section"><a href="#sec-heat-templates-register"><span class="title-number">15.4 </span><span class="title-name">Register SUSE CaaS Platform Cluster for Software Updates</span></a></span></li><li><span class="section"><a href="#id-1.4.5.7.7"><span class="title-number">15.5 </span><span class="title-name">More Information about SUSE CaaS Platform</span></a></span></li></ul></li><li><span class="chapter"><a href="#install-caasp-terraform"><span class="title-number">16 </span><span class="title-name">Installing SUSE CaaS Platform v4 using terraform</span></a></span><ul><li><span class="section"><a href="#id-1.4.5.8.2"><span class="title-number">16.1 </span><span class="title-name">CaaSP v4 deployment on SOC using terraform.</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-depl-nostack"><span class="title-number">IV </span><span class="title-name">Setting Up Non-<span class="productname">OpenStack</span> Services</span></a></span><ul><li><span class="chapter"><a href="#cha-depl-nostack"><span class="title-number">17 </span><span class="title-name">Deploying the Non-<span class="productname">OpenStack</span> Components</span></a></span><ul><li><span class="sect1"><a href="#sec-depl-nostack-crowbar-tuning"><span class="title-number">17.1 </span><span class="title-name">Tuning the Crowbar Service</span></a></span></li><li><span class="sect1"><a href="#sec-depl-nostack-ntp"><span class="title-number">17.2 </span><span class="title-name">Configuring the NTP Service</span></a></span></li><li><span class="sect1"><a href="#sec-depl-nostack-salt"><span class="title-number">17.3 </span><span class="title-name">Installing and using Salt</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-depl-troubleshooting"><span class="title-number">V </span><span class="title-name">Troubleshooting and Support</span></a></span><ul><li><span class="chapter"><a href="#cha-depl-trouble"><span class="title-number">18 </span><span class="title-name">Troubleshooting and Support</span></a></span><ul><li><span class="sect1"><a href="#sec-depl-trouble-faq"><span class="title-number">18.1 </span><span class="title-name">FAQ</span></a></span></li><li><span class="sect1"><a href="#sec-depl-trouble-support"><span class="title-number">18.2 </span><span class="title-name">Support</span></a></span></li></ul></li></ul></li><li><span class="appendix"><a href="#app-deploy-cisco"><span class="title-number">A </span><span class="title-name">Using Cisco Nexus Switches with neutron</span></a></span><ul><li><span class="sect1"><a href="#app-deploy-cisco-requirements"><span class="title-number">A.1 </span><span class="title-name">Requirements</span></a></span></li><li><span class="sect1"><a href="#app-deploy-cisco-deploy"><span class="title-number">A.2 </span><span class="title-name">Deploying neutron with the Cisco Plugin</span></a></span></li></ul></li><li><span class="appendix"><a href="#app-deploy-docupdates"><span class="title-number">B </span><span class="title-name">Documentation Updates</span></a></span><ul><li><span class="sect1"><a href="#sec-deploy-docupdates-c8-gm"><span class="title-number">B.1 </span><span class="title-name">April 2018 (Initial Release <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8)</span></a></span></li></ul></li><li><span class="glossary"><a href="#gl-cloud"><span class="title-name">Glossary of Terminology and Product Names</span></a></span></li></ul></div><div class="list-of-figures"><div class="toc-title">List of Figures</div><ul><li><span class="figure"><a href="#id-1.4.3.2.6"><span class="number">1.1 </span><span class="name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Infrastructure</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.3.2.4"><span class="number">2.1 </span><span class="name">SUSE <span class="productname">OpenStack</span> Cloud Network: Overview</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.3.2.12"><span class="number">2.2 </span><span class="name">SUSE <span class="productname">OpenStack</span> Cloud Network: Details</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.6.2.3"><span class="number">7.1 </span><span class="name">YaST Crowbar Setup: User Settings</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.6.3.3"><span class="number">7.2 </span><span class="name">YaST Crowbar Setup: Network Settings</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.6.3.7.4"><span class="number">7.3 </span><span class="name">YaST Crowbar Setup: Network Settings for the BMC Network</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.6.4.6.6"><span class="number">7.4 </span><span class="name">YaST Crowbar Setup: Network Settings for the Bastion Network</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.6.5.4"><span class="number">7.5 </span><span class="name">YaST Crowbar Setup: Repository Settings</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.7.11"><span class="number">8.1 </span><span class="name">The SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation Web interface</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.7.13"><span class="number">8.2 </span><span class="name">Crowbar Web Interface: The Dashboard</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.2.5.3"><span class="number">10.1 </span><span class="name">Crowbar UI—Dashboard (Main Screen)</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.3.6.3.3.2"><span class="number">11.1 </span><span class="name">Discovered Nodes</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.3.6.3.4.2.2.2"><span class="number">11.2 </span><span class="name">Grouping Nodes</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.3.6.3.5.2"><span class="number">11.3 </span><span class="name">Editing a Single Node</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.3.6.3.7.5"><span class="number">11.4 </span><span class="name">Bulk Editing Nodes</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.3.6.3.9.2"><span class="number">11.5 </span><span class="name">All Nodes Have Been Installed</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.3.8.3.6.3.3"><span class="number">11.6 </span><span class="name">SUSE Updater barclamp: Configuration</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.3.8.3.6.4.2"><span class="number">11.7 </span><span class="name">SUSE Updater barclamp: Node Deployment</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.3.8.4.6.6.2"><span class="number">11.8 </span><span class="name">SUSE Manager barclamp</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.3.8.5.4.4.2"><span class="number">11.9 </span><span class="name">NFS barclamp</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.3.8.5.4.5.3"><span class="number">11.10 </span><span class="name">Editing an NFS barclamp Proposal</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.3.9.4"><span class="number">11.11 </span><span class="name">Node Information</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.8.10"><span class="number">12.1 </span><span class="name">The Pacemaker Barclamp</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.8.13"><span class="number">12.2 </span><span class="name">The Pacemaker Barclamp: Node Deployment Example</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.9.5"><span class="number">12.3 </span><span class="name">The Database Barclamp</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.9.6.5.2"><span class="number">12.4 </span><span class="name">MariaDB Configuration</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.10.4"><span class="number">12.5 </span><span class="name">The RabbitMQ Barclamp</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.10.6.7.1.2"><span class="number">12.6 </span><span class="name">SSL Settings for RabbitMQ Barclamp</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.11.3.6.2.2"><span class="number">12.7 </span><span class="name">The keystone Barclamp</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.11.3.7.2.2.4.2.3"><span class="number">12.8 </span><span class="name">The SSL Dialog</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.12.4"><span class="number">12.9 </span><span class="name">The monasca barclamp Raw Mode</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.12.30"><span class="number">12.10 </span><span class="name">The monasca Barclamp: Node Deployment Example</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.13.7"><span class="number">12.11 </span><span class="name">The swift Barclamp</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.13.12"><span class="number">12.12 </span><span class="name">The swift Barclamp: Node Deployment Example</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.14.6"><span class="number">12.13 </span><span class="name">The glance Barclamp</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.15.33"><span class="number">12.14 </span><span class="name">The cinder Barclamp</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.15.36"><span class="number">12.15 </span><span class="name">The cinder Barclamp: Node Deployment Example</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.16.14"><span class="number">12.16 </span><span class="name">The neutron Barclamp</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.16.17"><span class="number">12.17 </span><span class="name">The neutron barclamp</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.17.5"><span class="number">12.18 </span><span class="name">The nova Barclamp</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.17.8"><span class="number">12.19 </span><span class="name">The nova Barclamp: Node Deployment Example with Two KVM Nodes</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.18.5"><span class="number">12.20 </span><span class="name">The horizon Barclamp</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.19.6"><span class="number">12.21 </span><span class="name">The heat Barclamp</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.19.7.7"><span class="number">12.22 </span><span class="name">the heat barclamp: Raw Mode</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.20.7"><span class="number">12.23 </span><span class="name">The ceilometer Barclamp</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.20.10"><span class="number">12.24 </span><span class="name">The ceilometer Barclamp: Node Deployment</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.21.17"><span class="number">12.25 </span><span class="name">The manila Barclamp</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.21.20"><span class="number">12.26 </span><span class="name">The manila Barclamp: Node Deployment Example</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.22.7"><span class="number">12.27 </span><span class="name">The Tempest Barclamp</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.23.7"><span class="number">12.28 </span><span class="name">The Magnum Barclamp</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.24.4"><span class="number">12.29 </span><span class="name">The barbican Barclamp: Raw Mode</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.24.7.1.2.2.4.2.3"><span class="number">12.30 </span><span class="name">The SSL Dialog</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.25.4"><span class="number">12.31 </span><span class="name">The sahara Barclamp</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.27.5.4"><span class="number">12.32 </span><span class="name">The ironic barclamp Custom view</span></a></span></li><li><span class="figure"><a href="#id-1.4.6.2.4.3"><span class="number">17.1 </span><span class="name">The Crowbar barclamp: Raw Mode</span></a></span></li><li><span class="figure"><a href="#id-1.4.8.5.2.5.2"><span class="number">A.1 </span><span class="name">The neutron barclamp: Cisco Plugin</span></a></span></li></ul></div><div class="list-of-tables"><div class="toc-title">List of Tables</div><ul><li><span class="table"><a href="#tab-netw-range-ip-min"><span class="number">2.1 </span><span class="name">Minimum Number of IP Addresses for Network Ranges</span></a></span></li><li><span class="table"><a href="#id-1.4.3.3.2.13.6"><span class="number">2.2 </span><span class="name"><code class="systemitem">192.168.124.0/24</code> (Admin/BMC) Network Address Allocation</span></a></span></li><li><span class="table"><a href="#id-1.4.3.3.2.13.7"><span class="number">2.3 </span><span class="name"><code class="systemitem">192.168.125/24</code> (Storage) Network Address Allocation</span></a></span></li><li><span class="table"><a href="#id-1.4.3.3.2.13.8"><span class="number">2.4 </span><span class="name"><code class="systemitem">192.168.123/24</code> (Private Network/nova-fixed) Network Address Allocation</span></a></span></li><li><span class="table"><a href="#id-1.4.3.3.2.13.9"><span class="number">2.5 </span><span class="name"><code class="systemitem">192.168.126/24</code> (Public Network nova-floating, public) Network Address Allocation</span></a></span></li><li><span class="table"><a href="#id-1.4.3.3.2.13.10"><span class="number">2.6 </span><span class="name"><code class="systemitem">192.168.130/24</code> (Software Defined Network) Network Address Allocation</span></a></span></li><li><span class="table"><a href="#id-1.4.4.4.2.6"><span class="number">5.1 </span><span class="name">Local Product Repositories for SUSE <span class="productname">OpenStack</span> Cloud</span></a></span></li><li><span class="table"><a href="#tab-smt-repos-local"><span class="number">5.2 </span><span class="name">SMT Repositories Hosted on the Administration Server</span></a></span></li><li><span class="table"><a href="#tab-smt-repos-remote"><span class="number">5.3 </span><span class="name">SMT Repositories hosted on a Remote Server</span></a></span></li><li><span class="table"><a href="#tab-depl-adm-conf-susemgr-repos"><span class="number">5.4 </span><span class="name">SUSE Manager Repositories (Channels)</span></a></span></li><li><span class="table"><a href="#tab-depl-adm-conf-local-repos"><span class="number">5.5 </span><span class="name">Default Repository Locations on the Administration Server</span></a></span></li><li><span class="table"><a href="#id-1.4.4.6.3.7.3"><span class="number">7.1 </span><span class="name">Separate BMC Network Example Configuration</span></a></span></li><li><span class="table"><a href="#id-1.4.4.6.4.6.4"><span class="number">7.2 </span><span class="name">Example Addresses for a Bastion Network</span></a></span></li><li><span class="table"><a href="#id-1.4.4.6.6.12.5"><span class="number">7.3 </span><span class="name">VLANs used by the SUSE <span class="productname">OpenStack</span> Cloud Default Network Setup</span></a></span></li></ul></div><div class="list-of-examples"><div class="toc-title">List of Examples</div><ul><li><span class="example"><a href="#interface-map-example"><span class="number">7.1 </span><span class="name">Changing the Network Interface Order on a Machine with four NICs</span></a></span></li><li><span class="example"><a href="#ex-conduits-nic-number"><span class="number">7.2 </span><span class="name">Network Modes for Different NIC Numbers</span></a></span></li><li><span class="example"><a href="#ex-conduits-role"><span class="number">7.3 </span><span class="name">Network Modes for Certain Roles</span></a></span></li><li><span class="example"><a href="#ex-conduits-machine"><span class="number">7.4 </span><span class="name">Network Modes for Certain Machines</span></a></span></li><li><span class="example"><a href="#id-1.4.4.6.6.12.3"><span class="number">7.5 </span><span class="name">Example Network Definition for the External Network 192.168.150.0/16</span></a></span></li><li><span class="example"><a href="#ex-ironic-network-json"><span class="number">12.1 </span><span class="name">Example network.json</span></a></span></li><li><span class="example"><a href="#ex-ironic-network-json-diff"><span class="number">12.2 </span><span class="name">Diff of ironic Configuration</span></a></span></li><li><span class="example"><a href="#id-1.4.8.4.3.6.2"><span class="number">A.1 </span><span class="name">Exclusively Mapping <span class="guimenu">nova-fixed</span> to conduit <span class="guimenu">intf1</span> in dual mode</span></a></span></li></ul></div><div><div class="legalnotice" id="id-1.4.1.5"><p>
  Copyright © 2006–
2022

  SUSE LLC and contributors. All rights reserved.
 </p><p>
  Except where otherwise noted, this document is licensed under
  <span class="bold"><strong>Creative Commons Attribution 3.0 License
  </strong></span>: 
   <a class="link" href="https://creativecommons.org/licenses/by/3.0/legalcode" target="_blank">https://creativecommons.org/licenses/by/3.0/legalcode</a>.
 </p><p>
  For SUSE trademarks, see
  <a class="link" href="https://www.suse.com/company/legal/" target="_blank">https://www.suse.com/company/legal/</a>. All other
  third-party trademarks are the property of their respective owners. Trademark
  symbols (®, ™ etc.) denote trademarks of SUSE and its affiliates.
  Asterisks (*) denote third-party trademarks.
 </p><p>
  All information found in this book has been compiled with utmost attention
  to detail. However, this does not guarantee complete accuracy. Neither
  SUSE LLC, its affiliates, the authors nor the translators shall be held
  liable for possible errors or the consequences thereof.
 </p></div></div><section class="preface" id="pre-cloud-deploy" data-id-title="About This Guide"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number"> </span><span class="title-name">About This Guide</span></span> <a title="Permalink" class="permalink" href="#pre-cloud-deploy">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE® <span class="productname">OpenStack</span> Cloud Crowbar</span></span> is an open source software solution that provides the
  fundamental capabilities to deploy and manage a cloud infrastructure
  based on SUSE Linux Enterprise. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> is powered by <span class="productname">OpenStack</span>, the leading
  community-driven, open source cloud infrastructure project. It seamlessly
  manages and provisions workloads across a heterogeneous cloud environment
  in a secure, compliant, and fully-supported manner. The product tightly
  integrates with other SUSE technologies and with the SUSE maintenance
  and support infrastructure.
 </p><p>
  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>, there are several different high-level user roles:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.2.5.1"><span class="term">SUSE <span class="productname">OpenStack</span> Cloud Operator</span></dt><dd><p>
     Installs and deploys <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> on bare-metal, then
     installs the operating system and the <span class="productname">OpenStack</span> components. For detailed
     information about the operator's tasks and how to solve them, refer
     to SUSE <span class="productname">OpenStack</span> Cloud <em class="citetitle">Deployment Guide using Crowbar</em>.
    </p></dd><dt id="id-1.4.2.5.2"><span class="term">SUSE <span class="productname">OpenStack</span> Cloud Administrator</span></dt><dd><p>
     Manages projects, users, images, flavors, and quotas within
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.  For detailed information about the administrator's
     tasks and how to solve them, refer to the <span class="productname">OpenStack</span> <em class="citetitle">Administrator Guide</em> and the
     SUSE <span class="productname">OpenStack</span> Cloud <em class="citetitle">Supplement to <em class="citetitle">Administrator Guide</em> and <em class="citetitle">User Guide</em></em>.
    </p></dd><dt id="id-1.4.2.5.3"><span class="term">SUSE <span class="productname">OpenStack</span> Cloud User</span></dt><dd><p>
     End user who launches and manages instances, creates snapshots, and
     uses volumes for persistent storage within <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. For detailed
     information about the user's tasks and how to solve them, refer to
     <span class="productname">OpenStack</span> <em class="citetitle">User Guide</em> and the SUSE <span class="productname">OpenStack</span> Cloud <em class="citetitle">Supplement to <em class="citetitle">Administrator Guide</em> and <em class="citetitle">User Guide</em></em>.
    </p></dd></dl></div><p>
  This guide provides cloud operators with the information needed to deploy
  and maintain SUSE <span class="productname">OpenStack</span> Cloud administrative units, the Administration Server, the
  Control Nodes, and the Compute and Storage Nodes. The Administration Server
  provides all services needed to manage and deploy all other nodes in the
  cloud. The Control Node hosts all <span class="productname">OpenStack</span> components needed to operate
  virtual machines deployed on the Compute Nodes in the SUSE <span class="productname">OpenStack</span> Cloud. Each
  virtual machine (instance) started in the cloud will be hosted on one
  of the Compute Nodes. Object storage is managed by the Storage Nodes.

 </p><p>
  Many chapters in this manual contain links to additional documentation
  resources. These include additional documentation that is available on the
  system, and documentation available on the Internet.
 </p><p>
  For an overview of the documentation available for your product and the
  latest documentation updates, refer to
  <a class="link" href="https://documentation.suse.com" target="_blank">https://documentation.suse.com</a>.
 </p><section class="sect1" id="id-1.4.2.9" data-id-title="Available Documentation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1 </span><span class="title-name">Available Documentation</span></span> <a title="Permalink" class="permalink" href="#id-1.4.2.9">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div id="id-1.4.2.9.3" data-id-title="Online Documentation and Latest Updates" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Online Documentation and Latest Updates</div><p>
  Documentation for our products is available at
  <a class="link" href="http://documentation.suse.com" target="_blank">http://documentation.suse.com</a>, where you can also
  find the latest updates, and browse or download the documentation in various formats.
 </p></div><p>
  In addition, the product documentation
  is usually available in your installed system under
  <code class="filename">/usr/share/doc/manual</code>. You can also access the
  product-specific manuals and the upstream documentation from
  the <span class="guimenu">Help</span> links in the graphical Web interfaces.
 </p><p>The following documentation is available for this product:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.2.9.6.1"><span class="term"><em class="citetitle">Deployment Guide using Crowbar</em></span></dt><dd><p>
     Gives an introduction to the <span class="phrase"><span class="phrase">SUSE® <span class="productname">OpenStack</span> Cloud Crowbar</span></span> architecture, lists
     the requirements, and describes how to set up, deploy, and maintain the
     individual components. Also contains information about troubleshooting,
     support, and a glossary listing the most important terms and concepts
     for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
    </p></dd><dt id="id-1.4.2.9.6.2"><span class="term"><em class="citetitle">Administrator Guide</em>
   </span></dt><dd><p>
     Introduces the <span class="productname">OpenStack</span> services and their components.
    </p><p>
     Also guides you through tasks like managing images, roles, instances, flavors,
     volumes, shares, quotas, host aggregates, and viewing cloud resources. To
     complete these tasks, use either the graphical Web interface (<span class="productname">OpenStack</span> Dashboard,
     code name <code class="literal">horizon</code>) or the <span class="productname">OpenStack</span> command line clients.
    </p></dd><dt id="id-1.4.2.9.6.3"><span class="term"><em class="citetitle">User Guide</em>
   </span></dt><dd><p>
     Describes how to manage images, instances, networks, object containers,
     volumes, shares, stacks, and databases. To complete these tasks, use either the graphical Web interface (<span class="productname">OpenStack</span> Dashboard, code name
     <code class="literal">horizon</code>) or the <span class="productname">OpenStack</span> command line clients.
    </p></dd><dt id="id-1.4.2.9.6.4"><span class="term"><em class="citetitle">Operations Guide Crowbar</em>
   </span></dt><dd/><dt id="id-1.4.2.9.6.5"><span class="term"><em class="citetitle">Supplement to <em class="citetitle">Administrator Guide</em> and <em class="citetitle">User Guide</em></em></span></dt><dd><p>
     A supplement to the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <em class="citetitle">Administrator Guide</em> and
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> <em class="citetitle">User Guide</em>. It contains additional information for
     admins and end users that is specific to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
    </p></dd></dl></div></section><section class="sect1" id="id-1.4.2.10" data-id-title="Feedback"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2 </span><span class="title-name">Feedback</span></span> <a title="Permalink" class="permalink" href="#id-1.4.2.10">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
  Several feedback channels are available:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.2.10.4.1"><span class="term">Services and Support Options</span></dt><dd><p>
     For services and support options available for your product, refer to
     <a class="link" href="http://www.suse.com/support/" target="_blank">http://www.suse.com/support/</a>.
    </p></dd><dt id="id-1.4.2.10.4.2"><span class="term">User Comments/Bug Reports</span></dt><dd><p>
     We want to hear your comments about and suggestions for this manual and
     the other documentation included with this product. If you are reading the
     HTML version of this guide, use the Comments feature at the bottom of each page
     in the online documentation
     at <a class="link" href="http://documentation.suse.com" target="_blank">http://documentation.suse.com</a>.
    </p><p>If you are reading the single-page HTML version of this guide, you can
     use the <span class="guimenu">Report Bug</span> link next to each section to open
     a bug report at <a class="link" href="https://bugzilla.suse.com/" target="_blank">https://bugzilla.suse.com/</a>. A user
    account is needed for this.</p></dd><dt id="id-1.4.2.10.4.3"><span class="term">Mail</span></dt><dd><p>
     For feedback on the documentation of this product, you can also send a
     mail to <code class="literal">doc-team@suse.com</code>. Make sure to include the
     document title, the product version, and the publication date of the
     documentation. To report errors or suggest enhancements, provide a
     concise description of the problem and refer to the respective section
     number and page (or URL).
    </p></dd></dl></div></section><section class="sect1" id="id-1.4.2.11" data-id-title="Documentation Conventions"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3 </span><span class="title-name">Documentation Conventions</span></span> <a title="Permalink" class="permalink" href="#id-1.4.2.11">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
  The following notices and typographical conventions are used
  in this documentation:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><div id="id-1.4.2.11.4.1.1" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
    Vital information you must be aware of before proceeding. Warns you about
    security issues, potential loss of data, damage to hardware, or physical
    hazards.
   </p></div><div id="id-1.4.2.11.4.1.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
    Important information you should be aware of before proceeding.
   </p></div><div id="id-1.4.2.11.4.1.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    Additional information, for example about differences in software
    versions.
   </p></div><div id="id-1.4.2.11.4.1.4" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip</div><p>
    Helpful information, like a guideline or a piece of practical advice.
   </p></div></li><li class="listitem"><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code><code class="command">command</code></pre></div><p>
    Commands that can be run by any user, including the <code class="systemitem">root</code> user.
   </p></li><li class="listitem"><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">command</code></pre></div><p>
    Commands that must be run with <code class="systemitem">root</code> privileges. Often you
    can also prefix these commands with the <code class="command">sudo</code> command to
    run them.
   </p></li><li class="listitem"><p>
    <code class="filename">/etc/passwd</code>: directory names and file names
   </p></li><li class="listitem"><p>
    <em class="replaceable">PLACEHOLDER</em>: replace
    <em class="replaceable">PLACEHOLDER</em> with the actual value
   </p></li><li class="listitem"><p>
    <code class="envar">PATH</code>: the environment variable PATH
   </p></li><li class="listitem"><p>
    <code class="command">ls</code>, <code class="option">--help</code>: commands, options, and
    parameters
   </p></li><li class="listitem"><p>
    <code class="systemitem">user</code>: users or groups
   </p></li><li class="listitem"><p>
    <span class="keycap">Alt</span>, <span class="keycap">Alt</span><span class="key-connector">–</span><span class="keycap">F1</span>: a key to press or a key combination;
    keys are shown in uppercase as on a keyboard
   </p></li><li class="listitem"><p>
    <span class="guimenu">File</span>, <span class="guimenu">File</span> › <span class="guimenu">Save As</span>: menu items, buttons
   </p></li><li class="listitem"><p><strong class="arch-arrow-start">AMD/Intel</strong>
    This paragraph is only relevant for the AMD64/Intel 64 architecture. The
    arrows mark the beginning and the end of the text block.
   <strong class="arch-arrow-end"> </strong></p><p><strong class="arch-arrow-start">IBM Z, POWER</strong>
    This paragraph is only relevant for the architectures
    <code class="literal">z Systems</code> and <code class="literal">POWER</code>. The arrows
    mark the beginning and the end of the text block.
   <strong class="arch-arrow-end"> </strong></p></li><li class="listitem"><p>
    <span class="emphasis"><em>Dancing Penguins</em></span> (Chapter
    <span class="emphasis"><em>Penguins</em></span>, ↑Another Manual): This is a
    reference to a chapter in another manual.
   </p></li></ul></div></section><section class="sect1" id="id-1.4.2.12" data-id-title="About the Making of This Manual"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4 </span><span class="title-name">About the Making of This Manual</span></span> <a title="Permalink" class="permalink" href="#id-1.4.2.12">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   This documentation is written in SUSEDoc, a subset of <a class="link" href="http://www.docbook.org" target="_blank">DocBook 5</a>. The XML source
   files were validated by <code class="command">jing</code>, processed by
   <code class="command">xsltproc</code>, and converted into XSL-FO using a customized
   version of Norman Walsh's stylesheets. The final PDF is formatted through
   <span class="phrase">FOP</span> from Apache Software Foundation. The open source tools
   and the environment used to build this documentation are provided by the
   DocBook Authoring and Publishing Suite (DAPS). The project's home page can
   be found at <a class="link" href="https://github.com/openSUSE/daps" target="_blank">https://github.com/openSUSE/daps</a>.
  </p><p>
   The XML source code of this documentation can be found at <a class="link" href="https://github.com" target="_blank">https://github.com</a>.
  </p></section></section><div class="part" id="part-depl-intro" data-id-title="Architecture and Requirements"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">Part I </span><span class="title-name">Architecture and Requirements </span></span><a title="Permalink" class="permalink" href="#part-depl-intro">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-depl-arch"><span class="title-number">1 </span><span class="title-name">The SUSE <span class="productname">OpenStack</span> Cloud Architecture</span></a></span></li><dd class="toc-abstract"><p>
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> is a managed cloud infrastructure solution that provides a
    full stack of cloud deployment and management services.
   </p></dd><li><span class="chapter"><a href="#cha-depl-req"><span class="title-number">2 </span><span class="title-name">Considerations and Requirements</span></a></span></li><dd class="toc-abstract"><p>
    Before deploying <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>, there are some requirements to meet and
    architectural decisions to make. Read this chapter thoroughly first, as
    some decisions need to be made <span class="emphasis"><em>before</em></span> deploying
    SUSE <span class="productname">OpenStack</span> Cloud, and they cannot be changed afterward.
   </p></dd></ul></div><section class="chapter" id="cha-depl-arch" data-id-title="The SUSE OpenStack Cloud Architecture"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">1 </span><span class="title-name">The SUSE <span class="productname">OpenStack</span> Cloud Architecture</span></span> <a title="Permalink" class="permalink" href="#cha-depl-arch">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><div><div class="abstract"><p>
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> is a managed cloud infrastructure solution that provides a
    full stack of cloud deployment and management services.
   </p></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>  9 provides the following features:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Open source software that is based on the <span class="productname">OpenStack</span> Rocky
    release.
   </p></li><li class="listitem"><p>
    Centralized resource tracking providing insight into activities and
    capacity of the cloud infrastructure for optimized automated deployment
    of services.
   </p></li><li class="listitem"><p>
    A self-service portal enabling end users to configure and deploy
    services as necessary, and to track resource
    consumption (horizon).
   </p></li><li class="listitem"><p>
    An image repository from which standardized, pre-configured virtual
    machines are published (glance).
   </p></li><li class="listitem"><p>
    Automated installation processes via Crowbar using pre-defined scripts
    for configuring and deploying the Control Node(s) and Compute
    and Storage Nodes.
   </p></li><li class="listitem"><p>
    Multi-tenant, role-based provisioning and access control for multiple
    departments and users within your organization.
   </p></li><li class="listitem"><p>
    APIs enabling the integration of third-party software, such as identity
    management and billing solutions.
   </p></li><li class="listitem"><p>
    An optional monitoring as a service solution, that allows to manage, track,
    and optimize the cloud infrastructure and the services provided to end
    users (<span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring, monasca).
   </p></li></ul></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> is based on SUSE Linux Enterprise Server, <span class="productname">OpenStack</span>, Crowbar, and
  Chef. SUSE Linux Enterprise Server is the underlying operating system for all
  cloud infrastructure machines (also called nodes). The cloud management layer,
  <span class="productname">OpenStack</span>, works as the <span class="quote">“<span class="quote">Cloud Operating
  System</span>”</span>. Crowbar and Chef automatically deploy
  and manage the <span class="productname">OpenStack</span> nodes from a central Administration Server.
 </p><div class="figure" id="id-1.4.3.2.6"><div class="figure-contents"><div class="mediaobject"><a href="images/cloud_node_structure.png"><img src="images/cloud_node_structure.png" width="100%" alt="SUSE OpenStack Cloud Crowbar Infrastructure" title="SUSE OpenStack Cloud Crowbar Infrastructure"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 1.1: </span><span class="title-name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Infrastructure </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.2.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> is deployed to four different types of machines:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    one Administration Server for node deployment and management
   </p></li><li class="listitem"><p>
    one or more Control Nodes hosting the cloud management services
   </p></li><li class="listitem"><p>
    several Compute Nodes on which the instances are started
   </p></li><li class="listitem"><p>
    several Monitoring Node for monitoring services and servers.
   </p></li></ul></div><section class="sect1" id="sec-depl-arch-components-admin" data-id-title="The Administration Server"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.1 </span><span class="title-name">The Administration Server</span></span> <a title="Permalink" class="permalink" href="#sec-depl-arch-components-admin">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   The Administration Server provides all services needed to manage and deploy all
   other nodes in the cloud. Most of these services are provided by the
   Crowbar tool that—together with Chef—automates all the
   required installation and configuration tasks. Among the services
   provided by the server are DHCP, DNS, NTP, PXE, and TFTP.
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/cloud_admin-node-crowbar.png"><img src="images/cloud_admin-node-crowbar.png" width="100%" alt="Administration Server Diagram" title="Administration Server Diagram"/></a></div></div><p>
   The Administration Server also hosts the software repositories for SUSE Linux Enterprise Server and
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>, which are needed for node deployment. If no other
   sources for the software repositories are available, it can host the Subscription Management Tool (SMT), providing up-to-date repositories
   with updates and patches for all nodes.
  </p></section><section class="sect1" id="sec-depl-arch-components-control" data-id-title="The Control Node(s)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.2 </span><span class="title-name">The Control Node(s)</span></span> <a title="Permalink" class="permalink" href="#sec-depl-arch-components-control">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   The Control Node(s) hosts all <span class="productname">OpenStack</span> components needed to
   orchestrate virtual machines deployed on the Compute Nodes in the
   SUSE <span class="productname">OpenStack</span> Cloud. <span class="productname">OpenStack</span> on SUSE <span class="productname">OpenStack</span> Cloud uses a MariaDB database,
   which is hosted on the Control Node(s). The following <span class="productname">OpenStack</span>
   components—if deployed—run on the Control Node(s):
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     MariaDB database.
    </p></li><li class="listitem"><p>
     Image (glance) for managing virtual images.
    </p></li><li class="listitem"><p>
     Identity (keystone), providing authentication and authorization
     for all <span class="productname">OpenStack</span> components.
    </p></li><li class="listitem"><p>
     Networking (neutron), providing <span class="quote">“<span class="quote">networking as a
     service</span>”</span> between interface devices managed by other <span class="productname">OpenStack</span>
     services.
    </p></li><li class="listitem"><p>
     Block Storage (cinder), providing block storage.
    </p></li><li class="listitem"><p>
     <span class="productname">OpenStack</span> Dashboard (horizon), providing the Dashboard,
     a user Web interface for the <span class="productname">OpenStack</span> components.
    </p></li><li class="listitem"><p>
     Compute (nova) management (<code class="literal">nova-controller</code>) including API and
     scheduler.
    </p></li><li class="listitem"><p>
     Message broker (RabbitMQ).
    </p></li><li class="listitem"><p>
     swift proxy server plus dispersion tools (health monitor) and
     swift ring (index of objects, replicas, and
     devices). swift provides object storage.
    </p></li><li class="listitem"><p>
     Hawk, a monitor for a pacemaker cluster in a High Availability (HA)
     setup.
    </p></li><li class="listitem"><p>
     heat, an orchestration engine.
    </p></li><li class="listitem"><p>
     designate provides DNS as a Service (DNSaaS)
    </p></li><li class="listitem"><p>
     ironic, the <span class="productname">OpenStack</span> bare metal service for provisioning physical machines.
    </p></li></ul></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> requires a three-node cluster for any production deployment
   since it leverages a MariaDB Galera Cluster for high availability.
  </p><p>
   We recommend deploying certain parts of Networking (neutron) on separate nodes for production clouds. See
   <a class="xref" href="#sec-depl-ostack-neutron" title="12.10. Deploying neutron">Section 12.10, “Deploying neutron”</a> for details.
  </p><p>
   You can separate authentication and authorization services from other cloud
   services, for stronger security, by hosting Identity (keystone) on a
   separate node. Hosting Block Storage (cinder, particularly the
   cinder-volume role) on a separate node when using local disks for storage
   enables you to customize your storage and network hardware to best meet your
   requirements.
  </p><div id="id-1.4.3.2.10.7" data-id-title="Moving Services in an Existing Setup" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Moving Services in an Existing Setup</div><p>
    If you plan to move a service from one Control Node to another, we strongly recommended shutting down or saving <span class="emphasis"><em>all</em></span> instances before doing so. Restart
    them after having successfully re-deployed the services. Moving
    services also requires stopping them manually on the original
    Control Node.


   </p></div></section><section class="sect1" id="sec-depl-arch-components-compute" data-id-title="The Compute Nodes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.3 </span><span class="title-name">The Compute Nodes</span></span> <a title="Permalink" class="permalink" href="#sec-depl-arch-components-compute">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   The Compute Nodes are the pool of machines on which your instances
   are running. These machines need to be equipped with a sufficient number
   of CPUs and enough RAM to start several instances. They also need to
   provide sufficient hard disk space, see
   <a class="xref" href="#sec-depl-req-storage-hardware-compute" title="2.2.2.3. Compute Nodes">Section 2.2.2.3, “Compute Nodes”</a> for details. The
   Control Node distributes instances within the pool of
   Compute Nodes and provides them with the necessary network resources. The
   <span class="productname">OpenStack</span> component Compute (nova) runs on the Compute Nodes and
   provides the means for setting up, starting, and stopping virtual machines.
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> supports several hypervisors, including KVM and  VMware
   vSphere. Each image that is started with an instance is
   bound to one hypervisor. Each Compute Node can only run one hypervisor
   at a time. You will choose which hypervisor to run on each Compute Node
   when deploying the nova barclamp.
  </p></section><section class="sect1" id="sec-depl-arch-components-storage" data-id-title="The Storage Nodes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.4 </span><span class="title-name">The Storage Nodes</span></span> <a title="Permalink" class="permalink" href="#sec-depl-arch-components-storage">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   The Storage Nodes are the pool of machines providing object or block
   storage. Object storage is provided by the <span class="productname">OpenStack</span> swift component,
   while block storage is provided by cinder. The latter supports several
   back-ends, including Ceph, that are deployed during the
   installation. Deploying swift and Ceph is optional.
  </p></section><section class="sect1" id="sec-depl-arch-components-monitoring" data-id-title="The Monitoring Node"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.5 </span><span class="title-name">The Monitoring Node</span></span> <a title="Permalink" class="permalink" href="#sec-depl-arch-components-monitoring">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
The Monitoring Node is the node that has the <code class="literal">monasca-server</code> role assigned.
It hosts most services needed for SUSE <span class="productname">OpenStack</span> Cloud
Monitoring, our monasca-based monitoring and logging solution. The following
services run on this node:
</p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.2.13.3.1"><span class="term">Monitoring API</span></dt><dd><p>
          The monasca Web API that is used for sending metrics by monasca agents, and
  retrieving metrics with the monasca command line client and the monasca Grafana dashboard.
      </p></dd><dt id="id-1.4.3.2.13.3.2"><span class="term">Message Queue</span></dt><dd><p>
         A Kafka instance used exclusively by SUSE <span class="productname">OpenStack</span> Cloud Monitoring.
      </p></dd><dt id="id-1.4.3.2.13.3.3"><span class="term">Persister</span></dt><dd><p>
         Stores metrics and alarms in InfluxDB.
      </p></dd><dt id="id-1.4.3.2.13.3.4"><span class="term">Notification Engine</span></dt><dd><p>
         Consumes alarms sent by the Threshold Engine and sends
notifications (e.g. via email).
      </p></dd><dt id="id-1.4.3.2.13.3.5"><span class="term">Threshold Engine</span></dt><dd><p>
         Based on Apache Storm. Computes thresholds on metrics and handles alarming.
      </p></dd><dt id="id-1.4.3.2.13.3.6"><span class="term">Metrics and Alarms Database</span></dt><dd><p>
         A Cassandra database for storing metrics alarm history.
      </p></dd><dt id="id-1.4.3.2.13.3.7"><span class="term">Config Database</span></dt><dd><p>
         A dedicated MariaDB instance used only for monitoring related data.
      </p></dd><dt id="id-1.4.3.2.13.3.8"><span class="term">Log API</span></dt><dd><p>
         The monasca Web API that is used for sending log entries by monasca agents, and
  retrieving log entries with the Kibana Server.
      </p></dd><dt id="id-1.4.3.2.13.3.9"><span class="term">Log Transformer</span></dt><dd><p>
         Transforms raw log entries sent to the Log API into a format
suitable for storage.
      </p></dd><dt id="id-1.4.3.2.13.3.10"><span class="term">Log Metrics</span></dt><dd><p>
         Sends metrics about high severity log messages to the Monitoring API.
      </p></dd><dt id="id-1.4.3.2.13.3.11"><span class="term">Log Persister</span></dt><dd><p>
         Stores logs processed by monasca Log Transformer in the Log Database.
      </p></dd><dt id="id-1.4.3.2.13.3.12"><span class="term">Kibana Server</span></dt><dd><p>
         A graphical web frontend for querying the Log Database.
      </p></dd><dt id="id-1.4.3.2.13.3.13"><span class="term">Log Database</span></dt><dd><p>
         An Elasticsearch database for storing logs.
      </p></dd><dt id="id-1.4.3.2.13.3.14"><span class="term">Zookeeper</span></dt><dd><p>
         Cluster synchronization for Kafka and Storm.
      </p></dd></dl></div><p>
Currently there can only be one Monitoring node. Clustering support is
planned for a future release. We strongly recommend using a dedicated physical
node without any other services as a Monitoring Node.
</p></section><section class="sect1" id="sec-depl-arch-components-ha" data-id-title="HA Setup"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.6 </span><span class="title-name">HA Setup</span></span> <a title="Permalink" class="permalink" href="#sec-depl-arch-components-ha">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   A failure of components in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> can lead to system downtime
   and data loss. To prevent this, set up a High Availability (HA) cluster
   consisting of several nodes. You can assign certain roles to this cluster
   instead of assigning them to individual nodes. As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>
   8, Control Nodes and Compute Nodes can be made highly available.
  </p><p>
   For all HA-enabled roles, their respective functions are automatically
   handled by the clustering software SUSE Linux Enterprise High Availability Extension. The High Availability Extension uses
   the Pacemaker cluster stack with Pacemaker as cluster resource manager,
   and Corosync as the messaging/infrastructure layer.
  </p><p>
   View the cluster status and configuration with the cluster
   management tools HA Web Console (Hawk) or the
   <code class="command">crm</code> shell.
  </p><div id="id-1.4.3.2.14.5" data-id-title="Do Not Change the Configuration" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Do Not Change the Configuration</div><p>
    Use the cluster management tools only for <span class="emphasis"><em>viewing</em></span>.
    All of the clustering configuration is done automatically via Crowbar
    and Chef. If you change anything via the cluster management tools
    you risk breaking the cluster. Changes done there may be reverted by the
    next run of Chef anyway.
   </p></div><p>
   A failure of the <span class="productname">OpenStack</span> infrastructure services (running on the
   Control Nodes) can be critical and may cause downtime within the cloud. For more information on making those services highly-available and avoiding other potential points of
   failure in your cloud setup, refer to <a class="xref" href="#sec-depl-req-ha" title="2.6. High Availability">Section 2.6, “High Availability”</a>.
  </p></section></section><section class="chapter" id="cha-depl-req" data-id-title="Considerations and Requirements"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">2 </span><span class="title-name">Considerations and Requirements</span></span> <a title="Permalink" class="permalink" href="#cha-depl-req">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><div><div class="abstract"><p>
    Before deploying <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>, there are some requirements to meet and
    architectural decisions to make. Read this chapter thoroughly first, as
    some decisions need to be made <span class="emphasis"><em>before</em></span> deploying
    SUSE <span class="productname">OpenStack</span> Cloud, and they cannot be changed afterward.
   </p></div></div></div></div><section class="sect1" id="sec-depl-req-network" data-id-title="Network"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.1 </span><span class="title-name">Network</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-network">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> requires a complex network setup consisting of several
   networks that are configured during installation. These networks are for
   exclusive cloud usage. You need a router to access them from an existing network.
  </p><p>
   The network configuration on the nodes in the SUSE <span class="productname">OpenStack</span> Cloud network is
   entirely controlled by Crowbar. Any network configuration not created with
   Crowbar (for example, with YaST) will automatically be
   overwritten. After the cloud is deployed, network settings cannot be
   changed.
  </p><div class="figure" id="id-1.4.3.3.2.4"><div class="figure-contents"><div class="mediaobject"><a href="images/cloud_network_overview.png"><img src="images/cloud_network_overview.png" width="75%" alt="SUSE OpenStack Cloud Network: Overview" title="SUSE OpenStack Cloud Network: Overview"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 2.1: </span><span class="title-name">SUSE <span class="productname">OpenStack</span> Cloud Network: Overview </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.3.2.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><p>
   The following networks are pre-defined when setting up <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
   The IP addresses listed are the default addresses and can be changed
   using the YaST Crowbar module (see
   <a class="xref" href="#sec-depl-adm-inst-crowbar" title="Chapter 7. Crowbar Setup">Chapter 7, <em>Crowbar Setup</em></a>). It is also possible to
   customize the network setup by manually editing
   the network barclamp template. See
   <a class="xref" href="#sec-depl-inst-admserv-post-network" title="7.5. Custom Network Configuration">Section 7.5, “Custom Network Configuration”</a> for detailed instructions.
  </p><div class="variablelist"><dl class="variablelist"><dt id="vle-netw-adm"><span class="term">
     Admin Network (192.168.124/24)
    </span></dt><dd><p>
      A private network to access the Administration Server and all nodes for
      administration purposes. The default setup also allows access to the
      BMC (Baseboard Management Controller) data via IPMI (Intelligent
      Platform Management Interface) from this network. If required, BMC
      access can be swapped to a separate network.
     </p><p>
      You have the following options for controlling access to this network:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Do not allow access from the outside and keep the admin network
        completely separated.
       </p></li><li class="listitem"><p>
        Allow access to the Administration Server from a single network (for example,
        your company's administration network) via the <span class="quote">“<span class="quote">bastion
        network</span>”</span> option configured on an additional network card with
        a fixed IP address.
       </p></li><li class="listitem"><p>
        Allow access from one or more networks via a gateway.
       </p></li></ul></div></dd><dt id="vle-netw-stor"><span class="term">
     Storage Network (192.168.125/24)
    </span></dt><dd><p>
      Private SUSE <span class="productname">OpenStack</span> Cloud internal virtual network. This network is used by
      Ceph and swift only. It should not be accessed by
      users.
     </p></dd><dt id="id-1.4.3.3.2.6.3"><span class="term">
     Private Network (nova-fixed, 192.168.123/24)
    </span></dt><dd><p>
      Private SUSE <span class="productname">OpenStack</span> Cloud internal virtual network. This network is used for
      inter-instance communication and provides access to the outside
      world for the instances. The required gateway is automatically provided by SUSE <span class="productname">OpenStack</span> Cloud.
     </p></dd><dt id="id-1.4.3.3.2.6.4"><span class="term">
     Public Network (nova-floating, public, 192.168.126/24)
    </span></dt><dd><p>
      The only public network provided by SUSE <span class="productname">OpenStack</span> Cloud. You can access the
      nova Dashboard and all instances (provided they have been equipped
      with floating IP addresses) on this network. This network can only be accessed
      via a gateway, which must be provided externally. All SUSE <span class="productname">OpenStack</span> Cloud
      users and administrators must have access to the public network.
     </p></dd><dt id="vle-netw-sdn"><span class="term">
     Software Defined Network (os_sdn, 192.168.130/24)
    </span></dt><dd><p>
      Private SUSE <span class="productname">OpenStack</span> Cloud internal virtual network. This network is used
      when neutron is configured to use openvswitch with
      <code class="literal">GRE</code> tunneling for the virtual networks. It should
      not be accessible to users.
     </p></dd><dt id="id-1.4.3.3.2.6.6"><span class="term">The monasca Monitoring Network</span></dt><dd><p>
      The monasca monitoring node needs to have an interface on both the
      admin network and the public network. monasca's backend services will
      listen on the admin network, the API services
      (<code class="systemitem">openstack-monasca-api</code>,
      <code class="systemitem">openstack-monasca-log-api</code>) will listen on all
      interfaces. <code class="systemitem">openstack-monasca-agent</code> and
      <code class="systemitem">openstack-monasca-log-agent</code> will send their logs
      and metrics to the
      <code class="systemitem">monasca-api</code>/<code class="systemitem">monasca-log-api</code>
      services to the monitoring node's public network IP address.
     </p></dd></dl></div><p>
   If the Octavia barclamp will be deployed, the Octavia Management Network
   also needs to be configured.
   Octavia uses a set of instances running on one or more compute nodes
   called amphorae and the Octavia services need to communicate with the
   amphorae over a dedicated management network.
   This network is not pre-defined when setting up <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. It needs
   to be explicitly configured as covered under <a class="xref" href="#sec-depl-ostack-octavia-mgmtnet" title="12.20.1.1. Management network">Section 12.20.1.1, “Management network”</a>.
  </p><div id="id-1.4.3.3.2.8" data-id-title="Protect Networks from External Access" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Protect Networks from External Access</div><p>
    For security reasons, protect the following networks from external
    access:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <a class="xref" href="#vle-netw-adm">
     Admin Network (192.168.124/24)
    </a>
     </p></li><li class="listitem"><p>
      <a class="xref" href="#vle-netw-stor">
     Storage Network (192.168.125/24)
    </a>
     </p></li><li class="listitem"><p>
      <a class="xref" href="#vle-netw-sdn">
     Software Defined Network (os_sdn, 192.168.130/24)
    </a>
     </p></li></ul></div><p>
    Especially traffic from the cloud instances must not be able to pass
    through these networks.
   </p></div><div id="id-1.4.3.3.2.9" data-id-title="VLAN Settings" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: VLAN Settings</div><p>
    As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8, using a VLAN for the admin network is
    only supported on a native/untagged VLAN. If you need VLAN support for the
    admin network, it must be handled at switch level.
   </p><p>
    When changing the network configuration with YaST or by editing
    <code class="filename">/etc/crowbar/network.json</code> you can define VLAN
    settings for each network. For the networks <code class="literal">nova-fixed</code>
    and <code class="literal">nova-floating</code>, however, special rules apply:
   </p><p>
    <span class="bold"><strong>nova-fixed</strong></span>: The <span class="guimenu">USE
    VLAN</span> setting will be ignored. However, VLANs will automatically
    be used if deploying neutron with VLAN support (using the plugins
    linuxbridge, openvswitch plus VLAN, or cisco plus VLAN). In this case, you
    need to specify a correct <span class="guimenu">VLAN ID</span> for this network.
   </p><p>
    <span class="bold"><strong>nova-floating</strong></span>: When using a VLAN for
    <code class="literal">nova-floating</code> (which is the default), the <span class="guimenu">USE
    VLAN</span> and <span class="guimenu">VLAN ID</span> settings for
    <span class="guimenu">nova-floating</span> and <span class="guimenu">public</span> must be
    the same. When not using a VLAN for <code class="literal">nova-floating</code>, it
    must have a different physical network interface than the
    <code class="literal">nova_fixed</code> network.
   </p></div><div id="id-1.4.3.3.2.10" data-id-title="No IPv6 Support" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: No IPv6 Support</div><p>
    As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8, IPv6 is not supported. This applies to the cloud
    internal networks and to the instances. You must use static IPv4 addresses
    for all network interfaces on the Admin Node, and disable IPv6 before
    deploying Crowbar on the Admin Node.
   </p></div><p>
   The following diagram shows the pre-defined SUSE <span class="productname">OpenStack</span> Cloud network in more
   detail. It demonstrates how the <span class="productname">OpenStack</span> nodes and services use the
   different networks.
  </p><div class="figure" id="id-1.4.3.3.2.12"><div class="figure-contents"><div class="mediaobject"><a href="images/cloud_network_detail.png"><img src="images/cloud_network_detail.png" width="100%" alt="SUSE OpenStack Cloud Network: Details" title="SUSE OpenStack Cloud Network: Details"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 2.2: </span><span class="title-name">SUSE <span class="productname">OpenStack</span> Cloud Network: Details </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.3.2.12">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><section class="sect2" id="sec-depl-req-network-allocation" data-id-title="Network Address Allocation"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.1.1 </span><span class="title-name">Network Address Allocation</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-network-allocation">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    The default networks set up in SUSE <span class="productname">OpenStack</span> Cloud are class C networks with 256
    IP addresses each. This limits the maximum number of instances that
    can be started simultaneously. Addresses within the networks are
    allocated as outlined in the following table. Use the YaST
    Crowbar module to make customizations (see
    <a class="xref" href="#sec-depl-adm-inst-crowbar" title="Chapter 7. Crowbar Setup">Chapter 7, <em>Crowbar Setup</em></a>). The last address in the IP address
    range of each network is always reserved as the broadcast address. This
    assignment cannot be changed.
   </p><p>For an overview of the minimum number of IP addresses needed for each
    of the ranges in the network settings, see <a class="xref" href="#tab-netw-range-ip-min" title="Minimum Number of IP Addresses for Network Ranges">Table 2.1, “Minimum Number of IP Addresses for Network Ranges”</a>.
   </p><div class="table" id="tab-netw-range-ip-min" data-id-title="Minimum Number of IP Addresses for Network Ranges"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 2.1: </span><span class="title-name">Minimum Number of IP Addresses for Network Ranges </span></span><a title="Permalink" class="permalink" href="#tab-netw-range-ip-min">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Network
        </p>
       </th><th style="border-bottom: 1px solid ; ">
        <p>
         Required Number of IP addresses
        </p>
       </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Admin Network
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>1 IP address per node (Administration Server, Control Nodes, and
           Compute Nodes)</p></li><li class="listitem"><p>1 VIP address for PostgreSQL</p></li><li class="listitem"><p>1 VIP address for RabbitMQ</p></li><li class="listitem"><p>1 VIP address per cluster (per Pacemaker barclamp proposal)</p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Public Network
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>1 IP address per node (Control Nodes and Compute Nodes)</p></li><li class="listitem"><p>1 VIP address per cluster</p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         BMC Network
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>1 IP address per node (Administration Server, Control Nodes, and
           Compute Nodes)</p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; ">
        <p>
         Software Defined Network
        </p>
       </td><td>
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>1 IP address per node (Control Nodes and
           Compute Nodes)</p></li></ul></div>
       </td></tr></tbody></table></div></div><div id="id-1.4.3.3.2.13.5" data-id-title="Limitations of the Default Network Proposal" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Limitations of the Default Network Proposal</div><p>
     The default network proposal as described below limits the maximum
     number of Compute Nodes to 80, the maximum number of floating IP
     addresses to 61 and the maximum number of addresses in the nova_fixed
     network to 204.
    </p><p>
     To overcome these limitations you need to reconfigure the network setup
     by using appropriate address ranges. Do this by either using the
     YaST Crowbar module as described in
     <a class="xref" href="#sec-depl-adm-inst-crowbar" title="Chapter 7. Crowbar Setup">Chapter 7, <em>Crowbar Setup</em></a>, or by manually editing the
     network template file as described in
     <a class="xref" href="#sec-depl-inst-admserv-post-network" title="7.5. Custom Network Configuration">Section 7.5, “Custom Network Configuration”</a>.
    </p></div><div class="table" id="id-1.4.3.3.2.13.6" data-id-title="192.168.124.0/24 (Admin/BMC) Network Address Allocation"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 2.2: </span><span class="title-name"><code class="systemitem">192.168.124.0/24</code> (Admin/BMC) Network Address Allocation </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.3.2.13.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Function
        </p>
       </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Address
        </p>
       </th><th style="border-bottom: 1px solid ; ">
        <p>
         Remark
        </p>
       </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         router
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="systemitem">192.168.124.1</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Provided externally.
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         admin
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="systemitem">192.168.124.10</code> -
         <code class="systemitem">192.168.124.11</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Fixed addresses reserved for the Administration Server.
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         DHCP
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="systemitem">192.168.124.21</code> -
         <code class="systemitem">192.168.124.80</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Address range reserved for node allocation/installation. Determines
         the maximum number of parallel allocations/installations.
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         host
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="systemitem">192.168.124.81</code> -
         <code class="systemitem">192.168.124.160</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Fixed addresses for the <span class="productname">OpenStack</span> nodes. Determines the maximum
         number of <span class="productname">OpenStack</span> nodes that can be deployed.
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         bmc vlan host
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="systemitem">192.168.124.161</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Fixed address for the BMC VLAN. Used to generate a VLAN tagged
         interface on the Administration Server that can access the BMC
         network. The BMC VLAN must be in the same ranges as BMC, and
         BMC must have VLAN enabled.
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         bmc host
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="systemitem">192.168.124.162</code> -
         <code class="systemitem">192.168.124.240</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Fixed addresses for the <span class="productname">OpenStack</span> nodes. Determines the maximum
         number of <span class="productname">OpenStack</span> nodes that can be deployed.
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; ">
        <p>
         switch
        </p>
       </td><td style="border-right: 1px solid ; ">
        <p>
         <code class="systemitem">192.168.124.241</code> -
         <code class="systemitem">192.168.124.250</code>
        </p>
       </td><td>
        <p>
         This range is not used in current releases and might be removed in
         the future.
        </p>
       </td></tr></tbody></table></div></div><div class="table" id="id-1.4.3.3.2.13.7" data-id-title="192.168.125/24 (Storage) Network Address Allocation"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 2.3: </span><span class="title-name"><code class="systemitem">192.168.125/24</code> (Storage) Network Address Allocation </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.3.2.13.7">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Function
        </p>
       </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Address
        </p>
       </th><th style="border-bottom: 1px solid ; ">
        <p>
         Remark
        </p>
       </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; ">
        <p>
         host
        </p>
       </td><td style="border-right: 1px solid ; ">
        <p>
         <code class="systemitem">192.168.125.10</code> -
         <code class="systemitem">192.168.125.239</code>
        </p>
       </td><td>
        <p>
         Each Storage Node will get an address from this range.
        </p>
       </td></tr></tbody></table></div></div><div class="table" id="id-1.4.3.3.2.13.8" data-id-title="192.168.123/24 (Private Network/nova-fixed) Network Address Allocation"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 2.4: </span><span class="title-name"><code class="systemitem">192.168.123/24</code> (Private Network/nova-fixed) Network Address Allocation </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.3.2.13.8">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Function
        </p>
       </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Address
        </p>
       </th><th style="border-bottom: 1px solid ; ">
        <p>
         Remark
        </p>
       </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; ">
        <p>
         DHCP
        </p>
       </td><td style="border-right: 1px solid ; ">
        <p>
         <code class="systemitem">192.168.123.1</code> -
         <code class="systemitem">192.168.123.254</code>
        </p>
       </td><td>
        <p>
         Address range for instances, routers and DHCP/DNS agents.
        </p>
       </td></tr></tbody></table></div></div><div class="table" id="id-1.4.3.3.2.13.9" data-id-title="192.168.126/24 (Public Network nova-floating, public) Network Address Allocation"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 2.5: </span><span class="title-name"><code class="systemitem">192.168.126/24</code> (Public Network nova-floating, public) Network Address Allocation </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.3.2.13.9">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Function
        </p>
       </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Address
        </p>
       </th><th style="border-bottom: 1px solid ; ">
        <p>
         Remark
        </p>
       </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         router
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="systemitem">192.168.126.1</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Provided externally.
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         public host
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="systemitem">192.168.126.2</code> -
         <code class="systemitem">192.168.126.127</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Public address range for external SUSE <span class="productname">OpenStack</span> Cloud components such as the
         <span class="productname">OpenStack</span> Dashboard or the API.
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; ">
        <p>
         floating host
        </p>
       </td><td style="border-right: 1px solid ; ">
        <p>
         <code class="systemitem">192.168.126.129</code> -
         <code class="systemitem">192.168.126.254</code>
        </p>
       </td><td>
        <p>
         Floating IP address range. Floating IP addresses can be manually assigned to
         a running instance to allow to access the guest from the
         outside. Determines the maximum number of instances that can
         concurrently be accessed from the outside.
        </p>
        <p>
         The nova_floating network is set up with a netmask of
         255.255.255.192, allowing a maximum number of 61 IP addresses. This
         range is pre-allocated by default and managed by neutron.
        </p>
       </td></tr></tbody></table></div></div><div class="table" id="id-1.4.3.3.2.13.10" data-id-title="192.168.130/24 (Software Defined Network) Network Address Allocation"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 2.6: </span><span class="title-name"><code class="systemitem">192.168.130/24</code> (Software Defined Network) Network Address Allocation </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.3.2.13.10">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Function
        </p>
       </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Address
        </p>
       </th><th style="border-bottom: 1px solid ; ">
        <p>
         Remark
        </p>
       </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; ">
        <p>
         host
        </p>
       </td><td style="border-right: 1px solid ; ">
        <p>
         <code class="systemitem">192.168.130.10</code> -
         <code class="systemitem">192.168.130.254</code>
        </p>
       </td><td>
        <p>
         If neutron is configured with <code class="literal">openvswitch</code>
         and <code class="literal">gre</code>, each network node and all
         Compute Nodes will get an IP address from this range.
        </p>
       </td></tr></tbody></table></div></div><div id="id-1.4.3.3.2.13.11" data-id-title="Addresses for Additional Servers" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Addresses for Additional Servers</div><p>
     Addresses not used in the ranges mentioned above can be used to add
     additional servers with static addresses to SUSE <span class="productname">OpenStack</span> Cloud. Such servers
     can be used to provide additional services. A SUSE Manager server
     inside SUSE <span class="productname">OpenStack</span> Cloud, for example, must be configured using one of
     these addresses.
    </p></div></section><section class="sect2" id="sec-depl-req-network-modes" data-id-title="Network Modes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.1.2 </span><span class="title-name">Network Modes</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-network-modes">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> supports different network modes defined in Crowbar: <code class="literal">single</code>, <code class="literal">dual</code>, and
    <code class="literal">team</code>. As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8, the networking mode
    is applied to all nodes and the Administration Server. That means that all
    machines need to meet the hardware requirements for the chosen mode. The
    network mode can be configured using the YaST Crowbar module
    (<a class="xref" href="#sec-depl-adm-inst-crowbar" title="Chapter 7. Crowbar Setup">Chapter 7, <em>Crowbar Setup</em></a>). The network mode cannot
    be changed after the cloud is deployed.
   </p><p>
    Other, more flexible network mode setups can be configured by manually
    editing the Crowbar network configuration files. See <a class="xref" href="#sec-depl-inst-admserv-post-network" title="7.5. Custom Network Configuration">Section 7.5, “Custom Network Configuration”</a> for more information. SUSE or a
    partner can assist you in creating a custom setup within the scope of a
    consulting services agreement (see <a class="link" href="http://www.suse.com/consulting/" target="_blank">http://www.suse.com/consulting/</a> for more information on
    SUSE consulting).
   </p><div id="id-1.4.3.3.2.14.4" data-id-title="Network Device Bonding is Required for HA" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Network Device Bonding is Required for HA</div><p>Network device bonding is required for an HA setup of
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. If you are planning to move your cloud to an
     HA setup at a later point in time, make sure to use a network
     mode in the YaST Crowbar that supports network device bonding.</p><p>Otherwise a migration to an HA setup is not supported.</p></div><section class="sect3" id="sec-depl-req-network-modes-single" data-id-title="Single Network Mode"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">2.1.2.1 </span><span class="title-name">Single Network Mode</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-network-modes-single">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
     In single mode you use one Ethernet card for all the traffic:
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/cloud_networking_single_mode.png"><img src="images/cloud_networking_single_mode.png" width="50%" alt="Single Network Mode" title="Single Network Mode"/></a></div></div></section><section class="sect3" id="sec-depl-req-network-modes-dual" data-id-title="Dual Network Mode"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">2.1.2.2 </span><span class="title-name">Dual Network Mode</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-network-modes-dual">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
     Dual mode needs two Ethernet cards (on all nodes but Administration Server) to completely separate traffic between the Admin Network and the public network:
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/cloud_networking_dual_mode.png"><img src="images/cloud_networking_dual_mode.png" width="70%" alt="Dual Network Mode" title="Dual Network Mode"/></a></div></div></section><section class="sect3" id="sec-depl-req-network-modes-teaming" data-id-title="Team Network Mode"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">2.1.2.3 </span><span class="title-name">Team Network Mode</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-network-modes-teaming">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
     Team mode is similar to single mode, except that you
     combine several Ethernet cards to a <span class="quote">“<span class="quote">bond</span>”</span> (network device
     bonding). Team mode needs two or more Ethernet cards.
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/cloud_networking_team_mode.png"><img src="images/cloud_networking_team_mode.png" width="50%" alt="Team Network Mode" title="Team Network Mode"/></a></div></div><p>
     When using team mode, you must choose a <span class="quote">“<span class="quote">bonding
     policy</span>”</span> that defines how to use the combined Ethernet cards. You
     can either set them up for fault tolerance, performance (load balancing),
     or a combination of both.
    </p></section></section><section class="sect2" id="sec-depl-req-network-bastion" data-id-title="Accessing the Administration Server via a Bastion Network"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.1.3 </span><span class="title-name">Accessing the Administration Server via a Bastion Network</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-network-bastion">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    Enabling access to the Administration Server from another network requires an external gateway. This option offers
    maximum flexibility, but requires additional hardware and may be less
    secure than you require. Therefore SUSE <span class="productname">OpenStack</span> Cloud offers a second option for
    accessing the Administration Server: the bastion network. You only need a
    dedicated Ethernet card and a static IP address from the external
    network to set it up.
   </p><p>
    The bastion network setup (see <a class="xref" href="#sec-depl-adm-inst-crowbar-mode-bastion" title="7.3.1. Setting Up a Bastion Network">Section 7.3.1, “Setting Up a Bastion Network”</a> for setup instructions)
    enables logging in to the Administration Server via SSH from the company network. A
    direct login to other nodes in the cloud is not possible. However, the
    Administration Server can act as a <span class="quote">“<span class="quote">jump host</span>”</span>: First
    log in to the Administration Server via SSH, then log in via SSH to
    other nodes.
   </p></section><section class="sect2" id="sec-depl-req-network-dns" data-id-title="DNS and Host Names"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.1.4 </span><span class="title-name">DNS and Host Names</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-network-dns">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    The Administration Server acts as a name server for all nodes in the cloud. If
    the Administration Server has access to the outside, then you can add additional
    name servers that are automatically used to forward requests. If
    additional name servers are found on your cloud deployment, the name server
    on the Administration Server is automatically configured to forward requests
    for non-local records to these servers.
   </p><p>
    The Administration Server must have a fully qualified host
    name. The domain name you specify is used for the DNS zone. It is
    required to use a sub-domain such as
    <em class="replaceable">cloud.example.com</em>. The Administration Server
    must have authority over the domain it is on so that it can
    create records for discovered nodes. As a result, it will not forward
    requests for names it cannot resolve in this domain, and thus cannot
    resolve names for the second-level domain, .e.g. <em class="replaceable">example.com</em>, other
    than for nodes in the cloud.
   </p><p>
    This host name must not be changed after SUSE <span class="productname">OpenStack</span> Cloud has been deployed.
    The <span class="productname">OpenStack</span> nodes are named after their MAC address by default,
    but you can provide aliases, which are easier to remember when
    allocating the nodes. The aliases for the <span class="productname">OpenStack</span> nodes can be
    changed at any time. It is useful to have a list of MAC addresses and
    the intended use of the corresponding host at hand when deploying the
    <span class="productname">OpenStack</span> nodes.
   </p></section></section><section class="sect1" id="sec-depl-req-storage" data-id-title="Persistent Storage"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.2 </span><span class="title-name">Persistent Storage</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-storage">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   When talking about <span class="quote">“<span class="quote">persistent storage</span>”</span> on <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>,
   there are two completely different aspects to discuss: 1) the block and
   object storage services SUSE <span class="productname">OpenStack</span> Cloud offers, 2) the
   hardware related storage aspects on the different node types.
  </p><div id="id-1.4.3.3.3.3" data-id-title="Persistent vs. Ephemeral Storage" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Persistent vs. Ephemeral Storage</div><p>
    Block and object storage are persistent storage models where files or
    images are stored until they are explicitly deleted. SUSE <span class="productname">OpenStack</span> Cloud also
    offers ephemeral storage for images attached to instances. These
    ephemeral images only exist during the life of an instance and are
    deleted when the guest is terminated. See
    <a class="xref" href="#sec-depl-req-storage-hardware-compute" title="2.2.2.3. Compute Nodes">Section 2.2.2.3, “Compute Nodes”</a> for more
    information.
   </p></div><section class="sect2" id="sec-depl-req-storage-services" data-id-title="Cloud Storage Services"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.2.1 </span><span class="title-name">Cloud Storage Services</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-storage-services">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    SUSE <span class="productname">OpenStack</span> Cloud offers two different types of services
    for persistent storage: object and block storage. Object storage lets
    you upload and download files (similar to an FTP server), whereas a
    block storage provides mountable devices (similar to a hard disk
    partition). SUSE <span class="productname">OpenStack</span> Cloud provides a repository to store the
    virtual disk images used to start instances.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.3.3.4.3.1"><span class="term">Object Storage with swift</span></dt><dd><p>
       The <span class="productname">OpenStack</span> object storage service is called swift. The
       storage component of swift (swift-storage) must be
       deployed on dedicated nodes where no other cloud services run. Deploy at
       least two swift nodes to provide redundant storage. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> is configured to
       always use all unused disks on a node for storage.
      </p><p>
       swift can optionally be used by glance, the service
       that manages the images used to boot the instances. Offering
       object storage with swift is optional.
      </p></dd><dt id="id-1.4.3.3.3.4.3.2"><span class="term">Block Storage</span></dt><dd><p>
       Block storage on SUSE <span class="productname">OpenStack</span> Cloud is provided by cinder.
       cinder can use a variety of storage back-ends, including
       network storage solutions like NetApp or EMC. It is also possible to
       use local disks for block storage. A list of drivers available for
       cinder and the features supported for each driver is
       available from the <em class="citetitle">CinderSupportMatrix</em> at
       <a class="link" href="https://wiki.openstack.org/wiki/CinderSupportMatrix" target="_blank">https://wiki.openstack.org/wiki/CinderSupportMatrix</a>.
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 9 ships with <span class="productname">OpenStack</span>
       Rocky.
      </p><p>
       Alternatively, cinder can use Ceph RBD as a back-end.  Ceph
       offers data security and speed by storing the the content on a dedicated
       Ceph cluster.
      </p></dd><dt id="id-1.4.3.3.3.4.3.3"><span class="term">The glance Image Repository</span></dt><dd><p>
       glance provides a catalog and repository for virtual disk images
       used to start the instances. glance is installed on a
       Control Node. It uses swift, Ceph, or a
       directory on the Control Node to store the images. The image
       directory can either be a local directory or an NFS share.
      </p></dd></dl></div></section><section class="sect2" id="sec-depl-req-storage-hardware" data-id-title="Storage Hardware Requirements"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.2.2 </span><span class="title-name">Storage Hardware Requirements</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-storage-hardware">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    Each node in SUSE <span class="productname">OpenStack</span> Cloud needs sufficient disk space to store both the operating system and  additional data.
    Requirements and recommendations for the various node types are listed
    below.
   </p><div id="id-1.4.3.3.3.5.3" data-id-title="Choose a Hard Disk for the Operating System Installation" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Choose a Hard Disk for the Operating System Installation</div><p>
     The operating system will always be installed on the
     <span class="emphasis"><em>first</em></span> hard disk. This is the disk that is listed
     <span class="emphasis"><em>first</em></span> in the BIOS, the one from which the machine
     will boot. Make sure that the hard disk the operating system is installed on will be recognized as the first disk.
    </p></div><section class="sect3" id="sec-depl-req-storage-hardware-admin" data-id-title="Administration Server"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">2.2.2.1 </span><span class="title-name">Administration Server</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-storage-hardware-admin">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
     If you store the update repositories directly on the Administration Server (see
     <a class="xref" href="#sec-depl-req-repos" title="2.5.2. Product and Update Repositories">Section 2.5.2, “Product and Update Repositories”</a>), we recommend mounting <code class="filename">/srv</code> on a separate partition or volume with a minimum of 30 GB space.
    </p><p>
     Log files from all nodes in SUSE <span class="productname">OpenStack</span> Cloud are stored on the Administration Server
     under <code class="filename">/var/log</code>.
     The message service RabbitMQ requires 1 GB of free space
     in <code class="filename">/var</code> (see
     <span class="intraxref">Book “<em class="citetitle">Operations Guide Crowbar</em>”, Chapter 5 “Log Management”, Section 5.4 “Log Files”, Section 5.4.1 “On the Administration Server”</span> for a complete list).
    </p></section><section class="sect3" id="sec-depl-req-storage-hardware-control" data-id-title="Control Nodes"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">2.2.2.2 </span><span class="title-name">Control Nodes</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-storage-hardware-control">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
     Depending on how the services are set up, glance and
     cinder may require additional disk space on the
     Control Node on which they are running. glance may be configured
     to use a local directory, whereas cinder may use a local
     image file for storage. For performance and scalability reasons this is
     only recommended for test setups. Make sure there is sufficient free
     disk space available if you use a local file for storage.
    </p><p>
     cinder may be configured to use local disks for storage (configuration option <code class="literal">raw</code>). If you choose this setup, we recommend deploying the <span class="guimenu">cinder-volume</span> role to one or more dedicated Control Nodes. Those should be equipped with several disks providing sufficient storage space. It may also be necessary to equip this node with two or more bonded network cards, since it will generate heavy network traffic. Bonded network cards require a special setup for this node. For details, refer to <a class="xref" href="#sec-depl-inst-admserv-post-network" title="7.5. Custom Network Configuration">Section 7.5, “Custom Network Configuration”</a>.
    </p></section><section class="sect3" id="sec-depl-req-storage-hardware-compute" data-id-title="Compute Nodes"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">2.2.2.3 </span><span class="title-name">Compute Nodes</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-storage-hardware-compute">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
     Unless an instance is started via “Boot from Volume”, it is started with at least one disk, which is a copy of the image from which it has been started. Depending on the flavor you start, the instance may also have a second, so-called <span class="quote">“<span class="quote">ephemeral</span>”</span>
     disk. The size of the root disk depends on the image itself.
     Ephemeral disks are always created as sparse image files that grow up
     to a defined size as they are <span class="quote">“<span class="quote">filled</span>”</span>. By default
     ephemeral disks have a size of 10 GB.
    </p><p>
     Both disks, root images and ephemeral disk, are directly bound to the
     instance and are deleted when the instance is terminated.
     These disks are bound to the Compute Node on which the
     instance has been started. The disks are created under
     <code class="filename">/var/lib/nova</code> on the Compute Node. Your
     Compute Nodes should be equipped with enough disk space to store the
     root images and ephemeral disks.
    </p><div id="id-1.4.3.3.3.5.6.4" data-id-title="Ephemeral Disks vs. Block Storage" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Ephemeral Disks vs. Block Storage</div><p>
      Do not confuse ephemeral disks with persistent block storage. In
      addition to an ephemeral disk, which is automatically provided with
      most instance flavors, you can optionally add a persistent storage
      device provided by cinder. Ephemeral disks are deleted when
      the instance terminates, while persistent storage devices can be
      reused in another instance.
     </p></div><p>
     The maximum disk space required on a compute node depends on the
     available flavors. A flavor specifies the number of CPUs, RAM, and disk
     size of an instance. Several flavors ranging from
     <span class="guimenu">tiny</span> (1 CPU, 512 MB RAM, no ephemeral disk) to
     <span class="guimenu">xlarge</span> (8 CPUs, 8 GB RAM, 10 GB ephemeral disk) are
     available by default. Adding custom flavors, and editing and deleting
     existing flavors is also supported.
    </p><p>
     To calculate the minimum disk space needed on a compute node, you need
     to determine the highest disk-space-to-RAM ratio from your flavors.
     For example:
    </p><table style="border: 0; " class="simplelist"><tr><td>
      Flavor small: 2 GB RAM, 100 GB ephemeral disk =&gt; 50 GB disk /1 GB RAM
     </td></tr><tr><td>
      Flavor large: 8 GB RAM, 200 GB ephemeral disk =&gt; 25 GB disk /1 GB RAM
     </td></tr></table><p>
     So, 50 GB disk /1 GB RAM is the ratio that matters. If you multiply
     that value by the amount of RAM in GB available on your compute node,
     you have the minimum disk space required by ephemeral disks. Pad that
     value with sufficient space for the root disks plus a buffer to leave room for flavors with a higher disk-space-to-RAM ratio in
     the future.
    </p><div id="id-1.4.3.3.3.5.6.9" data-id-title="Overcommitting Disk Space" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Overcommitting Disk Space</div><p>
      The scheduler that decides in which node an instance is started
      does not check for available disk space. If there is no disk space
      left on a compute node, this will not only cause data loss on the
      instances, but the compute node itself will also stop operating.
      Therefore you must make sure all compute nodes are equipped with
      enough hard disk space.
     </p></div></section><section class="sect3" id="sec-depl-req-storage-hardware-store" data-id-title="Storage Nodes (optional)"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">2.2.2.4 </span><span class="title-name">Storage Nodes (optional)</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-storage-hardware-store">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p> The block storage service
    Ceph RBD and the object storage service swift need to be deployed onto
    dedicated nodes—it is not possible to mix these services. The swift
    component requires at least two machines (more are recommended) to store
    data redundantly. For information on hardware requirements for Ceph, see
    <a class="link" href="https://documentation.suse.com/ses/5.5/single-html/ses-deployment/#storage-bp-hwreq" target="_blank">https://documentation.suse.com/ses/5.5/single-html/ses-deployment/#storage-bp-hwreq</a>
    </p><p>
     Each Ceph/swift Storage Node needs at least two hard disks.
     The first one will be used for the operating system installation, while
     the others can be used for storage. We recommend equipping
     the storage nodes with as many disks as possible.
    </p><p>
     Using RAID on swift storage nodes is not supported.
     swift takes care of redundancy and replication on its own. Using
     RAID with swift would also result in a huge performance
     penalty.
    </p></section></section></section><section class="sect1" id="sec-depl-req-ssl" data-id-title="SSL Encryption"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.3 </span><span class="title-name">SSL Encryption</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-ssl">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   Whenever non-public data travels over a network it must be encrypted.
   Encryption protects the integrity and confidentiality of data. Therefore
   you should enable SSL support when deploying SUSE <span class="productname">OpenStack</span> Cloud to production. (SSL
   is not enabled by default as it requires you to provide certificates.)
   The following services (and their APIs, if available) can use SSL:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     cinder
    </p></li><li class="listitem"><p>
     horizon
    </p></li><li class="listitem"><p>
     glance
    </p></li><li class="listitem"><p>
     heat
    </p></li><li class="listitem"><p>
     keystone
    </p></li><li class="listitem"><p>
     manila
    </p></li><li class="listitem"><p>
     neutron
    </p></li><li class="listitem"><p>
     nova
    </p></li><li class="listitem"><p>
     swift
    </p></li><li class="listitem"><p>
     VNC
    </p></li><li class="listitem"><p>
    RabbitMQ
    </p></li><li class="listitem"><p>
    ironic
    </p></li><li class="listitem"><p>
    Magnum
    </p></li></ul></div><p>
   You have two options for deploying your SSL certificates. You may use a single shared certificate for all services on each node, or provide individual certificates for each service. The minimum requirement is a single certificate for the Control Node and all services installed on it.
  </p><p>
   Certificates must be signed by a trusted authority. Refer to
   <a class="link" href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-admin/#sec-apache2-ssl" target="_blank">https://documentation.suse.com/sles/15-SP1/single-html/SLES-admin/#sec-apache2-ssl</a>
   for instructions on how to create and sign them.
  </p><div id="id-1.4.3.3.4.6" data-id-title="Host Names" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Host Names</div><p>
    Each SSL certificate is issued for a certain host name and, optionally,
    for alternative host names (via the <code class="literal">AlternativeName</code>
    option). Each publicly available node in SUSE <span class="productname">OpenStack</span> Cloud has two host
    names—an internal and a public one. The SSL certificate needs
    to be issued for <span class="emphasis"><em>both</em></span> internal and public names.
   </p><p>
    The internal name has the following scheme:
   </p><div class="verbatim-wrap"><pre class="screen">d<em class="replaceable">MACADDRESS</em>.<em class="replaceable">FQDN</em></pre></div><p>
    <em class="replaceable">MACADDRESS</em> is the MAC address of the
    interface used to boot the machine via PXE. All letters are turned
    lowercase and all colons are replaced with dashes. For example,
    <code class="literal">00-00-5E-00-53-00</code>. <em class="replaceable">FQDN</em> is
    the fully qualified domain name. An example name looks like this:
   </p><div class="verbatim-wrap"><pre class="screen">d00-00-5E-00-53-00.example.com</pre></div><p>
    Unless you have entered a custom <span class="guimenu">Public Name</span> for a
    client (see <a class="xref" href="#sec-depl-inst-nodes-install" title="11.2. Node Installation">Section 11.2, “Node Installation”</a> for details),
    the public name is the same as the internal name prefixed by
    <code class="literal">public</code>:
   </p><div class="verbatim-wrap"><pre class="screen">public-d00-00-5E-00-53-00.example.com</pre></div><p>
    To look up the node names open the Crowbar Web interface and click the
    name of a node in the <span class="guimenu">Node Dashboard</span>. The names are
    listed as <span class="guimenu">Full Name</span> and <span class="guimenu">Public
    Name</span>.
   </p></div></section><section class="sect1" id="sec-depl-req-hardware" data-id-title="Hardware Requirements"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.4 </span><span class="title-name">Hardware Requirements</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-hardware">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   Precise hardware requirements can only be listed for the Administration Server and
   the <span class="productname">OpenStack</span> Control Node. The requirements of the <span class="productname">OpenStack</span>
   Compute and Storage Nodes depends on the number of concurrent
   instances and their virtual hardware equipment.
  </p><p>
   A minimum of three machines are required for a SUSE <span class="productname">OpenStack</span> Cloud:
   one Administration Server, one Control Node, and one Compute Node. You also need a gateway providing access to the public network.
   Deploying storage requires additional nodes: at least two nodes for
   swift and a minimum of four nodes for Ceph.
  </p><div id="id-1.4.3.3.5.4" data-id-title="Virtual/Physical Machines and Architecture" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Virtual/Physical Machines and Architecture</div><p>
    Deploying SUSE <span class="productname">OpenStack</span> Cloud functions to virtual machines is only supported for the
    Administration Server—all other nodes need to be physical hardware. Although the
    Control Node can be virtualized in test environments, this is not
    supported for production systems.
   </p><p>
    SUSE <span class="productname">OpenStack</span> Cloud currently only runs on <code class="literal">x86_64</code> hardware.
   </p></div><section class="sect2" id="sec-depl-req-hardware-admserv" data-id-title="Administration Server"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.4.1 </span><span class="title-name">Administration Server</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-hardware-admserv">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Architecture: x86_64.
     </p></li><li class="listitem"><p>
      RAM: at least 4 GB, 8 GB recommended. The demand for memory depends on
      the total number of nodes in SUSE <span class="productname">OpenStack</span> Cloud—the higher the number of
      nodes, the more RAM is needed. A deployment with 50 nodes requires a
      minimum of 24 GB RAM for each Control Node.
     </p></li><li class="listitem"><p>
      Hard disk: at least 50 GB. We recommend putting
      <code class="filename">/srv</code> on a separate partition with at least
      additional 30 GB of space. Alternatively, you can mount the update
      repositories from another server (see <a class="xref" href="#sec-depl-req-repos" title="2.5.2. Product and Update Repositories">Section 2.5.2, “Product and Update Repositories”</a> for details).
     </p></li><li class="listitem"><p>
      Number of network cards: 1 for single and dual mode, 2 or more for
      team mode. Additional networks such as the bastion network and/or a
      separate BMC network each need an additional network card. See
      <a class="xref" href="#sec-depl-req-network" title="2.1. Network">Section 2.1, “Network”</a> for details.
     </p></li><li class="listitem"><p>
      Can be deployed on physical hardware or a virtual machine.
     </p></li></ul></div></section><section class="sect2" id="sec-depl-req-hardware-contrnode" data-id-title="Control Node"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.4.2 </span><span class="title-name">Control Node</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-hardware-contrnode">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Architecture: x86_64.
     </p></li><li class="listitem"><p>
      RAM: at least 8 GB, 12 GB when deploying a single Control Node, and 32 GB
      recommended.
     </p></li><li class="listitem"><p>
      Number of network cards: 1 for single mode, 2 for dual mode, 2 or more
      for team mode. See <a class="xref" href="#sec-depl-req-network" title="2.1. Network">Section 2.1, “Network”</a> for details.
     </p></li><li class="listitem"><p>
      Hard disk: See
      <a class="xref" href="#sec-depl-req-storage-hardware-control" title="2.2.2.2. Control Nodes">Section 2.2.2.2, “Control Nodes”</a>.
     </p></li></ul></div></section><section class="sect2" id="sec-depl-req-hardware-compnode" data-id-title="Compute Node"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.4.3 </span><span class="title-name">Compute Node</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-hardware-compnode">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    The Compute Nodes need to be equipped with a sufficient amount of RAM
    and CPUs, matching the numbers required by the maximum number of
    instances running concurrently. An instance started in
    SUSE <span class="productname">OpenStack</span> Cloud cannot share resources from several physical nodes. It uses
    the resources of the node on which it was started. So if you
    offer a flavor (see <a class="xref" href="#gloss-flavor" title="Flavor">Flavor</a> for a definition)
    with 8 CPUs and 12 GB RAM, at least one of your nodes should be able to
    provide these resources. Add 1 GB RAM for every two nodes
    (including Control Nodes and Storage Nodes) deployed in your cloud.
   </p><p>
    See <a class="xref" href="#sec-depl-req-storage-hardware-compute" title="2.2.2.3. Compute Nodes">Section 2.2.2.3, “Compute Nodes”</a> for storage
    requirements.
   </p></section><section class="sect2" id="sec-depl-req-hardware-stornode" data-id-title="Storage Node"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.4.4 </span><span class="title-name">Storage Node</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-hardware-stornode">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    Usually a single CPU and a minimum of 4 GB RAM are sufficient for the Storage Nodes. Memory requirements increase depending on the total number of
    nodes in SUSE <span class="productname">OpenStack</span> Cloud—the higher the number of nodes, the more RAM you need. A deployment with 50 nodes requires a minimum of 20 GB for each
    Storage Node. If you use Ceph as storage, the storage nodes should be
    equipped with an additional 2 GB RAM per OSD (Ceph object storage
    daemon).
   </p><p>
    For storage requirements, see <a class="xref" href="#sec-depl-req-storage-hardware-store" title="2.2.2.4. Storage Nodes (optional)">Section 2.2.2.4, “Storage Nodes (optional)”</a>.
   </p></section><section class="sect2" id="sec-depl-req-hardware-monnode" data-id-title="monasca Node"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.4.5 </span><span class="title-name">monasca Node</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-hardware-monnode">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
     The monasca Node is a dedicated physical machine that runs the
     <code class="literal">monasca-server</code> role. This node is used for
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring.  Hardware requirements for the monasca Node are as
     follows:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Architecture: x86_64
     </p></li><li class="listitem"><p>
       RAM: At least 32 GB, 64 GB or more is recommended
     </p></li><li class="listitem"><p>
       CPU: At least 8 cores, 16 cores or more is recommended
     </p></li><li class="listitem"><p>
       Hard Disk: SSD is strongly recommended
     </p></li></ul></div><p>
     The following formula can be used to calculate the required disk space:
   </p><div class="verbatim-wrap"><pre class="screen">200 GB + ["number of nodes" * "retention period" * ("space for log
   data/day" + "space for metrics data/day") ]</pre></div><p>
    The recommended values for the formula are as follows:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Retention period = 60 days for InfluxDB and Elasticsearch
     </p></li><li class="listitem"><p>
       Space for daily log data = 2GB
     </p></li><li class="listitem"><p>
       Space for daily metrics data = 50MB
     </p></li></ul></div><p>
     The formula is based on the following log data assumptions:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Approximately 50 log files per node
     </p></li><li class="listitem"><p>
       Approximately 1 log entry per file per sec
     </p></li><li class="listitem"><p>
       200 bytes in size
     </p></li></ul></div><p>
     The formula is based on the following metrics data assumptions:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       400 metrics per node
     </p></li><li class="listitem"><p>
      Time interval of 30 seconds
     </p></li><li class="listitem"><p>
      20 bytes in size
     </p></li></ul></div><p>
    The formula provides only a rough estimation of the required disk
    space. There are several factors that can affect disk space
    requirements. This includes the exact combination of services that run on
    your <span class="productname">OpenStack</span> node actual cloud usage pattern, and whether any or all
    services have debug logging enabled.
   </p></section></section><section class="sect1" id="sec-depl-req-software" data-id-title="Software Requirements"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.5 </span><span class="title-name">Software Requirements</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-software">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   All nodes and the Administration Server in SUSE <span class="productname">OpenStack</span> Cloud run on SUSE Linux Enterprise Server 12 SP4. Subscriptions for
   the following components are available as one- or three-year subscriptions
   including priority support:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Control Node + <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Administration Server (including
     entitlements for High Availability and SUSE Linux Enterprise Server 12 SP4)
    </p></li><li class="listitem"><p>
     Additional <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Control Node (including
     entitlements for High Availability and SUSE Linux Enterprise Server 12 SP4)
    </p></li><li class="listitem"><p>
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Compute Node (excluding entitlements for High Availability and
     SUSE Linux Enterprise Server 12 SP4)
    </p></li><li class="listitem"><p>
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> swift node (excluding entitlements for High Availability
     and SUSE Linux Enterprise Server 12 SP4)
    </p></li></ul></div><p>
   SUSE Linux Enterprise Server 12 SP4, HA entitlements for Compute Nodes and swift Storage Nodes, and entitlements for guest operating systems need
   to be purchased separately. Refer to
   <a class="link" href="http://www.suse.com/products/suse-openstack-cloud/how-to-buy/" target="_blank">http://www.suse.com/products/suse-openstack-cloud/how-to-buy/</a>
   for more information on licensing and pricing.
  </p><p>
   Running an external Ceph cluster (optional) with SUSE <span class="productname">OpenStack</span> Cloud  requires an additional SUSE Enterprise Storage
   subscription. Refer to <a class="link" href="https://www.suse.com/products/suse-enterprise-storage/" target="_blank">https://www.suse.com/products/suse-enterprise-storage/</a> and
   <a class="link" href="https://www.suse.com/products/suse-openstack-cloud/frequently-asked-questions" target="_blank">https://www.suse.com/products/suse-openstack-cloud/frequently-asked-questions</a>
   for more information.
  </p><div id="id-1.4.3.3.6.6" data-id-title="SUSE Account" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: SUSE Account</div><p>
    A SUSE account is needed for product registration and access to
    update repositories. If you do not already have one, go to
    <a class="link" href="http://www.suse.com/" target="_blank">http://www.suse.com/</a> to create it.
   </p></div><section class="sect2" id="sec-depl-req-software-optional" data-id-title="Optional Component: SUSE Enterprise Storage"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.5.1 </span><span class="title-name">Optional Component: SUSE Enterprise Storage</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-software-optional">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> can be extended by SUSE Enterprise Storage for setting up a Ceph cluster
    providing block storage services. To store virtual disks for instances, SUSE <span class="productname">OpenStack</span> Cloud uses block storage provided by the cinder
    module. cinder itself needs a back-end providing storage. In
    production environments this usually is a network storage
    solution. cinder can use a variety of network storage back-ends, among them solutions from EMC, Fujitsu, or NetApp. In case your
    organization does not provide a network storage solution that can be used
    with SUSE <span class="productname">OpenStack</span> Cloud, you can set up a Ceph cluster with SUSE Enterprise Storage. SUSE Enterprise Storage
    provides a reliable and fast distributed storage architecture using
    commodity hardware platforms.
   </p></section><section class="sect2" id="sec-depl-req-repos" data-id-title="Product and Update Repositories"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.5.2 </span><span class="title-name">Product and Update Repositories</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-repos">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    You need seven software repositories to deploy SUSE <span class="productname">OpenStack</span> Cloud and to keep a running SUSE <span class="productname">OpenStack</span> Cloud up-to-date. This includes the static product repositories, which do not change over the
    product life cycle, and the update repositories, which constantly change.
    The following repositories are needed:
   </p><div class="variablelist"><div class="title-container"><div class="variablelist-title-wrap"><div class="variablelist-title"><span class="title-number-name"><span class="title-name">Mandatory Repositories </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.3.6.8.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><dl class="variablelist"><dt id="id-1.4.3.3.6.8.3.2"><span class="term">SUSE Linux Enterprise Server 12 SP4 Product</span></dt><dd><p>
       The SUSE Linux Enterprise Server 12 SP4 product repository is a copy of the installation
       media (DVD #1) for SUSE Linux Enterprise Server. As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>
       8, it is required to have it available locally on the
       Administration Server. This repository requires approximately 3.5 GB of hard
       disk space.
      </p></dd><dt id="id-1.4.3.3.6.8.3.3"><span class="term"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 9 Product</span></dt><dd><p>
       The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 9 product repository is a copy
       of the installation media (DVD #1) for SUSE <span class="productname">OpenStack</span> Cloud. It can either be
       made available remotely via HTTP, or locally on the Administration Server. We recommend the latter since it makes the setup of the Administration Server
       easier. This repository requires approximately 500 MB of hard disk
       space.
      </p></dd><dt id="id-1.4.3.3.6.8.3.4"><span class="term">PTF</span></dt><dd><p>
       This repository is created automatically on the Administration Server when you install the
       SUSE <span class="productname">OpenStack</span> Cloud add-on product. It serves as a repository for
       <span class="quote">“<span class="quote">Program Temporary Fixes</span>”</span> (PTF), which are part of the
       SUSE support program.
      </p></dd><dt id="id-1.4.3.3.6.8.3.5"><span class="term">SLES12-SP4-Pool and <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-9</span></span>-Pool</span></dt><dd><p>
       The SUSE Linux Enterprise Server and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> repositories contain all binary
       RPMs from the installation media, plus pattern information and
       support status metadata. These repositories are served from SUSE Customer Center
       and need to be kept in synchronization with their sources. Make them  available remotely via an existing SMT or SUSE Manager
       server. Alternatively, make them available locally on the Administration Server
       by installing a local SMT server, by mounting or synchronizing a
       remote directory, or by copying them.
      </p></dd><dt id="id-1.4.3.3.6.8.3.6"><span class="term">SLES12-SP4-Updates and <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-9</span></span>-Updates</span></dt><dd><p>
       These repositories contain maintenance updates to packages in the
       corresponding Pool repositories. These repositories are served from
       SUSE Customer Center and need to be kept synchronized with their sources. Make them available remotely via an existing SMT or
       SUSE Manager server, or locally on the Administration Server by installing a
       local SMT server, by mounting or synchronizing a remote
       directory, or by regularly copying them.
      </p></dd></dl></div><p>
    As explained in <a class="xref" href="#sec-depl-req-ha" title="2.6. High Availability">Section 2.6, “High Availability”</a>, Control Nodes in SUSE <span class="productname">OpenStack</span> Cloud
    can optionally be made highly available with the SUSE Linux Enterprise
    High Availability Extension. The following repositories are
    required to deploy SLES High Availability Extension nodes:
   </p><div class="variablelist"><div class="title-container"><div class="variablelist-title-wrap"><div class="variablelist-title"><span class="title-number-name"><span class="title-name">Optional Repositories </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.3.6.8.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><dl class="variablelist"><dt id="id-1.4.3.3.6.8.5.2"><span class="term">SLE-HA12-SP4-Pool</span></dt><dd><p>
       The pool repositories contain all binary RPMs from the installation
       media, plus pattern information and support status metadata. These
       repositories are served from SUSE Customer Center and need to be kept in
       synchronization with their sources. Make them available
       remotely via an existing SMT or SUSE Manager server. Alternatively, make
       them available locally on the Administration Server by installing a local SMT server, by
       mounting or synchronizing a remote directory, or by copying them.
      </p></dd><dt id="id-1.4.3.3.6.8.5.3"><span class="term">SLE-HA12-SP4-Updates</span></dt><dd><p>
       These repositories contain maintenance updates to packages in the
       corresponding pool repositories. These repositories are served from
       SUSE Customer Center and need to be kept synchronized with their sources. Make them
       available remotely via an existing SMT or SUSE Manager server, or locally
       on the Administration Server by installing a local SMT server, by mounting or
       synchronizing a remote directory, or by regularly copying them.
      </p></dd></dl></div><p>
    The product repositories for SUSE Linux Enterprise Server 12 SP4 and
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 9 do not change during the life cycle
    of a product. Thus, they can be copied to the destination directory from the
    installation media. However, the pool and update repositories must be
    kept synchronized with their sources on the SUSE Customer Center. SUSE
    offers two products that synchronize repositories and make
    them available within your organization: SUSE Manager
    (<a class="link" href="http://www.suse.com/products/suse-manager/" target="_blank">http://www.suse.com/products/suse-manager/</a>, and
    Subscription Management Tool (which ships with SUSE Linux Enterprise Server 12 SP4).
   </p><p>
    All repositories must be served via HTTP to be
    available for SUSE <span class="productname">OpenStack</span> Cloud deployment. Repositories that are installed on the Administration Server are made available by the Apache Web
    server running on the Administration Server. If your organization already uses
    SUSE Manager or SMT, you can use the repositories provided by these
    servers.
   </p><p>
    Making the repositories locally available on the Administration Server has the
    advantage of a simple network setup within SUSE <span class="productname">OpenStack</span> Cloud, and it allows you to
    seal off the SUSE <span class="productname">OpenStack</span> Cloud network from other networks in your
    organization. Hosting the repositories on a remote server has
    the advantage of using existing resources and services, and it makes
    setting up the Administration Server much easier. However, this requires a custom network setup
    for SUSE <span class="productname">OpenStack</span> Cloud, since the Administration Server needs access to the remote
    server.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.3.6.8.9.1"><span class="term">Installing a Subscription Management Tool (SMT) Server on the Administration Server</span></dt><dd><p>
       The SMT server, shipping with SUSE Linux Enterprise Server 12 SP4, regularly
       synchronizes repository data from SUSE Customer Center with your local host.
       Installing the SMT server on the Administration Server is recommended if
       you do not have access to update repositories from elsewhere within
       your organization. This option requires the Administration Server to have Internet access.
      </p></dd><dt id="id-1.4.3.3.6.8.9.2"><span class="term">Using a Remote SMT Server</span></dt><dd><p>
       If you already run an SMT server within your organization, you
       can use it within SUSE <span class="productname">OpenStack</span> Cloud. When using a remote SMT server,
       update repositories are served directly from the SMT server.
       Each node is configured with these repositories upon its initial
       setup.
      </p><p>
       The SMT server needs to be accessible from the Administration Server and
       all nodes in SUSE <span class="productname">OpenStack</span> Cloud (via one or more gateways). Resolving the
       server's host name also needs to work.
      </p></dd><dt id="id-1.4.3.3.6.8.9.3"><span class="term">Using a SUSE Manager Server</span></dt><dd><p>
       Each client that is managed by SUSE Manager needs to register with the
       SUSE Manager server. Therefore the SUSE Manager support can only be installed
       after the nodes have been deployed. SUSE Linux Enterprise Server 12 SP4 must be set up
       for autoinstallation on the SUSE Manager server in order to use repositories
       provided by SUSE Manager during node deployment.
      </p><p>
       The server needs to be accessible from the Administration Server and all nodes
       in SUSE <span class="productname">OpenStack</span> Cloud (via one or more gateways). Resolving the server's host
       name also needs to work.
      </p></dd><dt id="id-1.4.3.3.6.8.9.4"><span class="term">Using Existing Repositories</span></dt><dd><p>
       If you can access existing repositories from within your company
       network from the Administration Server, you have the following options: mount, synchronize, or
       manually transfer these repositories to the required locations on the
       Administration Server.
      </p></dd></dl></div></section></section><section class="sect1" id="sec-depl-req-ha" data-id-title="High Availability"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.6 </span><span class="title-name">High Availability</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-ha">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   Several components and services in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> are potentially single
   points of failure that may cause system downtime and data loss if they
   fail.
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> provides various mechanisms to ensure that the crucial
   components and services are highly available. The following sections
   provide an overview of components on each node that can be made highly available. For making the Control Node functions and the
   Compute Nodes highly available, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> uses the cluster software SUSE Linux Enterprise
   High Availability Extension. Make sure to thoroughly read <a class="xref" href="#sec-depl-reg-ha-general" title="2.6.5. Cluster Requirements and Recommendations">Section 2.6.5, “Cluster Requirements and Recommendations”</a> to learn about additional requirements for high availability deployments.
  </p><section class="sect2" id="sec-depl-req-ha-admin" data-id-title="High Availability of the Administration Server"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.6.1 </span><span class="title-name">High Availability of the Administration Server</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-ha-admin">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    The Administration Server provides all services needed to manage and deploy all
    other nodes in the cloud. If the Administration Server is not available, new
    cloud nodes cannot be allocated, and you cannot add new roles to cloud
    nodes.
   </p><p>
    However, only two services on the Administration Server are single points of
    failure, without which the cloud cannot continue to run properly: DNS
    and NTP.
   </p><section class="sect3" id="sec-depl-req-ha-admin-spof" data-id-title="Administration Server—Avoiding Points of Failure"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">2.6.1.1 </span><span class="title-name">Administration Server—Avoiding Points of Failure</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-ha-admin-spof">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
     To avoid DNS and NTP as potential points of failure, deploy the roles
     <code class="systemitem">dns-server</code> and
     <code class="systemitem">ntp-server</code> to multiple nodes.
    </p><div id="id-1.4.3.3.7.4.4.3" data-id-title="Access to External Network" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Access to External Network</div><p>
      If any configured DNS forwarder or NTP external server is not
      reachable through the admin network from these nodes, allocate an
      address in the public network for each node that has the
      <code class="systemitem">dns-server</code> and
      <code class="systemitem">ntp-server</code> roles:
     </p><div class="verbatim-wrap"><pre class="screen">crowbar network allocate_ip default `hostname -f` public host</pre></div><p>
      Then the nodes can use the public gateway to reach the external
      servers. The change will only become effective after the next run of
      <code class="command">chef-client</code> on the affected nodes.
     </p></div></section></section><section class="sect2" id="sec-depl-reg-ha-control" data-id-title="High Availability of the Control Node(s)"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.6.2 </span><span class="title-name">High Availability of the Control Node(s)</span></span> <a title="Permalink" class="permalink" href="#sec-depl-reg-ha-control">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    The Control Node(s) usually run a variety of services without which
    the cloud would not be able to run properly.
   </p><section class="sect3" id="sec-depl-reg-ha-control-spof" data-id-title="Control Node(s)—Avoiding Points of Failure"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">2.6.2.1 </span><span class="title-name">Control Node(s)—Avoiding Points of Failure</span></span> <a title="Permalink" class="permalink" href="#sec-depl-reg-ha-control-spof">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
     To prevent the cloud from avoidable downtime if one or more
     Control Nodes fail, you can make the
     following roles highly available:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="systemitem">database-server</code>
       (<code class="systemitem">database</code> barclamp)
      </p></li><li class="listitem"><p>
       <code class="systemitem">keystone-server</code>
       (<code class="systemitem">keystone</code> barclamp)
      </p></li><li class="listitem"><p>
       <code class="systemitem">rabbitmq-server</code>
       (<code class="systemitem">rabbitmq</code> barclamp)
      </p></li><li class="listitem"><p>
       <code class="systemitem">swift-proxy</code> (<code class="systemitem">swift</code>
       barclamp)
      </p></li><li class="listitem"><p>
       <code class="systemitem">glance-server</code>
       (<code class="systemitem">glance</code> barclamp)
      </p></li><li class="listitem"><p>
       <code class="systemitem">cinder-controller</code>
       (<code class="systemitem">cinder</code> barclamp)
      </p></li><li class="listitem"><p>
       <code class="systemitem">neutron-server</code>
       (<code class="systemitem">neutron</code> barclamp)
      </p></li><li class="listitem"><p>
       <code class="systemitem">neutron-network</code> (<code class="systemitem">neutron</code>
       barclamp)
      </p></li><li class="listitem"><p>
       <code class="systemitem">nova-controller</code>
       (<code class="systemitem">nova</code> barclamp)
      </p></li><li class="listitem"><p>
       <code class="systemitem">nova_dashboard-server</code>
       (<code class="systemitem">nova_dashboard</code> barclamp)
      </p></li><li class="listitem"><p>
       <code class="systemitem">ceilometer-server</code>
       (<code class="systemitem">ceilometer</code> barclamp)
      </p></li><li class="listitem"><p>
       <code class="systemitem">ceilometer-control</code>
       (<code class="systemitem">ceilometer</code> barclamp)
      </p></li><li class="listitem"><p>
       <code class="systemitem">heat-server</code>
       (<code class="systemitem">heat</code> barclamp)
      </p></li><li class="listitem"><p>
       <code class="systemitem">octavia-api</code>
       (<code class="systemitem">octavia</code> barclamp)
      </p></li><li class="listitem"><p>
       <code class="systemitem">octavia-backend</code>
       (<code class="systemitem">octavia</code> barclamp)
      </p></li></ul></div><p>
     Instead of assigning these roles to individual cloud nodes, you can
     assign them to one or several High Availability clusters. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> will
     then use the Pacemaker cluster stack (shipped with the SUSE Linux Enterprise
     High Availability Extension) to manage the services. If one Control Node fails, the services
     will fail over to another
     Control Node. For details on the
     Pacemaker cluster stack and the SUSE Linux Enterprise High Availability Extension, refer to the
     <em class="citetitle">High Availability Guide</em>, available at
     <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/</a>.
     Note that SUSE Linux Enterprise High Availability Extension includes Linux Virtual Server as the load-balancer, and
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> uses HAProxy for this purpose
     (<a class="link" href="http://haproxy.1wt.eu/" target="_blank">http://haproxy.1wt.eu/</a>).
    </p><div id="id-1.4.3.3.7.5.3.5" data-id-title="Recommended Setup" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Recommended Setup</div><p>
      Though it is possible to use the same cluster for all of the roles
      above, the recommended setup is to use three clusters and to deploy
      the roles as follows:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        <code class="literal">data</code> cluster:
        <code class="systemitem">database-server</code> and
        <code class="systemitem">rabbitmq-server</code>
       </p></li><li class="listitem"><p>
        <code class="literal">network</code> cluster:
        <code class="systemitem">neutron-network</code> (as the
        <code class="systemitem">neutron-network</code> role may result in heavy network
        load and CPU impact)
       </p></li><li class="listitem"><p>
        <code class="systemitem">services</code> cluster: all other roles listed
        above (as they are related to API/schedulers)
       </p></li></ul></div></div><div id="id-1.4.3.3.7.5.3.6" data-id-title="Cluster Requirements and Recommendations" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Cluster Requirements and Recommendations</div><p>
      For setting up the clusters, some special requirements and
      recommendations apply. For details, refer to
      <a class="xref" href="#sec-depl-reg-ha-general" title="2.6.5. Cluster Requirements and Recommendations">Section 2.6.5, “Cluster Requirements and Recommendations”</a>.
     </p></div></section><section class="sect3" id="sec-depl-reg-ha-control-recover" data-id-title="Control Node(s)—Recovery"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">2.6.2.2 </span><span class="title-name">Control Node(s)—Recovery</span></span> <a title="Permalink" class="permalink" href="#sec-depl-reg-ha-control-recover">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
     Recovery of the Control Node(s) is done automatically by the cluster
     software: if one Control Node fails, Pacemaker will fail over the
     services to another Control Node. If a failed Control Node is
     repaired and rebuilt via Crowbar, it will be automatically configured
     to join the cluster. At this point Pacemaker will have the option to
     fail back services if required.
    </p></section></section><section class="sect2" id="sec-depl-reg-ha-compute" data-id-title="High Availability of the Compute Node(s)"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.6.3 </span><span class="title-name">High Availability of the Compute Node(s)</span></span> <a title="Permalink" class="permalink" href="#sec-depl-reg-ha-compute">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    If a Compute Node fails, all VMs running on that node will go down. While it cannot protect against failures of individual VMs, a
    High Availability setup for Compute Nodes helps to minimize VM downtime caused by
    Compute Node failures.  If the <code class="literal">nova-compute</code> service or
    <code class="literal">libvirtd</code> fail on a Compute Node, Pacemaker will
    try to automatically recover them.  If recovery fails, or the
    node itself should become unreachable, the node will be fenced and the
    VMs will be moved to a different Compute Node.
   </p><p>
    If you decide to use High Availability for Compute Nodes, your Compute Node will
    be run as Pacemaker remote nodes. With the <code class="literal">pacemaker-remote</code>
    service, High Availability clusters can be extended to control remote nodes without any
    impact on scalability, and without having to install the full cluster stack
    (including <code class="literal">corosync</code>) on the remote nodes.  Instead, each
    Compute Node only runs the <code class="literal">pacemaker-remote</code> service. The service
    acts as a proxy, allowing the cluster stack on the <span class="quote">“<span class="quote">normal</span>”</span>
    cluster nodes to connect to it and to control services remotely. Thus, the
    node is effectively integrated into the cluster as a remote node. In this way,
    the services running on the <span class="productname">OpenStack</span> compute nodes can be controlled from the core
    Pacemaker cluster in a lightweight, scalable fashion.
   </p><p> Find more information about the <code class="literal">pacemaker_remote</code>
    service in
    <em class="citetitle">Pacemaker Remote—Extending High Availability into
    Virtual Nodes</em>,
    available at <a class="link" href="http://www.clusterlabs.org/doc/" target="_blank">http://www.clusterlabs.org/doc/</a>. </p><p>To configure High Availability for Compute Nodes, you need to adjust the following
   barclamp proposals:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Pacemaker—for details, see <a class="xref" href="#sec-depl-ostack-pacemaker" title="12.2. Deploying Pacemaker (Optional, HA Setup Only)">Section 12.2, “Deploying Pacemaker (Optional, HA Setup Only)”</a>.</p></li><li class="listitem"><p>nova—for details, see <a class="xref" href="#sec-depl-ostack-nova-ha" title="12.11.1. HA Setup for nova">Section 12.11.1, “HA Setup for nova”</a>.</p></li></ul></div></section><section class="sect2" id="sec-depl-reg-ha-storage" data-id-title="High Availability of the Storage Node(s)"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.6.4 </span><span class="title-name">High Availability of the Storage Node(s)</span></span> <a title="Permalink" class="permalink" href="#sec-depl-reg-ha-storage">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> offers two different types of storage that can be used
    for the Storage Nodes: object storage (provided by the <span class="productname">OpenStack</span>
    swift component) and block storage (provided by Ceph).
   </p><p>
    Both already consider High Availability aspects by design, therefore it does not
    require much effort to make the storage highly available.
   </p><section class="sect3" id="sec-depl-reg-ha-storage-swift" data-id-title="swift—Avoiding Points of Failure"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">2.6.4.1 </span><span class="title-name">swift—Avoiding Points of Failure</span></span> <a title="Permalink" class="permalink" href="#sec-depl-reg-ha-storage-swift">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
     The <span class="productname">OpenStack</span> Object Storage replicates the data by design, provided
     the following requirements are met:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       The option <span class="guimenu">Replicas</span> in the swift
       barclamp is set to <code class="literal">3</code>, the tested and recommended
       value.
      </p></li><li class="listitem"><p>
       The number of Storage Nodes needs to be greater than the value set in
       the <span class="guimenu">Replicas</span> option.
      </p></li></ul></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       To avoid single points of failure, assign the
       <code class="systemitem">swift-storage</code> role to multiple nodes.
      </p></li><li class="step"><p>
       To make the API highly available, assign the
       <code class="systemitem">swift-proxy</code> role to a cluster instead of
       assigning it to a single Control Node. See
       <a class="xref" href="#sec-depl-reg-ha-control-spof" title="2.6.2.1. Control Node(s)—Avoiding Points of Failure">Section 2.6.2.1, “Control Node(s)—Avoiding Points of Failure”</a>. Other swift roles
       must not be deployed on a cluster.
      </p></li></ol></div></div></section><section class="sect3" id="sec-depl-reg-ha-storage-ceph" data-id-title="Ceph—Avoiding Points of Failure"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">2.6.4.2 </span><span class="title-name">Ceph—Avoiding Points of Failure</span></span> <a title="Permalink" class="permalink" href="#sec-depl-reg-ha-storage-ceph">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
     Ceph is a distributed storage solution that can provide High Availability.  For High Availability
     redundant storage and monitors need to be configured in the Ceph
     cluster. For more information refer to the SUSE Enterprise Storage documentation at
     <a class="link" href="https://documentation.suse.com/ses/5.5/" target="_blank">https://documentation.suse.com/ses/5.5/</a>.
    </p></section></section><section class="sect2" id="sec-depl-reg-ha-general" data-id-title="Cluster Requirements and Recommendations"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.6.5 </span><span class="title-name">Cluster Requirements and Recommendations</span></span> <a title="Permalink" class="permalink" href="#sec-depl-reg-ha-general">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    When considering setting up one or more High Availability clusters, refer to the
    chapter <em class="citetitle">System Requirements</em> in the
    <em class="citetitle">High Availability Guide</em> for SUSE Linux Enterprise High Availability Extension. The guide is available at
    <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/</a>.
   </p><p>
    The HA requirements for Control Node also apply to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. Note that by
    buying <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>, you automatically get an entitlement for
    SUSE Linux Enterprise High Availability Extension.
   </p><p>
    Especially note the following requirements:
   </p><div class="variablelist"><dl class="variablelist"><dt id="vle-ha-req-nodes"><span class="term">Number of Cluster Nodes</span></dt><dd><p>
       Each cluster needs to consist of at least three cluster nodes.
      </p><div id="id-1.4.3.3.7.8.5.1.2.2" data-id-title="Odd Number of Cluster Nodes" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Odd Number of Cluster Nodes</div><p>
        The Galera cluster needs an <span class="emphasis"><em>odd</em></span> number of cluster
        nodes with a <span class="emphasis"><em>minimum</em></span> of three nodes.
       </p><p>
        A cluster needs
        <a class="xref" href="#gloss-quorum" title="Quorum">Quorum</a> to
        keep services running. A three-node cluster can tolerate
         failure of only one node at a time, whereas a five-node cluster can
        tolerate failures of two nodes.
       </p></div></dd><dt id="vle-ha-req-stonith"><span class="term">STONITH</span></dt><dd><p>
       The cluster software will shut down <span class="quote">“<span class="quote">misbehaving</span>”</span> nodes
       in a cluster to prevent them from causing trouble. This mechanism is
       called <code class="literal">fencing</code> or
       <a class="xref" href="#gloss-stonith" title="STONITH">STONITH</a>.
      </p><div id="id-1.4.3.3.7.8.5.2.2.2" data-id-title="No Support Without STONITH" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: No Support Without STONITH</div><p>
        A cluster without STONITH is not supported.
       </p></div><p>
       For a supported HA setup, ensure the following:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         Each node in the High Availability cluster needs to have at least one
         STONITH device (usually a hardware device). We strongly
         recommend multiple STONITH devices per node, unless STONITH Block Device (SBD) is used.
        </p></li><li class="listitem"><p>
         The global cluster options <code class="systemitem">stonith-enabled</code>
         and <code class="systemitem">startup-fencing</code> must be set to
         <code class="literal">true</code>. These options are set automatically when
         deploying the <code class="systemitem">Pacemaker</code> barclamp. When you change them, you will lose support.
        </p></li><li class="listitem"><p>
         When deploying the <code class="literal">Pacemaker</code> service, select a
         <a class="xref" href="#vle-pacemaker-barcl-stonith">STONITH: Configuration mode for STONITH
    </a>
         that matches your setup. If your STONITH devices support the
         IPMI protocol, choosing the IPMI option is the easiest way to
         configure STONITH. Another alternative is SBD. It provides a way to enable STONITH and fencing
         in clusters without external power switches, but it requires shared
         storage. For SBD requirements, see
         <a class="link" href="http://linux-ha.org/wiki/SBD_Fencing" target="_blank">http://linux-ha.org/wiki/SBD_Fencing</a>, section
         <em class="citetitle">Requirements</em>.
        </p></li></ul></div><p>
       For more information, refer to the <em class="citetitle">High Availability Guide</em>, available at
       <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/</a>.
       Especially read the following chapters: <em class="citetitle">Configuration and
       Administration Basics</em>, and <em class="citetitle">Fencing and
       STONITH</em>, <em class="citetitle"> Storage Protection</em>.
      </p></dd><dt id="vle-ha-req-communication"><span class="term">Network Configuration</span></dt><dd><div id="id-1.4.3.3.7.8.5.3.2.1" data-id-title="Redundant Communication Paths" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Redundant Communication Paths</div><p>
        For a supported HA setup, it is required to set up cluster
        communication via two or more redundant paths. For this purpose, use
        network device bonding and team network mode in your Crowbar network setup. For details, see
        <a class="xref" href="#sec-depl-req-network-modes-teaming" title="2.1.2.3. Team Network Mode">Section 2.1.2.3, “Team Network Mode”</a>. At least two
        Ethernet cards per cluster node are required for network redundancy.
        We advise using team network mode everywhere (not only
        between the cluster nodes) to ensure redundancy.
       </p></div><p>
       For more information, refer to the <em class="citetitle">High Availability Guide</em>, available at
       <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/</a>.
       Especially read the following chapter: <em class="citetitle">Network Device
       Bonding</em>.
      </p><p>
       Using a second communication channel (ring) in Corosync (as an
       alternative to network device bonding) is not supported yet in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
       By default, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> uses the admin network (typically
       <code class="literal">eth0</code>) for the first Corosync ring.
      </p><div id="id-1.4.3.3.7.8.5.3.2.4" data-id-title="Dedicated Networks" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Dedicated Networks</div><p>
         The <code class="literal">corosync</code> network communication layer is
         crucial to the health of the cluster. <code class="literal">corosync</code> traffic always
         goes over the admin network.
        </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           Use redundant communication paths for the <code class="literal">corosync</code>
           network communication layer.
          </p></li><li class="listitem"><p>
           Do not place the <code class="literal">corosync</code> network communication layer
           on interfaces shared with any other networks that could experience heavy
           load, such as the <span class="productname">OpenStack</span> public / private / SDN / storage networks.
          </p></li></ul></div><p>
         Similarly, if SBD over iSCSI is used as a STONITH device (see
         <a class="xref" href="#vle-ha-req-stonith">STONITH</a>), do not place the iSCSI traffic on
         interfaces that could experience heavy load, because this might disrupt
         the SBD mechanism.
        </p></div></dd><dt id="vle-ha-req-storage"><span class="term">Storage Requirements</span></dt><dd><p>
       When using SBD as STONITH device, additional requirements apply
       for the shared storage. For details, see
       <a class="link" href="http://linux-ha.org/wiki/SBD_Fencing" target="_blank">http://linux-ha.org/wiki/SBD_Fencing</a>, section
       <em class="citetitle">Requirements</em>.
      </p></dd></dl></div></section><section class="sect2" id="sec-depl-reg-ha-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.6.6 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-depl-reg-ha-more">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    For a basic understanding and detailed information on the SUSE Linux Enterprise
    High Availability Extension (including the Pacemaker cluster stack), read the
    <em class="citetitle">High Availability Guide</em>. It is available at
    <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/</a>.
   </p><p>
    In addition to the chapters mentioned in
    <a class="xref" href="#sec-depl-reg-ha-general" title="2.6.5. Cluster Requirements and Recommendations">Section 2.6.5, “Cluster Requirements and Recommendations”</a>, the following
    chapters are especially recommended:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <em class="citetitle">Product Overview</em>
     </p></li><li class="listitem"><p>
      <em class="citetitle">Configuration and Administration Basics</em>
     </p></li></ul></div><p>
    The <em class="citetitle">High Availability Guide</em> also provides comprehensive information about the
    cluster management tools with which you can view and check the cluster status
    in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. They can also be used to look up details like
    configuration of cluster resources or global cluster options. Read the
    following chapters for more information:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      HA Web Console: <em class="citetitle">Configuring and Managing Cluster Resources (Web
      Interface)</em>
     </p></li><li class="listitem"><p>
      <code class="command">crm.sh</code>: <em class="citetitle"> Configuring and Managing
      Cluster Resources (Command Line)</em>
     </p></li></ul></div></section></section><section class="sect1" id="sec-depl-req-summary" data-id-title="Summary: Considerations and Requirements"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.7 </span><span class="title-name">Summary: Considerations and Requirements</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-summary">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   As outlined above, there are some important considerations to be made
   before deploying SUSE <span class="productname">OpenStack</span> Cloud. The following briefly summarizes what was
   discussed in detail in this chapter. Keep in mind that as of
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8, it is not possible to change some
   aspects such as the network setup when SUSE <span class="productname">OpenStack</span> Cloud is deployed!
   
  </p><div class="itemizedlist"><div class="title-container"><div class="itemizedlist-title-wrap"><div class="itemizedlist-title"><span class="title-number-name"><span class="title-name">Network </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.3.8.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><ul class="itemizedlist"><li class="listitem"><p>
     If you do not want to stick with the default networks and addresses,
     define custom networks and addresses. You need five different networks.
     If you need to separate the admin and the BMC network, a sixth network
     is required. See <a class="xref" href="#sec-depl-req-network" title="2.1. Network">Section 2.1, “Network”</a> for details.
     Networks that share interfaces need to be configured as VLANs.
    </p></li><li class="listitem"><p>
     The SUSE <span class="productname">OpenStack</span> Cloud networks are completely isolated, therefore it is not
     required to use public IP addresses for them. A class C network as used
     in this documentation may not provide enough addresses for a cloud that
     is supposed to grow. You may alternatively choose addresses from a
     class B or A network.
    </p></li><li class="listitem"><p>
     Determine how to allocate addresses from your network. Make sure not to
     allocate IP addresses twice. See
     <a class="xref" href="#sec-depl-req-network-allocation" title="2.1.1. Network Address Allocation">Section 2.1.1, “Network Address Allocation”</a> for the default
     allocation scheme.
    </p></li><li class="listitem"><p>
     Define which network mode to use. Keep in mind that all machines within
     the cloud (including the Administration Server) will be set up with the chosen
     mode and therefore need to meet the hardware requirements. See
     <a class="xref" href="#sec-depl-req-network-modes" title="2.1.2. Network Modes">Section 2.1.2, “Network Modes”</a> for details.
    </p></li><li class="listitem"><p>
     Define how to access the admin and BMC network(s): no access from the
     outside (no action is required), via an external gateway (gateway needs
     to be provided), or via bastion network. See
     <a class="xref" href="#sec-depl-req-network-bastion" title="2.1.3. Accessing the Administration Server via a Bastion Network">Section 2.1.3, “Accessing the Administration Server via a Bastion Network”</a> for details.
    </p></li><li class="listitem"><p>
     Provide a gateway to access the public network (public, nova-floating).
    </p></li><li class="listitem"><p>
     Make sure the Administration Server's host name is correctly configured
     (<code class="command">hostname</code> <code class="option">-f</code> needs to return a
     fully qualified host name). If this is not the case, run <span class="guimenu">YaST</span> › <span class="guimenu">Network Services</span> › <span class="guimenu">Hostnames</span> and add a fully qualified
     host name.
    </p></li><li class="listitem"><p>
     Prepare a list of MAC addresses and the intended use of the
     corresponding host for all <span class="productname">OpenStack</span> nodes.
    </p></li></ul></div><div class="itemizedlist"><div class="title-container"><div class="itemizedlist-title-wrap"><div class="itemizedlist-title"><span class="title-number-name"><span class="title-name">Update Repositories </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.3.8.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><ul class="itemizedlist"><li class="listitem"><p>
     Depending on your network setup you have different options for
     providing up-to-date update repositories for SUSE Linux Enterprise Server and SUSE <span class="productname">OpenStack</span> Cloud for
     SUSE <span class="productname">OpenStack</span> Cloud deployment: using an existing SMT or SUSE Manager
     server, installing SMT on the Administration Server, synchronizing data
     with an existing repository, mounting remote repositories, or using physical media. Choose the option that best matches your
     needs.
    </p></li></ul></div><div class="itemizedlist"><div class="title-container"><div class="itemizedlist-title-wrap"><div class="itemizedlist-title"><span class="title-number-name"><span class="title-name">Storage </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.3.8.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><ul class="itemizedlist"><li class="listitem"><p>
     Decide whether you want to deploy the object storage service
     swift. If so, you need to deploy at least two nodes with
     sufficient disk space exclusively dedicated to swift.
    </p></li><li class="listitem"><p>
     Decide which back-end to use with cinder. If using the
     <span class="guimenu">raw</span> back-end (local disks) we strongly
     recommend using a separate node equipped with several hard disks for
     deploying <code class="literal">cinder-volume</code>. Ceph needs a minimum of four exclusive nodes with sufficient disk space.
    </p></li><li class="listitem"><p>
     Make sure all Compute Nodes are equipped with sufficient hard disk
     space.
    </p></li></ul></div><div class="itemizedlist"><div class="title-container"><div class="itemizedlist-title-wrap"><div class="itemizedlist-title"><span class="title-number-name"><span class="title-name">SSL Encryption </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.3.8.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><ul class="itemizedlist"><li class="listitem"><p>
     Decide whether to use different SSL certificates for the services and
     the API, or whether to use a single certificate.
    </p></li><li class="listitem"><p>
     Get one or more SSL certificates certified by a trusted third party
     source.
    </p></li></ul></div><div class="itemizedlist"><div class="title-container"><div class="itemizedlist-title-wrap"><div class="itemizedlist-title"><span class="title-number-name"><span class="title-name">Hardware and Software Requirements </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.3.8.7">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><ul class="itemizedlist"><li class="listitem"><p>
     Make sure the hardware requirements for the different node types are
     met.
    </p></li><li class="listitem"><p>
     Make sure to have all required software at hand.
    </p></li></ul></div></section><section class="sect1" id="sec-depl-req-installation" data-id-title="Overview of the SUSE OpenStack Cloud Installation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.8 </span><span class="title-name">Overview of the SUSE <span class="productname">OpenStack</span> Cloud Installation</span></span> <a title="Permalink" class="permalink" href="#sec-depl-req-installation">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   Deploying and installing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> is a multi-step process.
   Start by deploying a basic SUSE Linux Enterprise Server installation and the
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> add-on product to the Administration Server. Then the product and
   update repositories need to be set up and the SUSE <span class="productname">OpenStack</span> Cloud network needs to
   be configured. Next, complete the Administration Server setup. After the
   Administration Server is ready, you can start deploying and configuring the
   <span class="productname">OpenStack</span> nodes. The complete node deployment is done automatically via
   Crowbar and Chef from the Administration Server. All you need to do is to
   boot the nodes using PXE and to deploy the <span class="productname">OpenStack</span> components to them.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Install SUSE Linux Enterprise Server 12 SP4 on the Administration Server with the add-on product
     SUSE <span class="productname">OpenStack</span> Cloud. Optionally select the Subscription Management Tool (SMT) pattern for installation. See
     <a class="xref" href="#cha-depl-adm-inst" title="Chapter 3. Installing the Administration Server">Chapter 3, <em>Installing the Administration Server</em></a>.
    </p></li><li class="step"><p>
     Optionally set up and configure the SMT server on the Administration Server. See
     <a class="xref" href="#app-deploy-smt" title="Chapter 4. Installing and Setting Up an SMT Server on the Administration Server (Optional)">Chapter 4, <em>Installing and Setting Up an SMT Server on the Administration Server (Optional)</em></a>.
    </p></li><li class="step"><p>
     Make all required software repositories available on the Administration Server. See
     <a class="xref" href="#cha-depl-repo-conf" title="Chapter 5. Software Repository Setup">Chapter 5, <em>Software Repository Setup</em></a>.
    </p></li><li class="step"><p>
     Set up the network on the Administration Server. See
     <a class="xref" href="#sec-depl-adm-inst-network" title="Chapter 6. Service Configuration: Administration Server Network Configuration">Chapter 6, <em>Service Configuration:  Administration Server Network Configuration</em></a>.
    </p></li><li class="step"><p>
     Perform the Crowbar setup to configure the SUSE <span class="productname">OpenStack</span> Cloud network and to make the
     repository locations known. When the configuration is done, start the
     SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation. See <a class="xref" href="#sec-depl-adm-inst-crowbar" title="Chapter 7. Crowbar Setup">Chapter 7, <em>Crowbar Setup</em></a>.
    </p></li><li class="step"><p>
     Boot all nodes onto which the <span class="productname">OpenStack</span> components should be deployed
     using PXE and allocate them in the Crowbar Web interface to start the
     automatic SUSE Linux Enterprise Server installation. See
     <a class="xref" href="#cha-depl-inst-nodes" title="Chapter 11. Installing the OpenStack Nodes">Chapter 11, <em>Installing the <span class="productname">OpenStack</span> Nodes</em></a>.
    </p></li><li class="step"><p>
     Configure and deploy the <span class="productname">OpenStack</span> components via the Crowbar Web
     interface or command line tools. See <a class="xref" href="#cha-depl-ostack" title="Chapter 12. Deploying the OpenStack Services">Chapter 12, <em>Deploying the <span class="productname">OpenStack</span> Services</em></a>.
    </p></li><li class="step"><p>
     When all <span class="productname">OpenStack</span> components are up and running, SUSE <span class="productname">OpenStack</span> Cloud is ready.
     The cloud administrator can now upload images to enable users to start
     deploying instances. See the <em class="citetitle">Administrator Guide</em> and the
     <em class="citetitle">Supplement to <em class="citetitle">Administrator Guide</em> and <em class="citetitle">User Guide</em></em>.

    </p></li></ol></div></div></section></section></div><div class="part" id="part-depl-admserv" data-id-title="Setting Up the Administration Server"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">Part II </span><span class="title-name">Setting Up the Administration Server </span></span><a title="Permalink" class="permalink" href="#part-depl-admserv">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-depl-adm-inst"><span class="title-number">3 </span><span class="title-name">Installing the Administration Server</span></a></span></li><dd class="toc-abstract"><p>
    In this chapter you will learn how to install the Administration Server from
    scratch. It will run on SUSE Linux Enterprise Server 12 SP4 and include the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>
    extension and, optionally, the Subscription Management Tool (SMT) server. Prior to starting
    the installation, refer to <a class="xref" href="#sec-depl-req-hardware" title="2.4. Hardware Requirements">Section 2.4, “Hardware Requirements”</a> and
    <a class="xref" href="#sec-depl-req-software" title="2.5. Software Requirements">Section 2.5, “Software Requirements”</a>.
   </p></dd><li><span class="chapter"><a href="#app-deploy-smt"><span class="title-number">4 </span><span class="title-name">Installing and Setting Up an SMT Server on the Administration Server (Optional)</span></a></span></li><dd class="toc-abstract"><p>
    One way to provide the repositories needed to set up the nodes in
    SUSE <span class="productname">OpenStack</span> Cloud is to install a Subscription Management Tool (SMT) server on the Administration Server, and then mirror all
    repositories from SUSE Customer Center via this server. Installing an SMT server
    on the Administration Server is optional. If your organization already provides an
    SMT server or a SUSE Manager server that can be accessed from the
    Administration Server, skip this step.
   </p></dd><li><span class="chapter"><a href="#cha-depl-repo-conf"><span class="title-number">5 </span><span class="title-name">Software Repository Setup</span></a></span></li><dd class="toc-abstract"><p>
    Nodes in SUSE <span class="productname">OpenStack</span> Cloud are automatically installed from the Administration Server. For this
    to happen, software repositories containing products, extensions, and the
    respective updates for all software need to be available on or accessible
    from the Administration Server. In this configuration step, these repositories are made
    available. There are two types of repositories:
   </p><p>
    <span class="bold"><strong>Product Media Repositories</strong></span>: Product media
    repositories are copies of the installation media. They need to be
    directly copied to the Administration Server, <span class="quote">“<span class="quote">loop-mounted</span>”</span> from an iso
    image, or mounted from a remote server via NFS. Affected are SUSE Linux Enterprise Server 12 SP4 and
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 9. These are static repositories; they do not change or receive updates. See <a class="xref" href="#sec-depl-adm-conf-repos-product" title="5.1. Copying the Product Media Repositories">Section 5.1, “Copying the Product Media Repositories”</a> for setup
    instructions.
   </p><p>
    <span class="bold"><strong>Update and Pool Repositories</strong></span>: Update and
    Pool repositories are provided by the SUSE Customer Center. They contain all updates and
    patches for the products and extensions. To make them available for
    SUSE <span class="productname">OpenStack</span> Cloud they need to be mirrored from the SUSE Customer Center. Since their content is
    regularly updated, they must be kept in synchronization with SUSE Customer Center. For
    these purposes, SUSE provides either the Subscription Management Tool (SMT) or the
    SUSE Manager.
   </p></dd><li><span class="chapter"><a href="#sec-depl-adm-inst-network"><span class="title-number">6 </span><span class="title-name">Service Configuration:  Administration Server Network Configuration</span></a></span></li><dd class="toc-abstract"><p>
    Prior to starting the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation, make sure the first network
    interface (<code class="systemitem">eth0</code>) gets a
    fixed IP address from the admin network. A host and domain name
    also need to be provided. Other interfaces will be automatically
    configured during the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation.
   </p></dd><li><span class="chapter"><a href="#sec-depl-adm-inst-crowbar"><span class="title-number">7 </span><span class="title-name">Crowbar Setup</span></a></span></li><dd class="toc-abstract"><p>
    The YaST Crowbar module enables you to configure all networks within the
    cloud, to set up additional repositories, and to manage the Crowbar users.
    This module should be launched before starting the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation. To start
    this module, either run <code class="command">yast crowbar</code> or <span class="guimenu">YaST</span> › <span class="guimenu">Miscellaneous</span> › <span class="guimenu">Crowbar</span>.
   </p></dd><li><span class="chapter"><a href="#sec-depl-adm-start-crowbar"><span class="title-number">8 </span><span class="title-name">Starting the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation</span></a></span></li><dd class="toc-abstract"><p>
    The last step in configuring the Administration Server is starting Crowbar.
   </p></dd><li><span class="chapter"><a href="#sec-depl-adm-crowbar-extra-features"><span class="title-number">9 </span><span class="title-name">Customizing Crowbar</span></a></span></li><dd class="toc-abstract"><p>In large deployments with many nodes, there are always some nodes that are in a fail or unknown state. New barclamps cannot be applied to them and values cannot be updated in some barclamps that are already deployed. This happens because Crowbar will refuse to apply a barclamp to a list of nodes if …</p></dd></ul></div><section class="chapter" id="cha-depl-adm-inst" data-id-title="Installing the Administration Server"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">3 </span><span class="title-name">Installing the Administration Server</span></span> <a title="Permalink" class="permalink" href="#cha-depl-adm-inst">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><div><div class="abstract"><p>
    In this chapter you will learn how to install the Administration Server from
    scratch. It will run on SUSE Linux Enterprise Server 12 SP4 and include the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>
    extension and, optionally, the Subscription Management Tool (SMT) server. Prior to starting
    the installation, refer to <a class="xref" href="#sec-depl-req-hardware" title="2.4. Hardware Requirements">Section 2.4, “Hardware Requirements”</a> and
    <a class="xref" href="#sec-depl-req-software" title="2.5. Software Requirements">Section 2.5, “Software Requirements”</a>.
   </p></div></div></div></div><section class="sect1" id="sec-depl-adm-inst-os" data-id-title="Starting the Operating System Installation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3.1 </span><span class="title-name">Starting the Operating System Installation</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-os">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   Start the installation by booting into the SUSE Linux Enterprise Server 12 SP4 installation system.
   For an overview of a default SUSE Linux Enterprise Server installation, refer to <a class="link" href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-deployment/#cha-install" target="_blank">https://documentation.suse.com/sles/15-SP1/single-html/SLES-deployment/#cha-install</a>.
  </p><p>
   The following sections will only cover the differences from the default
   installation process.
  </p></section><section class="sect1" id="sec-depl-adm-inst-online-update" data-id-title="Registration and Online Updates"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3.2 </span><span class="title-name">Registration and Online Updates</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-online-update">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   Registering SUSE Linux Enterprise Server 12 SP4 during the installation process is required for
   getting product updates and for installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>
   extension.</p><p>
   After a successful registration you will be asked whether
   to add the update repositories. If you agree, the latest updates will
   automatically be installed, ensuring that your system is on the latest
   patch level after the initial installation. We strongly recommend adding the update repositories immediately. If you choose to skip this step you need to perform
   an online update later, before starting the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation.
  </p><div id="id-1.4.4.2.3.4" data-id-title="SUSE Login Required" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: SUSE Login Required</div><p>
    To register a product, you need to have a SUSE login.
    If you do not have such a login, create it at
    <a class="link" href="http://www.suse.com/" target="_blank">http://www.suse.com/</a>.
   </p></div></section><section class="sect1" id="sec-depl-adm-inst-add-on" data-id-title="Installing the SUSE OpenStack Cloud Crowbar Extension"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3.3 </span><span class="title-name">Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Extension</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-add-on">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   SUSE <span class="productname">OpenStack</span> Cloud is an extension to SUSE Linux Enterprise Server. Installing it during the SUSE Linux Enterprise Server
   installation is the easiest and recommended way to set up the Administration Server. To get access to the extension selection dialog, you need to register
   SUSE Linux Enterprise Server 12 SP4 during the installation. After a successful registration, the
   SUSE Linux Enterprise Server 12 SP4 installation continues with the <span class="guimenu">Extension &amp; Module
   Selection</span>. Choose <span class="guimenu"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 9</span>
   and provide the registration key you obtained by purchasing
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. The registration and the extension installation require an
   Internet connection.
  </p><p>
   Alternatively, install the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> after the
   SUSE Linux Enterprise Server 12 SP4 installation via <span class="guimenu">YaST</span> › <span class="guimenu">Software</span> › <span class="guimenu">Add-On Products</span>.
   For details, refer to the section <a class="link" href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-deployment/#sec-yast-install-modules" target="_blank">https://documentation.suse.com/sles/15-SP1/single-html/SLES-deployment/#sec-yast-install-modules</a>.
  </p></section><section class="sect1" id="sec-depl-adm-inst-partition" data-id-title="Partitioning"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3.4 </span><span class="title-name">Partitioning</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-partition">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   Currently, Crowbar requires <code class="filename">/opt</code> to be writable. We recommend creating a separate partition
   or volume formatted with XFS for <code class="filename">/srv</code> with a size of
   at least 30 GB.
  </p><p>
   The default file system on SUSE Linux Enterprise Server 12 SP4 is Btrfs with snapshots enabled.
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> installs into <code class="filename">/opt</code>, a directory that is
   excluded from snapshots. Reverting to a snapshot may therefore break the
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> installation. We recommend disabling Btrfs snapshots on
   the Administration Server.
  </p><p>
   Help on using the partitioning tool is available at the section <a class="link" href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-deployment/#sec-expert-partitioner" target="_blank">https://documentation.suse.com/sles/15-SP1/single-html/SLES-deployment/#sec-expert-partitioner</a>.
  </p></section><section class="sect1" id="sec-depl-adm-inst-settings" data-id-title="Installation Settings"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3.5 </span><span class="title-name">Installation Settings</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-settings">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   In the final installation step, <span class="guimenu">Installation Settings</span>, you need to adjust
   the software selection and the firewall settings for your Administration Server setup. For more information
   refer to the <a class="link" href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-deployment/#sec-yast-install-perform" target="_blank">https://documentation.suse.com/sles/15-SP1/single-html/SLES-deployment/#sec-yast-install-perform</a>.
  </p><section class="sect2" id="sec-depl-adm-inst-settings-software" data-id-title="Software Selection"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">3.5.1 </span><span class="title-name">Software Selection</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-settings-software">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    Installing a minimal base system is sufficient to set up the
    Administration Server. The following patterns are the minimum required:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <span class="guimenu">Base System</span>
     </p></li><li class="listitem"><p>
      <span class="guimenu">Minimal System (Appliances)</span>
     </p></li><li class="listitem"><p>
      <span class="guimenu">Meta Package for Pattern cloud_admin</span> (in case you have
      chosen to install the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Extension)
     </p></li><li class="listitem"><p>
      <span class="guimenu">Subscription Management Tool</span> (optional, also see <a class="xref" href="#tip-depl-adm-inst-settings-smt" title="Tip: Installing a Local SMT Server (Optional)">Tip: Installing a Local SMT Server (Optional)</a>)
     </p></li></ul></div><div id="tip-depl-adm-inst-settings-smt" data-id-title="Installing a Local SMT Server (Optional)" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Installing a Local SMT Server (Optional)</div><p>
     If you do not have a SUSE Manager or SMT server in your
     organization, or are planning to manually update the repositories required for deployment of the SUSE <span class="productname">OpenStack</span> Cloud nodes, you need to set up an SMT server on
     the Administration Server. Choose the pattern <span class="guimenu">Subscription Management
     Tool</span> in addition to the patterns listed above to install the
     SMT server software.
    </p></div></section><section class="sect2" id="sec-depl-adm-inst-settings-firewall" data-id-title="Firewall Settings"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">3.5.2 </span><span class="title-name">Firewall Settings</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-settings-firewall">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> requires disabling the firewall on the Administration Server. You can
    disable the firewall during installation in the <span class="guimenu">Firewall and SSH</span>
    section. If your environment requires a firewall to be active at this
    stage of the installation, you can disable the firewall during your final network configuration (see <a class="xref" href="#sec-depl-adm-inst-network" title="Chapter 6. Service Configuration: Administration Server Network Configuration">Chapter 6, <em>Service Configuration:  Administration Server Network Configuration</em></a>). Optionally, you can also enable SSH
    access to the Administration Server in this section.
   </p><div id="warn-depl-adm-inst-settings-proxy" data-id-title="HTTP_PROXY and NO_PROXY" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: HTTP_PROXY and NO_PROXY</div><p>
      Setting HTTP_PROXY without properly configuring NO_PROXY for the
      Administration Server might result in chef-client failing in non-obvious ways.
     </p></div></section></section></section><section class="chapter" id="app-deploy-smt" data-id-title="Installing and Setting Up an SMT Server on the Administration Server (Optional)"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">4 </span><span class="title-name">Installing and Setting Up an SMT Server on the Administration Server (Optional)</span></span> <a title="Permalink" class="permalink" href="#app-deploy-smt">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><div><div class="abstract"><p>
    One way to provide the repositories needed to set up the nodes in
    SUSE <span class="productname">OpenStack</span> Cloud is to install a Subscription Management Tool (SMT) server on the Administration Server, and then mirror all
    repositories from SUSE Customer Center via this server. Installing an SMT server
    on the Administration Server is optional. If your organization already provides an
    SMT server or a SUSE Manager server that can be accessed from the
    Administration Server, skip this step.
   </p></div></div></div></div><div id="id-1.4.4.3.3" data-id-title="Use of SMT Server and Ports" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Use of SMT Server and Ports</div><p>
   When installing an SMT server on the Administration Server, use it exclusively
   for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. To use the SMT server for other
   products, run it outside of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. Make sure it can be accessed
   from the Administration Server for mirroring the repositories needed for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
  </p><p>
   When the SMT server is installed on the Administration Server, Crowbar
   provides the mirrored repositories on port <code class="literal">8091</code>.
  </p></div><section class="sect1" id="app-deploy-smt-install" data-id-title="SMT Installation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.1 </span><span class="title-name">SMT Installation</span></span> <a title="Permalink" class="permalink" href="#app-deploy-smt-install">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   If you have not installed the SMT server during the initial Administration Server
   installation as suggested in <a class="xref" href="#sec-depl-adm-inst-settings-software" title="3.5.1. Software Selection">Section 3.5.1, “Software Selection”</a>, run the following command
   to install it:
  </p><div class="verbatim-wrap"><pre class="screen">sudo zypper in -t pattern smt</pre></div></section><section class="sect1" id="app-deploy-smt-config" data-id-title="SMT Configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.2 </span><span class="title-name">SMT Configuration</span></span> <a title="Permalink" class="permalink" href="#app-deploy-smt-config">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   No matter whether the SMT server was installed during the initial
   installation or in the running system, it needs to be configured with the
   following steps.
  </p><div id="id-1.4.4.3.5.3" data-id-title="Prerequisites" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Prerequisites</div><p>
    To configure the SMT server, a SUSE account is required. If you do not
    have such an account, register at <a class="link" href="http://www.suse.com/" target="_blank">http://www.suse.com/</a>. All products and
    extensions for which you want to mirror updates with the SMT
    server should be registered at the SUSE Customer Center (<a class="link" href="http://scc.suse.com/" target="_blank">http://scc.suse.com/</a>).
   </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Configuring the SMT server requires you to have your mirroring
     credentials (user name and password) and your registration e-mail
     address at hand. To access them, proceed as follows:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Open a Web browser and log in to the SUSE Customer Center at
       <a class="link" href="http://scc.suse.com/" target="_blank">http://scc.suse.com/</a>.
      </p></li><li class="step"><p>
       Click your name to see the e-mail address which you have registered.
      </p></li><li class="step"><p>
       Click <span class="guimenu">Organization</span> › <span class="guimenu">Organization Credentials</span> to obtain
       your mirroring credentials (user name and password).
      </p></li></ol></li><li class="step"><p>
     Start <span class="guimenu">YaST</span> › <span class="guimenu">Network
     Services</span> › <span class="guimenu">SMT Configuration
     Wizard</span>.
    </p></li><li class="step"><p>
     Activate <span class="guimenu">Enable Subscription Management Tool Service
     (SMT)</span>.
    </p></li><li class="step"><p>
     Enter the <span class="guimenu">Customer Center Configuration</span> data as
     follows:
    </p><table style="border: 0; " class="simplelist"><tr><td><span class="guimenu">Use Custom Server</span>:
     Do <span class="emphasis"><em>not</em></span> activate this option</td></tr><tr><td><span class="guimenu">User</span>: The user name you retrieved from the
     SUSE Customer Center</td></tr><tr><td><span class="guimenu">Password</span>: The password you retrieved from the
     SUSE Customer Center</td></tr></table><p>
     Check your input with <span class="guimenu">Test</span>. If the test does not
     return <code class="literal">success</code>, check the credentials you entered.
    </p></li><li class="step"><p>
     Enter the e-mail address you retrieved from the SUSE Customer Center at
     <span class="guimenu">SCC E-Mail Used for Registration</span>.
    </p></li><li class="step"><p>
     <span class="guimenu">Your SMT Server URL</span> shows the HTTP address of your
     server. Usually it should not be necessary to change it.
    </p></li><li class="step"><p>
     Select <span class="guimenu">Next</span> to proceed to step two of the <span class="guimenu">SMT Configuration Wizard</span>.
    </p></li><li class="step"><p>
     Enter a <span class="guimenu">Database Password for SMT User</span> and confirm
     it by entering it once again.
    </p></li><li class="step"><p>
     Enter one or more e-mail addresses to which SMT status reports are
     sent by selecting <span class="guimenu">Add</span>.
    </p></li><li class="step"><p>
     Select <span class="guimenu">Next</span> to save your SMT configuration. When
     setting up the database you will be prompted for the MariaDB root
     password. If you have not already created one then create it in this step. Note that this is
     the global MariaDB root password, not the database password for the SMT
     user you specified before.
    </p><p>
     The SMT server requires a server certificate at
     <code class="filename">/etc/pki/trust/anchors/YaST-CA.pem</code>. Choose
     <span class="guimenu">Run CA Management</span>, provide a password and choose
     <span class="guimenu">Next</span> to create such a certificate. If your
     organization already provides a CA certificate, <span class="guimenu">Skip</span>
     this step and import the certificate via <span class="guimenu">YaST</span> › <span class="guimenu">Security and Users</span> › <span class="guimenu">CA Management</span> after the SMT
     configuration is done. See
   <a class="link" href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-security/#cha-security-yast-security" target="_blank">https://documentation.suse.com/sles/15-SP1/single-html/SLES-security/#cha-security-yast-security</a>
   for more information.
    </p><p>
     After you complete your configuration a synchronization check with the SUSE Customer Center will run, which may take several minutes.
    </p></li></ol></div></div></section><section class="sect1" id="app-deploy-smt-repos" data-id-title="Setting up Repository Mirroring on the SMT Server"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.3 </span><span class="title-name">Setting up Repository Mirroring on the SMT Server</span></span> <a title="Permalink" class="permalink" href="#app-deploy-smt-repos">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   The final step in setting up the SMT server is configuring it to
   mirror the repositories needed for SUSE <span class="productname">OpenStack</span> Cloud. The SMT server
   mirrors the repositories from the SUSE Customer Center. Make
   sure to have the appropriate subscriptions registered in SUSE Customer Center with the
   same e-mail address you specified when configuring SMT. For
   details on the required subscriptions refer to
   <a class="xref" href="#sec-depl-req-software" title="2.5. Software Requirements">Section 2.5, “Software Requirements”</a>.
  </p><section class="sect2" id="app-deploy-smt-repos-mandatory" data-id-title="Adding Mandatory Repositories"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.3.1 </span><span class="title-name">Adding Mandatory Repositories</span></span> <a title="Permalink" class="permalink" href="#app-deploy-smt-repos-mandatory">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    Mirroring the SUSE Linux Enterprise Server 12 SP4 and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 9
    repositories is mandatory. Run the following commands as user
    <code class="systemitem">root</code> to add them to the list of mirrored repositories:
   </p><div class="verbatim-wrap"><pre class="screen">for REPO in SLES12-SP4-{Pool,Updates} <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-9</span></span>-{Pool,Updates}; do
  smt-repos $REPO sle-12-x86_64 -e
done</pre></div></section><section class="sect2" id="app-deploy-smt-repos-optional" data-id-title="Adding Optional Repositories"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.3.2 </span><span class="title-name">Adding Optional Repositories</span></span> <a title="Permalink" class="permalink" href="#app-deploy-smt-repos-optional">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    The following optional repositories provide high availability and storage:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.3.6.4.3.1"><span class="term">High Availability</span></dt><dd><p>
       For the optional HA setup you need to mirror the SLE-HA12-SP4
       repositories. Run the following commands as user <code class="systemitem">root</code> to add
       them to the list of mirrored repositories:
      </p><div class="verbatim-wrap"><pre class="screen">for REPO in SLE-HA12-SP4-{Pool,Updates}; do
  smt-repos $REPO sle-12-x86_64 -e
done</pre></div></dd><dt id="id-1.4.4.3.6.4.3.2"><span class="term">SUSE Enterprise Storage</span></dt><dd><p>
       The SUSE Enterprise Storage repositories are needed if you plan to use an external
       Ceph with SUSE <span class="productname">OpenStack</span> Cloud. Run the following commands as user <code class="systemitem">root</code> to
       add them to the list of mirrored repositories:
      </p><div class="verbatim-wrap"><pre class="screen">for REPO in SUSE-Enterprise-Storage-5-{Pool,Updates}; do
  smt-repos $REPO sle-12-x86_64 -e
done</pre></div></dd></dl></div></section><section class="sect2" id="app-deploy-smt-repos-mirror" data-id-title="Updating the Repositories"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.3.3 </span><span class="title-name">Updating the Repositories</span></span> <a title="Permalink" class="permalink" href="#app-deploy-smt-repos-mirror">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    New repositories added to SMT must be updated immediately by running the following command as user <code class="systemitem">root</code>:
   </p><div class="verbatim-wrap"><pre class="screen">smt-mirror -L /var/log/smt/smt-mirror.log</pre></div><p>
    This command will download several GB of patches. This process may last
    up to several hours. A log file is written to
    <code class="filename">/var/log/smt/smt-mirror.log</code>. After this first manual update the repositories are updated automatically via cron
    job. A list of all
    repositories and their location in the file system on the Administration Server can be
    found at <a class="xref" href="#tab-smt-repos-local" title="SMT Repositories Hosted on the Administration Server">Table 5.2, “SMT Repositories Hosted on the Administration Server”</a>.
   </p></section></section><section class="sect1" id="app-deploy-smt-info" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.4 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#app-deploy-smt-info">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   For detailed information about SMT refer to the Subscription Management Tool manual at <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-smt/" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-smt/</a>.
  </p></section></section><section class="chapter" id="cha-depl-repo-conf" data-id-title="Software Repository Setup"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">5 </span><span class="title-name">Software Repository Setup</span></span> <a title="Permalink" class="permalink" href="#cha-depl-repo-conf">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><div><div class="abstract"><p>
    Nodes in SUSE <span class="productname">OpenStack</span> Cloud are automatically installed from the Administration Server. For this
    to happen, software repositories containing products, extensions, and the
    respective updates for all software need to be available on or accessible
    from the Administration Server. In this configuration step, these repositories are made
    available. There are two types of repositories:
   </p><p>
    <span class="bold"><strong>Product Media Repositories</strong></span>: Product media
    repositories are copies of the installation media. They need to be
    directly copied to the Administration Server, <span class="quote">“<span class="quote">loop-mounted</span>”</span> from an iso
    image, or mounted from a remote server via NFS. Affected are SUSE Linux Enterprise Server 12 SP4 and
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 9. These are static repositories; they do not change or receive updates. See <a class="xref" href="#sec-depl-adm-conf-repos-product" title="5.1. Copying the Product Media Repositories">Section 5.1, “Copying the Product Media Repositories”</a> for setup
    instructions.
   </p><p>
    <span class="bold"><strong>Update and Pool Repositories</strong></span>: Update and
    Pool repositories are provided by the SUSE Customer Center. They contain all updates and
    patches for the products and extensions. To make them available for
    SUSE <span class="productname">OpenStack</span> Cloud they need to be mirrored from the SUSE Customer Center. Since their content is
    regularly updated, they must be kept in synchronization with SUSE Customer Center. For
    these purposes, SUSE provides either the Subscription Management Tool (SMT) or the
    SUSE Manager.
   </p></div></div></div></div><section class="sect1" id="sec-depl-adm-conf-repos-product" data-id-title="Copying the Product Media Repositories"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.1 </span><span class="title-name">Copying the Product Media Repositories</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-conf-repos-product">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   The files in the product repositories for SUSE Linux Enterprise Server and SUSE <span class="productname">OpenStack</span> Cloud do not
   change, therefore they do not need to be synchronized with a remote
   source. It is sufficient to either copy the data (from a remote host or
   the installation media), to mount the product repository from a remote
    server via <code class="literal">NFS</code>, or to loop mount a copy of the
    installation images.
   </p><div id="id-1.4.4.4.2.3" data-id-title="No Symbolic Links for the SUSE Linux Enterprise Server Repository" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: No Symbolic Links for the SUSE Linux Enterprise Server Repository</div><p>
     Note that the SUSE Linux Enterprise Server product repository <span class="emphasis"><em>must</em></span> be
     directly available from the local directory listed below. It is not
     possible to use a symbolic link to a directory located elsewhere, since
     this will cause booting via PXE to fail.
    </p></div><div id="id-1.4.4.4.2.4" data-id-title="Providing the SUSE OpenStack Cloud Crowbar Repository via HTTP" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Providing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Repository via HTTP</div><p>
     The SUSE Linux Enterprise Server product repositories need to be available locally
     to enable booting via PXE for node deployment. The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>
     repository may also be served via HTTP from a remote
     host. In this case, enter the URL to the <code class="literal">Cloud</code>
     repository as described in <a class="xref" href="#sec-depl-adm-inst-crowbar-repos" title="7.4. Repositories">Section 7.4, “<span class="guimenu">Repositories</span>”</a>.
    </p><p>
     We recommend copying the data to the Administration Server as the best solution. It does not require much hard disk space (approximately
     900 MB). Nor does it require the Administration Server to access a remote host from a different network.
    </p></div><p>
    The following product media must be copied to the specified
    directories:
   </p><div class="table" id="id-1.4.4.4.2.6" data-id-title="Local Product Repositories for SUSE OpenStack Cloud"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 5.1: </span><span class="title-name">Local Product Repositories for SUSE <span class="productname">OpenStack</span> Cloud </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.4.2.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Repository
        </p>
       </th><th style="border-bottom: 1px solid ; ">
        <p>
         Directory
        </p>
       </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         SUSE Linux Enterprise Server 12 SP4 DVD #1
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         <code class="filename">/srv/tftpboot/suse-12.4/x86_64/install</code>
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; ">
        <p>
         <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 9 DVD #1
        </p>
       </td><td>
        <p>
         <code class="filename">/srv/tftpboot/suse-12.4/x86_64/repos/Cloud</code>
        </p>
       </td></tr></tbody></table></div></div><p>
    The data can be copied by a variety of methods:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.4.2.8.1"><span class="term">Copying from the Installation Media</span></dt><dd><p>
      We recommended using <code class="command">rsync</code> for copying. If the
      installation data is located on a removable device, make sure to mount
      it first (for example, after inserting the DVD1 in the Administration Server and
      waiting for the device to become ready):
     </p><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.4.4.2.8.1.2.2"><span class="name">SUSE Linux Enterprise Server 12 SP4 DVD#1</span><a title="Permalink" class="permalink" href="#id-1.4.4.4.2.8.1.2.2">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">mkdir -p /srv/tftpboot/suse-12.4/x86_64/install
mount /dev/dvd /mnt
rsync -avP /mnt/ /srv/tftpboot/suse-12.4/x86_64/install/
umount /mnt</pre></div><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.4.4.2.8.1.2.4"><span class="name">
      <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 9 DVD#1
     </span><a title="Permalink" class="permalink" href="#id-1.4.4.4.2.8.1.2.4">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">mkdir -p /srv/tftpboot/suse-12.4/x86_64/repos/Cloud
mount /dev/dvd /mnt
rsync -avP /mnt/ /srv/tftpboot/suse-12.4/x86_64/repos/Cloud/
umount /mnt</pre></div></dd><dt id="id-1.4.4.4.2.8.2"><span class="term">Copying from a Remote Host</span></dt><dd><p>
       If the data is provided by a remote machine, log in to that machine and
       push the data to the Administration Server (which has the IP address <code class="systemitem">192.168.124.10</code> in the following
       example):
      </p><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.4.4.2.8.2.2.2"><span class="name">SUSE Linux Enterprise Server 12 SP4 DVD#1</span><a title="Permalink" class="permalink" href="#id-1.4.4.4.2.8.2.2.2">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">mkdir -p /srv/tftpboot/suse-12.4/x86_64/install
rsync -avPz <em class="replaceable">/data/SLES-12-SP4/DVD1/</em> <em class="replaceable">192.168.124.10</em>:/srv/tftpboot/suse-12.4/x86_64/install/</pre></div><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.4.4.2.8.2.2.4"><span class="name">
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 9 DVD#1
      </span><a title="Permalink" class="permalink" href="#id-1.4.4.4.2.8.2.2.4">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">mkdir -p /srv/tftpboot/suse-12.4/x86_64/repos/Cloud
rsync -avPz <em class="replaceable">/data/SUSE-OPENSTACK-CLOUD//DVD1/</em> <em class="replaceable">192.168.124.10</em>:/srv/tftpboot/suse-12.4/x86_64/repos/Cloud/</pre></div></dd><dt id="id-1.4.4.4.2.8.3"><span class="term">Mounting from an NFS Server</span></dt><dd><p>
       If the installation data is provided via NFS by a remote machine, mount
       the respective shares as follows. To automatically mount these
       directories either create entries in <code class="filename">/etc/fstab</code> or
       set up the automounter.
      </p><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.4.4.2.8.3.2.2"><span class="name">SUSE Linux Enterprise Server 12 SP4 DVD#1</span><a title="Permalink" class="permalink" href="#id-1.4.4.4.2.8.3.2.2">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">mkdir -p /srv/tftpboot/suse-12.4/x86_64/install
mount -t nfs <em class="replaceable">nfs.example.com:/exports/SLES-12-SP4/x86_64/DVD1/</em> /srv/tftpboot/suse-12.4/x86_64/install</pre></div><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.4.4.2.8.3.2.4"><span class="name">
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 9 DVD#1
      </span><a title="Permalink" class="permalink" href="#id-1.4.4.4.2.8.3.2.4">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">mkdir -p /srv/tftpboot/suse-12.4/x86_64/repos/Cloud/
mount -t nfs <em class="replaceable">nfs.example.com:/exports/SUSE-OPENSTACK-CLOUD/DVD1/</em> /srv/tftpboot/suse-12.4/x86_64/repos/Cloud</pre></div></dd><dt id="id-1.4.4.4.2.8.4"><span class="term">Mounting the ISO Images</span></dt><dd><p>
       The product repositories can also be made available by copying the
       respective ISO images to the Administration Server and mounting them. To
       automatically mount these directories either create entries in
       <code class="filename">/etc/fstab</code> or set up the automounter.
      </p><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.4.4.2.8.4.2.2"><span class="name">SUSE Linux Enterprise Server 12 SP4 DVD#1</span><a title="Permalink" class="permalink" href="#id-1.4.4.4.2.8.4.2.2">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">mkdir -p /srv/tftpboot/suse-12.4/x86_64/install/
mount -o loop <em class="replaceable">/local/SLES-12-SP4-x86_64-DVD1.iso</em> /srv/tftpboot/suse-12.4/x86_64/install</pre></div><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.4.4.2.8.4.2.4"><span class="name">
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 9 DVD#1
      </span><a title="Permalink" class="permalink" href="#id-1.4.4.4.2.8.4.2.4">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">mkdir -p /srv/tftpboot/suse-12.4/x86_64/repos/Cloud/
mount -o loop <em class="replaceable">/local/SUSE-OPENSTACK-CLOUD-9-x86_64-DVD1.iso</em> /srv/tftpboot/suse-12.4/x86_64/repos/Cloud</pre></div></dd></dl></div></section><section class="sect1" id="sec-depl-adm-conf-repos-scc" data-id-title="Update and Pool Repositories"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.2 </span><span class="title-name">Update and Pool Repositories</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-conf-repos-scc">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    Update and Pool Repositories are required on the Administration Server to set up and
    maintain the SUSE <span class="productname">OpenStack</span> Cloud nodes. They are provided by SUSE Customer Center and contain all
    software packages needed to install SUSE Linux Enterprise Server 12 SP4 and the extensions (pool
    repositories). In addition, they contain all updates and patches (update repositories). Update
    repositories are used when deploying the nodes that build
    SUSE <span class="productname">OpenStack</span> Cloud to ensure they are initially equipped with the latest software
    versions available.
   </p><p>
    The repositories can be made available on the Administration Server using one or more of the
    following methods:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <a class="xref" href="#sec-depl-adm-conf-repos-scc-local-smt" title="5.2.1.  Repositories Hosted on an SMT Server Installed on the Administration Server">Section 5.2.1, “
     Repositories Hosted on an SMT Server Installed on the Administration Server
    ”</a>
     </p></li><li class="listitem"><p>
      <a class="xref" href="#sec-depl-adm-conf-repos-scc-remote-smt" title="5.2.2. Repositories Hosted on a Remote SMT Server">Section 5.2.2, “Repositories Hosted on a Remote SMT Server”</a>
     </p></li><li class="listitem"><p>
      <a class="xref" href="#sec-depl-adm-conf-repos-scc-remote-susemgr" title="5.2.3. Repositories Hosted on a SUSE Manager Server">Section 5.2.3, “Repositories Hosted on a SUSE Manager Server”</a>
     </p></li><li class="listitem"><p>
      <a class="xref" href="#sec-depl-adm-conf-repos-scc-alternatives" title="5.2.4. Alternative Ways to Make the Repositories Available">Section 5.2.4, “Alternative Ways to Make the Repositories Available”</a>
     </p></li></ul></div><section class="sect2" id="sec-depl-adm-conf-repos-scc-local-smt" data-id-title="Repositories Hosted on an SMT Server Installed on the Administration Server"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.2.1 </span><span class="title-name">
     Repositories Hosted on an SMT Server Installed on the Administration Server
    </span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-conf-repos-scc-local-smt">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
     When all update and pool repositories are managed by an SMT server
     installed on the Administration Server (see <a class="xref" href="#app-deploy-smt" title="Chapter 4. Installing and Setting Up an SMT Server on the Administration Server (Optional)">Chapter 4, <em>Installing and Setting Up an SMT Server on the Administration Server (Optional)</em></a>), make
     sure the repository location in YaST Crowbar is set to <span class="guimenu">Local
     SMT Server</span> (this is the default). For details, see <a class="xref" href="#sec-depl-adm-inst-crowbar-repos" title="7.4. Repositories">Section 7.4, “<span class="guimenu">Repositories</span>”</a>. No
     further action is required. The SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation automatically
     detects all available repositories.
    </p></section><section class="sect2" id="sec-depl-adm-conf-repos-scc-remote-smt" data-id-title="Repositories Hosted on a Remote SMT Server"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.2.2 </span><span class="title-name">Repositories Hosted on a Remote SMT Server</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-conf-repos-scc-remote-smt">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
     To use repositories from a remote SMT server, you first need to make
     sure all required repositories are mirrored on the server. Refer to <a class="xref" href="#app-deploy-smt-repos" title="4.3. Setting up Repository Mirroring on the SMT Server">Section 4.3, “Setting up Repository Mirroring on the SMT Server”</a> for more information. When all update
     and pool repositories are managed by a remote SMT server, make sure the
     repository location in YaST Crowbar is set to <span class="guimenu">Remote SMT
     Server</span>. For details, see <a class="xref" href="#sec-depl-adm-inst-crowbar-repos" title="7.4. Repositories">Section 7.4, “<span class="guimenu">Repositories</span>”</a>. No further action is
     required. The SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation automatically detects all
     available repositories.
    </p><div id="id-1.4.4.4.3.6.3" data-id-title="Accessing an External SMT Server" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Accessing an External SMT Server</div><p>
        When using an external SMT server, it needs to be reachable by all
        nodes. This means that the SMT server either needs to be part of the admin network
        or it needs to be accessible via the default route of the
        nodes. The latter can be either the gateway of the admin network or the gateway
        of the public network.
     </p></div></section><section class="sect2" id="sec-depl-adm-conf-repos-scc-remote-susemgr" data-id-title="Repositories Hosted on a SUSE Manager Server"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.2.3 </span><span class="title-name">Repositories Hosted on a SUSE Manager Server</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-conf-repos-scc-remote-susemgr">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
     To use repositories from SUSE Manager you first need to make sure all
     required products and extensions are registered, and the corresponding
     channels are mirrored in SUSE Manager (refer to
     <a class="xref" href="#tab-depl-adm-conf-susemgr-repos" title="SUSE Manager Repositories (Channels)">Table 5.4, “SUSE Manager Repositories (Channels)”</a> for a list of
     channels).
    </p><div id="id-1.4.4.4.3.7.3" data-id-title="Accessing a SUSE Manager Server" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Accessing a SUSE Manager Server</div><p>
      An external SUSE Manager server needs to be accessible to
      <span class="emphasis"><em>all</em></span> nodes in SUSE <span class="productname">OpenStack</span> Cloud. The network hosting the SUSE Manager server must be added to the
      network definitions as described in
      <a class="xref" href="#sec-depl-inst-admserv-post-network-external" title="7.5.8. Providing Access to External Networks">Section 7.5.8, “Providing Access to External Networks”</a>.
     </p></div><p>
     By default SUSE Manager does not expose repositories for direct access. To
     access them via HTTPS, you need to create a
     <span class="guimenu">Distribution</span> for auto-installation for the SUSE Linux Enterprise Server 12 SP4
     (x86_64) product. Creating this distribution makes the update
     repositories for this product available, including the repositories
     for all registered add-on products (like <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>, SLES High Availability Extension and
     SUSE Enterprise Storage). Instructions for creating a distribution are in the SUSE Manager documentation in <a class="link" href="https://documentation.suse.com/suma/4.0/" target="_blank">https://documentation.suse.com/suma/4.0/</a>.
    </p><p>
     During the distribution setups you need to provide a
     <span class="guimenu">Label</span> for each the distribution. This label will be
     part of the URL under which the repositories are available. We
     recommend choosing a name consisting of characters that do not need to
     be URL-encoded. In <a class="xref" href="#tab-depl-adm-conf-susemgr-repos" title="SUSE Manager Repositories (Channels)">Table 5.4, “SUSE Manager Repositories (Channels)”</a> we
     assume the following label has been provided:
     <code class="literal">sles12-sp4-x86_64</code>.
    </p><p>
     When all update and pool repositories are managed by a SUSE Manager server,
     make sure the repository location in YaST Crowbar is set to
     <span class="guimenu">SUSE Manager Server</span>. For details, see <a class="xref" href="#sec-depl-adm-inst-crowbar-repos" title="7.4. Repositories">Section 7.4, “<span class="guimenu">Repositories</span>”</a>. No further action is
     required. The SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation automatically detects all
     available repositories.
    </p><p>
     The autoinstallation tree provided by SUSE Manager does not provide the
     SLES Pool repository. Although this repository is not used
     for node installation, it needs to be present. To work around this issue,
     it is sufficient to create an empty Pool repository for SUSE Linux Enterprise Server 12 SP4:
    </p><div class="verbatim-wrap"><pre class="screen">mkdir /srv/tftpboot/suse-12.4/x86_64/repos/SLES12-SP4-Pool/
createrepo /srv/tftpboot/suse-12.4/x86_64/repos/SLES12-SP4-Pool/</pre></div></section><section class="sect2" id="sec-depl-adm-conf-repos-scc-alternatives" data-id-title="Alternative Ways to Make the Repositories Available"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.2.4 </span><span class="title-name">Alternative Ways to Make the Repositories Available</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-conf-repos-scc-alternatives">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
     If you want to keep your SUSE <span class="productname">OpenStack</span> Cloud network as isolated from the company
     network as possible, or your infrastructure does not allow accessing a
     SUSE Manager or an SMT server, you can alternatively provide access to the
     required repositories by one of the following methods:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Mount the repositories from a remote server.
      </p></li><li class="listitem"><p>
       Synchronize the repositories from a remote server (for example via
       <code class="command">rsync</code> and cron).
      </p></li><li class="listitem"><p>
        Manually synchronize the update repositories from removable media.
      </p></li></ul></div><p>
     We strongly recommended making the repositories available at the
     default locations on the Administration Server as listed in <a class="xref" href="#tab-depl-adm-conf-local-repos" title="Default Repository Locations on the Administration Server">Table 5.5, “Default Repository Locations on the Administration Server”</a>. When choosing these locations,
     it is sufficient to set the repository location in YaST Crowbar to
     <span class="guimenu">Custom</span>. You do not need to specify a detailed location
     for each repository. Refer to <a class="xref" href="#sec-depl-adm-inst-crowbar-repos" title="7.4. Repositories">Section 7.4, “<span class="guimenu">Repositories</span>”</a> for details. If you prefer to
     use different locations, you need to announce each location with YaST
     Crowbar.
    </p></section></section><section class="sect1" id="sec-depl-inst-admserv-post-adm-repos" data-id-title="Software Repository Sources for the Administration Server Operating System"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.3 </span><span class="title-name">
    Software Repository Sources for the Administration Server Operating System
   </span></span> <a title="Permalink" class="permalink" href="#sec-depl-inst-admserv-post-adm-repos">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   During the installation of the Administration Server, repository locations for SUSE Linux Enterprise Server 12 SP4
   are automatically added to the Administration Server. They point to the source
   used to install the Administration Server and to the SUSE Customer Center. These repository locations
   have no influence on the repositories used to set up nodes in the cloud. They
   are solely used to maintain and update the Administration Server itself.
  </p><p>
   However, as the Administration Server and all nodes in the cloud use the same
   operating system—SUSE Linux Enterprise Server 12 SP4—it makes sense to use the same
   repositories for the cloud and the Administration Server. To avoid
   downloading the same patches twice, change this setup so that the repositories set up for SUSE <span class="productname">OpenStack</span> Cloud deployment are also used
   on the Administration Server.
  </p><p>
   To do so, you need to disable or delete all services. In a second step all
   SUSE Linux Enterprise Server and SUSE <span class="productname">OpenStack</span> Cloud repositories need to be edited to point to the
   alternative sources. Use either Zypper or YaST to edit the repository setup. Note that changing the repository setup on the Administration Server
   is optional.
  </p></section><section class="sect1" id="sec-deploy-repo-locations" data-id-title="Repository Locations"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.4 </span><span class="title-name">Repository Locations</span></span> <a title="Permalink" class="permalink" href="#sec-deploy-repo-locations">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
  The following tables show the locations of all repositories that can be
  used for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
 </p><div class="table" id="tab-smt-repos-local" data-id-title="SMT Repositories Hosted on the Administration Server"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 5.2: </span><span class="title-name">SMT Repositories Hosted on the Administration Server </span></span><a title="Permalink" class="permalink" href="#tab-smt-repos-local">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Repository
      </p>
     </th><th style="border-bottom: 1px solid ; ">
      <p>
       Directory
      </p>
     </th></tr></thead><tbody><tr><td style="border-bottom: 1px solid ; " colspan="2">
      <p>
       Mandatory Repositories
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       SLES12-SP4-Pool
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Products/SLE-SERVER/12-SP4/x86_64/product/</code>
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       SLES12-SP4-Updates
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Updates/SLE-SERVER/12-SP4/x86_64/update/</code>
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-9</span></span>-Pool
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Products/OpenStack-Cloud-Crowbar/9/x86_64/product/</code>
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-9</span></span>-Updates
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Updates/OpenStack-Cloud-Crowbar/9/x86_64/update/</code>
      </p>
     </td></tr><tr><td style="border-bottom: 1px solid ; " colspan="2">
      <p>
       Optional Repositories
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       SLE-HA12-SP4-Pool
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Products/SLE-HA/12-SP4/x86_64/product/</code>
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       SLE-HA12-SP4-Updates
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Updates/SLE-HA/12-SP4/x86_64/update/</code>
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       SUSE-Enterprise-Storage-5-Pool
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Products/Storage/5/x86_64/product/</code>
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; ">
      <p>
       SUSE-Enterprise-Storage-5-Updates
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Updates/Storage/5/x86_64/update/</code>
      </p>
     </td></tr></tbody></table></div></div><div class="table" id="tab-smt-repos-remote" data-id-title="SMT Repositories hosted on a Remote Server"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 5.3: </span><span class="title-name">SMT Repositories hosted on a Remote Server </span></span><a title="Permalink" class="permalink" href="#tab-smt-repos-remote">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Repository
      </p>
     </th><th style="border-bottom: 1px solid ; ">
      <p>
       URl
      </p>
     </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Mandatory Repositories
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       SLES12-SP4-Pool
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>http://<em class="replaceable">smt.example.com</em>/repo/SUSE/Products/SLE-SERVER/12-SP4/x86_64/product/</p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       SLES12-SP4-Updates
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>http://<em class="replaceable">smt.example.com</em>/repo/SUSE/Updates/SLE-SERVER/12-SP4/x86_64/update/</p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-9</span></span>-Pool
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>http://<em class="replaceable">smt.example.com</em>/repo/SUSE/Products/OpenStack-Cloud/9/x86_64/product/</p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-9</span></span>-Updates
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>http://<em class="replaceable">smt.example.com</em>/repo/SUSE/Updates/OpenStack-Cloud/9/x86_64/update/</p>
     </td></tr><tr><td style="border-bottom: 1px solid ; " colspan="2">
      <p>
       Optional Repositories
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       SLE-HA12-SP4-Pool
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>http://<em class="replaceable">smt.example.com</em>/repo/SUSE/Products/SLE-HA/12-SP4/x86_64/product/</p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       SLE-HA12-SP4-Updates
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>http://<em class="replaceable">smt.example.com</em>/repo/SUSE/Updates/SLE-HA/12-SP4/x86_64/update/</p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       SUSE-Enterprise-Storage-5-Pool
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>http://<em class="replaceable">smt.example.com</em>/repo/SUSE/Products/Storage/4/x86_64/product/</p>
     </td></tr><tr><td style="border-right: 1px solid ; ">
      <p>
       SUSE-Enterprise-Storage-5-Updates
      </p>
     </td><td>
      <p>http://<em class="replaceable">smt.example.com</em>/repo/SUSE/Updates/Storage/4/x86_64/update/</p>
     </td></tr></tbody></table></div></div><div class="table" id="tab-depl-adm-conf-susemgr-repos" data-id-title="SUSE Manager Repositories (Channels)"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 5.4: </span><span class="title-name">SUSE Manager Repositories (Channels) </span></span><a title="Permalink" class="permalink" href="#tab-depl-adm-conf-susemgr-repos">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Repository
      </p>
     </th><th style="border-bottom: 1px solid ; ">
      <p>
       URL
      </p>
     </th></tr></thead><tbody><tr><td style="border-bottom: 1px solid ; " colspan="2">
      <p>
       Mandatory Repositories
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       SLES12-SP4-Updates
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       http://manager.example.com/ks/dist/child/sles12-sp4-updates-x86_64/sles12-sp4-x86_64/
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-9</span></span>-Pool
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       
       http://manager.example.com/ks/dist/child/suse-openstack-cloud-9-pool-x86_64/sles12-sp4-x86_64/
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-9</span></span>--Updates
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       
       http://manager.example.com/ks/dist/child/suse-openstack-cloud-9-updates-x86_64/sles12-sp4-x86_64/
      </p>
     </td></tr><tr><td style="border-bottom: 1px solid ; " colspan="2">
      <p>
       Optional Repositories
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       SLE-HA12-SP4-Pool
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       http://manager.example.com/ks/dist/child/sle-ha12-sp4-pool-x86_64/sles12-sp4-x86_64/
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       SLE-HA12-SP4-Updates
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       http://manager.example.com/ks/dist/child/sle-ha12-sp4-updates-x86_64/sles12-sp4-x86_64/
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       SUSE-Enterprise-Storage-5-Pool
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       
       http://manager.example.com/ks/dist/child/suse-enterprise-storage-2.1-pool-x86_64/sles12-sp4-x86_64/
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; ">
      <p>
       SUSE-Enterprise-Storage-5-Updates
      </p>
     </td><td>
      <p>
       
       http://manager.example.com/ks/dist/child/suse-enterprise-storage-5-updates-x86_64/sles12-sp4-x86_64/
      </p>
     </td></tr></tbody></table></div></div><p>
  The following table shows the recommended default repository locations to use when manually copying, synchronizing, or mounting the
  repositories. When choosing these locations, it is sufficient to set the
  repository location in YaST Crowbar to <span class="guimenu">Custom</span>. You do
  not need to specify a detailed location for each repository. Refer to <a class="xref" href="#sec-depl-adm-conf-repos-scc-alternatives" title="5.2.4. Alternative Ways to Make the Repositories Available">Section 5.2.4, “Alternative Ways to Make the Repositories Available”</a> and <a class="xref" href="#sec-depl-adm-inst-crowbar-repos" title="7.4. Repositories">Section 7.4, “<span class="guimenu">Repositories</span>”</a> for details.
 </p><div class="table" id="tab-depl-adm-conf-local-repos" data-id-title="Default Repository Locations on the Administration Server"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 5.5: </span><span class="title-name">Default Repository Locations on the Administration Server </span></span><a title="Permalink" class="permalink" href="#tab-depl-adm-conf-local-repos">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Channel
      </p>
     </th><th style="border-bottom: 1px solid ; ">
      <p>
       Directory on the Administration Server
      </p>
     </th></tr></thead><tbody><tr><td style="border-bottom: 1px solid ; " colspan="2">
      <p>
       Mandatory Repositories
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       SLES12-SP4-Pool
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       <code class="filename">/srv/tftpboot/suse-12.4/x86_64/repos/SLES12-SP4-Pool/</code>
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       SLES12-SP4-Updates
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       <code class="filename">/srv/tftpboot/suse-12.4/x86_64/repos/SLES12-SP4-Updates/</code>
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-9</span></span>-Pool
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       <code class="filename">/srv/tftpboot/suse-12.4/x86_64/repos/<span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-9</span></span>-Pool/</code>
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-9</span></span>-Updates
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       <code class="filename">/srv/tftpboot/suse-12.4/x86_64/repos/<span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-9</span></span>-Updates</code>
      </p>
     </td></tr><tr><td style="border-bottom: 1px solid ; " colspan="2">
      <p>
       Optional Repositories
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       SLE-HA12-SP4-Pool
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       <code class="filename">/srv/tftpboot/suse-12.4/x86_64/repos/SLE-HA12-SP4-Pool</code>
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       SLE-HA12-SP4-Updates
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       <code class="filename">/srv/tftpboot/suse-12.4/x86_64/repos/SLE-HA12-SP4-Updates</code>
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       SUSE-Enterprise-Storage-5-Pool
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       <code class="filename">/srv/tftpboot/suse-12.4/x86_64/repos/SUSE-Enterprise-Storage-5-Pool</code>
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; ">
      <p>
       SUSE-Enterprise-Storage-5-Updates
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/tftpboot/suse-12.4/x86_64/repos/SUSE-Enterprise-Storage-5-Updates</code>
      </p>
     </td></tr></tbody></table></div></div></section></section><section class="chapter" id="sec-depl-adm-inst-network" data-id-title="Service Configuration: Administration Server Network Configuration"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">6 </span><span class="title-name">Service Configuration:  Administration Server Network Configuration</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-network">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><div><div class="abstract"><p>
    Prior to starting the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation, make sure the first network
    interface (<code class="systemitem">eth0</code>) gets a
    fixed IP address from the admin network. A host and domain name
    also need to be provided. Other interfaces will be automatically
    configured during the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation.
   </p></div></div></div></div><p>
  To configure the network interface proceed as follows:
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Start <span class="guimenu">YaST</span> › <span class="guimenu">System</span> › <span class="guimenu">Network Settings</span>.
   </p></li><li class="step"><p>
    Switch to the <span class="guimenu">Overview</span> tab, select the interface
    with the <span class="guimenu">Device</span> identifier, <code class="literal">eth0</code> and
    choose <span class="guimenu">Edit</span>.
   </p></li><li class="step"><p>
    Switch to the <span class="guimenu">Address</span> tab and activate
    <span class="guimenu">Statically Assigned IP Address</span>. Provide an IPv4
    <span class="guimenu">IP Address</span>, a <span class="guimenu">Subnet Mask</span>, and a
    fully qualified <span class="guimenu">Hostname</span>. Examples in this book assume
    the default IP address of <code class="systemitem">192.168.124.10</code> and a network mask of
    <code class="systemitem">255.255.255.0</code>. Using a
    different IP address requires adjusting the Crowbar configuration in a
    later step as described in <a class="xref" href="#sec-depl-adm-inst-crowbar" title="Chapter 7. Crowbar Setup">Chapter 7, <em>Crowbar Setup</em></a>.
   </p></li><li class="step"><p>
    Check the settings on the <span class="guimenu">General</span> tab. The device needs
    to be activated <span class="guimenu">At Boot Time</span>. Confirm your settings
    with <span class="guimenu">Next</span>.
   </p></li><li class="step"><p>
    Back on the <span class="guimenu">Network Settings</span> dialog, switch to the
    <span class="guimenu">Routing</span> tab and enter a <span class="guimenu">Default IPv4
    Gateway</span>. The address depends on whether you
    have provided an external gateway for the admin network. In that case, use the address
    of that gateway. If not, use <em class="replaceable">xxx.xxx.xxx</em>.1, for
    example, <code class="systemitem">192.168.124.1</code>. Confirm your settings
    with <span class="guimenu">OK</span>.
   </p></li><li class="step"><p>
    Choose <span class="guimenu">Hostname/DNS</span> from the <span class="guimenu">Network
    Settings</span> dialog and set the <span class="guimenu">Hostname</span> and
    <span class="guimenu">Domain Name</span>. Examples in this book assume     <em class="replaceable">admin.cloud.example.com</em> for the host/domain
    name.
   </p><p>
    If the Administration Server has access to the outside, you can add additional name
    servers here that will automatically be used to forward requests. The
    Administration Server's name server will automatically be configured during the
    SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation to forward requests for non-local records to those
    server(s).
   </p></li><li class="step"><p>
    Last, check if the firewall is disabled. Return to YaST's main menu
    (<span class="guimenu">YaST Control Center</span>) and start <span class="guimenu">Security and Users</span> › <span class="guimenu">Firewall</span>. On <span class="guimenu">Start-Up</span> › <span class="guimenu">Service Start</span>, the firewall needs to be
    disabled. Confirm your settings with <span class="guimenu">Next</span>.
   </p></li></ol></div></div><div id="id-1.4.4.5.4" data-id-title="Administration Server Domain Name and Host name" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Administration Server Domain Name and Host name</div><p>
   Setting up the SUSE <span class="productname">OpenStack</span> Cloud will also install a DNS server for all nodes in the
   cloud. The domain name you specify for the Administration Server will be used for the
   DNS zone. It is required to use a sub-domain such as
   <em class="replaceable">cloud.example.com</em>. See <a class="xref" href="#sec-depl-req-network-dns" title="2.1.4. DNS and Host Names">Section 2.1.4, “DNS and Host Names”</a> for more information.
  </p><p>
   The host name and the FQDN need to be resolvable with
   <code class="command">hostname</code> <code class="option">-f</code>. Double-check whether
   <code class="filename">/etc/hosts</code> contains an appropriate entry for the
   Administration Server. It should look like the following:
  </p><div class="verbatim-wrap"><pre class="screen">192.168.124.10 admin.cloud.example.com admin</pre></div><p>
   It is <span class="emphasis"><em>not</em></span> possible to change the Administration Server host name
   or the FQDN after the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation has been completed.
  </p></div></section><section class="chapter" id="sec-depl-adm-inst-crowbar" data-id-title="Crowbar Setup"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">7 </span><span class="title-name">Crowbar Setup</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-crowbar">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><div><div class="abstract"><p>
    The YaST Crowbar module enables you to configure all networks within the
    cloud, to set up additional repositories, and to manage the Crowbar users.
    This module should be launched before starting the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation. To start
    this module, either run <code class="command">yast crowbar</code> or <span class="guimenu">YaST</span> › <span class="guimenu">Miscellaneous</span> › <span class="guimenu">Crowbar</span>.
   </p></div></div></div></div><section class="sect1" id="sec-depl-adm-inst-crowbar-user" data-id-title="User Settings"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.1 </span><span class="title-name"><span class="guimenu">User Settings</span></span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-crowbar-user">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   In this section, you can manage the administration user for the Crowbar Web
   interface. Use the entries to change the user name and the password. The
   preconfigured user is <code class="systemitem">crowbar</code>
   (password <code class="literal">crowbar</code>). This administration user configured
   here has no relation to any existing system user on the
   Administration Server.
  </p><div class="figure" id="id-1.4.4.6.2.3"><div class="figure-contents"><div class="mediaobject"><a href="images/yast_crowbar_user.png"><img src="images/yast_crowbar_user.png" alt="YaST Crowbar Setup: User Settings" title="YaST Crowbar Setup: User Settings"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 7.1: </span><span class="title-name">YaST Crowbar Setup: User Settings </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.6.2.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></section><section class="sect1" id="sec-depl-adm-inst-crowbar-network" data-id-title="Networks"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.2 </span><span class="title-name"><span class="guimenu">Networks</span></span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-crowbar-network">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   Use the <span class="guimenu">Networks</span> tab to change the default network setup
   (described in <a class="xref" href="#sec-depl-req-network" title="2.1. Network">Section 2.1, “Network”</a>). Change the IP address
   assignment for each network under <span class="guimenu">Edit Ranges</span>. You may
   also add a bridge (<span class="guimenu">Add Bridge</span>) or a VLAN (<span class="guimenu">Use
   VLAN</span>, <span class="guimenu">VLAN ID</span>) to a network. Only change the
   latter two settings if you really know what you require; we recommend
   sticking with the defaults.
  </p><div class="figure" id="id-1.4.4.6.3.3"><div class="figure-contents"><div class="mediaobject"><a href="images/yast_crowbar_networks.png"><img src="images/yast_crowbar_networks.png" alt="YaST Crowbar Setup: Network Settings" title="YaST Crowbar Setup: Network Settings"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 7.2: </span><span class="title-name">YaST Crowbar Setup: Network Settings </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.6.3.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><div id="id-1.4.4.6.3.4" data-id-title="No Network Changes After Completing the SUSE OpenStack Cloud Crowbar installation" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: No Network Changes After Completing the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation</div><p>
    After you have completed the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation, you cannot change the network
    setup. If you do need to change it, you must completely set up the
    Administration Server again.
   </p></div><div id="id-1.4.4.6.3.5" data-id-title="VLAN Settings" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: VLAN Settings</div><p>
    As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8, using a VLAN for the admin network is
    only supported on a native/untagged VLAN. If you need VLAN support for the
    admin network, it must be handled at switch level.
   </p><p>
    When changing the network configuration with YaST or by editing
    <code class="filename">/etc/crowbar/network.json</code>, you can define VLAN
    settings for each network. For the networks <code class="literal">nova-fixed</code>
    and <code class="literal">nova-floating</code>, however, special rules apply:
   </p><p>
    <span class="bold"><strong>nova-fixed</strong></span>: The <span class="guimenu">USE
    VLAN</span> setting will be ignored. However, VLANs will automatically
    be used if deploying neutron with VLAN support (using the drivers
    linuxbridge, openvswitch plus VLAN, or cisco_nexus). In this case, you need
    to specify a correct <span class="guimenu">VLAN ID</span> for this network.
   </p><p>
    <span class="bold"><strong>nova-floating</strong></span>: When using a VLAN for
    <code class="literal">nova-floating</code> (which is the default), the <span class="guimenu">USE
    VLAN</span> and <span class="guimenu">VLAN ID</span> settings for
    <span class="guimenu">nova-floating</span> and <span class="guimenu">public</span> default to
    the same.
   </p><p>
    You have the option of separating public and floating networks with a
    custom configuration. Configure your own separate floating network (not as
    a subnet of the public network), and give the floating network its own
    router. For example, define <code class="literal">nova-floating</code> as part of an
    external network with a custom <code class="literal">bridge-name</code>. When you are
    using different networks and OpenVSwitch is configured, the pre-defined
    <code class="literal">bridge-name</code> won't work.
   </p></div><p>
   Other, more flexible network mode setups, can be configured by manually
   editing the Crowbar network configuration files. See
   <a class="xref" href="#sec-depl-inst-admserv-post-network" title="7.5. Custom Network Configuration">Section 7.5, “Custom Network Configuration”</a> for more information.
   SUSE or a partner can assist you in creating a custom setup within the
   scope of a consulting services agreement. See
   <a class="link" href="http://www.suse.com/consulting/" target="_blank">http://www.suse.com/consulting/</a> for more information on
   SUSE consulting.
  </p><section class="sect2" id="sec-depl-adm-inst-crowbar-network-bmc" data-id-title="Separating the Admin and the BMC Network"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.2.1 </span><span class="title-name">Separating the Admin and the BMC Network</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-crowbar-network-bmc">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    If you want to separate the admin and the BMC network, you must change the
    settings for the networks <span class="guimenu">bmc</span> and
    <span class="guimenu">bmc_vlan</span>. The <span class="guimenu">bmc_vlan</span> is used to
    generate a VLAN tagged interface on the Administration Server that can access the
    <span class="guimenu">bmc</span> network. The <span class="guimenu">bmc_vlan</span> needs to be
    in the same ranges as <span class="guimenu">bmc</span>, and <span class="guimenu">bmc</span>
    needs to have <span class="guimenu">VLAN</span> enabled.
   </p><div class="table" id="id-1.4.4.6.3.7.3" data-id-title="Separate BMC Network Example Configuration"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 7.1: </span><span class="title-name">Separate BMC Network Example Configuration </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.6.3.7.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        
       </th><th style="text-align: center; border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         bmc
        </p>
       </th><th style="text-align: center; border-bottom: 1px solid ; ">
        <p>
         bmc_vlan
        </p>
       </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Subnet
        </p>
       </td><td style="text-align: center; border-bottom: 1px solid ; " colspan="2">
        <p>
         <code class="systemitem">192.168.128.0</code>
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Netmask
        </p>
       </td><td style="text-align: center; border-bottom: 1px solid ; " colspan="2">
        <p>
         <code class="systemitem">255.255.255.0</code>
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Router
        </p>
       </td><td style="text-align: center; border-bottom: 1px solid ; " colspan="2">
        <p>
         <code class="systemitem">192.168.128.1</code>
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Broadcast
        </p>
       </td><td style="text-align: center; border-bottom: 1px solid ; " colspan="2">
        <p>
         <code class="systemitem">192.168.128.255</code>
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Host Range
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="systemitem">192.168.128.10</code> -
         <code class="systemitem">192.168.128.100</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         <code class="systemitem">192.168.128.101</code> -
         <code class="systemitem">192.168.128.101</code>
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         VLAN
        </p>
       </td><td style="text-align: center; border-bottom: 1px solid ; " colspan="2">
        <p>
         yes
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         VLAN ID
        </p>
       </td><td style="text-align: center; border-bottom: 1px solid ; " colspan="2">
        <p>
         100
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; ">
        <p>
         Bridge
        </p>
       </td><td style="text-align: center; " colspan="2">
        <p>
         no
        </p>
       </td></tr></tbody></table></div></div><div class="figure" id="id-1.4.4.6.3.7.4"><div class="figure-contents"><div class="mediaobject"><a href="images/yast_crowbar_networks_bmc.png"><img src="images/yast_crowbar_networks_bmc.png" alt="YaST Crowbar Setup: Network Settings for the BMC Network" title="YaST Crowbar Setup: Network Settings for the BMC Network"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 7.3: </span><span class="title-name">YaST Crowbar Setup: Network Settings for the BMC Network </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.6.3.7.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></section></section><section class="sect1" id="sec-depl-adm-inst-crowbar-mode" data-id-title="Network Mode"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.3 </span><span class="title-name"><span class="guimenu">Network Mode</span></span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-crowbar-mode">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   On the <span class="guimenu">Network Mode</span> tab you can choose between
   <span class="guimenu">single</span>, <span class="guimenu">dual</span>, and
   <span class="guimenu">team</span>. In single mode, all traffic is handled by a single
   Ethernet card. Dual mode requires two Ethernet cards and separates traffic
   for private and public networks. See
   <a class="xref" href="#sec-depl-req-network-modes" title="2.1.2. Network Modes">Section 2.1.2, “Network Modes”</a> for details.
  </p><p>
   Team mode is similar to single mode, except that you combine several
   Ethernet cards to a “bond”. It is required for an HA setup of SUSE <span class="productname">OpenStack</span> Cloud.
   When choosing this mode, you also need to specify a <span class="guimenu">Bonding
   Policy</span>. This option lets you define whether to focus on
   reliability (fault tolerance), performance (load balancing), or a
   combination of both. You can choose from the following modes:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.6.4.4.1"><span class="term"><span class="guimenu">0</span> (balance-rr)</span></dt><dd><p>
      Default mode in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. Packets are transmitted in round-robin
      fashion from the first to the last available interface. Provides fault
      tolerance and load balancing.
     </p></dd><dt id="id-1.4.4.6.4.4.2"><span class="term"><span class="guimenu">1</span> (active-backup)</span></dt><dd><p>
      Only one network interface is active. If it fails, a different interface
      becomes active. This setting is the default for SUSE <span class="productname">OpenStack</span> Cloud. Provides fault
      tolerance.
     </p></dd><dt id="id-1.4.4.6.4.4.3"><span class="term"><span class="guimenu">2</span> (balance-xor)</span></dt><dd><p>
      Traffic is split between all available interfaces based on the following
      policy: <code class="literal">[(source MAC address XOR'd with destination MAC address
      XOR packet type ID) modulo slave count]</code> Requires support from
      the switch. Provides fault tolerance and load balancing.
     </p></dd><dt id="id-1.4.4.6.4.4.4"><span class="term"><span class="guimenu">3</span> (broadcast)</span></dt><dd><p>
      All traffic is broadcast on all interfaces. Requires support from the
      switch. Provides fault tolerance.
     </p></dd><dt id="id-1.4.4.6.4.4.5"><span class="term"><span class="guimenu">4</span> (802.3ad)</span></dt><dd><p>
      Aggregates interfaces into groups that share the same speed and duplex
      settings. Requires <code class="command">ethtool</code> support in the interface
      drivers, and a switch that supports and is configured for IEEE 802.3ad
      Dynamic link aggregation. Provides fault tolerance and load balancing.
     </p></dd><dt id="id-1.4.4.6.4.4.6"><span class="term"><span class="guimenu">5</span> (balance-tlb)</span></dt><dd><p>
      Adaptive transmit load balancing. Requires <code class="command">ethtool</code>
      support in the interface drivers but no switch support. Provides fault
      tolerance and load balancing.
     </p></dd><dt id="id-1.4.4.6.4.4.7"><span class="term"><span class="guimenu">6</span> (balance-alb)</span></dt><dd><p>
      Adaptive load balancing. Requires <code class="command">ethtool</code> support in
      the interface drivers but no switch support. Provides fault tolerance and
      load balancing.
     </p></dd></dl></div><p>
   For a more detailed description of the modes, see
   <a class="link" href="https://www.kernel.org/doc/Documentation/networking/bonding.txt" target="_blank">https://www.kernel.org/doc/Documentation/networking/bonding.txt</a>.
  </p><section class="sect2" id="sec-depl-adm-inst-crowbar-mode-bastion" data-id-title="Setting Up a Bastion Network"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.3.1 </span><span class="title-name">Setting Up a Bastion Network</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-crowbar-mode-bastion">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    The <span class="guimenu">Network Mode</span> tab of the YaST Crowbar module also
    lets you set up a Bastion network. As outlined in
    <a class="xref" href="#sec-depl-req-network" title="2.1. Network">Section 2.1, “Network”</a>, one way to access the Administration Server
    from a defined external network is via a Bastion network and a second
    network card (as opposed to providing an external gateway).
   </p><p>
    To set up the Bastion network, you need to have a static IP address for the
    Administration Server from the external network. The example configuration used below
    assumes that the external network from which to access the admin network
    has the following addresses. Adjust them according to your needs.
   </p><div class="table" id="id-1.4.4.6.4.6.4" data-id-title="Example Addresses for a Bastion Network"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 7.2: </span><span class="title-name">Example Addresses for a Bastion Network </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.6.4.6.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Subnet
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         <code class="systemitem">10.10.1.0</code>
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Netmask
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         <code class="systemitem">255.255.255.0</code>
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Broadcast
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         <code class="systemitem">10.10.1.255</code>
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Gateway
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         <code class="systemitem">10.10.1.1</code>
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; ">
        <p>
         Static Administration Server address
        </p>
       </td><td>
        <p>
         <code class="systemitem">10.10.1.125</code>
        </p>
       </td></tr></tbody></table></div></div><p>
    In addition to the values above, you need to enter the <span class="guimenu">Physical
    Interface Mapping</span>. With this value you specify the Ethernet card
    that is used for the bastion network. See
    <a class="xref" href="#sec-deploy-network-json-conduits" title="7.5.5. Network Conduits">Section 7.5.5, “Network Conduits”</a> for details on the
    syntax. The default value <code class="literal">?1g2</code> matches the second
    interface (<span class="quote">“<span class="quote">eth1</span>”</span>) of the system.
   </p><div class="figure" id="id-1.4.4.6.4.6.6"><div class="figure-contents"><div class="mediaobject"><a href="images/yast_crowbar_networks_bastion.png"><img src="images/yast_crowbar_networks_bastion.png" alt="YaST Crowbar Setup: Network Settings for the Bastion Network" title="YaST Crowbar Setup: Network Settings for the Bastion Network"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 7.4: </span><span class="title-name">YaST Crowbar Setup: Network Settings for the Bastion Network </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.6.4.6.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><div id="id-1.4.4.6.4.6.7" data-id-title="No Network Changes After Completing the SUSE OpenStack Cloud Crowbar installation" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: No Network Changes After Completing the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation</div><p>
     After you have completed the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation, you cannot change the network
     setup. If you do need to change it, you must completely set up the
     Administration Server again.
    </p></div><div id="id-1.4.4.6.4.6.8" data-id-title="Accessing Nodes From Outside the Bastion Network" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Accessing Nodes From Outside the Bastion Network</div><p>
     The example configuration from above allows access to the SUSE <span class="productname">OpenStack</span> Cloud nodes
     from <span class="emphasis"><em>within</em></span> the bastion network. If you want to
     access nodes from outside the bastion network, make the router for the
     bastion network the default router for the Administration Server. This is achieved by
     setting the value for the bastion network's <span class="guimenu">Router
     preference</span> entry to a lower value than the corresponding entry
     for the admin network. By default no router preference is set for the
     Administration Server—in this case, set the preference for the bastion network
     to <code class="literal">5</code>.
    </p><p>
     If you use a Linux gateway between the outside and the bastion network,
     you also need to disable route verification (rp_filter) on the Administration Server.
     Do so by running the following command on the Administration Server:
    </p><div class="verbatim-wrap"><pre class="screen">echo 0 &gt; /proc/sys/net/ipv4/conf/all/rp_filter</pre></div><p>
     That command disables route verification for the current session, so the
     setting will not survive a reboot. Make it permanent by editing
     <code class="filename">/etc/sysctl.conf</code> and setting the value for
     <span class="guimenu">net.ipv4.conf.all.rp_filter</span> to <code class="literal">0</code>.
    </p></div></section></section><section class="sect1" id="sec-depl-adm-inst-crowbar-repos" data-id-title="Repositories"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.4 </span><span class="title-name"><span class="guimenu">Repositories</span></span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-crowbar-repos">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   This dialog lets you announce the locations of the product, pool, and update
   repositories (see <a class="xref" href="#cha-depl-repo-conf" title="Chapter 5. Software Repository Setup">Chapter 5, <em>Software Repository Setup</em></a> for details). You can
   choose between four alternatives:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.6.5.3.1"><span class="term"><span class="guimenu">Local SMT Server</span>
    </span></dt><dd><p>
      If you have an SMT server installed on the Administration Server as explained in
      <a class="xref" href="#app-deploy-smt" title="Chapter 4. Installing and Setting Up an SMT Server on the Administration Server (Optional)">Chapter 4, <em>Installing and Setting Up an SMT Server on the Administration Server (Optional)</em></a>, choose this option. The repository
      details do not need to be provided as they will be configured
      automatically. This option will be applied by default if the repository
      configuration has not been changed manually.
     </p></dd><dt id="id-1.4.4.6.5.3.2"><span class="term"><span class="guimenu">Remote SMT Server</span>
    </span></dt><dd><p>
      If you use a remote SMT for <span class="emphasis"><em>all</em></span> repositories,
      choose this option and provide the <span class="guimenu">Sever URL</span> (in the
      form of <code class="literal">http://smt.example.com</code>). The repository
      details do not need to be provided, they will be configured
      automatically.
     </p></dd><dt id="id-1.4.4.6.5.3.3"><span class="term"><span class="guimenu">SUSE Manager Server</span>
    </span></dt><dd><p>
      If you use a remote SUSE Manager server for <span class="emphasis"><em>all</em></span>
      repositories, choose this option and provide the <span class="guimenu">Sever
      URL</span> (in the form of
      <code class="literal">http://manager.example.com</code>).
     </p></dd><dt id="id-1.4.4.6.5.3.4"><span class="term">Custom</span></dt><dd><p>
      If you use different sources for your repositories or are using
      non-standard locations, choose this option and manually provide a
      location for each repository. This can either be a local directory
      (<code class="filename">/srv/tftpboot/suse-12.4/x86_64/repos/SLES12-SP4-Pool/</code>)
      or a remote location
      (<code class="literal">http://manager.example.com/ks/dist/child/sles12-sp4-updates-x86_64/sles12-sp4-x86_64/</code>).
      Activating <span class="guimenu">Ask On Error</span> ensures that you will be
      informed if a repository is not available during node deployment,
      otherwise errors will be silently ignored.
     </p><p>
      The <span class="guimenu">Add Repository</span> dialog allows adding additional
      repositories. See
      <a class="xref" href="#q-depl-trouble-faq-admin-custom-repos" title="Q:"><em>How to make custom software repositories from an external server (for example a remote SMT or SUSE M..?</em></a> for
      instructions.
     </p><div id="id-1.4.4.6.5.3.4.2.3" data-id-title="Default Locations" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Default Locations</div><p>
       If you have made the repositories available in the default locations on
       the Administration Server (see
       <a class="xref" href="#tab-depl-adm-conf-local-repos" title="Default Repository Locations on the Administration Server">Table 5.5, “Default Repository Locations on the Administration Server”</a> for a list),
       choose <span class="guimenu">Custom</span> and leave the <span class="guimenu">Repository
       URL</span> empty (default). The repositories will automatically be
       detected.
      </p></div></dd></dl></div><div class="figure" id="id-1.4.4.6.5.4"><div class="figure-contents"><div class="mediaobject"><a href="images/yast_crowbar_repos.png"><img src="images/yast_crowbar_repos.png" alt="YaST Crowbar Setup: Repository Settings" title="YaST Crowbar Setup: Repository Settings"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 7.5: </span><span class="title-name">YaST Crowbar Setup: Repository Settings </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.6.5.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></section><section class="sect1" id="sec-depl-inst-admserv-post-network" data-id-title="Custom Network Configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.5 </span><span class="title-name">Custom Network Configuration</span></span> <a title="Permalink" class="permalink" href="#sec-depl-inst-admserv-post-network">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   To adjust the pre-defined network setup of SUSE <span class="productname">OpenStack</span> Cloud beyond the scope of
   changing IP address assignments (as described in
   <a class="xref" href="#sec-depl-adm-inst-crowbar" title="Chapter 7. Crowbar Setup">Chapter 7, <em>Crowbar Setup</em></a>), modify the network barclamp
   template.
  </p><p>
   The Crowbar network barclamp provides two functions for the system. The first
   is a common role to instantiate network interfaces on the Crowbar managed
   systems. The other function is address pool management. While the addresses
   can be managed with the YaST Crowbar module, complex network setups require
   to manually edit the network barclamp template file
   <code class="filename">/etc/crowbar/network.json</code>. This section explains the
   file in detail. Settings in this file are applied to all nodes in SUSE <span class="productname">OpenStack</span> Cloud.
   (See <a class="xref" href="#sec-network-json-resolve" title="7.5.11. Matching Logical and Physical Interface Names with network-json-resolve">Section 7.5.11, “Matching Logical and Physical Interface Names with network-json-resolve”</a> to learn how to verify
   your correct network interface names.)
  </p><div id="id-1.4.4.6.6.4" data-id-title="No Network Changes After Completing the SUSE OpenStack Cloud Crowbar installation" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: No Network Changes After Completing the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation</div><p>
    After you have completed the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation installation, you cannot change
    the network setup. If you do need to change it, you must completely set up
    the Administration Server again.
   </p><p>
    The only exception to this rule is the interface map, which can be changed
    after setup. See <a class="xref" href="#sec-deploy-network-json-interface-map" title="7.5.3. Interface Map">Section 7.5.3, “Interface Map”</a>
    for details.
   </p></div><section class="sect2" id="sec-deploy-network-json-edit" data-id-title="Editing network.json"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.5.1 </span><span class="title-name">Editing <code class="filename">network.json</code></span></span> <a title="Permalink" class="permalink" href="#sec-deploy-network-json-edit">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    The <code class="filename">network.json</code> file is located in
    <code class="filename">/etc/crowbar/</code>. The template has the following general
    structure:
   </p><div class="verbatim-wrap"><pre class="screen">{
   "attributes" : {
      "network" : {
         "mode" : "<em class="replaceable">VALUE</em>",
         "start_up_delay" : <em class="replaceable">VALUE</em>,
         "teaming" : { "mode": <em class="replaceable">VALUE</em> },<span class="callout" id="structure-general">1</span>
         "enable_tx_offloading" : <em class="replaceable">VALUE</em>,
         "enable_rx_offloading" : <em class="replaceable">VALUE</em>,
         "interface_map"<span class="callout" id="structure-interface-map">2</span> : [
            ...
         ],
         "conduit_map"<span class="callout" id="structure-conduit">3</span> : [
            ...
         ],
         "networks"<span class="callout" id="structure-networks">4</span> : {
            ...
         },
      }
   }
}</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#structure-general"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      General attributes. Refer to
      <a class="xref" href="#sec-deploy-network-json-global" title="7.5.2. Global Attributes">Section 7.5.2, “Global Attributes”</a> for details.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#structure-interface-map"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Interface map section. Defines the order in which the physical network
      interfaces are to be used. Refer to
      <a class="xref" href="#sec-deploy-network-json-interface-map" title="7.5.3. Interface Map">Section 7.5.3, “Interface Map”</a> for details.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#structure-conduit"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Network conduit section defining the network modes and the network
      interface usage. Refer to
      <a class="xref" href="#sec-deploy-network-json-conduits" title="7.5.5. Network Conduits">Section 7.5.5, “Network Conduits”</a> for details.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#structure-networks"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Network definition section. Refer to
      <a class="xref" href="#sec-deploy-network-json-networks" title="7.5.7. Network Definitions">Section 7.5.7, “Network Definitions”</a> for details.
     </p></td></tr></table></div><div id="id-1.4.4.6.6.5.5" data-id-title="Order of Elements" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Order of Elements</div><p>
     The order in which the entries in the <code class="filename">network.json</code>
     file appear may differ from the one listed above. Use your editor's search
     function to find certain entries.
    </p></div></section><section class="sect2" id="sec-deploy-network-json-global" data-id-title="Global Attributes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.5.2 </span><span class="title-name">Global Attributes</span></span> <a title="Permalink" class="permalink" href="#sec-deploy-network-json-global">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    The most important options to define in the global attributes section are
    the default values for the network and bonding modes. The following global
    attributes exist:
   </p><div class="verbatim-wrap"><pre class="screen">{
   "attributes" : {
      "network" : {
         "mode" : "single",<span class="callout" id="global-mode">1</span>
         "start_up_delay" : 30,<span class="callout" id="global-startup">2</span>
         "teaming" : { "mode": 5 },<span class="callout" id="global-bonding">3</span>
         "enable_tx_offloading" : true, <span class="callout" id="global-tx">4</span>
         "enable_rx_offloading" : true, <a class="xref" href="#global-tx"><span class="callout">4</span></a>
         "interface_map" : [
            ...
         ],
         "conduit_map" : [
            ...
         ],
         "networks" : {
            ...
         },
      }
   }
}</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#global-mode"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Network mode. Defines the configuration name (or name space) to be used
      from the conduit_map (see
      <a class="xref" href="#sec-deploy-network-json-conduits" title="7.5.5. Network Conduits">Section 7.5.5, “Network Conduits”</a>). Your choices are
      single, dual, or team.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#global-startup"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Time (in seconds) the Chef-client waits for the network interfaces to
      come online before timing out.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#global-bonding"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Default bonding mode. For a list of available modes, see
      <a class="xref" href="#sec-depl-adm-inst-crowbar-mode" title="7.3. Network Mode">Section 7.3, “<span class="guimenu">Network Mode</span>”</a>.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#global-tx"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Turn on/off TX and RX checksum offloading. If set to
      <code class="literal">false</code>, disable offloading by running <code class="command">ethtool
      -K</code> and adding the setting to the respective ifcfg configuration
      file. If set to <code class="literal">true</code>, use the defaults of the network
      driver. If the network driver supports TX and/or RX checksum offloading
      and enables it by default, it will be used.
     </p><p>
      Checksum offloading is set to <code class="literal">true</code> in
      <code class="filename">network.json</code> by default. It is recommended to keep
      this setting. If you experience problems, such as package losses, try
      disabling this feature by setting the value to <code class="literal">false</code>.
     </p><div id="id-1.4.4.6.6.6.4.4.3" data-id-title="Change of the Default Value" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Change of the Default Value</div><p>
       Starting with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>, the default value for TX and RX checksum
       offloading changed from <code class="literal">false</code> to
       <code class="literal">true</code>.
      </p></div><p>
      To check which defaults a network driver uses, run <code class="command">ethtool
      -k</code>, for example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo ethtool -k eth0 | grep checksumming
rx-checksumming: on
tx-checksumming: on</pre></div><p>
      Note that if the output shows a value marked as
      <code class="literal">[fixed]</code>, this value cannot be changed. For more
      information on TX and RX checksum offloading refer to your hardware
      vendor's documentation. Detailed technical information can also be
      obtained from
      <a class="link" href="https://www.kernel.org/doc/Documentation/networking/checksum-offloads.txt" target="_blank">https://www.kernel.org/doc/Documentation/networking/checksum-offloads.txt</a>.
     </p></td></tr></table></div></section><section class="sect2" id="sec-deploy-network-json-interface-map" data-id-title="Interface Map"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.5.3 </span><span class="title-name">Interface Map</span></span> <a title="Permalink" class="permalink" href="#sec-deploy-network-json-interface-map">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    By default, physical network interfaces are used in the order they appear
    under <code class="filename">/sys/class/net/</code>. If you want to apply a
    different order, you need to create an interface map where you can specify
    a custom order of the bus IDs. Interface maps are created for specific
    hardware configurations and are applied to all machines matching this
    configuration.
   </p><div class="verbatim-wrap"><pre class="screen">{
   "attributes" : {
      "network" : {
         "mode" : "single",
         "start_up_delay" : 30,
         "teaming" : { "mode": 5 },
         "enable_tx_offloading" : true ,
         "enable_rx_offloading" : true ,
         "interface_map" : [
            {
              "pattern" : "PowerEdge R610"<span class="callout" id="interface-pattern">1</span>,
              "serial_number" : "0x02159F8E"<span class="callout" id="interface-serial">2</span>,
              "bus_order" : [<span class="callout" id="interface-bus">3</span>
                "0000:00/0000:00:01",
                "0000:00/0000:00:03"
              ]
            }
            ...
         ],
         "conduit_map" : [
            ...
         ],
         "networks" : {
            ...
         },
      }
   }
}</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#interface-pattern"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Hardware specific identifier. This identifier can be obtained by running
      the command <code class="command">dmidecode</code> <code class="option">-s
      system-product-name</code> on the machine you want to identify. You can
      log in to a node during the hardware discovery phase (when booting the
      SLEShammer image) via the Administration Server.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#interface-serial"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Additional hardware specific identifier. This identifier can be used in
      case two machines have the same value for <span class="guimenu">pattern</span>, but
      different interface maps are needed. Specifying this parameter is
      optional (it is not included in the default
      <code class="filename">network.json</code> file). The serial number of a machine
      can be obtained by running the command <code class="command">dmidecode</code>
      <code class="option">-s system-serial-number</code> on the machine you want to
      identify.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#interface-bus"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Bus IDs of the interfaces. The order in which they are listed here
      defines the order in which Chef addresses the interfaces. The IDs can
      be obtained by listing the contents of
      <code class="filename">/sys/class/net/</code>.
     </p></td></tr></table></div><div id="id-1.4.4.6.6.7.5" data-id-title="PXE Boot Interface Must be Listed First" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: PXE Boot Interface Must be Listed First</div><p>
     The physical interface used to boot the node via PXE must always be listed
     first.
    </p></div><div id="id-1.4.4.6.6.7.6" data-id-title="Interface Map Changes Allowed After Having Completed the SUSE OpenStack Cloud Crowbar Installation" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Interface Map Changes Allowed After Having Completed the SUSE <span class="productname">OpenStack</span> Cloud Crowbar Installation</div><p>
     Contrary to all other sections in <code class="filename">network.json</code>, you
     can change interface maps after completing the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation. However, nodes
     that are already deployed and affected by these changes must be deployed
     again. Therefore, we do not recommend making changes to the interface map
     that affect active nodes.
    </p><p>
     If you change the interface mappings after completing the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation you
     <span class="emphasis"><em>must not</em></span> make your changes by editing
     <code class="filename">network.json</code>. You must rather use the Crowbar Web
     interface and open <span class="guimenu">Barclamps</span> › <span class="guimenu">Crowbar</span> › <span class="guimenu">Network</span> › <span class="guimenu">Edit</span>. Activate your changes by clicking
     <span class="guimenu">Apply</span>.
    </p></div></section><section class="sect2" id="sec-deploy-network-json-interface-map-examples" data-id-title="Interface Map Example"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.5.4 </span><span class="title-name">Interface Map Example</span></span> <a title="Permalink" class="permalink" href="#sec-deploy-network-json-interface-map-examples">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="complex-example"><div class="example" id="interface-map-example" data-id-title="Changing the Network Interface Order on a Machine with four NICs"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 7.1: </span><span class="title-name">Changing the Network Interface Order on a Machine with four NICs </span></span><a title="Permalink" class="permalink" href="#interface-map-example">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="example-contents"><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
       Get the machine identifier by running the following command on the
       machine to which the map should be applied:
      </p><div class="verbatim-wrap"><pre class="screen">~ # dmidecode -s system-product-name
AS 2003R</pre></div><p>
       The resulting string needs to be entered on the
       <span class="guimenu">pattern</span> line of the map. It is interpreted as a Ruby
       regular expression (see
       <a class="link" href="http://www.ruby-doc.org/core-2.0/Regexp.html" target="_blank">http://www.ruby-doc.org/core-2.0/Regexp.html</a> for a
       reference). Unless the pattern starts with <code class="literal">^</code> and ends
       with <code class="literal">$</code>, a substring match is performed against the
       name returned from the above commands.
      </p></li><li class="listitem"><p>
       List the interface devices in <code class="filename">/sys/class/net</code> to get
       the current order and the bus ID of each interface:
      </p><div class="verbatim-wrap"><pre class="screen">~ # ls -lgG /sys/class/net/ | grep eth
lrwxrwxrwx 1 0 Jun 19 08:43 eth0 -&gt; ../../devices/pci0000:00/0000:00:1c.0/0000:09:00.0/net/eth0
lrwxrwxrwx 1 0 Jun 19 08:43 eth1 -&gt; ../../devices/pci0000:00/0000:00:1c.0/0000:09:00.1/net/eth1
lrwxrwxrwx 1 0 Jun 19 08:43 eth2 -&gt; ../../devices/pci0000:00/0000:00:1c.0/0000:09:00.2/net/eth2
lrwxrwxrwx 1 0 Jun 19 08:43 eth3 -&gt; ../../devices/pci0000:00/0000:00:1c.0/0000:09:00.3/net/eth3</pre></div><p>
       The bus ID is included in the path of the link target—it is the
       following string: <code class="filename">../../devices/pci<em class="replaceable">BUS
       ID</em>/net/eth0</code>
      </p></li><li class="listitem"><p>
       Create an interface map with the bus ID listed in the order the
       interfaces should be used. Keep in mind that the interface from which
       the node is booted using PXE must be listed first. In the following
       example the default interface order has been changed to
       <code class="systemitem">eth0</code>,
       <code class="systemitem">eth2</code>,
       <code class="systemitem">eth1</code> and
       <code class="systemitem">eth3</code>.
      </p><div class="verbatim-wrap"><pre class="screen">{
   "attributes" : {
      "network" : {
         "mode" : "single",
         "start_up_delay" : 30,
         "teaming" : { "mode": 5 },
         "enable_tx_offloading" : true,
         "enable_rx_offloading" : true,
         "interface_map" : [
            {
              "pattern" : "AS 2003R",
              "bus_order" : [
                "0000:00/0000:00:1c.0/0000:09:00.0",
                "0000:00/0000:00:1c.0/0000:09:00.2",
                "0000:00/0000:00:1c.0/0000:09:00.1",
                "0000:00/0000:00:1c.0/0000:09:00.3"
              ]
            }
            ...
         ],
         "conduit_map" : [
            ...
         ],
         "networks" : {
            ...
         },
      }
   }
}</pre></div></li></ol></div></div></div></div></section><section class="sect2" id="sec-deploy-network-json-conduits" data-id-title="Network Conduits"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.5.5 </span><span class="title-name">Network Conduits</span></span> <a title="Permalink" class="permalink" href="#sec-deploy-network-json-conduits">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    Network conduits define mappings for logical interfaces—one or more
    physical interfaces bonded together. Each conduit can be identified by a
    unique name, the <span class="guimenu">pattern</span>. This pattern is also called
    <span class="quote">“<span class="quote">Network Mode</span>”</span> in this document.
   </p><p>
    Three network modes are available:
   </p><table style="border: 0; " class="simplelist"><tr><td><span class="bold"><strong>single</strong></span>: Only use the first interface for
    all networks. VLANs will be added on top of this single interface.
   </td></tr><tr><td><span class="bold"><strong>dual</strong></span>: Use the first interface as the admin
    interface and the second one for all other networks. VLANs will be added
    on top of the second interface.
   </td></tr><tr><td><span class="bold"><strong>team</strong></span>: Bond the first two or more
    interfaces. VLANs will be added on top of the bond.
   </td></tr></table><p>
    See <a class="xref" href="#sec-depl-req-network-modes" title="2.1.2. Network Modes">Section 2.1.2, “Network Modes”</a> for detailed descriptions.
    Apart from these modes a fallback mode <code class="literal">".*/.*/.*"</code> is
    also pre-defined—it is applied in case no other mode matches the one
    specified in the global attributes section. These modes can be adjusted
    according to your needs. It is also possible to customize modes, but mode
    names must be either <code class="literal">single</code>, <code class="literal">dual</code>, or
    <code class="literal">team</code>.
   </p><p>
    The mode name that is specified with <code class="literal">mode</code> in the global
    attributes section is deployed on all nodes in SUSE <span class="productname">OpenStack</span> Cloud. It is not possible
    to use a different mode for a certain node. However, you can define
    <span class="quote">“<span class="quote">sub</span>”</span> modes with the same name that only match the following
    machines:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Machines with a certain number of physical network interfaces.
     </p></li><li class="listitem"><p>
      Machines with certain roles (all Compute Nodes for example).
     </p></li></ul></div><div class="verbatim-wrap"><pre class="screen">{
   "attributes" : {
      "network" : {
         "mode" : "single",
         "start_up_delay" : 30,
         "teaming" : { "mode": 5 },
         "enable_tx_offloading" : true,
         "enable_rx_offloading" : true,
         "interface_map" : [
            ...
         ],
         "conduit_map" : [
            {
              "pattern" : "single/.*/.*"<span class="callout" id="conduit-pattern">1</span>,
              "conduit_list" : {
                "intf2"<span class="callout" id="conduit-name">2</span> : {
                  "if_list" : ["1g1","1g2"]<span class="callout" id="conduit-interface">3</span>,
                  "team_mode" : 5<span class="callout" id="conduit-bonding">4</span>
                },
                "intf1" : {
                  "if_list" : ["1g1","1g2"],
                  "team_mode" : 5
                },
                "intf0" : {
                  "if_list" : ["1g1","1g2"],
                  "team_mode" : 5
                }
              }
            },
         ...
         ],
         "networks" : {
            ...
         },
      }
   }
}</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#conduit-pattern"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      This line contains the pattern definition for the
      <code class="systemitem">conduit_map</code>. The value for pattern must have the
      following form:
     </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable">MODE_NAME</em>/<em class="replaceable">NUMBER_OF_NICS</em>/<em class="replaceable">NODE_ROLE</em></pre></div><p>
      Each field in the pattern is interpreted as a Ruby regular expression
      (see <a class="link" href="http://www.ruby-doc.org/core-2.0/Regexp.html" target="_blank">http://www.ruby-doc.org/core-2.0/Regexp.html</a>
      for a reference).
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.6.6.9.9.1.4.1"><span class="term">mode_name
      </span></dt><dd><p>
         Name of the network mode. This string is used to reference the mode
         from the general attributes section.
        </p></dd><dt id="id-1.4.4.6.6.9.9.1.4.2"><span class="term">number_of_nics
      </span></dt><dd><p>
         Normally it is not possible to apply different network modes to
         different roles—you can only specify one mode in the global
         attributes section. However, it does not make sense to apply a network
         mode that bonds three interfaces on a machine with only two physical
         network interfaces. This option enables you to create modes for nodes
         with a given number of interfaces.
        </p></dd><dt id="id-1.4.4.6.6.9.9.1.4.3"><span class="term">node_role
      </span></dt><dd><p>
         This part of the pattern lets you create matches for a certain node
         role. This enables you to create network modes for certain roles, for
         example the Compute Nodes (role: <span class="guimenu">nova-compute</span>) or the
         swift nodes (role: <span class="guimenu">swift-storage</span>). See
         <a class="xref" href="#ex-conduits-role" title="Network Modes for Certain Roles">Example 7.3, “Network Modes for Certain Roles”</a> for the full list of roles.
        </p></dd></dl></div></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#conduit-name"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      The logical network interface definition. Each conduit list must contain
      at least one such definition. This line defines the name of the logical
      interface. This identifier must be unique and will also be referenced in
      the network definition section. We recommend sticking with the
      pre-defined naming scheme: <code class="literal">intf0</code> for <span class="quote">“<span class="quote">Interface
      0</span>”</span>, <code class="literal">intf1</code> for <span class="quote">“<span class="quote">Interface 1</span>”</span>, etc.
      If you change the name (not recommended), you also need to change all
      references in the network definition section.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#conduit-interface"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      This line maps one or more <span class="emphasis"><em>physical</em></span> interfaces to
      the logical interface. Each entry represents a physical interface. If
      more than one entry exists, the interfaces are bonded—either with
      the mode defined in the <span class="guimenu">team_mode</span> attribute of this
      conduit section. Or, if that is not present, by the globally defined
      <span class="guimenu">teaming</span> attribute.
     </p><p>
      The physical interfaces definition needs to fit the following pattern:
     </p><div class="verbatim-wrap"><pre class="screen">[Quantifier][Speed][Order]</pre></div><p>
      Valid examples are <code class="literal">+1g2</code>, <code class="literal">10g1</code> or
      <code class="literal">?1g2</code>.
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.6.6.9.9.3.5.1"><span class="term">Quantifier</span></dt><dd><p>
         Specifying the quantifier is optional. The following values may be
         entered:
        </p><table style="border: 0; " class="simplelist"><tr><td><code class="literal">+</code>: at least the speed specified afterwards
        (specified value or higher)</td></tr><tr><td><code class="literal">-</code>: at most the speed specified afterwards
        (specified value or lower)</td></tr><tr><td><code class="literal">?</code>: any speed (speed specified afterwards is
        ignored)</td></tr></table><p>
         If no quantifier is specified, the exact speed specified is used.
        </p></dd><dt id="id-1.4.4.6.6.9.9.3.5.2"><span class="term">Speed</span></dt><dd><p>
         Specifying the interface speed is mandatory (even if using the
         <code class="literal">?</code> quantifier). The following values may be entered:
        </p><table style="border: 0; " class="simplelist"><tr><td><code class="literal">10m</code>: 10 Mbit</td></tr><tr><td><code class="literal">100m</code>: 100 Mbit</td></tr><tr><td><code class="literal">1g</code>: 1 Gbit</td></tr><tr><td><code class="literal">10g</code>: 10 Gbit</td></tr><tr><td><code class="literal">20g</code>: 20 Gbit</td></tr><tr><td><code class="literal">40g</code>: 40 Gbit</td></tr><tr><td><code class="literal">56g</code>: 56 Gbit</td></tr></table></dd><dt id="id-1.4.4.6.6.9.9.3.5.3"><span class="term">Order</span></dt><dd><p>
         Position in the interface order. Specifying this value is mandatory.
         The interface order is defined by the order in which the interfaces
         appear in <code class="filename">/sys/class/net</code> (default) or, if it
         exists, by an interface map. The order is also linked to the speed in
         this context:
        </p><table style="border: 0; " class="simplelist"><tr><td><code class="literal">1g1</code>: the first 1Gbit interface</td></tr><tr><td><code class="literal">+1g1</code>: the first 1Gbit or 10Gbit
        interface. Crowbar will take the first 1Gbit interface. Only if such an
        interface does not exist, it will take the first 10Gbit interface
        available.</td></tr><tr><td><code class="literal">?1g3</code>: the third 1Gbit, 10Gbit, 100Mbit or
        10Mbit interface. Crowbar will take the third 1Gbit interface. Only if
        such an interface does not exist, it will take the third 10Gbit
        interface, then the third 100Mbit or 10Mbit interface.</td></tr></table><div id="id-1.4.4.6.6.9.9.3.5.3.2.3" data-id-title="Ordering Numbers" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Ordering Numbers</div><p>
          Ordering numbers start with <code class="literal">1</code> rather than with
          <code class="literal">0</code>.
         </p><p>
          Each interfaces that supports multiple speeds is referenced by
          multiple names—one for each speed it supports. A 10Gbit
          interface is therefore represented by four names:
          <code class="literal">10g<em class="replaceable">X</em></code>,
          <code class="literal">1g<em class="replaceable">X</em></code>,
          <code class="literal">100m<em class="replaceable">X</em></code>,
          <code class="literal">10m<em class="replaceable">X</em></code>, where
          <em class="replaceable">X</em> is the ordering number.
         </p><p>
          Ordering numbers always start with <code class="literal">1</code> and are
          assigned ascending for each speed, for example
          <code class="literal">1g1</code>, <code class="literal">1g2</code>, and
          <code class="literal">1g3</code>. Numbering starts with the first physical
          interface. On systems with network interfaces supporting different
          maximum speeds, ordering numbers for the individual speeds differ, as
          the following example shows:
         </p><table style="border: 0; " class="simplelist"><tr><td>
          100Mbit (first interface): <code class="literal">100m1</code>,
          <code class="literal">10m1</code>
          </td></tr><tr><td>
          1Gbit (second interface): <code class="literal">1g1</code>,
          <code class="literal">100m2</code>, <code class="literal">10m2</code>
          </td></tr><tr><td>
          10Gbit (third interface): <code class="literal">10g1</code>,
          <code class="literal">1g2</code>, <code class="literal">100m3</code>,
          <code class="literal">10m3</code>
          </td></tr></table><p>
          In this example the pattern <code class="literal">?1g3</code> would match
          <code class="literal">100m3</code>, since no third 1Gbit or 10Gbit interface
          exist.
         </p></div></dd></dl></div></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#conduit-bonding"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      The bonding mode to be used for this logical interface. Overwrites the
      default set in the global attributes section <span class="emphasis"><em>for this
      interface</em></span>. See
      <a class="link" href="https://www.kernel.org/doc/Documentation/networking/bonding.txt" target="_blank">https://www.kernel.org/doc/Documentation/networking/bonding.txt</a>
      for a list of available modes. Specifying this option is
      optional—if not specified here, the global setting applies.
     </p></td></tr></table></div></section><section class="sect2" id="sec-deploy-network-json-conduits-examples" data-id-title="Network Conduit Examples"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.5.6 </span><span class="title-name">Network Conduit Examples</span></span> <a title="Permalink" class="permalink" href="#sec-deploy-network-json-conduits-examples">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="complex-example"><div class="example" id="ex-conduits-nic-number" data-id-title="Network Modes for Different NIC Numbers"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 7.2: </span><span class="title-name">Network Modes for Different NIC Numbers </span></span><a title="Permalink" class="permalink" href="#ex-conduits-nic-number">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="example-contents"><p>
     The following example defines a team network mode for nodes with 6, 3, and
     an arbitrary number of network interfaces. Since the first mode that
     matches is applied, it is important that the specific modes (for 6 and 3
     NICs) are listed before the general mode:
    </p><div class="verbatim-wrap"><pre class="screen">{
   "attributes" : {
      "network" : {
         "mode" : "single",
         "start_up_delay" : 30,
         "teaming" : { "mode": 5 },
         "enable_tx_offloading" : true,
         "enable_rx_offloading" : true,
         "interface_map" : [
            ...
         ],
         "conduit_map" : [
           {
              "pattern" : "single/6/.*",
              "conduit_list" : {
                ...
              }
            },
            {
              "pattern" : "single/3/.*",
              "conduit_list" : {
                ...
              }
            },
            {
              "pattern" : "single/.*/.*",
              "conduit_list" : {
                ...
              }
            },
            ...
         ],
         "networks" : {
            ...
         },
      }
   }
}</pre></div></div></div></div><div class="complex-example"><div class="example" id="ex-conduits-role" data-id-title="Network Modes for Certain Roles"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 7.3: </span><span class="title-name">Network Modes for Certain Roles </span></span><a title="Permalink" class="permalink" href="#ex-conduits-role">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="example-contents"><p>
     The following example defines network modes for Compute Nodes with four
     physical interfaces, the Administration Server (role <code class="literal">crowbar</code>), the
     Control Node, and a general mode applying to all other nodes.
    </p><div class="verbatim-wrap"><pre class="screen">{
   "attributes" : {
      "network" : {
         "mode" : "team",
         "start_up_delay" : 30,
         "teaming" : { "mode": 5 },
         "enable_tx_offloading" : true,
         "enable_rx_offloading" : true,
         "interface_map" : [
            ...
         ],
         "conduit_map" : [
          {
            "pattern" : "team/4/nova-compute",
            "conduit_list" : {
              ...
            }
            },
            {
              "pattern" : "team/.*/^crowbar$",
              "conduit_list" : {
                ...
              }
            },
            {
              "pattern" : "team/.*/nova-controller",
              "conduit_list" : {
                ...
              }
            },
            {
              "pattern" : "team/.*/.*",
              "conduit_list" : {
                ...
              }
            },
            ...
         ],
         "networks" : {
            ...
         },
      }
   }
}</pre></div><p>
     The following values for <code class="literal">node_role</code> can be used:
    </p><table style="border: 0; " class="simplelist"><tr><td><code class="literal">ceilometer-central</code>
     </td></tr><tr><td><code class="literal">ceilometer-server</code>
     </td></tr><tr><td><code class="literal">cinder-controller</code>
     </td></tr><tr><td><code class="literal">cinder-volume</code>
     </td></tr><tr><td><code class="literal">crowbar</code>
     </td></tr><tr><td><code class="literal">database-server</code>
     </td></tr><tr><td><code class="literal">glance-server</code>
     </td></tr><tr><td><code class="literal">heat-server</code>
     </td></tr><tr><td><code class="literal">horizon-server</code>
     </td></tr><tr><td><code class="literal">keystone-server</code>
     </td></tr><tr><td><code class="literal">manila-server</code>
     </td></tr><tr><td><code class="literal">manila-share</code>
     </td></tr><tr><td><code class="literal">monasca-agent</code>
     </td></tr><tr><td><code class="literal">monasca-log-agent</code>
     </td></tr><tr><td><code class="literal">monasca-master</code>
     </td></tr><tr><td><code class="literal">monasca-server</code>
     </td></tr><tr><td><code class="literal">neutron-network</code>
     </td></tr><tr><td><code class="literal">neutron-server</code>
     </td></tr><tr><td><code class="literal">nova-controller</code>
     </td></tr><tr><td><code class="literal">nova-compute-*</code>
     </td></tr><tr><td><code class="literal">rabbitmq-server</code>
     </td></tr><tr><td><code class="literal">swift-dispersion</code>
     </td></tr><tr><td><code class="literal">swift-proxy</code>
     </td></tr><tr><td><code class="literal">swift-ring-compute</code>
     </td></tr><tr><td><code class="literal">swift-storage</code>
     </td></tr></table><p>
     The role <code class="literal">crowbar</code> refers to the Administration Server.
    </p></div></div></div><div id="id-1.4.4.6.6.10.5" data-id-title="The crowbar and Pattern Matching" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: The <code class="literal">crowbar</code> and Pattern Matching</div><p>
     As explained in <a class="xref" href="#ex-conduits-machine" title="Network Modes for Certain Machines">Example 7.4, “Network Modes for Certain Machines”</a>, each node has an
     additional, unique role named <code class="literal">crowbar-<em class="replaceable">FULLY
     QUALIFIED HOSTNAME</em></code>.
    </p><p>
     All three elements of the value of the <code class="literal">pattern</code> line
     are read as regular expressions. Therefore using the pattern
     <code class="literal"><em class="replaceable">mode-name</em>/.*/crowbar</code> will
     match all nodes in your installation. <code class="literal">crowbar</code> is
     considered a substring and therefore will also match all strings
     <code class="literal">crowbar-<em class="replaceable">FULLY QUALIFIED
     HOSTNAME</em></code>. As a consequence, all subsequent map
     definitions will be ignored. To make sure this does not happen, you must
     use the proper regular expression <code class="literal">^crowbar$</code>:
     <code class="literal"><em class="replaceable">mode-name</em>/.*/^crowbar$</code>.
    </p></div><div class="complex-example"><div class="example" id="ex-conduits-machine" data-id-title="Network Modes for Certain Machines"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 7.4: </span><span class="title-name">Network Modes for Certain Machines </span></span><a title="Permalink" class="permalink" href="#ex-conduits-machine">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="example-contents"><p>
     Apart from the roles listed under <a class="xref" href="#ex-conduits-role" title="Network Modes for Certain Roles">Example 7.3, “Network Modes for Certain Roles”</a>, each
     node in SUSE <span class="productname">OpenStack</span> Cloud has a unique role, which lets you create modes matching
     exactly one node. Each node can be addressed by its unique role name in
     the <code class="literal">pattern</code> entry of the
     <code class="literal">conduit_map</code>.
    </p><p>
     The role name depends on the fully qualified host name (FQHN) of the
     respective machine. The role is named after the scheme
     <code class="literal">crowbar-<em class="replaceable">FULLY QUALIFIED
     HOSTNAME</em></code> where colons are replaced with dashes,
     and periods are replaced with underscores. The FQHN depends on whether the
     respective node was booted via PXE or not.
    </p><p>
     To determine the host name of a node, log in to the Crowbar Web interface and got to
     <span class="guimenu">Nodes</span> › <span class="guimenu">Dashboard</span>. Click the respective node name to get detailed data for the
     node. The FQHN is listed first under <span class="guimenu">Full Name</span>.
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.6.6.10.6.5.1"><span class="term">Role Names for Nodes Booted via PXE</span></dt><dd><p>
        The <em class="replaceable">FULLY QUALIFIED HOSTNAME</em> for nodes
        booted via PXE is composed of the following: a prefix 'd', the MAC
        address of the network interface used to boot the node via PXE, and the
        domain name as configured on the Administration Server. A machine with the fully
        qualified host name
        <code class="literal">d1a-12-05-1e-35-49.cloud.example.com</code> would get the
        following role name:
       </p><div class="verbatim-wrap"><pre class="screen">crowbar-d1a-12-05-1e-35-49_cloud_example_com</pre></div></dd><dt id="id-1.4.4.6.6.10.6.5.2"><span class="term">Role Names for the Administration Server and Nodes Added Manually</span></dt><dd><p>
        The fully qualified hostnames of the Administration Server and all nodes added
        manually (as described in
        <a class="xref" href="#sec-depl-inst-nodes-install-external" title="11.3. Converting Existing SUSE Linux Enterprise Server 12 SP4 Machines Into SUSE OpenStack Cloud Nodes">Section 11.3, “Converting Existing SUSE Linux Enterprise Server 12 SP4 Machines Into SUSE <span class="productname">OpenStack</span> Cloud Nodes”</a>) are
        defined by the system administrator. They typically have the form
        hostname+domain, for example
        <em class="replaceable">admin.cloud.example.com</em>, which would
        result in the following role name:
       </p><div class="verbatim-wrap"><pre class="screen">crowbar-admin_cloud_example_com</pre></div></dd></dl></div><p>
     Network mode definitions for certain machines must be listed first in the
     conduit map. This prevents other, general rules which would also map from
     being applied.
    </p><div class="verbatim-wrap"><pre class="screen">{
   "attributes" : {
      "network" : {
         "mode" : "dual",
         "start_up_delay" : 30,
         "teaming" : { "mode": 5 },
         "enable_tx_offloading" : true,
         "enable_rx_offloading" : true,
         "interface_map" : [
            ...
         ],
         "conduit_map" : [
          {
            "pattern" : "dual/.*/crowbar-d1a-12-05-1e-35-49_cloud_example_com",
            "conduit_list" : {
               ...
            }
          },
            ...
         ],
         "networks" : {
            ...
         },
      }
   }
}</pre></div></div></div></div></section><section class="sect2" id="sec-deploy-network-json-networks" data-id-title="Network Definitions"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.5.7 </span><span class="title-name">Network Definitions</span></span> <a title="Permalink" class="permalink" href="#sec-deploy-network-json-networks">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    The network definitions contain IP address assignments, the bridge and VLAN
    setup, and settings for the router preference. Each network is also
    assigned to a logical interface defined in the network conduit section. In
    the following the network definition is explained using the example of the
    admin network definition:
   </p><div class="verbatim-wrap"><pre class="screen">{
   "attributes" : {
      "network" : {
         "mode" : "single",
         "start_up_delay" : 30,
         "teaming" : { "mode": 5 },
         "enable_tx_offloading" : true,
         "enable_rx_offloading" : true,
         "interface_map" : [
            ...
         ],
         "conduit_map" : [
             ...
         ],
         "networks" : {
           "admin" : {
             "conduit" : "intf0"<span class="callout" id="network-conduit">1</span>,
             "add_bridge" : false<span class="callout" id="network-bridge">2</span>,
             "use_vlan" : false<span class="callout" id="network-vlan">3</span>,
             "vlan" : 100<span class="callout" id="network-vlanid">4</span>,
             "router_pref" : 10<span class="callout" id="network-routerpref">5</span>,
             "subnet" : "192.168.124.0"<span class="callout" id="network-addresses">6</span>,
             "netmask" : "255.255.255.0",
             "router" : "192.168.124.1",
             "broadcast" : "192.168.124.255",
             "ranges" : {
               "admin" : {
                 "start" : "192.168.124.10",
                 "end" : "192.168.124.11"
               },
               "switch" : {
                 "start" : "192.168.124.241",
                 "end" : "192.168.124.250"
               },
               "dhcp" : {
                 "start" : "192.168.124.21",
                 "end" : "192.168.124.80"
               },
               "host" : {
                 "start" : "192.168.124.81",
                 "end" : "192.168.124.160"
               }
             }
           },
           "nova_floating": {
             "add_ovs_bridge": false<span class="callout" id="network-ovs-bridge">7</span>,
             "bridge_name": "br-public"<span class="callout" id="network-ovs-bridge-name">8</span>,
             ....
           }
         ...
         },
      }
   }
}</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#network-conduit"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Logical interface assignment. The interface must be defined in the
      network conduit section and must be part of the active network mode.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#network-bridge"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Bridge setup. Do not touch. Should be <code class="literal">false</code> for all
      networks.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#network-vlan"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Create a VLAN for this network. Changing this setting is not recommended.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#network-vlanid"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      ID of the VLAN. Change this to the VLAN ID you intend to use for the
      specific network, if required. This setting can also be changed using the
      YaST Crowbar interface. The VLAN ID for the
      <code class="literal">nova-floating</code> network must always match the ID for the
      <code class="literal">public network</code>.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#network-routerpref"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Router preference, used to set the default route. On nodes hosting
      multiple networks the router with the lowest
      <code class="literal">router_pref</code> becomes the default gateway. Changing this
      setting is not recommended.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#network-addresses"><span class="callout">6</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Network address assignments. These values can also be changed by using
      the YaST Crowbar interface.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#network-ovs-bridge"><span class="callout">7</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Openvswitch virtual switch setup. This attribute is maintained by Crowbar
      on a per-node level and should not be changed manually.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#network-ovs-bridge-name"><span class="callout">8</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Name of the openvswitch virtual switch. This attribute is maintained by
      Crowbar on a per-node level and should not be changed manually.
     </p></td></tr></table></div><div id="id-1.4.4.6.6.11.5" data-id-title="VLAN Settings" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: VLAN Settings</div><p>
     As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8, using a VLAN for the admin network is
     only supported on a native/untagged VLAN. If you need VLAN support for the
     admin network, it must be handled at switch level.
    </p><p>
     When changing the network configuration with YaST or by editing
     <code class="filename">/etc/crowbar/network.json</code>, you can define VLAN
     settings for each network. For the networks <code class="literal">nova-fixed</code>
     and <code class="literal">nova-floating</code>, however, special rules apply:
    </p><p>
     <span class="bold"><strong>nova-fixed</strong></span>: The <span class="guimenu">USE
     VLAN</span> setting will be ignored. However, VLANs will automatically
     be used if deploying neutron with VLAN support (using the plugins
     linuxbridge, openvswitch plus VLAN, or cisco plus VLAN). In this case, you
     need to specify a correct <span class="guimenu">VLAN ID</span> for this network.
    </p><p>
     <span class="bold"><strong>nova-floating</strong></span>: When using a VLAN for
     <code class="literal">nova-floating</code> (which is the default), the <span class="guimenu">USE
     VLAN</span> and <span class="guimenu">VLAN ID</span> settings for
     <span class="guimenu">nova-floating</span> and <span class="guimenu">public</span> default to
     the same.
    </p><p>
     You have the option of separating public and floating networks with a
     custom configuration. Configure your own separate floating network (not as
     a subnet of the public network), and give the floating network its own
     router. For example, define <code class="literal">nova-floating</code> as part of an
     external network with a custom <code class="literal">bridge-name</code>. When you
     are using different networks and OpenVSwitch is configured, the
     pre-defined <code class="literal">bridge-name</code> won't work.
    </p></div></section><section class="sect2" id="sec-depl-inst-admserv-post-network-external" data-id-title="Providing Access to External Networks"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.5.8 </span><span class="title-name">Providing Access to External Networks</span></span> <a title="Permalink" class="permalink" href="#sec-depl-inst-admserv-post-network-external">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    By default, external networks cannot be reached from nodes in the SUSE <span class="productname">OpenStack</span> Cloud.
    To access external services such as a SUSE Manager server, an SMT server, or
    a SAN, you need to make the external network(s) known to SUSE <span class="productname">OpenStack</span> Cloud. Do so by
    adding a network definition for each external network to
    <code class="filename">/etc/crowbar/network.json</code>. Refer to
    <a class="xref" href="#sec-depl-inst-admserv-post-network" title="7.5. Custom Network Configuration">Section 7.5, “Custom Network Configuration”</a> for setup
    instructions.
   </p><div class="example" id="id-1.4.4.6.6.12.3" data-id-title="Example Network Definition for the External Network 192.168.150.0/16"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 7.5: </span><span class="title-name">Example Network Definition for the External Network 192.168.150.0/16 </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.6.6.12.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">            "external" : {
               "add_bridge" : false,
               "vlan" : <em class="replaceable">XXX</em>,
               "ranges" : {
                  "host" : {
                     "start" : "192.168.150.1",
                     "end" : "192.168.150.254"
                  }
               },
               "broadcast" : "192.168.150.255",
               "netmask" : "255.255.255.0",
               "conduit" : "intf1",
               "subnet" : "192.168.150.0",
               "use_vlan" : true
            }</pre></div></div></div><p>
    Replace the value <em class="replaceable">XXX</em> for the VLAN by a value
    not used within the SUSE <span class="productname">OpenStack</span> Cloud network and not used by neutron. By default,
    the following VLANs are already used:
   </p><div class="table" id="id-1.4.4.6.6.12.5" data-id-title="VLANs used by the SUSE OpenStack Cloud Default Network Setup"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 7.3: </span><span class="title-name">VLANs used by the SUSE <span class="productname">OpenStack</span> Cloud Default Network Setup </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.6.6.12.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         VLAN ID
        </p>
       </th><th style="border-bottom: 1px solid ; ">
        <p>
         Used by
        </p>
       </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         100
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         BMC VLAN (bmc_vlan)
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         200
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Storage Network
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         300
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Public Network (nova-floating, public)
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         400
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Software-defined network (os_sdn)
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         500
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Private Network (nova-fixed)
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; ">
        <p>
         501 - 2500
        </p>
       </td><td>
        <p>
         neutron (value of nova-fixed plus 2000)
        </p>
       </td></tr></tbody></table></div></div></section><section class="sect2" id="sec-depl-inst-admserv-post-network-split" data-id-title="Split Public and Floating Networks on Different VLANs"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.5.9 </span><span class="title-name">Split Public and Floating Networks on Different VLANs</span></span> <a title="Permalink" class="permalink" href="#sec-depl-inst-admserv-post-network-split">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    For custom setups, the public and floating networks can be separated.
    Configure your own separate floating network (not as a subnet of the public
    network), and give the floating network its own router. For example, define
    <code class="literal">nova-floating</code> as part of an external network with a
    custom <code class="literal">bridge-name</code>. When you are using different
    networks and OpenVSwitch is configured, the pre-defined
    <code class="literal">bridge-name</code> won't work.
   </p></section><section class="sect2" id="sec-depl-inst-admserv-post-network-mtu" data-id-title="Adjusting the Maximum Transmission Unit for the Admin and Storage Network"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.5.10 </span><span class="title-name">Adjusting the Maximum Transmission Unit for the Admin and Storage Network</span></span> <a title="Permalink" class="permalink" href="#sec-depl-inst-admserv-post-network-mtu">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    If you need to adjust the Maximum Transmission Unit (MTU) for the Admin
    and/or Storage Network, adjust
    <code class="filename">/etc/crowbar/network.json</code> as shown below. You can also
    enable jumbo frames this way by setting the MTU to 9000. The following
    example enables jumbo frames for both, the storage and the admin network by
    setting <code class="literal">"mtu": 9000</code>.
   </p><div class="verbatim-wrap"><pre class="screen">        "admin": {
          "add_bridge": false,
          "broadcast": "192.168.124.255",
          "conduit": "intf0",
          "mtu": 9000,
          "netmask": "255.255.255.0",
          "ranges": {
            "admin": {
              "end": "192.168.124.11",
              "start": "192.168.124.10"
            },
            "dhcp": {
              "end": "192.168.124.80",
              "start": "192.168.124.21"
            },
            "host": {
              "end": "192.168.124.160",
              "start": "192.168.124.81"
            },
            "switch": {
              "end": "192.168.124.250",
              "start": "192.168.124.241"
            }
          },
          "router": "192.168.124.1",
          "router_pref": 10,
          "subnet": "192.168.124.0",
          "use_vlan": false,
          "vlan": 100
        },
        "storage": {
          "add_bridge": false,
          "broadcast": "192.168.125.255",
          "conduit": "intf1",
          "mtu": 9000,
          "netmask": "255.255.255.0",
          "ranges": {
            "host": {
              "end": "192.168.125.239",
              "start": "192.168.125.10"
            }
          },
          "subnet": "192.168.125.0",
          "use_vlan": true,
          "vlan": 200
        },</pre></div><div id="id-1.4.4.6.6.14.4" data-id-title="No Network Changes After Completing the SUSE OpenStack Cloud Crowbar installation" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: No Network Changes After Completing the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation</div><p>
     After you have completed the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation, you cannot change the network
     setup, and you cannot change the MTU size.
    </p></div></section><section class="sect2" id="sec-network-json-resolve" data-id-title="Matching Logical and Physical Interface Names with network-json-resolve"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.5.11 </span><span class="title-name">Matching Logical and Physical Interface Names with network-json-resolve</span></span> <a title="Permalink" class="permalink" href="#sec-network-json-resolve">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    SUSE <span class="productname">OpenStack</span> Cloud includes a new script, <code class="filename">network-json-resolve</code>,
    which matches the physical and logical names of network interfaces, and
    prints them to stdout. Use this to verify that you are using the correct
    interface names in <code class="filename">network.json</code>. Note that it will
    only work if <span class="productname">OpenStack</span> nodes have been deployed. The following command
    prints a help menu:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="command">sudo /opt/dell/bin/network-json-resolve -h</code></pre></div><p>
    <code class="filename">network-json-resolve</code> reads your deployed
    <code class="filename">network.json</code> file. To use a different
    <code class="filename">network.json</code> file, specify its full path with the
    <code class="command">--network-json</code> option. The following example shows how
    to use a different <code class="filename">network.json</code> file, and prints the
    interface mappings of a single node:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="command">sudo /opt/dell/bin/network-json-resolve --network-json /opt/configs/network.json aliases compute1</code>
 eth0: 0g1, 1g1
 eth1: 0g1, 1g1</pre></div><p>
    You may query the mappings of a specific network interface:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="command">sudo /opt/dell/bin/network-json-resolve aliases compute1 eth0</code>
 eth0: 0g1, 1g1</pre></div><p>
    Print the bus ID order on a node. This returns <code class="literal">no bus order
    defined for <em class="replaceable">node</em></code> if you did not
    configure any bus ID mappings:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="command">sudo /opt/dell/bin/network-json-resolve bus_order compute1</code></pre></div><p>
    Print the defined conduit map for the node:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="command">sudo /opt/dell/bin/network-json-resolve conduit_map compute1</code>
bastion: ?1g1
intf0: ?1g1
intf1: ?1g1
intf2: ?1g1</pre></div><p>
    Resolve conduits to the standard interface names:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="command">sudo /opt/dell/bin/network-json-resolve conduits compute1</code>
bastion:
intf0: eth0
intf1: eth0
intf2: eth0</pre></div><p>
    Resolve the configured networks on a node to the standard interface names:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="command">sudo /opt/dell/bin/network-json-resolve networks compute1</code>
bastion:
bmc_vlan:  eth0
nova_fixed: eth0
nova_floating: eth0
os_sdn: eth0
public: eth0
storage: eth0</pre></div><p>
    Resolve the specified network to the standard interface name(s):
   </p><div class="verbatim-wrap"><pre class="screen"><code class="command">sudo /opt/dell/bin/network-json-resolve networks compute1 public</code>
   public: eth0</pre></div><p>
    Resolve a <code class="filename">network.json</code>-style interface to its standard
    interface name(s):
   </p><div class="verbatim-wrap"><pre class="screen"><code class="command">sudo /opt/dell/bin/network-json-resolve resolve compute1 1g1</code>
 eth0</pre></div></section></section></section><section class="chapter" id="sec-depl-adm-start-crowbar" data-id-title="Starting the SUSE OpenStack Cloud Crowbar installation"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">8 </span><span class="title-name">Starting the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-start-crowbar">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><div><div class="abstract"><p>
    The last step in configuring the Administration Server is starting Crowbar.
   </p></div></div></div></div><p>
   Before starting the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation to finish the configuration
   of the Administration Server, make sure to double-check the following items.
  </p><div class="itemizedlist"><div class="title-container"><div class="itemizedlist-title-wrap"><div class="itemizedlist-title"><span class="title-number-name"><span class="title-name">Final Check Points </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.7.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><ul class="itemizedlist"><li class="listitem"><p>
     Make sure the network configuration is correct. Run <span class="guimenu">YaST</span> › <span class="guimenu">Crowbar</span>
     to review/change the configuration. See
     <a class="xref" href="#sec-depl-adm-inst-crowbar" title="Chapter 7. Crowbar Setup">Chapter 7, <em>Crowbar Setup</em></a> for further instructions.
    </p><div id="id-1.4.4.7.3.2.2" data-id-title="An HA Setup Requires Team Network Mode" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: An HA Setup Requires Team Network Mode</div><p>
      If you are planning to make SUSE <span class="productname">OpenStack</span> Cloud highly available, whether upon the
      initial setup or later, set up the
      network in the team mode. Such a setup requires at least two network
      cards for each node.
     </p></div></li><li class="listitem"><p>
     Make sure <code class="command">hostname</code> <code class="option">-f</code> returns a
     fully qualified host name. See
     <a class="xref" href="#sec-depl-adm-inst-network" title="Chapter 6. Service Configuration: Administration Server Network Configuration">Chapter 6, <em>Service Configuration:  Administration Server Network Configuration</em></a> for further instructions.
    </p></li><li class="listitem"><p>
     Make sure all update and product repositories are available. See
     <a class="xref" href="#cha-depl-repo-conf" title="Chapter 5. Software Repository Setup">Chapter 5, <em>Software Repository Setup</em></a> for further instructions.
    </p></li><li class="listitem"><p>
     Make sure the operating system and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> are up-to-date and
     have the latest patches installed. Run <code class="command">zypper patch</code>
     to install them.
    </p></li><li class="listitem"><p>
     To use the Web interface for the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation you need network access to
     the Administration Server via a second network interface. As the network will be
     reconfigured during the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation, make sure to either have a bastion
     network or an external gateway configured. (For details on bastion
     networks, see <a class="xref" href="#sec-depl-adm-inst-crowbar-mode-bastion" title="7.3.1. Setting Up a Bastion Network">Section 7.3.1, “Setting Up a Bastion Network”</a>.)
    </p></li></ul></div><p>
   Now everything is in place to finally set up Crowbar and install the
   Administration Server. Crowbar requires a MariaDB database—you can either create
   one on the Administration Server or use an existing PostgreSQL database on a remote
   server.
  </p><div class="procedure" id="id-1.4.4.7.5" data-id-title="Setting up Crowbar with a Local Database"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 8.1: </span><span class="title-name">Setting up Crowbar with a Local Database </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.7.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Start Crowbar:
    </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl start crowbar-init</pre></div></li><li class="step"><p>
     Create a new database on the Administration Server. By default the credentials
     <code class="literal">crowbar</code>/<code class="literal">crowbar</code> are used:
    </p><div class="verbatim-wrap"><pre class="screen">crowbarctl database create</pre></div><p>
     To use a different user name and password, run the following command
     instead:
    </p><div class="verbatim-wrap"><pre class="screen">crowbarctl database create \
--db_username=<em class="replaceable">USERNAME</em> --db_password=<em class="replaceable">PASSWORD</em></pre></div><p>
     Run <code class="command">crowbarctl database help create</code> for help and more
     information.
    </p></li></ol></div></div><div class="procedure" id="id-1.4.4.7.6" data-id-title="Setting up Crowbar with a Remote MariaDB Database"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 8.2: </span><span class="title-name">Setting up Crowbar with a Remote MariaDB Database </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.7.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Start Crowbar:
    </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl start crowbar-init</pre></div></li><li class="step"><p>
     Make sure a user account that can be used for the Crowbar database exists
     on the remote MariaDB database. If not, create such an account.
    </p></li><li class="step"><p>
     Test the database connection using the credentials from the previous
     step:
    </p><div class="verbatim-wrap"><pre class="screen">crowbarctl database test --db-username=<em class="replaceable">USERNAME</em> \
--db-password=<em class="replaceable">PASSWORD</em> --database=<em class="replaceable">DBNAME</em> \
--host=<em class="replaceable">IP_or_FQDN</em> --port=<em class="replaceable">PORT</em></pre></div><p>
     You need to be able to successfully connect to the database before you
     can proceed. Run <code class="command">crowbarctl database help test</code> for
     help and more information.
    </p></li><li class="step"><p>
     To connect to the database, use the following command:
    </p><div class="verbatim-wrap"><pre class="screen">crowbarctl database connect --db-username=<em class="replaceable">USERNAME</em> \
--db-password=<em class="replaceable">PASSWORD</em> --database=<em class="replaceable">DBNAME</em> \
--host=<em class="replaceable">IP_or_FQDN</em> --port=<em class="replaceable">PORT</em></pre></div><p>
     Run <code class="command">crowbarctl database help connect</code> for help and more
     information.
    </p></li></ol></div></div><p>
   After the database is successfully created and you can connect to it, access
   the Web interface from a Web browser, using the following address:
   </p><div class="verbatim-wrap"><pre class="screen">http://<em class="replaceable">ADDRESS</em></pre></div><p>
   Replace <em class="replaceable">ADDRESS</em> either with the IP address of the
   second network interface or its associated host name.  Logging in to the
   Web interface requires the credentials you configured with YaST Crowbar (see <a class="xref" href="#sec-depl-adm-inst-crowbar-user" title="7.1. User Settings">Section 7.1, “<span class="guimenu">User Settings</span>”</a>). If you have not changed the
   defaults, user name and password are both <code class="literal">crowbar</code>. Refer
   to <a class="xref" href="#cha-depl-crowbar" title="Chapter 10. The Crowbar Web Interface">Chapter 10, <em>The Crowbar Web Interface</em></a> for details.
  </p><p>
    The Web interface shows the SUSE <span class="productname">OpenStack</span> Cloud installation wizard. Click <span class="guimenu">Start
    Installation</span> to begin. The installation progress is shown in the
    Web interface:
   </p><div class="figure" id="id-1.4.4.7.11"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_web_installer_init.png"><img src="images/depl_web_installer_init.png" width="75%" alt="The SUSE OpenStack Cloud Crowbar installation Web interface" title="The SUSE OpenStack Cloud Crowbar installation Web interface"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 8.1: </span><span class="title-name">The SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation Web interface </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.7.11">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><p>
    If the installation has successfully finished, you will be
    redirected to the Crowbar Dashboard:
   </p><div class="figure" id="id-1.4.4.7.13"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_node_dashboard_initial.png"><img src="images/depl_node_dashboard_initial.png" width="75%" alt="Crowbar Web Interface: The Dashboard" title="Crowbar Web Interface: The Dashboard"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 8.2: </span><span class="title-name">Crowbar Web Interface: The Dashboard </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.7.13">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><p>
    From here you can start allocating nodes and then deploy the <span class="productname">OpenStack</span>
    services. Refer to <a class="xref" href="#part-depl-ostack" title="Part III. Setting Up OpenStack Nodes and Services">Part III, “Setting Up <span class="productname">OpenStack</span> Nodes and Services”</a> for more information.
   </p></section><section class="chapter" id="sec-depl-adm-crowbar-extra-features" data-id-title="Customizing Crowbar"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">9 </span><span class="title-name">Customizing Crowbar</span></span> <a title="Permalink" class="permalink" href="#sec-depl-adm-crowbar-extra-features">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><section class="sect1" id="id-1.4.4.8.2" data-id-title="Skip Unready Nodes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">9.1 </span><span class="title-name">Skip Unready Nodes</span></span> <a title="Permalink" class="permalink" href="#id-1.4.4.8.2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   In large deployments with many nodes, there are always some nodes that are
   in a fail or unknown state. New barclamps cannot be applied to them and
   values cannot be updated in some barclamps that are already deployed. This
   happens because Crowbar will refuse to apply a barclamp to a list of nodes if
   they are not all in <code class="literal">ready</code> state.
  </p><p>
   To avoid having to manually take out nodes that are not
   <code class="literal">ready</code>, there is a feature called <code class="literal">skip unready
   nodes</code>. Instead of refusing to apply the barclamp, it will skip the
   nodes that it finds in any other state than <code class="literal">ready</code>.
  </p><p>
   <span class="bold"><strong>Enabling the Feature</strong></span>
  </p><p>
   In <code class="filename">/opt/dell/crowbar_framework/config/crowbar.yml</code>, set
   the option <code class="literal">skip_unready_nodes</code> to <code class="literal">true</code>.
  </p><div class="verbatim-wrap"><pre class="screen">default: &amp;default
skip_unready_nodes:
  enabled: false <span class="bold"><strong>&lt;&lt;&lt; change to true</strong></span></pre></div><p>
   <span class="bold"><strong>Roles Affected</strong></span>
  </p><p>
   All Barclamp roles are affected. The default config file includes all the
   roles that have been tested and found to be working. Adding roles to the
   default list is not supported for the <code class="literal">skip_unready_nodes</code>
   feature. Removing default roles is supported.
  </p><p>
   The list of currently supported roles to skip:
  </p><div class="verbatim-wrap"><pre class="screen">- bmc-nat-client
- ceilometer-agent
- deployer-client
- dns-client
- ipmi
- logging-client
- nova-compute-ironic
- nova-compute-kvm
- nova-compute-qemu
- nova-compute-vmware
- ntp-client
- provisioner-base
- suse-manager-client
- swift-storage
- updater</pre></div><p>
   <span class="bold"><strong>Determining Which Nodes Were Skipped</strong></span>
  </p><p>
   Skipped nodes are logged to the Crowbar log
   (<code class="filename">/var/log/crowbar/production.log</code>) where you can search
   for the text <code class="literal">skipped until next chef run</code>. This will print
   the log lines where nodes were skipped, the name of the node, and the
   barclamp which was being applied.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>grep "skipped until next chef run" /var/log/crowbar/production.log</pre></div><div id="id-1.4.4.8.2.14" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
    After enabling/disabling the <code class="literal">skip_unready_nodes</code> feature
    or adding/removing roles, the Crowbar framework service must be restarted
    (<code class="command">systemctl restart crowbar</code>) in order to use the updated
    settings.
   </p></div></section><section class="sect1" id="id-1.4.4.8.3" data-id-title="Skip Unchanged Nodes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">9.2 </span><span class="title-name">Skip Unchanged Nodes</span></span> <a title="Permalink" class="permalink" href="#id-1.4.4.8.3">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   When a barclamp is applied, all nodes will run <code class="literal">chef-client</code>. Sometimes it is
   not necessary to run chef for each node as attributes or roles may have not
   changed for some of the nodes.
  </p><p>
   The <code class="literal">skip unchanged nodes</code> feature allows nodes to be
   skipped if their attributes or roles have not changed. This can help speed
   up applying a barclamp, especially in large deployments and with roles that
   are usually applied to a lot of nodes, such as nova.
  </p><p>
   <span class="bold"><strong>Enabling the Feature</strong></span>
  </p><p>
   In <code class="filename">/opt/dell/crowbar_framework/config/crowbar.yml</code>, set
   the option <code class="literal">skip_unchanged_nodes</code> to
   <code class="literal">true</code>.
  </p><div class="verbatim-wrap"><pre class="screen">default: &amp;default
skip_unchanged_nodes:
  enabled: false <span class="bold"><strong>&lt;&lt;&lt; change to true</strong></span></pre></div></section><section class="sect1" id="id-1.4.4.8.4" data-id-title="Controlling Chef Restarts Manually"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">9.3 </span><span class="title-name">Controlling Chef Restarts Manually</span></span> <a title="Permalink" class="permalink" href="#id-1.4.4.8.4">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   When a service configuration has changed, Chef forces this service to
   restart. Sometimes it is useful to have manual control of these
   restarts. This feature supports avoiding automatic restart of services and
   including them in a pending restart list.  Disabling restarts is
   enabled/disabled by barclamp and cannot be done on a service level. In other
   words, enabling this feature on the cinder barclamp will disable
   automatic restarts for all cinder-* services.
  </p><p>
   Two steps are necessary to activate this feature:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Enable <code class="literal">disallow_restart</code> in the Crowbar configuration file
    </p></li><li class="step"><p>
     Set the <code class="literal">disable_restart</code> flag for a specific barclamp
     using <code class="literal">crowbar-client</code> or API
    </p></li></ol></div></div><p>
   <span class="bold"><strong>Enabling the Feature</strong></span>
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     In <code class="filename">/opt/dell/crowbar_framework/config/crowbar.yml</code>, set
     the option <code class="literal">disallow_restart</code> to <code class="literal">true</code>.
    </p><div class="verbatim-wrap"><pre class="screen">default: &amp;default
disallow_restart:
  enabled: false <span class="bold"><strong>&lt;&lt;&lt; change to true</strong></span></pre></div></li><li class="step"><p>
     The <code class="literal">disable_restart</code> flag can be set with the Crowbar
     client or with the API.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Set Flag with Crowbar Client
      </p><p>
       The crowbar client options for this feature are accessed with the
       <code class="command">crowbarctl services</code> command, and only work for
       <span class="productname">OpenStack</span> services. The options are:
      </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.8.4.6.2.2.1.3.1"><span class="term">disable_restart</span></dt><dd><p>
          The parameters are the barclamp name and the flag value
          (<code class="literal">true</code> to disable automatic restart and
          <code class="literal">false</code> to enable automatic restart). The command is
          <code class="literal">crowbarctl services disable_restart
          <em class="replaceable">BARCLAMP</em> &lt;true|false&gt;</code>. For
          example, to disable restart of the keystone barclamp, enter the
          command <code class="command">crowbarctl services disable_restart keystone
          true</code>.
         </p></dd><dt id="id-1.4.4.8.4.6.2.2.1.3.2"><span class="term">restart_flags</span></dt><dd><p>
          Used to check the <code class="literal">disable_restart</code> flag for each
          barclamp
         </p><div class="verbatim-wrap"><pre class="screen">crowbarctl services restart_flags
{
  "nova": true,
  "keystone": true,
  "database": true
}</pre></div></dd><dt id="id-1.4.4.8.4.6.2.2.1.3.3"><span class="term">list_restarts</span></dt><dd><p>
          Displays the list of pending restarts. The <code class="literal">pending
          restart</code> flag indicates that a service tried to
          restart due to the Chef run but it did not due to the automatic
          restart being disabled. It also indicates that the service might have
          a new configuration and it will not be applied until it is manually
          restarted.
         </p><p>
          In the following example, the <code class="literal">pacemaker_service</code>
          attribute indicates whether this service is managed by Pacemaker
          (usually in an HA environment) or it is a standalone service managed
          by <code class="literal">systemd</code> (usually in non-HA environments). More
          on Pacemaker at <a class="xref" href="#sec-depl-ostack-pacemaker" title="12.2. Deploying Pacemaker (Optional, HA Setup Only)">Section 12.2, “Deploying Pacemaker (Optional, HA Setup Only)”</a>. The
          service to restart will be <code class="literal">apache2</code> not managed by
          Pacemaker.
         </p><div class="verbatim-wrap"><pre class="screen">crowbarctl services list_restarts
{
  "<em class="replaceable">NODE_IP</em>": {
    "alias": "controller1",
    "keystone": {
      "apache2": {
        "pacemaker_service": false,
        "timestamp": "2017-11-22 11:17:49 UTC"
      }
    }
  }
}</pre></div></dd><dt id="id-1.4.4.8.4.6.2.2.1.3.4"><span class="term">clear_restart</span></dt><dd><p>
          Removes the flag on a specific node for a specific service. It can
          be executed when the service has restarted manually. The command is
          <code class="literal">crowbarctl services clear_restart
          <em class="replaceable">NODE</em>
          <em class="replaceable">SERVICE</em></code>. For example, <code class="command">crowbarctl
          services clear_restart <em class="replaceable">NODE_IP</em>
          apache2</code>
         </p></dd></dl></div></li><li class="listitem"><p>
       Using the API to List, Get Status, Set and Clear Flags
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         /restart_management/configuration
        </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.8.4.6.2.2.2.2.1.2.1"><span class="term">GET</span></dt><dd><p>
           Lists the barclamps and the status of service reboots disallowed
          </p></dd><dt id="id-1.4.4.8.4.6.2.2.2.2.1.2.2"><span class="term">POST (parameters: disallow_restarts, barclamp)</span></dt><dd><p>
            Sets the disallow_restart flag for a barclamp
           </p></dd></dl></div></li><li class="listitem"><p>
         /restart_management/restarts
        </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.8.4.6.2.2.2.2.2.2.1"><span class="term">GET</span></dt><dd><p>
           Lists all of the services needing restarts
          </p></dd><dt id="id-1.4.4.8.4.6.2.2.2.2.2.2.2"><span class="term">POST (parameters: node, service)</span></dt><dd><p>
            Clears the restart flag for the given service in the given node
           </p></dd></dl></div></li></ul></div></li></ul></div></li></ol></div></div></section><section class="sect1" id="id-1.4.4.8.5" data-id-title="Prevent Automatic Restart"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">9.4 </span><span class="title-name">Prevent Automatic Restart</span></span> <a title="Permalink" class="permalink" href="#id-1.4.4.8.5">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   Sometimes a change in a proposal requires services to be restarted on all
   implicated nodes. This could be a problem in a production environment where
   interrupting a service completely is not an option, and where manual restart
   control is needed. With the service <code class="literal">disable_restart</code>
   feature in <code class="literal">crowbarctl</code>, you can set a flag for certain
   proposals to prevent the automatic restart.
  </p><p>
   <span class="bold"><strong>Enabling the Feature</strong></span>
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     In <code class="filename">/opt/dell/crowbar_framework/config/crowbar.yml</code>, set
     the option <code class="literal">disallow_restart</code> to <code class="literal">true</code>.
    </p><div class="verbatim-wrap"><pre class="screen">default: &amp;default
disallow_restart:
  enabled: false <span class="bold"><strong>&lt;&lt;&lt; change to true</strong></span></pre></div></li><li class="step"><p>
     Restart Crowbar
    </p></li><li class="step"><p>
     Enable the <code class="literal">disable_restart</code> flag in the affected
     cookbook.
    </p><div class="verbatim-wrap"><pre class="screen">crowbarctl services disable_restart <em class="replaceable">COOKBOOK</em> true</pre></div><p>
     For example, to disable keystone and RabbitMQ restarts:
    </p><div class="verbatim-wrap"><pre class="screen">crowbarctl services disable_restart keystone true
crowbarctl services disable_restart rabbitmq true</pre></div></li><li class="step"><p>
     To check the proposals that have <code class="literal">disable_restart</code>
     enabled:
    </p><div class="verbatim-wrap"><pre class="screen">crowbarctl services restart_flags
{
  "rabbitmq": true,
  "keystone": true,
}</pre></div></li></ol></div></div><p>
   <span class="bold"><strong>Check Pending Restarts</strong></span>
  </p><p>
   When a proposal is applied with the <code class="literal">disable_restart</code> flag
   enabled, the implicated nodes will not restart. They will be listed as
   pending restarts. To check this list, run the command <code class="command">crowbarctl
   services list_restarts</code>.
  </p><p>
   In the following example, <code class="literal">disable_restart</code> is enabled for
   RabbitMQ. After applying rabbitmq, <code class="literal">list_restarts</code> will
   show the affected nodes, the proposal, and the services to restart.
  </p><div class="verbatim-wrap"><pre class="screen">crowbarctl services list_restarts
{
   "d52-54-77-77-01-01.vo5.cloud.suse.de": {
    "alias": "controller1",
    "rabbitmq": {
      "rabbitmq-server": {
        "pacemaker_service": false,
        "timestamp": "2018-03-07 15:30:30 UTC"
      }
    }
  }
}</pre></div><p>
   After a manual restart of the service, the flags should be cleaned. The
   following command will clean the flag for a specific node, for a specific
   proposal or all proposals, and for a specific service.
  </p><div class="verbatim-wrap"><pre class="screen">crowbarctl services clear_restart <em class="replaceable">NODE [COOKBOOK [SERVICE]]</em></pre></div><p>
   For example, to clean the RabbitMQ flag from the
   <code class="literal">controller1</code> node, run the command:
  </p><div class="verbatim-wrap"><pre class="screen">crowbarctl service clear_restart controller1 rabbitmq</pre></div><p>
   To clean the <code class="literal">controller1</code> node, run the command:
  </p><div class="verbatim-wrap"><pre class="screen">crowbarctl service clear_restart controller1</pre></div></section></section></div><div class="part" id="part-depl-ostack" data-id-title="Setting Up OpenStack Nodes and Services"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">Part III </span><span class="title-name">Setting Up <span class="productname">OpenStack</span> Nodes and Services </span></span><a title="Permalink" class="permalink" href="#part-depl-ostack">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-depl-crowbar"><span class="title-number">10 </span><span class="title-name">The Crowbar Web Interface</span></a></span></li><dd class="toc-abstract"><p>The Crowbar Web interface runs on the Administration Server. It provides an
    overview of the most important deployment details in your cloud. This includes a
    view of the nodes and which roles are deployed on which nodes, and the
    barclamp proposals that can be edited and deployed. In addition, the
    Crowbar Web interface shows details about the networks and switches in your
    cloud. It also provides graphical access to tools for managing
    your repositories, backing up or restoring the Administration Server, exporting the
    Chef configuration, or generating a <code class="literal">supportconfig</code> TAR
    archive with the most important log files.</p></dd><li><span class="chapter"><a href="#cha-depl-inst-nodes"><span class="title-number">11 </span><span class="title-name">Installing the <span class="productname">OpenStack</span> Nodes</span></a></span></li><dd class="toc-abstract"><p>
  The <span class="productname">OpenStack</span> nodes represent the actual cloud infrastructure. Node
  installation and service deployment is done automatically from the
  Administration Server. Before deploying the <span class="productname">OpenStack</span> services, SUSE Linux Enterprise Server 12 SP4 will be installed on all Control Nodes and Storage Nodes.
 </p></dd><li><span class="chapter"><a href="#cha-depl-ostack"><span class="title-number">12 </span><span class="title-name">Deploying the <span class="productname">OpenStack</span> Services</span></a></span></li><dd class="toc-abstract"><p>After the nodes are installed and configured you can start deploying the OpenStack components to finalize the installation. The components need to be deployed in a given order, because they depend on one another. The Pacemaker component for an HA setup is the only exception from this rule—it can be …</p></dd><li><span class="chapter"><a href="#sec-deploy-policy-json"><span class="title-number">13 </span><span class="title-name">Limiting Users' Access Rights</span></a></span></li><dd class="toc-abstract"><p>To limit users' access rights (or to define more fine-grained access rights), you can use Role Based Access Control (RBAC, only available with keystone v3). In the example below, we will create a new role (ProjectAdmin). It allows users with this role to add and remove other users to the member role…</p></dd><li><span class="chapter"><a href="#cha-depl-ostack-configs"><span class="title-number">14 </span><span class="title-name">Configuration Files for <span class="productname">OpenStack</span> Services</span></a></span></li><dd class="toc-abstract"><p>
    Typically, each <span class="productname">OpenStack</span> component comes with a configuration file, for
    example: <code class="filename">/etc/nova/nova.conf</code>.
   </p><p>
    These configuration files can still be used. However, to configure an
    <span class="productname">OpenStack</span> component and its different components and roles, it is now
    preferred to add custom configuration file snippets to a
    <code class="filename"><em class="replaceable">SERVICE</em>.conf.d/</code> directory
    instead.

   </p></dd><li><span class="chapter"><a href="#install-heat-templates"><span class="title-number">15 </span><span class="title-name">Installing SUSE CaaS Platform heat Templates</span></a></span></li><dd class="toc-abstract"><p>
  This chapter describes how to install SUSE CaaS Platform heat template on
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
 </p></dd><li><span class="chapter"><a href="#install-caasp-terraform"><span class="title-number">16 </span><span class="title-name">Installing SUSE CaaS Platform v4 using terraform</span></a></span></li><dd class="toc-abstract"><p>
   More information about the SUSE CaaS Platform v4 is available at <a class="link" href="https://documentation.suse.com/suse-caasp/4.0/html/caasp-deployment/_deployment_instructions.html#_deployment_on_suse_openstack_cloud" target="_blank">https://documentation.suse.com/suse-caasp/4.0/html/caasp-deployment/_deployment_instructions.html#_deployment_on_suse_openstack_cloud</a>
  </p></dd></ul></div><section class="chapter" id="cha-depl-crowbar" data-id-title="The Crowbar Web Interface"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">10 </span><span class="title-name">The Crowbar Web Interface</span></span> <a title="Permalink" class="permalink" href="#cha-depl-crowbar">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><div><div class="abstract"><p>The Crowbar Web interface runs on the Administration Server. It provides an
    overview of the most important deployment details in your cloud. This includes a
    view of the nodes and which roles are deployed on which nodes, and the
    barclamp proposals that can be edited and deployed. In addition, the
    Crowbar Web interface shows details about the networks and switches in your
    cloud. It also provides graphical access to tools for managing
    your repositories, backing up or restoring the Administration Server, exporting the
    Chef configuration, or generating a <code class="literal">supportconfig</code> TAR
    archive with the most important log files.</p></div></div></div></div><div id="tip-crow-api" data-id-title="Crowbar API Documentation" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Crowbar API Documentation</div><p>
   You can access the Crowbar API documentation from the following static page:
   <code class="literal">http://<em class="replaceable">CROWBAR_SERVER</em>/apidoc</code>.
  </p><p>
    The documentation contains information about the crowbar API endpoints and
    its parameters, including response examples, possible errors (and their
    HTTP response codes), parameter validations, and required headers.
   </p></div><section class="sect1" id="sec-depl-crow-login" data-id-title="Logging In"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.1 </span><span class="title-name">Logging In</span></span> <a title="Permalink" class="permalink" href="#sec-depl-crow-login">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p> The Crowbar Web interface uses the HTTP protocol and port
   <code class="literal">80</code>. </p><div class="procedure" id="pro-depl-crow-login" data-id-title="Logging In to the Crowbar Web Interface"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 10.1: </span><span class="title-name">Logging In to the Crowbar Web Interface </span></span><a title="Permalink" class="permalink" href="#pro-depl-crow-login">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p> On any machine, start a Web browser and make sure that JavaScript and
     cookies are enabled.</p></li><li class="step"><p> As URL, enter the IP address of the Administration Server, for example:</p><div class="verbatim-wrap"><pre class="screen">http://192.168.124.10/</pre></div></li><li class="step"><p>Log in as user
     <code class="systemitem">crowbar</code>. If you have not changed
     the password, it is <code class="literal">crowbar</code> by default.
    </p></li></ol></div></div><div class="procedure" id="pro-depl-crow-password" data-id-title="Changing the Password for the Crowbar Web Interface"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 10.2: </span><span class="title-name">Changing the Password for the Crowbar Web Interface </span></span><a title="Permalink" class="permalink" href="#pro-depl-crow-password">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     On the Administration Server, open the following file in a text editor:
     <code class="filename">/etc/crowbarrc</code>. It contains the following:
    </p><div class="verbatim-wrap"><pre class="screen">[default]
username=crowbar
password=crowbar</pre></div><p>
      Change the <code class="literal">password</code> entry and save the file.
    </p></li><li class="step"><p>
      Alternatively, use the YaST Crowbar module to edit the password as
      described in <a class="xref" href="#sec-depl-adm-inst-crowbar-user" title="7.1. User Settings">Section 7.1, “<span class="guimenu">User Settings</span>”</a>.
    </p></li><li class="step"><p>
      Manually run <code class="command">chef-client</code>. This step is not needed if
      the installation has not been completed yet.
    </p></li></ol></div></div></section><section class="sect1" id="sec-depl-crow-overview" data-id-title="Overview: Main Elements"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.2 </span><span class="title-name">Overview: Main Elements</span></span> <a title="Permalink" class="permalink" href="#sec-depl-crow-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>After logging in to Crowbar, you will see a navigation bar at the
   top-level row. Its menus and the respective views are described in the
   following sections.</p><div class="figure" id="id-1.4.5.2.5.3"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_node_dashboard_groups_installed.png"><img src="images/depl_node_dashboard_groups_installed.png" width="100%" alt="Crowbar UI—Dashboard (Main Screen)" title="Crowbar UI—Dashboard (Main Screen)"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 10.1: </span><span class="title-name">Crowbar UI—Dashboard (Main Screen) </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.2.5.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><section class="sect2" id="sec-depl-crow-overview-nodes" data-id-title="Nodes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.2.1 </span><span class="title-name">Nodes</span></span> <a title="Permalink" class="permalink" href="#sec-depl-crow-overview-nodes">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.2.5.4.2.1"><span class="term"><span class="guimenu">Dashboard</span></span></dt><dd><p>This is the default view after logging in to the Crowbar
       Web interface. The Dashboard shows the groups (which you can create to
       arrange nodes according to their purpose), which nodes belong to each
       group, and which state the nodes and groups are in. In addition, the
       total number of nodes is displayed in the top-level row.</p><p>The color of the dot in front of each node or group indicates the status. If the dot for
       a group shows more than one color, hover the mouse pointer over the dot to view the total
       number of nodes and the statuses they are in.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Gray means the node is being discovered by the Administration Server, or that there is no
         up-to-date information about a deployed node. If the status is shown for a node longer than
         expected, check if the chef-client is still running on the node.</p></li><li class="listitem"><p>
         Yellow means the node has been successfully
         <code class="literal">Discovered</code>. As long as the node has not been
         allocated the dot will flash. A solid (non-flashing) yellow dot
         indicates that the node has been allocated, but installation has not
         yet started.
        </p></li><li class="listitem"><p>
         Flashing from yellow to green means the node has been allocated and is
         currently being installed.
        </p></li><li class="listitem"><p>Solid green means the node is in status <code class="literal">Ready</code>.
        </p></li><li class="listitem"><p>Red means the node is in status <code class="literal">Problem</code>.</p></li></ul></div><p>During the initial state of the setup, the Dashboard only shows
       one group called <code class="literal">sw_unknown</code> into which the
       Administration Server is automatically sorted. Initially, all nodes (except
       the Administration Server) are listed with their MAC address as a name.
       However, we recommend creating an alias for each node. This makes
       it easier to identify the node in the admin network and on the
       Dashboard. For details on how to create groups, how to assign nodes
       to a group, and how to create node aliases, see <a class="xref" href="#sec-depl-inst-nodes-install" title="11.2. Node Installation">Section 11.2, “Node Installation”</a>.</p></dd><dt id="id-1.4.5.2.5.4.2.2"><span class="term"><span class="guimenu">Bulk Edit</span></span></dt><dd><p>This screen allows you to edit multiple nodes at once instead of
       editing them individually. It lists all nodes, including
        <span class="guimenu">Name</span> (in form of the MAC address),
<span class="guimenu">Hardware</span> configuration, <span class="guimenu">Alias</span> (used within the admin network),
        <span class="guimenu">Public Name</span> (name used outside of the SUSE <span class="productname">OpenStack</span> Cloud
       network), <span class="guimenu">Group</span>, <span class="guimenu">Intended
        Role</span>, <span class="guimenu">Platform</span> (the operating
       system that is going to be installed on the node),
        <span class="guimenu">License</span> (if available), and allocation
       status. You can toggle the list view between <span class="guimenu">Show
        unallocated</span> or <span class="guimenu">Show all</span> nodes.</p><p>For details on how to fill in the data for all nodes and how to
       start the installation process, see <a class="xref" href="#sec-depl-inst-nodes-install" title="11.2. Node Installation">Section 11.2, “Node Installation”</a>.</p></dd><dt id="id-1.4.5.2.5.4.2.3"><span class="term"><span class="guimenu">HA Clusters</span></span></dt><dd><p>This menu entry only appears if your cloud contains a High Availability setup. The overview
       shows all clusters in your setup, including the <span class="guimenu">Nodes</span> that are members of
       the respective cluster and the <span class="guimenu">Roles</span> assigned to the cluster. It also
       shows if a cluster contains <span class="guimenu">Remote Nodes</span> and which roles are assigned to
       the remote nodes.</p></dd><dt id="id-1.4.5.2.5.4.2.4"><span class="term"><span class="guimenu">Actives Roles</span></span></dt><dd><p>This overview shows which roles have been deployed on which
       node(s). The roles are grouped according to the service to which they
       belong. You cannot edit anything here. To change role deployment, you
       need to edit and redeploy the appropriate barclamps as described in
        <a class="xref" href="#cha-depl-ostack" title="Chapter 12. Deploying the OpenStack Services">Chapter 12, <em>Deploying the <span class="productname">OpenStack</span> Services</em></a>.</p></dd></dl></div></section><section class="sect2" id="sec-depl-crow-overview-barclamps" data-id-title="Barclamps"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.2.2 </span><span class="title-name">Barclamps</span></span> <a title="Permalink" class="permalink" href="#sec-depl-crow-overview-barclamps">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.2.5.5.2.1"><span class="term"><span class="guimenu">All Barclamps</span></span></dt><dd><p>This screen shows a list of all available barclamp proposals, including
       their <span class="guimenu">Status</span>, <span class="guimenu">Name</span>, and a short
       <span class="guimenu">Description</span>. From here, you can
       <span class="guimenu">Edit</span> individual barclamp
       proposals as described in <a class="xref" href="#sec-depl-ostack-barclamps" title="10.3. Deploying Barclamp Proposals">Section 10.3, “Deploying Barclamp Proposals”</a>.
      </p></dd><dt id="id-1.4.5.2.5.5.2.2"><span class="term"><span class="guimenu">Crowbar</span></span></dt><dd><p>This screen only shows the barclamps that are included with the core
       Crowbar framework. They contain general recipes for setting up and
       configuring all nodes. From here, you can <span class="guimenu">Edit</span>
       individual barclamp proposals.</p></dd><dt id="id-1.4.5.2.5.5.2.3"><span class="term"><span class="guimenu"><span class="productname">OpenStack</span></span></span></dt><dd><p>This screen only shows the barclamps that are dedicated to <span class="productname">OpenStack</span>
       service deployment and configuration.  From here, you can
       <span class="guimenu">Edit</span> individual barclamp
       proposals. </p></dd><dt id="id-1.4.5.2.5.5.2.4"><span class="term"><span class="guimenu">Deployment Queue</span></span></dt><dd><p>If barclamps are applied to one or more nodes that are not yet
       available for deployment (for example, because they are rebooting or have
       not been fully installed yet), the proposals will be put in a queue. This
       screen shows the proposals that are <span class="guimenu">Currently deploying</span>
       or <span class="guimenu">Waiting in queue</span>.</p></dd></dl></div></section><section class="sect2" id="sec-depl-crow-overview-utilities" data-id-title="Utilities"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.2.3 </span><span class="title-name">Utilities</span></span> <a title="Permalink" class="permalink" href="#sec-depl-crow-overview-utilities">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.2.5.6.2.1"><span class="term"><span class="guimenu">Exported Items</span></span></dt><dd><p>The <span class="guimenu">Exported Files</span> screen allows you to export the
       Chef configuration and the <code class="literal">supportconfig</code> TAR
       archive. The <code class="literal">supportconfig</code> archive contains system
        information such as the current kernel version being used, the hardware, RPM
        database, partitions, and the most important log files for analysis of any
        problems. To access the export options, click <span class="guimenu">New
        Export</span>. After the export has been successfully finished, the
        <span class="guimenu">Exported Files</span> screen will show any files that are
       available for download.</p></dd><dt id="id-1.4.5.2.5.6.2.2"><span class="term"><span class="guimenu">Repositories</span></span></dt><dd><p>This screen shows an overview of the mandatory, recommended, and
       optional repositories for all architectures of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. On each
       reload of the screen the Crowbar Web interface checks the availability and
       status of the repositories. If a mandatory repository is not present, it
       is marked red in the screen. Any repositories marked green are usable and
       available to each node in the cloud. Usually, the available repositories
       are also shown as <span class="guimenu">Active</span> in the rightmost column. This
       means that the managed nodes will automatically be configured to use this
       repository. If you disable the <span class="guimenu">Active</span> check box for a
       repository, managed nodes will not use that repository.</p><p>You cannot edit any repositories in this screen. If you need additional, third-party
       repositories, or want to modify the repository metadata, edit
        <code class="filename">/etc/crowbar/repos.yml</code>. Find an example of a repository
       definition below:</p><div class="verbatim-wrap"><pre class="screen">suse-12.2:
  x86_64:
    Custom-Repo-12.2:
      url: 'http://example.com/12-SP2:/x86_64/custom-repo/'
      ask_on_error: true # sets the ask_on_error flag in
                         # the autoyast profile for that repo
      priority: 99 # sets the repo priority for zypper</pre></div><p>Alternatively, use the YaST Crowbar module to add or edit repositories as described in <a class="xref" href="#sec-depl-adm-inst-crowbar-repos" title="7.4. Repositories">Section 7.4, “<span class="guimenu">Repositories</span>”</a>.</p></dd><dt id="id-1.4.5.2.5.6.2.3"><span class="term"><span class="guimenu">swift Dashboard</span></span></dt><dd><p>This screen allows you to run
       <code class="command">swift-dispersion-report</code> on the node or nodes
       to which it has been deployed. Use this tool to measure the
       overall health of the swift cluster. For details, see <a class="link" href="http://docs.openstack.org/liberty/config-reference/content/object-storage-dispersion.html" target="_blank">http://docs.openstack.org/liberty/config-reference/content/object-storage-dispersion.html</a>.
       </p></dd><dt id="id-1.4.5.2.5.6.2.4"><span class="term"><span class="guimenu">Backup &amp; Restore</span></span></dt><dd><p>This screen is for creating and downloading a backup of the Administration Server. You can also restore from a backup or upload a backup image from your
       local file system.
       
       </p></dd></dl></div></section><section class="sect2" id="sec-depl-crow-overview-help" data-id-title="Help"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.2.4 </span><span class="title-name">Help</span></span> <a title="Permalink" class="permalink" href="#sec-depl-crow-overview-help">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>From this screen you can access HTML and PDF versions of the
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> manuals that are installed on the Administration Server.</p></section></section><section class="sect1" id="sec-depl-ostack-barclamps" data-id-title="Deploying Barclamp Proposals"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.3 </span><span class="title-name">Deploying Barclamp Proposals</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-barclamps">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    Barclamps are a set of recipes, templates, and installation
    instructions. They are used to automatically install <span class="productname">OpenStack</span> components on the
    nodes. Each barclamp is configured via a so-called proposal. A proposal
    contains the configuration of the service(s) associated with the barclamp
    and a list of machines onto which to deploy the barclamp.
   </p><p>Most barclamps consist of two sections:</p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.2.6.4.1"><span class="term"><span class="guimenu">Attributes</span></span></dt><dd><p>For changing the barclamp's configuration, either by editing the
      respective Web forms (<span class="guimenu">Custom</span> view) or by switching to the
       <span class="guimenu">Raw</span> view, which exposes all configuration options for the barclamp. In
      the <span class="guimenu">Raw</span> view, you directly edit the configuration file. </p><div id="id-1.4.5.2.6.4.1.2.2" data-id-title="Saving Your Changes" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Saving Your Changes</div><p> Before you switch to <span class="guimenu">Raw</span> view or back again to <span class="guimenu">Custom</span> view,
       <span class="guimenu">Save</span> your changes. Otherwise they will
       be lost.</p></div></dd><dt id="id-1.4.5.2.6.4.2"><span class="term"><span class="guimenu">Deployment</span></span></dt><dd><p>Lets you choose onto which nodes to deploy the barclamp. On the left-hand side, you
      see a list of <span class="guimenu">Available Nodes</span>. The right-hand side shows a list of roles
      that belong to the barclamp.</p><p>Assign the nodes to the roles that should be deployed on that node. Some barclamps
      contain roles that can also be deployed to a cluster. If you have deployed the Pacemaker
      barclamp, the <span class="guimenu">Deployment</span> section additionally lists <span class="guimenu">Available
       Clusters</span> and <span class="guimenu">Available Clusters with Remote Nodes</span> in this case.
      The latter are clusters that contain both <span class="quote">“<span class="quote">normal</span>”</span> nodes and Pacemaker remote
      nodes. See <a class="xref" href="#sec-depl-reg-ha-compute" title="2.6.3. High Availability of the Compute Node(s)">Section 2.6.3, “High Availability of the Compute Node(s)”</a> for the basic details.</p><div id="id-1.4.5.2.6.4.2.2.3" data-id-title="Clusters with Remote Nodes" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Clusters with Remote Nodes</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Clusters (or clusters with remote nodes) cannot be assigned to roles that need to
         be deployed on individual nodes. If you try to do so, the Crowbar Web interface shows an error
         message.</p></li><li class="listitem"><p>If you assign a cluster with remote nodes to a role that can only be applied to
          <span class="quote">“<span class="quote">normal</span>”</span> (Corosync) nodes, the role will only be applied to the Corosync
         nodes of that cluster. The role will not be applied to the remote nodes of the same cluster.</p></li></ul></div></div></dd></dl></div><section class="sect2" id="sec-depl-ostack-barclamps-deploy" data-id-title="Creating, Editing and Deploying Barclamp Proposals"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.3.1 </span><span class="title-name">Creating, Editing and Deploying Barclamp Proposals</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-barclamps-deploy">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The following procedure shows how to generally edit, create and deploy barclamp
    proposals. For the description and deployment of the individual barclamps, see
    <a class="xref" href="#cha-depl-ostack" title="Chapter 12. Deploying the OpenStack Services">Chapter 12, <em>Deploying the <span class="productname">OpenStack</span> Services</em></a>.</p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Log in to the Crowbar Web interface.
     </p></li><li class="step"><p>Click
      <span class="guimenu">Barclamps</span> and select
      <span class="guimenu">All Barclamps</span>. Alternatively, filter for categories by selecting
      either <span class="guimenu">Crowbar</span> or <span class="guimenu"><span class="productname">OpenStack</span></span>.</p></li><li class="step"><p>To create a new proposal or edit an existing one, click <span class="guimenu">Create</span> or
      <span class="guimenu">Edit</span> next to the appropriate barclamp.</p></li><li class="step"><p>Change the configuration in the <span class="guimenu">Attributes</span> section:</p><ol type="a" class="substeps"><li class="step"><p>Change the available options via the Web form.</p></li><li class="step"><p>To edit the configuration file directly, first save changes made in the Web form. Click <span class="guimenu">Raw</span> to edit the configuration in the editor view.</p></li><li class="step"><p>After you have finished, <span class="guimenu">Save</span> your changes. (They
       are not applied yet).</p></li></ol></li><li class="step"><p>Assign nodes to a role in the <span class="guimenu">Deployment</span> section of the
     barclamp. By default, one or more nodes are automatically pre-selected for
     available roles.</p><ol type="a" class="substeps"><li class="step"><p>If this pre-selection does not meet your requirements, click the
       <span class="guimenu">Remove</span> icon next to the role to remove the assignment.</p></li><li class="step"><p>To assign a node or cluster of your choice, select the item you want from the list of
       nodes or clusters on the left-hand side, then drag and drop the item onto the desired role name on the right.</p><div id="id-1.4.5.2.6.5.3.5.2.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>Do <span class="emphasis"><em>not</em></span> drop a node or cluster onto the text box—this is
       used to filter the list of available nodes or clusters!</p></div></li><li class="step"><p>To save your changes without deploying them yet, click <span class="guimenu">Save</span>.</p></li></ol></li><li class="step"><p>
      Deploy the proposal by clicking <span class="guimenu">Apply</span>.
     </p><div id="id-1.4.5.2.6.5.3.6.2" data-id-title="Wait Until a Proposal Has Been Deployed" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Wait Until a Proposal Has Been Deployed</div><p>If you deploy a proposal onto a node where a previous one is still
      active, the new proposal will overwrite the old one.</p><p>
       Deploying a proposal might take some time (up to several minutes). Always
       wait until you see the message <span class="quote">“<span class="quote">Successfully applied the proposal</span>”</span>
       before proceeding to the next proposal.
      </p></div></li></ol></div></div><p>A proposal that has not been deployed yet can be deleted in the <span class="guimenu">Edit
     Proposal</span> view by clicking <span class="guimenu">Delete</span>. To delete a proposal that has
    already been deployed, see <a class="xref" href="#sec-depl-ostack-barclamps-delete" title="10.3.3. Deleting a Proposal That Already Has Been Deployed">Section 10.3.3, “Deleting a Proposal That Already Has Been Deployed”</a>.</p></section><section class="sect2" id="sec-depl-ostack-barclamps-failure" data-id-title="Barclamp Deployment Failure"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.3.2 </span><span class="title-name">Barclamp Deployment Failure</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-barclamps-failure">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div id="id-1.4.5.2.6.6.2" data-id-title="Deployment Failure" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Deployment Failure</div><p> A deployment failure of a barclamp may leave your node in an inconsistent state. If
    deployment of a barclamp fails:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
       Fix the reason that has caused the failure.
      </p></li><li class="listitem"><p>Re-deploy the barclamp.</p></li></ol></div><p>For help, see the respective troubleshooting section at <a class="xref" href="#sec-depl-trouble-faq-ostack" title="OpenStack Node Deployment">Q &amp; A 2, “<span class="productname">OpenStack</span> Node Deployment”</a>. </p></div></section><section class="sect2" id="sec-depl-ostack-barclamps-delete" data-id-title="Deleting a Proposal That Already Has Been Deployed"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.3.3 </span><span class="title-name">Deleting a Proposal That Already Has Been Deployed</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-barclamps-delete">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>To delete a proposal that has already been deployed, you first need to
     <span class="guimenu">Deactivate</span> it.</p><div class="procedure" id="id-1.4.5.2.6.7.3" data-id-title="Deactivating and Deleting a Proposal"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 10.3: </span><span class="title-name">Deactivating and Deleting a Proposal </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.2.6.7.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Log in to the Crowbar Web interface. </p></li><li class="step"><p>Click <span class="guimenu">Barclamps</span> › <span class="guimenu">All Barclamps</span>.</p></li><li class="step"><p>Click <span class="guimenu">Edit</span> to open the editing view. </p></li><li class="step"><p>Click <span class="guimenu">Deactivate</span> and confirm your choice in the following
      pop-up.</p><p>Deactivating a proposal removes the chef role from the nodes, so the routine that
      installed and set up the services is not executed anymore.</p></li><li class="step"><p>Click <span class="guimenu">Delete</span> to confirm your choice in the following
      pop-up.</p><p>This removes the barclamp configuration data from the server. </p></li></ol></div></div><p>However, deactivating and deleting a barclamp that already had been deployed does
    <span class="emphasis"><em>not</em></span> remove packages installed when the barclamp was deployed.
    Nor does it stop any services that were started during the barclamp deployment.
   On the affected node, proceed as follows to undo the deployment:</p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Stop the respective services:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl stop <em class="replaceable">service</em></pre></div></li><li class="step"><p>Disable the respective services:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl disable <em class="replaceable">service</em></pre></div></li></ol></div></div><p>Uninstalling the packages should not be necessary.</p></section><section class="sect2" id="sec-depl-ostack-barclamps-queues" data-id-title="Queuing/Dequeuing Proposals"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.3.4 </span><span class="title-name">Queuing/Dequeuing Proposals</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-barclamps-queues">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
     When a proposal is applied to one or more nodes that are not yet
     available for deployment (for example, because they are rebooting or have
     not been yet fully installed), the proposal will be put in a queue. A
     message like
    </p><div class="verbatim-wrap"><pre class="screen">Successfully queued the proposal until the following become ready: d52-54-00-6c-25-44</pre></div><p>
     will be shown when having applied the proposal. A new button
     <span class="guimenu">Dequeue</span> will also become available. Use it to cancel
     the deployment of the proposal by removing it from the queue.
    </p></section></section></section><section class="chapter" id="cha-depl-inst-nodes" data-id-title="Installing the OpenStack Nodes"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">11 </span><span class="title-name">Installing the <span class="productname">OpenStack</span> Nodes</span></span> <a title="Permalink" class="permalink" href="#cha-depl-inst-nodes">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
  The <span class="productname">OpenStack</span> nodes represent the actual cloud infrastructure. Node
  installation and service deployment is done automatically from the
  Administration Server. Before deploying the <span class="productname">OpenStack</span> services, SUSE Linux Enterprise Server 12 SP4 will be installed on all Control Nodes and Storage Nodes.
 </p><p>
  To prepare the installation, each node needs to be booted using PXE, which
  is provided by the <code class="systemitem">tftp</code> server
  from the Administration Server. Afterward you can allocate the nodes and trigger
  the operating system installation.
 </p><section class="sect1" id="sec-depl-inst-nodes-prep" data-id-title="Preparations"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.1 </span><span class="title-name">Preparations</span></span> <a title="Permalink" class="permalink" href="#sec-depl-inst-nodes-prep">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.3.5.2.1"><span class="term">Meaningful Node Names</span></dt><dd><p>
      Make a note of the MAC address and the purpose of each node (for
      example, controller, block storage, object storage, compute).
      This will make deploying the <span class="productname">OpenStack</span> components a lot easier and
      less error-prone. It also enables you to assign meaningful names
      (aliases) to the nodes, which are otherwise listed with the MAC
      address by default.
     </p></dd><dt id="id-1.4.5.3.5.2.2"><span class="term">BIOS Boot Settings</span></dt><dd><p>
      Make sure booting using PXE (booting from the network) is enabled and
      configured as the <span class="emphasis"><em>primary</em></span> boot-option for each
      node. The nodes will boot twice from the network during the allocation
      and installation phase. Booting from the first hard disk needs to be
      configured as the second boot option.
     </p></dd><dt id="id-1.4.5.3.5.2.3"><span class="term">Custom Node Configuration</span></dt><dd><p>
      All nodes are installed using AutoYaST with the same configuration
      located at
      <code class="filename">/opt/dell/chef/cookbooks/provisioner/templates/default/autoyast.xml.erb</code>.
      If this configuration does not match your needs (for example if you
      need special third party drivers) you need to make adjustments to this
      file. See the <a class="link" href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-autoyast/#book-autoyast" target="_blank">https://documentation.suse.com/sles/15-SP1/single-html/SLES-autoyast/#book-autoyast</a> for details.
      If you change the AutoYaST configuration file, you need to re-upload
      it to Chef using the following command:
     </p><div class="verbatim-wrap"><pre class="screen">knife cookbook upload -o /opt/dell/chef/cookbooks/ provisioner</pre></div></dd><dt id="var-depl-inst-nodes-prep-root-login"><span class="term">Direct <code class="systemitem">root</code> Login</span></dt><dd><p>
      By default, the <code class="systemitem">root</code> account on the nodes has no password
      assigned, so a direct <code class="systemitem">root</code> login is not possible. Logging in
      on the nodes as <code class="systemitem">root</code> is only possible via SSH public keys
      (for example, from the Administration Server).
     </p><p>
      If you want to allow direct <code class="systemitem">root</code> login, you can set a
      password via the Crowbar Provisioner barclamp before deploying the
      nodes. That password will be used for the <code class="systemitem">root</code> account on all
      <span class="productname">OpenStack</span> nodes. Using this method after the nodes are deployed is
      not possible. In that case you would need to log in to each node via
      SSH from the Administration Server and change the password manually with
      <code class="command">passwd</code>.
     </p><div class="orderedlist"><div class="title-container"><div class="orderedlist-title-wrap"><div class="orderedlist-title"><span class="title-number-name"><span class="title-name">Setting a <code class="systemitem">root</code> Password for the <span class="productname">OpenStack</span> Nodes </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.3.5.2.4.2.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><ol class="orderedlist" type="1"><li class="listitem"><p>
        Create an md5-hashed <code class="systemitem">root</code>-password, for example by using
        <code class="command">openssl passwd</code> <code class="option">-1</code>.
       </p></li><li class="listitem"><p>
        Open a browser and point it to the Crowbar Web interface on the
        Administration Server, for example <code class="literal">http://192.168.124.10</code>. Log
        in as user <code class="systemitem">crowbar</code>. The
        password is <code class="literal">crowbar</code> by default, if you have not
        changed it during the installation.
       </p></li><li class="listitem"><p>
        Open the barclamp menu by clicking <span class="guimenu">Barclamps</span> › <span class="guimenu">Crowbar</span>. Click the <span class="guimenu">Provisioner</span> barclamp
        entry and <span class="guimenu">Edit</span> the <span class="guimenu">Default</span>
        proposal.
       </p></li><li class="listitem"><p>
        Click <span class="guimenu">Raw</span> in the <span class="guimenu">Attributes</span>
        section to edit the configuration file.
       </p></li><li class="listitem"><p>
        Add the following line to the end of the file before the last
        closing curly bracket:
       </p><div class="verbatim-wrap"><pre class="screen">, "root_password_hash": "<em class="replaceable">HASHED_PASSWORD</em>"</pre></div><p>
        replacing "<em class="replaceable">HASHED_PASSWORD</em>" with the
        password you generated in the first step.
       </p></li><li class="listitem"><p>
        Click <span class="guimenu">Apply</span>.
       </p></li></ol></div></dd></dl></div></section><section class="sect1" id="sec-depl-inst-nodes-install" data-id-title="Node Installation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.2 </span><span class="title-name">Node Installation</span></span> <a title="Permalink" class="permalink" href="#sec-depl-inst-nodes-install">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   To install a node, you need to boot it first using PXE. It will be booted
   with an image that enables the Administration Server to discover the node and make
   it available for installation. When you have allocated the node, it will
   boot using PXE again and the automatic installation will start.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Boot all nodes that you want to deploy using PXE. The nodes will boot
     into the SLEShammer image, which performs the initial
     hardware discovery.
    </p><div id="id-1.4.5.3.6.3.1.2" data-id-title="Limit the Number of Concurrent Boots using PXE" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Limit the Number of Concurrent Boots using PXE</div><p>
      Booting many nodes at the same time using PXE will cause heavy load on
      the TFTP server, because all nodes will request the boot image at the
      same time. We recommend booting the nodes at different intervals.
     </p></div></li><li class="step"><p>
     Open a browser and point it to the Crowbar Web interface on the Administration Server,
     for example <code class="literal">http://192.168.124.10/</code>. Log in as user
     <code class="systemitem">crowbar</code>. The password is
     <code class="literal">crowbar</code> by default, if you have not changed it.
    </p><p>
     Click <span class="guimenu">Nodes</span> › <span class="guimenu">Dashboard</span> to open the <span class="guimenu">Node
     Dashboard</span>.
    </p></li><li class="step"><p>
     Each node that has successfully booted will be listed as being in state
     <code class="literal">Discovered</code>, indicated by a yellow bullet. The nodes
     will be listed with their MAC address as a name. Wait until all nodes
     are listed as <code class="literal">Discovered</code> before proceeding. If a node does not report as <code class="literal">Discovered</code>, it
     may need to be rebooted manually.
    </p><div class="figure" id="id-1.4.5.3.6.3.3.2"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_node_dashboard_initial_nodes.png"><img src="images/depl_node_dashboard_initial_nodes.png" width="75%" alt="Discovered Nodes" title="Discovered Nodes"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 11.1: </span><span class="title-name">Discovered Nodes </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.3.6.3.3.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></li><li class="step"><p>
     Although this step is optional, we recommend properly grouping
     your nodes at this stage, since it lets you clearly arrange all nodes.
     Grouping the nodes by role would be one option, for example control,
     compute and object storage (swift).
    </p><ol type="a" class="substeps"><li class="step"><p>
       Enter the name of a new group into the <span class="guimenu">New Group</span>
       text box and click <span class="guimenu">Add Group</span>.
      </p></li><li class="step"><p>
       Drag and drop a node onto the title of the newly created group.
       Repeat this step for each node you want to put into the group.
       
      </p><div class="figure" id="id-1.4.5.3.6.3.4.2.2.2"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_node_dashboard_groups_initial.png"><img src="images/depl_node_dashboard_groups_initial.png" width="75%" alt="Grouping Nodes" title="Grouping Nodes"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 11.2: </span><span class="title-name">Grouping Nodes </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.3.6.3.4.2.2.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></li></ol></li><li class="step"><p>
     To allocate all nodes, click <span class="guimenu">Nodes</span> › <span class="guimenu">Bulk Edit</span>. To allocate a single node,
     click the name of a node, then click <span class="guimenu">Edit</span>.
    </p><div class="figure" id="id-1.4.5.3.6.3.5.2"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_node_edit.png"><img src="images/depl_node_edit.png" width="75%" alt="Editing a Single Node" title="Editing a Single Node"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 11.3: </span><span class="title-name">Editing a Single Node </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.3.6.3.5.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><div id="id-1.4.5.3.6.3.5.3" data-id-title="Limit the Number of Concurrent Node Deployments" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Limit the Number of Concurrent Node Deployments</div><p>
      Deploying many nodes in bulk mode will cause heavy load
      on the Administration Server. The subsequent concurrent Chef client runs
      triggered by the nodes will require a lot of RAM on the Administration Server.
     </p><p>
      Therefore it is recommended to limit the number of concurrent
      <span class="quote">“<span class="quote">Allocations</span>”</span> in bulk mode. The maximum number depends on
      the amount of RAM on the Administration Server—limiting concurrent
      deployments to five up to ten is recommended.
     </p></div></li><li class="step"><p> In single node editing mode, you can also specify the
      <span class="guimenu">Filesystem Type</span> for the node. By default, it is set to
      <code class="literal">ext4</code> for all nodes. We recommended using the default.</p></li><li class="step"><p>
     Provide a meaningful <span class="guimenu">Alias</span>, <span class="guimenu">Public
     Name</span>, and a <span class="guimenu">Description</span> for each node, and then
     check the <span class="guimenu">Allocate</span> box. You can also specify the
     <span class="guimenu">Intended Role</span> for the node. This optional setting is
     used to make reasonable proposals for the barclamps.
    </p><p>
     By default the <span class="guimenu">Target Platform</span> is set to <span class="guimenu">SLES 12
     SP2</span>.
    </p><div id="id-1.4.5.3.6.3.7.3" data-id-title="Alias Names" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Alias Names</div><p>
      Providing an alias name will change the default node names (MAC
      address) to the name you provided, making it easier to identify the
      node. Furthermore, this alias will also be used as a DNS
      <code class="literal">CNAME</code> for the node in the admin network. As a
      result, you can access the node via this alias when, for example,
      logging in via SSH.
     </p></div><div id="id-1.4.5.3.6.3.7.4" data-id-title="Public Names" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Public Names</div><p>
      A node's <span class="guimenu">Alias Name</span> is resolved by the DNS server
      installed on the Administration Server and therefore only available within the
      cloud network. The <span class="productname">OpenStack</span> Dashboard or some APIs
      (<code class="systemitem">keystone-server</code>,
      <code class="systemitem">glance-server</code>,
      <code class="systemitem">cinder-controller</code>,
      <code class="systemitem">neutron-server</code>,
      <code class="systemitem">nova-controller</code>, and
      <code class="systemitem">swift-proxy</code>) can be accessed
      from outside the SUSE <span class="productname">OpenStack</span> Cloud network. To be able to access them by
      name, these names need to be resolved by a name server placed outside
      of the SUSE <span class="productname">OpenStack</span> Cloud network. If you have created DNS entries for nodes,
      specify the name in the <span class="guimenu">Public Name</span> field.
     </p><p>
      The <span class="guimenu">Public Name</span> is never used within the SUSE <span class="productname">OpenStack</span> Cloud
      network. However, if you create an SSL certificate for a node that has
      a public name, this name must be added as an
      <code class="literal">AlternativeName</code> to the certificate. See <a class="xref" href="#sec-depl-req-ssl" title="2.3. SSL Encryption">Section 2.3, “SSL Encryption”</a> for more information.
     </p></div><div class="figure" id="id-1.4.5.3.6.3.7.5"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_node_bulk_edit_allocate.png"><img src="images/depl_node_bulk_edit_allocate.png" width="75%" alt="Bulk Editing Nodes" title="Bulk Editing Nodes"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 11.4: </span><span class="title-name">Bulk Editing Nodes </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.3.6.3.7.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></li><li class="step"><p> When you have filled in the data for all nodes, click
      <span class="guimenu">Save</span>. The nodes will reboot and commence the
     AutoYaST-based SUSE Linux Enterprise Server installation (or installation of other target platforms,
     if selected) via a second boot using PXE. Click <span class="guimenu">Nodes</span> › <span class="guimenu">Dashboard</span> to return to the <span class="guimenu">Node Dashboard</span>. </p></li><li class="step"><p>
     Nodes that are being installed are listed with the status
     <code class="literal">Installing</code> (yellow/green bullet). When the
     installation of a node has finished, it is listed as being
     <code class="literal">Ready</code>, indicated by a green bullet. Wait until all
     nodes are listed as <code class="literal">Ready</code> before proceeding.
    </p><div class="figure" id="id-1.4.5.3.6.3.9.2"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_node_dashboard_groups_installed.png"><img src="images/depl_node_dashboard_groups_installed.png" width="75%" alt="All Nodes Have Been Installed" title="All Nodes Have Been Installed"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 11.5: </span><span class="title-name">All Nodes Have Been Installed </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.3.6.3.9.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></li></ol></div></div></section><section class="sect1" id="sec-depl-inst-nodes-install-external" data-id-title="Converting Existing SUSE Linux Enterprise Server 12 SP4 Machines Into SUSE OpenStack Cloud Nodes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.3 </span><span class="title-name">Converting Existing SUSE Linux Enterprise Server 12 SP4 Machines Into SUSE <span class="productname">OpenStack</span> Cloud Nodes</span></span> <a title="Permalink" class="permalink" href="#sec-depl-inst-nodes-install-external">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   SUSE <span class="productname">OpenStack</span> Cloud allows adding existing machines installed with SUSE Linux Enterprise Server 12 SP4 to
   the pool of nodes. This enables you to use spare machines for
   SUSE <span class="productname">OpenStack</span> Cloud, and offers an alternative way of provisioning and installing
   nodes (via SUSE Manager for example). The
   machine must run SUSE Linux Enterprise Server 12 SP4.
  </p><p>
   The machine also needs to be on the same network as the
   Administration Server, because it needs to communicate with this server. Since the
   Administration Server provides a DHCP server, we recommend configuring this machine to get its network assignments from DHCP. If it has a static IP address, make
   sure it is not already used in the admin network. Check the list of used
   IP addresses with the YaST Crowbar module as described in
   <a class="xref" href="#sec-depl-adm-inst-crowbar-network" title="7.2. Networks">Section 7.2, “<span class="guimenu">Networks</span>”</a>.
  </p><p>
   Proceed as follows to convert an existing SUSE Linux Enterprise Server 12 SP4 machine into a
   SUSE <span class="productname">OpenStack</span> Cloud node:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Download the <code class="filename">crowbar_register</code> script from the
     Administration Server at
     <code class="literal">http://<em class="replaceable">192.168.124.10</em>:8091/suse-12.4/x86_64/crowbar_register</code>.
     Replace the IP address with the IP address of your Administration Server using
     <code class="command">curl</code> or <code class="command">wget</code>. Note that the
     download only works from within the admin network.
    </p></li><li class="step"><p>
     Make the <code class="filename">crowbar_register</code> script executable
     (<code class="command">chmod</code> <code class="option">a+x</code> crowbar_register).
    </p></li><li class="step"><p>
     Run the <code class="filename">crowbar_register</code> script. If you have
     multiple network interfaces, the script tries to automatically detect
     the one that is connected to the admin network. You may also explicitly
     specify which network interface to use by using the
     <code class="option">--interface</code> switch, for example
     <code class="command">crowbar_register</code> <code class="option">--interface eth1</code>.
    </p></li><li class="step"><p>
     After the script has successfully run, the machine has been added to
     the pool of nodes in the SUSE <span class="productname">OpenStack</span> Cloud and can be used as any other node
     from the pool.
    </p></li></ol></div></div></section><section class="sect1" id="sec-depl-inst-nodes-post" data-id-title="Post-Installation Configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.4 </span><span class="title-name">Post-Installation Configuration</span></span> <a title="Permalink" class="permalink" href="#sec-depl-inst-nodes-post">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   The following lists some <span class="emphasis"><em>optional</em></span> configuration
   steps like configuring node updates, monitoring, access, and
   enabling SSL. You may entirely skip the following steps or perform any
   of them at a later stage.
  </p><section class="sect2" id="sec-depl-inst-nodes-post-updater" data-id-title="Deploying Node Updates with the Updater Barclamp"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.4.1 </span><span class="title-name">Deploying Node Updates with the Updater Barclamp</span></span> <a title="Permalink" class="permalink" href="#sec-depl-inst-nodes-post-updater">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    To keep the operating system and the SUSE <span class="productname">OpenStack</span> Cloud software itself
    up-to-date on the nodes, you can deploy either the Updater barclamp or
    the SUSE Manager barclamp. The latter requires access to a
    SUSE Manager server. The Updater barclamp uses Zypper to install
    updates and patches from repositories made available on the
    Administration Server.
   </p><p>
    The easiest way to provide the required repositories on the Administration Server
    is to set up an SMT server as described in
    <a class="xref" href="#app-deploy-smt" title="Chapter 4. Installing and Setting Up an SMT Server on the Administration Server (Optional)">Chapter 4, <em>Installing and Setting Up an SMT Server on the Administration Server (Optional)</em></a>. Alternatives to setting up an SMT
    server are described in <a class="xref" href="#cha-depl-repo-conf" title="Chapter 5. Software Repository Setup">Chapter 5, <em>Software Repository Setup</em></a>.
   </p><p>
    The Updater barclamp lets you deploy updates that are available on the
    update repositories at the moment of deployment. Each time you deploy
    updates with this barclamp you can choose a different set of nodes to
    which the updates are deployed. This lets you exactly control where and
    when updates are deployed.
   </p><p>
    To deploy the Updater barclamp, proceed as follows. For general
    instructions on how to edit barclamp proposals refer to
    <a class="xref" href="#sec-depl-ostack-barclamps" title="10.3. Deploying Barclamp Proposals">Section 10.3, “Deploying Barclamp Proposals”</a>.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open a browser and point it to the Crowbar Web interface on the
      Administration Server, for example <code class="literal">http://192.168.124.10/</code>. Log in
      as user <code class="systemitem">crowbar</code>. The password
      is <code class="literal">crowbar</code> by default, if you have not changed it
      during the installation.
     </p></li><li class="step"><p>
      Open the barclamp menu by clicking <span class="guimenu">Barclamps</span> › <span class="guimenu">Crowbar</span>.
      Click the <span class="guimenu">Updater</span> barclamp entry and
      <span class="guimenu">Create</span> to open the proposal.
     </p></li><li class="step"><p>
      Configure the barclamp by the following attributes. This
      configuration always applies to all nodes on which the barclamp is
      deployed. Individual configurations for certain nodes are only supported
      by creating a separate proposal.
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.3.8.3.6.3.2.1"><span class="term"><span class="guimenu">Use zypper</span>
       </span></dt><dd><p>
         Define which Zypper subcommand to use for updating.
         <span class="guimenu">patch</span> will install all patches applying to the
         system from the configured update repositories that are available.
         <span class="guimenu">update</span> will update packages from all configured
         repositories (not just the update repositories) that have a higher
         version number than the installed packages.
         <span class="guimenu">dist-upgrade</span> replaces each package installed
         with the version from the repository and deletes packages not
         available in the repositories.
        </p><p>
         We recommend using <span class="guimenu">patch</span>.
        </p></dd><dt id="id-1.4.5.3.8.3.6.3.2.2"><span class="term"><span class="guimenu">Enable GPG Checks</span>
       </span></dt><dd><p>
         If set to true (recommended), checks if packages are correctly
         signed.
        </p></dd><dt id="id-1.4.5.3.8.3.6.3.2.3"><span class="term"><span class="guimenu">Automatically Agree With Licenses</span>
       </span></dt><dd><p>
         If set to true (recommended), Zypper automatically accepts third
         party licenses.
        </p></dd><dt id="id-1.4.5.3.8.3.6.3.2.4"><span class="term"><span class="guimenu">Include Patches that need Reboots (Kernel)</span>
       </span></dt><dd><p>
         Installs patches that require a reboot (for example Kernel or glibc
         updates). Only set this option to <code class="literal">true</code> when you
         can safely reboot the affected nodes. Refer to
         <span class="intraxref">Book “<em class="citetitle">Operations Guide Crowbar</em>”, Chapter 1 “Maintenance”, Section 1.1 “Keeping the Nodes Up-To-Date”</span> for more information.
         Installing a new Kernel and not rebooting may result in an unstable
         system.
        </p></dd><dt id="id-1.4.5.3.8.3.6.3.2.5"><span class="term"><span class="guimenu">Reboot Nodes if Needed</span>
       </span></dt><dd><p>
         Automatically reboots the system in case a patch requiring a reboot
         has been installed. Only set this option to <code class="literal">true</code>
         when you can safely reboot the affected nodes. Refer to
         <span class="intraxref">Book “<em class="citetitle">Operations Guide Crowbar</em>”, Chapter 1 “Maintenance”, Section 1.1 “Keeping the Nodes Up-To-Date”</span> for more information.
        </p></dd></dl></div><div class="figure" id="id-1.4.5.3.8.3.6.3.3"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_updater_attributes.png"><img src="images/depl_barclamp_updater_attributes.png" width="75%" alt="SUSE Updater barclamp: Configuration" title="SUSE Updater barclamp: Configuration"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 11.6: </span><span class="title-name">SUSE Updater barclamp: Configuration </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.3.8.3.6.3.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></li><li class="step"><p>
      Choose the nodes on which the Updater barclamp should be deployed in
      the <span class="guimenu">Node Deployment</span> section by dragging them to the
      <span class="guimenu">Updater</span> column.
     </p><div class="figure" id="id-1.4.5.3.8.3.6.4.2"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_updater_nodes.png"><img src="images/depl_barclamp_updater_nodes.png" width="75%" alt="SUSE Updater barclamp: Node Deployment" title="SUSE Updater barclamp: Node Deployment"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 11.7: </span><span class="title-name">SUSE Updater barclamp: Node Deployment </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.3.8.3.6.4.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></li></ol></div></div><p>
    <code class="command">zypper</code> keeps track of the packages and patches it
    installs in <code class="filename">/var/log/zypp/history</code>. Review that log
    file on a node to find out which updates have been installed. A second
    log file recording debug information on the <code class="command">zypper</code>
    runs can be found at <code class="filename">/var/log/zypper.log</code> on each
    node.
   </p><div id="id-1.4.5.3.8.3.8" data-id-title="Updating Software Packages on Cluster Nodes" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Updating Software Packages on Cluster Nodes</div><p>
     Before starting an update for a cluster node, either stop the cluster
     stack on that node or put the cluster into maintenance mode. If the
     cluster resource manager on a node is active during the software update,
     this can lead to unpredictable results like fencing of active nodes. For
     detailed instructions refer to <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#sec-ha-clvm-migrate" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#sec-ha-clvm-migrate</a>.
    </p></div></section><section class="sect2" id="sec-depl-inst-nodes-post-manager" data-id-title="Configuring Node Updates with the SUSE Manager Client Barclamp"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.4.2 </span><span class="title-name">Configuring Node Updates with the <span class="guimenu">SUSE Manager Client</span>
    Barclamp</span></span> <a title="Permalink" class="permalink" href="#sec-depl-inst-nodes-post-manager">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    To keep the operating system and the SUSE <span class="productname">OpenStack</span> Cloud software itself
    up-to-date on the nodes, you can deploy either <span class="guimenu">SUSE Manager
    Client</span> barclamp or the Updater barclamp. The latter uses
    Zypper to install updates and patches from repositories made available
    on the Administration Server.
   </p><p>
    To enable the SUSE Manager server to manage the SUSE <span class="productname">OpenStack</span> Cloud nodes, you must make the
    respective <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 9 channels, the SUSE Linux Enterprise Server 12 SP4 channels,
    and the channels for extensions used with your deployment (High Availability Extension,
    SUSE Enterprise Storage) available via an activation key.
   </p><p>
    The <span class="guimenu">SUSE Manager Client</span> barclamp requires access to
    the SUSE Manager server from every node it is deployed to.
   </p><p>
    To deploy the <span class="guimenu">SUSE Manager Client</span> barclamp, proceed
    as follows. For general instructions on how to edit barclamp proposals
    refer to <a class="xref" href="#sec-depl-ostack-barclamps" title="10.3. Deploying Barclamp Proposals">Section 10.3, “Deploying Barclamp Proposals”</a>.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Download the package
      <code class="literal">rhn-org-trusted-ssl-cert-<em class="replaceable">VERSION</em>-<em class="replaceable">RELEASE</em>.noarch.rpm</code>
      from
      https://<em class="replaceable">susemanager.example.com</em>/pub/.
      <em class="replaceable">VERSION</em> and
      <em class="replaceable">RELEASE</em> may vary, ask the administrator of
      the SUSE Manager for the correct values.
      <em class="replaceable">susemanager.example.com</em> needs to be
      replaced by the address of your SUSE Manager server. Copy the file you
      downloaded to
      <code class="filename">/opt/dell/chef/cookbooks/suse-manager-client/files/default/ssl-cert.rpm</code>
      on the Administration Server. The package contains the SUSE Manager's CA SSL
      Public Certificate. The certificate installation has not been
      automated on purpose, because downloading the certificate manually
      enables you to check it before copying it.
     </p></li><li class="step"><p>
      Re-install the barclamp by running the following command:
     </p><div class="verbatim-wrap"><pre class="screen">/opt/dell/bin/barclamp_install.rb --rpm core</pre></div></li><li class="step"><p>
      Open a browser and point it to the Crowbar Web interface on the
      Administration Server, for example <code class="literal">http://192.168.124.10/</code>. Log in
      as user <code class="systemitem">crowbar</code>. The password
      is <code class="literal">crowbar</code> by default, if you have not changed it
      during the installation.
     </p></li><li class="step"><p>
      Open the barclamp menu by clicking <span class="guimenu">Barclamps</span> › <span class="guimenu">Crowbar</span>.
      Click the <span class="guimenu">SUSE Manager Client</span> barclamp entry and
      <span class="guimenu">Create</span> to open the proposal.
     </p></li><li class="step"><p>
      Specify the URL of the script for activation of the clients in the <span class="guimenu">URL of the bootstrap script</span> field.
     </p></li><li class="step"><p>
      Choose the nodes on which the SUSE Manager barclamp should be
      deployed in the <span class="guimenu">Deployment</span> section by dragging
      them to the <span class="guimenu">suse-manager-client</span> column. We
      recommend deploying it on all nodes in the SUSE <span class="productname">OpenStack</span> Cloud.
     </p><div class="figure" id="id-1.4.5.3.8.4.6.6.2"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_susemgr.png"><img src="images/depl_barclamp_susemgr.png" width="75%" alt="SUSE Manager barclamp" title="SUSE Manager barclamp"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 11.8: </span><span class="title-name">SUSE Manager barclamp </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.3.8.4.6.6.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></li></ol></div></div><div id="id-1.4.5.3.8.4.7" data-id-title="Updating Software Packages on Cluster Nodes" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Updating Software Packages on Cluster Nodes</div><p>
     Before starting an update for a cluster node, either stop the cluster
     stack on that node or put the cluster into maintenance mode. If the
     cluster resource manager on a node is active during the software update,
     this can lead to unpredictable results like fencing of active nodes. For
     detailed instructions refer to <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#sec-ha-clvm-migrate" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#sec-ha-clvm-migrate</a>.
    </p></div></section><section class="sect2" id="sec-depl-inst-nodes-post-nfs" data-id-title="Mounting NFS Shares on a Node"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.4.3 </span><span class="title-name">Mounting NFS Shares on a Node</span></span> <a title="Permalink" class="permalink" href="#sec-depl-inst-nodes-post-nfs">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    The NFS barclamp allows you to mount NFS share from a remote host on
    nodes in the cloud. This feature can, for example, be used to provide an
    image repository for glance. Note that all nodes which are to mount
    an NFS share must be able to reach the NFS server. This requires manually adjusting the network configuration.
   </p><p>
    To deploy the NFS barclamp, proceed as follows. For general
    instructions on how to edit barclamp proposals refer to
    <a class="xref" href="#sec-depl-ostack-barclamps" title="10.3. Deploying Barclamp Proposals">Section 10.3, “Deploying Barclamp Proposals”</a>.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open a browser and point it to the Crowbar Web interface on the
      Administration Server, for example <code class="literal">http://192.168.124.10/</code>. Log in
      as user <code class="systemitem">crowbar</code>. The password
      is <code class="literal">crowbar</code> by default, if you have not changed it
      during the installation.
     </p></li><li class="step"><p>
      Open the barclamp menu by clicking <span class="guimenu">Barclamps</span> › <span class="guimenu">Crowbar</span>.
      Click the <span class="guimenu">NFS Client</span> barclamp entry and
      <span class="guimenu">Create</span> to open the proposal.
     </p></li><li class="step"><p>
      Configure the barclamp by the following attributes. Each set of
      attributes is used to mount a single NFS share.
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.3.8.5.4.3.2.1"><span class="term"><span class="guimenu">Name</span>
       </span></dt><dd><p>
         Unique name for the current configuration. This name is used in the
         Web interface only to distinguish between different shares.
        </p></dd><dt id="id-1.4.5.3.8.5.4.3.2.2"><span class="term"><span class="guimenu">NFS Server</span>
       </span></dt><dd><p>
         Fully qualified host name or IP address of the NFS server.
        </p></dd><dt id="id-1.4.5.3.8.5.4.3.2.3"><span class="term"><span class="guimenu">Export</span>
       </span></dt><dd><p>
         Export name for the share on the NFS server.
        </p></dd><dt id="id-1.4.5.3.8.5.4.3.2.4"><span class="term"><span class="guimenu">Path</span>
       </span></dt><dd><p>
         Mount point on the target machine.
        </p></dd><dt id="id-1.4.5.3.8.5.4.3.2.5"><span class="term"><span class="guimenu">Mount Options</span>
       </span></dt><dd><p>
         Mount options that will be used on the node. See <code class="command">man 8
         mount </code> for general mount options and <code class="command">man 5
         nfs</code> for a list of NFS-specific options. Note that the
         general option <code class="option">nofail</code> (do not report errors if
         device does not exist) is automatically set.
        </p></dd></dl></div></li><li class="step"><p>
      After having filled in all attributes, click <span class="guimenu">Add</span>. If
      you want to mount more than one share, fill in the data for another
      NFS mount. Otherwise click <span class="guimenu">Save</span> to save the data,
      or <span class="guimenu">Apply</span> to deploy the proposal. Note that you must
      always click <span class="guimenu">Add</span> before saving or applying the
      barclamp, otherwise the data that was entered will be lost.
     </p><div class="figure" id="id-1.4.5.3.8.5.4.4.2"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_nfs.png"><img src="images/depl_barclamp_nfs.png" width="75%" alt="NFS barclamp" title="NFS barclamp"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 11.9: </span><span class="title-name">NFS barclamp </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.3.8.5.4.4.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></li><li class="step"><p>
      Go to the <span class="guimenu">Node Deployment</span> section and drag and drop
      all nodes, on which the NFS shares defined above should be mounted, to
      the <span class="guimenu">nfs-client</span> column. Click
      <span class="guimenu">Apply</span> to deploy the proposal.
     </p><p>
      The NFS barclamp is the only barclamp that lets you create
      different proposals, enabling you to mount different NFS
      shares on different nodes. When you have created an NFS proposal, a
      special <span class="guimenu">Edit</span> is shown in the barclamp overview of the
      Crowbar Web interface. Click it to either
      <span class="guimenu">Edit</span> an existing proposal or
      <span class="guimenu">Create</span> a new one. New proposals must have unique names.
     </p><div class="figure" id="id-1.4.5.3.8.5.4.5.3"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_nfs_edit.png"><img src="images/depl_barclamp_nfs_edit.png" width="75%" alt="Editing an NFS barclamp Proposal" title="Editing an NFS barclamp Proposal"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 11.10: </span><span class="title-name">Editing an NFS barclamp Proposal </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.3.8.5.4.5.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></li></ol></div></div></section><section class="sect2" id="sec-depl-inst-nodes-post-ceph-ext" data-id-title="Using an Externally Managed Ceph Cluster"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.4.4 </span><span class="title-name">Using an Externally Managed Ceph Cluster</span></span> <a title="Permalink" class="permalink" href="#sec-depl-inst-nodes-post-ceph-ext">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    The following chapter provides instructions on using an external Ceph
    cluster in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
   </p><section class="sect3" id="sec-depl-inst-nodes-post-ceph-ext-requirements" data-id-title="Requirements"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">11.4.4.1 </span><span class="title-name">Requirements</span></span> <a title="Permalink" class="permalink" href="#sec-depl-inst-nodes-post-ceph-ext-requirements">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.3.8.6.3.2.1"><span class="term">Ceph Release</span></dt><dd><p>
        External Ceph cluster are supported with SUSE Enterprise Storage 5 or higher. The
        version of Ceph should be compatible with the version of the Ceph
        client supplied with SUSE Linux Enterprise Server 12 SP4.
       </p></dd><dt id="id-1.4.5.3.8.6.3.2.2"><span class="term">Network Configuration</span></dt><dd><p>
        The external Ceph cluster needs to be connected to a separate
        VLAN, which is mapped to the SUSE <span class="productname">OpenStack</span> Cloud storage VLAN. See
        <a class="xref" href="#sec-depl-req-network" title="2.1. Network">Section 2.1, “Network”</a> for more information.
       </p></dd></dl></div></section><section class="sect3" id="sec-depl-inst-nodes-post-ceph-ext-install" data-id-title="Making Ceph Available on the SUSE OpenStack Cloud Nodes"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">11.4.4.2 </span><span class="title-name">Making Ceph Available on the SUSE <span class="productname">OpenStack</span> Cloud Nodes</span></span> <a title="Permalink" class="permalink" href="#sec-depl-inst-nodes-post-ceph-ext-install">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
     Ceph can be used from the KVM Compute Nodes, with
     cinder, and with glance. The following installation
     steps need to be executed on each node accessing Ceph:
    </p><div id="id-1.4.5.3.8.6.4.3" data-id-title="Installation Workflow" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Installation Workflow</div><p>
      The following steps need to be executed before the barclamps get
      deployed.
     </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Log in as user <code class="systemitem">root</code> to a machine in the Ceph cluster
       and generate keyring files for cinder users. Optionally, you can
       generate keyring files for the glance users (only needed
       when using glance with Ceph/Rados). The keyring file that will be
       generated for cinder will also be used on the Compute Nodes.
       To do so, you need to specify pool names and user names for both services. The default
       names are:
      </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
           
          </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
           <p>
            glance
           </p>
          </th><th style="border-bottom: 1px solid ; ">
           <p>
            cinder
           </p>
          </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
           <p>
            <span class="bold"><strong>User</strong></span>
           </p>
          </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
           <p>
            glance
           </p>
          </td><td style="border-bottom: 1px solid ; ">
           <p>
            cinder
           </p>
          </td></tr><tr><td style="border-right: 1px solid ; ">
           <p>
            <span class="bold"><strong>Pool</strong></span>
           </p>
          </td><td style="border-right: 1px solid ; ">
           <p>
            images
           </p>
          </td><td>
           <p>
            volumes
           </p>
          </td></tr></tbody></table></div><p>
       Make a note of user and pool names in case you do not use the default
       values. You will need this information later, when deploying
       glance and cinder.
      </p></li><li class="step"><div id="id-1.4.5.3.8.6.4.4.2.1" data-id-title="Automatic Changes to the Cluster" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Automatic Changes to the Cluster</div><p>
        If you decide to use the admin keyring file to connect the external
        Ceph cluster, be aware that after Crowbar discovers this admin keyring,
        it will create client keyring files, pools, and capabilities needed to run
        glance, cinder, or nova integration.
       </p></div><p>
       If you have access to the admin keyring file and agree that automatic
       changes will be done to the cluster as described above, copy it together
       with the Ceph configuration file to the Administration Server. If you cannot
       access this file, create a keyring:
      </p><ol type="a" class="substeps"><li class="step"><p>
         When you can access the admin keyring file
         <code class="filename">ceph.client.admin.keyring</code>, copy it together with
         <code class="filename">ceph.conf</code> (both files are usually located in
         <code class="filename">/etc/ceph</code>) to a temporary location on the
         Administration Server, for example <code class="filename">/root/tmp/</code>.
        </p></li><li class="step"><p>
         If you cannot access the admin keyring file create a new keyring file
         with the following commands. Re-run the commands for glance, too, if
         needed. First create a key:
        </p><div class="verbatim-wrap"><pre class="screen">ceph auth get-or-create-key client.<em class="replaceable">USERNAME</em> mon "allow r" \
osd 'allow class-read object_prefix rbd_children, allow rwx \
pool=<em class="replaceable">POOLNAME</em>'</pre></div><p>
         Replace <em class="replaceable">USERNAME</em> and
         <em class="replaceable">POOLNAME</em> with the respective values.
        </p><p>
         Now use the key to generate the keyring file
         <code class="filename">/etc/ceph/ceph.client.<em class="replaceable">USERNAME</em>.keyring</code>:
        </p><div class="verbatim-wrap"><pre class="screen">ceph-authtool \
/etc/ceph/ceph.client.<em class="replaceable">USERNAME</em>.keyring \
--create-keyring --name=client.<em class="replaceable">USERNAME</em>&gt; \
--add-key=<em class="replaceable">KEY</em></pre></div><p>
         Replace <em class="replaceable">USERNAME</em> with the respective
         value.
        </p><p>
         Copy the Ceph configuration file <code class="filename">ceph.conf</code>
         (usually located in <code class="filename">/etc/ceph</code>) and the keyring
         file(s) generated above to a temporary location on the Administration Server, for
         example <code class="filename">/root/tmp/</code>.
        </p></li></ol></li><li class="step"><p>
       Log in to the Crowbar Web interface and check whether the nodes
       which should have access to the Ceph cluster already have an IP
       address from the storage network. Do so by going to the
       <span class="guimenu">Dashboard</span> and clicking the node name. An
       <span class="guimenu">IP address</span> should be listed for
       <span class="guimenu">storage</span>. Make a note of the <span class="guimenu">Full
       name</span> of each node that has <span class="emphasis"><em>no</em></span> storage
       network IP address.
      </p></li><li class="step"><p>
       Log in to the Administration Server as user <code class="systemitem">root</code> and run the
       following command for all nodes you noted down in the previous step:
      </p><div class="verbatim-wrap"><pre class="screen">crowbar network allocate_ip "default" <em class="replaceable">NODE</em> "storage" "host"
chef-client</pre></div><p>
       <em class="replaceable">NODE</em> needs to be replaced by the node's
       name.
      </p></li><li class="step"><p>
       After executing the command in the previous step for all
       affected nodes, run the command <code class="command">chef-client</code> on the
       Administration Server.
      </p></li><li class="step"><p>
       Log in to each affected node as user <code class="systemitem">root</code>. See
       <a class="xref" href="#var-depl-trouble-faq-ostack-login" title="Q:"><em>
       How can I log in to a node as root?
      </em></a> for instructions.
       On each node, do the following:
      </p><ol type="a" class="substeps"><li class="step"><p>
         Manually install nova, cinder (if using cinder) and/or glance
         (if using glance) packages with the following commands:
        </p><div class="verbatim-wrap"><pre class="screen">zypper in openstack-glance
zypper in openstack-cinder
zypper in openstack-nova</pre></div></li><li class="step"><p>
         Copy the ceph.conf file from the Administration Server to
         <code class="filename">/etc/ceph</code>:
        </p><div class="verbatim-wrap"><pre class="screen">mkdir -p /etc/ceph
scp root@admin:/root/tmp/ceph.conf /etc/ceph
chmod 664 /etc/ceph/ceph.conf</pre></div></li><li class="step"><p>
         Copy the keyring file(s) to <code class="filename">/etc/ceph</code>. The
         exact process depends on whether you have copied the admin keyring
         file or whether you have created your own keyrings:
        </p><ol type="i" class="substeps"><li class="step"><p>
           If you have copied the admin keyring file, run the following
           command on the Control Node(s) on which cinder and glance
           will be deployed, and on all KVM Compute Nodes:
          </p><div class="verbatim-wrap"><pre class="screen">scp root@admin:/root/tmp/ceph.client.admin.keyring /etc/ceph
chmod 640 /etc/ceph/ceph.client.admin.keyring</pre></div></li><li class="step"><p>
           If you have created you own keyrings, run the following command on
           the Control Node on which cinder will be deployed, and on all
           KVM Compute Nodes to copy the cinder keyring:
          </p><div class="verbatim-wrap"><pre class="screen">scp root@admin:/root/tmp/ceph.client.cinder.keyring /etc/ceph
chmod 640 /etc/ceph/ceph.client.cinder.keyring</pre></div><p>
	    On Control Node on which cinder will be deployed run the
	    following command to update file ownership:
          </p><div class="verbatim-wrap"><pre class="screen">chown root.cinder /etc/ceph/ceph.client.cinder.keyring</pre></div><p>
	    On KVM Compute Nodes run the following command to update file ownership:
          </p><div class="verbatim-wrap"><pre class="screen">chown root.nova /etc/ceph/ceph.client.cinder.keyring</pre></div><p>
           Now copy the glance keyring to the Control Node on which glance
           will be deployed:
          </p><div class="verbatim-wrap"><pre class="screen">scp root@admin:/root/tmp/ceph.client.glance.keyring /etc/ceph
chmod 640 /etc/ceph/ceph.client.glance.keyring
chown root.glance /etc/ceph/ceph.client.glance.keyring</pre></div></li></ol></li></ol></li></ol></div></div></section></section><section class="sect2" id="sec-depl-inst-nodes-post-access" data-id-title="Accessing the Nodes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.4.5 </span><span class="title-name">Accessing the Nodes</span></span> <a title="Permalink" class="permalink" href="#sec-depl-inst-nodes-post-access">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    The nodes can only be accessed via SSH from the Administration Server—it
    is not possible to connect to them from any other host in the network.
   </p><p>
    The <code class="systemitem">root</code> account <span class="emphasis"><em>on the nodes</em></span> has no
    password assigned, therefore logging in to a node as
    <code class="systemitem">root</code>@<em class="replaceable">node</em> is only possible via SSH
    with key authentication. By default, you can only log in with the key of
    the <code class="systemitem">root</code> of the Administration Server
    (root@<em class="replaceable">admin</em>) via SSH only.
   </p><p>
    If you have added users to the Administration Server and want to
    give them permission to log in to the nodes as well, you need to add
    these users' public SSH keys to <code class="systemitem">root</code>'s
    <code class="filename">authorized_keys</code> file on all nodes. Proceed as
    follows:
   </p><div class="procedure" id="id-1.4.5.3.8.7.5" data-id-title="Copying SSH Keys to All Nodes"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 11.1: </span><span class="title-name">Copying SSH Keys to All Nodes </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.3.8.7.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      If they do not already exist, generate an SSH key pair with
      <code class="command">ssh-keygen</code>. This key pair belongs to the user that you use to log in to the nodes. Alternatively, copy an existing public
      key with <code class="command">ssh-copy-id</code>. Refer to the respective man
      pages for more information.
     </p></li><li class="step"><p>
      Log in to the Crowbar Web interface on the Administration Server, for
      example <code class="literal">http://192.168.124.10/</code> (user name and default
      password: <code class="literal">crowbar</code>).
     </p></li><li class="step"><p>
      Open the barclamp menu by clicking <span class="guimenu">Barclamps</span> › <span class="guimenu">Crowbar</span>. Click the <span class="guimenu">Provisioner</span> barclamp
      entry and <span class="guimenu">Edit</span> the <span class="guimenu">Default</span>
      proposal.
     </p></li><li class="step"><p>
      Copy and paste the <span class="emphasis"><em>public</em></span> SSH key of the user
      into the <span class="guimenu">Additional SSH Keys</span> text box. If adding
      keys for multiple users, note that each key needs to be placed on a
      new line.
     </p></li><li class="step"><p>
      Click <span class="guimenu">Apply</span> to deploy the keys and save your
      changes to the proposal.
     </p></li></ol></div></div></section><section class="sect2" id="sec-depl-inst-nodes-post-ssl" data-id-title="Enabling SSL"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.4.6 </span><span class="title-name">Enabling SSL</span></span> <a title="Permalink" class="permalink" href="#sec-depl-inst-nodes-post-ssl">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    To enable SSL to encrypt communication within the cloud (see
    <a class="xref" href="#sec-depl-req-ssl" title="2.3. SSL Encryption">Section 2.3, “SSL Encryption”</a> for details), all nodes running encrypted services need SSL certificates. An SSL certificate is, at a minimum, required on the Control Node.
   </p><p>
    Each certificate consists
    of a pair of files: the certificate file (for example,
    <code class="filename">signing_cert.pem</code>) and the key file (for example,
    <code class="filename">signing_key.pem</code>). If you use your own certificate
    authority (CA) for signing, you will also need a certificate file for
    the CA (for example, <code class="filename">ca.pem</code>). We recommend copying the files to the <code class="filename">/etc</code> directory using the
    directory structure outlined below. If you use a dedicated certificate
    for each service, create directories named after the services (for
    example, <code class="filename">/etc/keystone</code>). If you are using shared
    certificates, use a directory such as <code class="filename">/etc/cloud</code>.
   </p><div class="variablelist"><div class="title-container"><div class="variablelist-title-wrap"><div class="variablelist-title"><span class="title-number-name"><span class="title-name">Recommended Locations for Shared Certificates </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.3.8.8.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><dl class="variablelist"><dt id="id-1.4.5.3.8.8.4.2"><span class="term">SSL Certificate File</span></dt><dd><p>
       <code class="filename">/etc/cloud/ssl/certs/signing_cert.pem</code>
      </p></dd><dt id="id-1.4.5.3.8.8.4.3"><span class="term">SSL Key File</span></dt><dd><p>
       <code class="filename">/etc/cloud/private/signing_key.pem</code>
      </p></dd><dt id="id-1.4.5.3.8.8.4.4"><span class="term">CA Certificates File</span></dt><dd><p>
       <code class="filename">/etc/cloud/ssl/certs/ca.pem</code>
      </p></dd></dl></div></section></section><section class="sect1" id="sec-depl-inst-nodes-edit" data-id-title="Editing Allocated Nodes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.5 </span><span class="title-name">Editing Allocated Nodes</span></span> <a title="Permalink" class="permalink" href="#sec-depl-inst-nodes-edit">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   All nodes that have been allocated can be decommissioned or re-installed.
   Click a node's name in the <span class="guimenu">Node Dashboard</span> to open a
   screen with the node details. The following options are available:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.3.9.3.1"><span class="term"><span class="guimenu">Forget</span>
    </span></dt><dd><p>
      Deletes a node from the pool. If you want to re-use this node again,
      it needs to be reallocated and re-installed from scratch.
     </p></dd><dt id="id-1.4.5.3.9.3.2"><span class="term"><span class="guimenu">Reinstall</span>
    </span></dt><dd><p>
      Triggers a reinstallation. The machine stays allocated. Any barclamps that were deployed on the machine will be re-applied after the installation.
     </p></dd><dt id="id-1.4.5.3.9.3.3"><span class="term"><span class="guimenu">Deallocate</span>
    </span></dt><dd><p>
      Temporarily removes the node from the pool of nodes. After you
      reallocate the node it will take its former role. Useful for adding
      additional machines in times of high load or for decommissioning
      machines in times of low load.
     </p></dd><dt id="id-1.4.5.3.9.3.4"><span class="term">
     <span class="guimenu">Power Actions</span> › <span class="guimenu">Reboot</span>
    </span></dt><dd><p>
      Reboots the node.
     </p></dd><dt id="id-1.4.5.3.9.3.5"><span class="term">
     <span class="guimenu">Power Actions</span> › <span class="guimenu">Shutdown</span>
    </span></dt><dd><p>
      Shuts the node down.
     </p></dd><dt id="id-1.4.5.3.9.3.6"><span class="term">
     <span class="guimenu">Power Actions</span> › <span class="guimenu">Power Cycle</span>
    </span></dt><dd><p>
      Forces a (non-clean) shuts down and a restart afterward. Only use if a
      reboot does not work.
     </p></dd><dt id="id-1.4.5.3.9.3.7"><span class="term">
     <span class="guimenu">Power Actions</span> › <span class="guimenu">Power Off</span>
    </span></dt><dd><p>
      Forces a (non-clean) node shut down. Only use if a clean shut down does
      not work.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.3.9.4"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_nodeinfo.png"><img src="images/depl_nodeinfo.png" width="75%" alt="Node Information" title="Node Information"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 11.11: </span><span class="title-name">Node Information </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.3.9.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><div id="id-1.4.5.3.9.5" data-id-title="Editing Nodes in a Production System" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Editing Nodes in a Production System</div><p>
    When de-allocating nodes that provide essential services, the complete
    cloud will become unusable. If you have not disabled redundancy, you can disable single storage nodes or single
    compute nodes. However, disabling Control Node(s) will cause major problems. It
    will either <span class="quote">“<span class="quote">kill</span>”</span> certain services (for example
    swift) or, at worst the complete cloud (when deallocating the Control Node
    hosting neutron). You should also not disable the nodes providing
    swift ring and proxy services.
   </p></div></section></section><section class="chapter" id="cha-depl-ostack" data-id-title="Deploying the OpenStack Services"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">12 </span><span class="title-name">Deploying the <span class="productname">OpenStack</span> Services</span></span> <a title="Permalink" class="permalink" href="#cha-depl-ostack">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
  After the nodes are installed and configured you can start deploying the
  <span class="productname">OpenStack</span> components to finalize the installation. The components need to be
  deployed in a given order, because they depend on one another. The
  <span class="guimenu">Pacemaker</span> component for an HA setup is the only exception
  from this rule—it can be set up at any time. However, when deploying
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> from scratch, we recommend deploying the
  <span class="guimenu">Pacemaker</span> proposal(s) first. Deployment for all components
  is done from the Crowbar Web interface through recipes, so-called
  <span class="quote">“<span class="quote">barclamps</span>”</span>. (See <a class="xref" href="#sec-depl-services" title="12.24. Roles and Services in SUSE OpenStack Cloud Crowbar">Section 12.24, “Roles and Services in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>”</a> for a table
  of all roles and services, and how to start and stop them.)
 </p><p>
  The components controlling the cloud, including storage management and
  control components, need to be installed on the Control Node(s) (refer to
  <a class="xref" href="#sec-depl-arch-components-control" title="1.2. The Control Node(s)">Section 1.2, “The Control Node(s)”</a> for more information).
  However, you may <span class="emphasis"><em>not</em></span> use your Control Node(s) as a
  compute node or storage host for swift. Do not install the components
  <span class="guimenu">swift-storage</span> and <span class="guimenu">nova-compute-*</span> on the
  Control Node(s). These components must be installed on dedicated Storage Nodes
  and Compute Nodes.
 </p><p>
  When deploying an HA setup, the  Control Nodes are replaced by one or more
  controller clusters consisting of at least two nodes, and three are
  recommended. We recommend setting up three separate clusters for data,
  services, and networking. See <a class="xref" href="#sec-depl-req-ha" title="2.6. High Availability">Section 2.6, “High Availability”</a> for more
  information on requirements and recommendations for an HA setup.
 </p><p>
  The <span class="productname">OpenStack</span> components need to be deployed in the following order. For
  general instructions on how to edit and deploy barclamps, refer to
  <a class="xref" href="#sec-depl-ostack-barclamps" title="10.3. Deploying Barclamp Proposals">Section 10.3, “Deploying Barclamp Proposals”</a>. Any optional components that you
  elect to use must be installed in their correct order.
 </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
    <a class="xref" href="#sec-depl-ostack-designate" title="12.1. Deploying designate">Deploying designate</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-depl-ostack-pacemaker" title="12.2. Deploying Pacemaker (Optional, HA Setup Only)">Deploying Pacemaker (Optional, HA Setup Only)</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-depl-ostack-db" title="12.3. Deploying the Database">Deploying the Database</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-depl-ostack-rabbit" title="12.4. Deploying RabbitMQ">Deploying RabbitMQ</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-depl-ostack-keystone" title="12.5. Deploying keystone">Deploying keystone</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-depl-ostack-monasca" title="12.6. Deploying monasca (Optional)">Deploying monasca (Optional)</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-depl-ostack-swift" title="12.7. Deploying swift (optional)">Deploying swift (optional)</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-depl-ostack-glance" title="12.8. Deploying glance">Deploying glance</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-depl-ostack-cinder" title="12.9. Deploying cinder">Deploying cinder</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-depl-ostack-neutron" title="12.10. Deploying neutron">Deploying neutron</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-depl-ostack-nova" title="12.11. Deploying nova">Deploying nova</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-depl-ostack-dash" title="12.12. Deploying horizon (OpenStack Dashboard)">Deploying horizon (<span class="productname">OpenStack</span> Dashboard)</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-depl-ostack-heat" title="12.13. Deploying heat (Optional)">Deploying heat (Optional)</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-depl-ostack-ceilometer" title="12.14. Deploying ceilometer (Optional)">Deploying ceilometer (Optional)</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-depl-ostack-manila" title="12.15. Deploying manila">Deploying manila</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-depl-ostack-tempest" title="12.16. Deploying Tempest (Optional)">Deploying Tempest (Optional)</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-depl-ostack-magnum" title="12.17. Deploying Magnum (Optional)">Deploying Magnum (Optional)</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-depl-ostack-octavia" title="12.20. Deploying Octavia">Deploying Octavia</a>
   </p></li></ol></div><section class="sect1" id="sec-depl-ostack-designate" data-id-title="Deploying designate"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.1 </span><span class="title-name">Deploying designate</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-designate">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    designate provides <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> DNS as a Service (DNSaaS). It is used to
    create and propagate zones and records over the network using pools of DNS
    servers. Deployment defaults are in place, so not much is required to
    configure designate. neutron needs additional settings for integration with
    designate, which are also present in the <code class="literal">[designate]</code> section in neutron configuration.
  </p><p>
    The designate barclamp relies heavily on the DNS barclamp and expects
    it to be applied without any failures.
  </p><div id="id-1.4.5.4.7.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    In order to deploy designate, at least one node is necessary in the DNS
    barclamp that is not the admin node. The admin node is not added to the
    public network. So another node is needed that can be attached to the public
    network and appear in the designate default pool.
   </p><p>
    We recommend that DNS services are running in a cluster in highly available
    deployments where Designate services are running in a cluster.
    For example, in a typical HA deployment where the controllers
    are deployed in a 3-node cluster, the DNS barclamp should be applied to all
    the controllers, in the same manner as Designate.
   </p></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.7.5.1"><span class="term">designate-server role</span></dt><dd><p>
          Installs the designate server packages and configures the mini-dns (mdns)
          service required by designate.
        </p></dd><dt id="id-1.4.5.4.7.5.2"><span class="term">designate-worker role</span></dt><dd><p>
        Configures a designate worker on the selected nodes. designate uses the
        workers to distribute its workload.
        </p></dd></dl></div><p>
    <code class="literal">designate Sink</code> is an optional service and is not configured as part
    of this barclamp.
  </p><p>
    designate uses pool(s) over which it can distribute zones and
    records. Pools can have varied configuration. Any misconfiguration can lead to
    information leakage.
  </p><p>
    The designate barclamp creates default Bind9 pool out of the box, which can be
    modified later as needed. The default Bind9 pool configuration is created by Crowbar
    on a node with <code class="literal">designate-server</code> role in
    <code class="filename">/etc/designate/pools.crowbar.yaml</code>. You can copy
    this file and edit it according to your requirements. Then provide this
    configuration to designate using the command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>designate-manage pool update --file /etc/designate/pools.crowbar.yaml</pre></div><p>
    The <code class="literal">dns_domain</code> specified in neutron configuration in <code class="literal">[designate]</code> section
    is the default Zone where DNS records for neutron resources are created via
    neutron-designate integration. If this is desired, you have to create this zone
    explicitly using the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack zone create &lt; email &gt; &lt; dns_domain &gt;</pre></div><p>
   Editing the designate proposal:
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/designate-edit-proposal.png"><img src="images/designate-edit-proposal.png" width="75%" alt="Edit designate Proposal" title="Edit designate Proposal"/></a></div></div><section class="sect2" id="sec-depl-ostack-designate-powerdns" data-id-title="Using PowerDNS Backend"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.1.1 </span><span class="title-name">Using PowerDNS Backend</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-designate-powerdns">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    Designate uses Bind9 backend by default. It is also possible to
    use PowerDNS backend in addition to, or as an alternative, to Bind9 backend.
    To do so PowerDNS must be manually deployed as The designate barclamp
    currently does not provide any facility to automatically install and
    configure PowerDNS. This section outlines the steps to deploy PowerDNS
    backend.
   </p><div id="id-1.4.5.4.7.14.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
     If PowerDNS is already deployed, you may skip the
     <a class="xref" href="#sec-depl-ostack-install-powerdns" title="12.1.1.1. Install PowerDNS">Section 12.1.1.1, “Install PowerDNS”</a> section and jump to
     the <a class="xref" href="#sec-depl-ostack-designate-use-powerdns-backend" title="12.1.1.2. Configure Designate To Use PowerDNS Backend">Section 12.1.1.2, “Configure Designate To Use PowerDNS Backend”</a>
     section.
    </p></div><section class="sect3" id="sec-depl-ostack-install-powerdns" data-id-title="Install PowerDNS"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">12.1.1.1 </span><span class="title-name">Install PowerDNS</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-install-powerdns">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
     Follow these steps to install and configure PowerDNS on a Crowbar node.
     Keep in mind that PowerDNS must be deployed with MySQL backend.
    </p><div id="id-1.4.5.4.7.14.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      We recommend that PowerDNS are running in a cluster in highly
      availability deployments where Designate services are running in a
      cluster. For example, in a typical HA deployment
      where the controllers are deployed in a 3-node cluster, PowerDNS should
      be running on all the controllers, in the same manner as Designate.
     </p></div><div class="procedure" id="pro-deploy-powerdns"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Install PowerDNS packages.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper install pdns pdns-backend-mysql</pre></div></li><li class="step"><p>
       Edit <code class="literal">/etc/pdns/pdns.conf</code> and provide these options:
       (See
       <a class="link" href="https://doc.powerdns.com/authoritative/settings.html" target="_blank">https://doc.powerdns.com/authoritative/settings.html</a> for a complete reference).
      </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.7.14.4.4.2.2.1"><span class="term">api</span></dt><dd><p>
          Set it to <code class="literal">yes</code> to enable Web service Rest API.
         </p></dd><dt id="id-1.4.5.4.7.14.4.4.2.2.2"><span class="term">api-key</span></dt><dd><p>
          Static Rest API access key. Use a secure random string here.
         </p></dd><dt id="id-1.4.5.4.7.14.4.4.2.2.3"><span class="term">launch</span></dt><dd><p>
          Must set to <code class="literal">gmysql</code> to use MySQL backend.
         </p></dd><dt id="id-1.4.5.4.7.14.4.4.2.2.4"><span class="term">gmysql-host</span></dt><dd><p>
          Hostname (i.e. FQDN) or IP address of the MySQL server.
         </p></dd><dt id="id-1.4.5.4.7.14.4.4.2.2.5"><span class="term">gmysql-user</span></dt><dd><p>
          MySQL user which have full access to the PowerDNS database.
         </p></dd><dt id="id-1.4.5.4.7.14.4.4.2.2.6"><span class="term">gmysql-password</span></dt><dd><p>
          Password for the MySQL user.
         </p></dd><dt id="id-1.4.5.4.7.14.4.4.2.2.7"><span class="term">gmysql-dbname</span></dt><dd><p>
          MySQL database name for PowerDNS.
         </p></dd><dt id="id-1.4.5.4.7.14.4.4.2.2.8"><span class="term">local-port</span></dt><dd><p>
          Port number where PowerDNS is listening for upcoming requests.
         </p></dd><dt id="id-1.4.5.4.7.14.4.4.2.2.9"><span class="term">setgid</span></dt><dd><p>
          The group where the PowerDNS process is running under.
         </p></dd><dt id="id-1.4.5.4.7.14.4.4.2.2.10"><span class="term">setuid</span></dt><dd><p>
          The user where the PowerDNS process is running under.
         </p></dd><dt id="id-1.4.5.4.7.14.4.4.2.2.11"><span class="term">webserver</span></dt><dd><p>
          Must set to <code class="literal">yes</code> to enable web service RestAPI.
         </p></dd><dt id="id-1.4.5.4.7.14.4.4.2.2.12"><span class="term">webserver-address</span></dt><dd><p>
          Hostname (FQDN) or IP address of the PowerDNS web service.
         </p></dd><dt id="id-1.4.5.4.7.14.4.4.2.2.13"><span class="term">webserver-allow-from</span></dt><dd><p>
          List of IP addresses (IPv4 or IPv6) of the nodes that are permitted
          to talk to the PowerDNS web service. These must include the IP
          address of the Designate worker nodes.
         </p></dd></dl></div><p>
       For example:
      </p><div class="verbatim-wrap"><pre class="screen">api=yes
api-key=Sfw234sDFw90z
launch=gmysql
gmysql-host=mysql.acme.com
gmysql-user=powerdns
gmysql-password=SuperSecured123
gmysql-dbname=powerdns
local-port=54
setgid=pdns
setuid=pdns
webserver=yes
webserver-address=192.168.124.83
webserver-allow-from=0.0.0.0/0,::/0</pre></div></li><li class="step"><p>
       Login to MySQL from a Crowbar MySQL node and create the PowerDNS database
       and the user which has full access to the PowerDNS database. Remember,
       the database name, username, and password must match
       <code class="literal">gmysql-dbname</code>, <code class="literal">gmysql-user</code>,
       and <code class="literal">gmysql-password</code> that were specified above
       respectively.
      </p><p>
       For example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 20075
Server version: 10.2.29-MariaDB-log SUSE package

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]&gt; CREATE DATABASE powerdns;
Query OK, 1 row affected (0.01 sec)

MariaDB [(none)]&gt; GRANT ALL ON powerdns.* TO 'powerdns'@'localhost' IDENTIFIED BY 'SuperSecured123';
Query OK, 0 rows affected (0.00 sec)

MariaDB [(none)]&gt; GRANT ALL ON powerdns.* TO 'powerdns'@'192.168.124.83' IDENTIFIED BY 'SuperSecured123';
Query OK, 0 rows affected, 1 warning (0.02 sec)

MariaDB [(none)]&gt; FLUSH PRIVILEGES;
Query OK, 0 rows affected (0.01 sec)

MariaDB [(none)]&gt; exit
Bye</pre></div></li><li class="step"><p>
       Create a MySQL schema file, named <code class="literal">powerdns-schema.sql</code>,
       with the following content:
      </p><div class="verbatim-wrap"><pre class="screen">/*
 SQL statements to create tables in designate_pdns DB.
 Note: This file is taken as is from:
 https://raw.githubusercontent.com/openstack/designate/master/devstack/designate_plugins/backend-pdns4-mysql-db.sql
*/
CREATE TABLE domains (
  id                    INT AUTO_INCREMENT,
  name                  VARCHAR(255) NOT NULL,
  master                VARCHAR(128) DEFAULT NULL,
  last_check            INT DEFAULT NULL,
  type                  VARCHAR(6) NOT NULL,
  notified_serial       INT DEFAULT NULL,
  account               VARCHAR(40) DEFAULT NULL,
  PRIMARY KEY (id)
) Engine=InnoDB;

CREATE UNIQUE INDEX name_index ON domains(name);


CREATE TABLE records (
  id                    INT AUTO_INCREMENT,
  domain_id             INT DEFAULT NULL,
  name                  VARCHAR(255) DEFAULT NULL,
  type                  VARCHAR(10) DEFAULT NULL,
  -- Changed to "TEXT", as VARCHAR(65000) is too big for most MySQL installs
  content               TEXT DEFAULT NULL,
  ttl                   INT DEFAULT NULL,
  prio                  INT DEFAULT NULL,
  change_date           INT DEFAULT NULL,
  disabled              TINYINT(1) DEFAULT 0,
  ordername             VARCHAR(255) BINARY DEFAULT NULL,
  auth                  TINYINT(1) DEFAULT 1,
  PRIMARY KEY (id)
) Engine=InnoDB;

CREATE INDEX nametype_index ON records(name,type);
CREATE INDEX domain_id ON records(domain_id);
CREATE INDEX recordorder ON records (domain_id, ordername);


CREATE TABLE supermasters (
  ip                    VARCHAR(64) NOT NULL,
  nameserver            VARCHAR(255) NOT NULL,
  account               VARCHAR(40) NOT NULL,
  PRIMARY KEY (ip, nameserver)
) Engine=InnoDB;


CREATE TABLE comments (
  id                    INT AUTO_INCREMENT,
  domain_id             INT NOT NULL,
  name                  VARCHAR(255) NOT NULL,
  type                  VARCHAR(10) NOT NULL,
  modified_at           INT NOT NULL,
  account               VARCHAR(40) NOT NULL,
  -- Changed to "TEXT", as VARCHAR(65000) is too big for most MySQL installs
  comment               TEXT NOT NULL,
  PRIMARY KEY (id)
) Engine=InnoDB;

CREATE INDEX comments_domain_id_idx ON comments (domain_id);
CREATE INDEX comments_name_type_idx ON comments (name, type);
CREATE INDEX comments_order_idx ON comments (domain_id, modified_at);


CREATE TABLE domainmetadata (
  id                    INT AUTO_INCREMENT,
  domain_id             INT NOT NULL,
  kind                  VARCHAR(32),
  content               TEXT,
  PRIMARY KEY (id)
) Engine=InnoDB;

CREATE INDEX domainmetadata_idx ON domainmetadata (domain_id, kind);


CREATE TABLE cryptokeys (
  id                    INT AUTO_INCREMENT,
  domain_id             INT NOT NULL,
  flags                 INT NOT NULL,
  active                BOOL,
  content               TEXT,
  PRIMARY KEY(id)
) Engine=InnoDB;

CREATE INDEX domainidindex ON cryptokeys(domain_id);


CREATE TABLE tsigkeys (
  id                    INT AUTO_INCREMENT,
  name                  VARCHAR(255),
  algorithm             VARCHAR(50),
  secret                VARCHAR(255),
  PRIMARY KEY (id)
) Engine=InnoDB;

CREATE UNIQUE INDEX namealgoindex ON tsigkeys(name, algorithm);</pre></div></li><li class="step"><p>
       Create the PowerDNS schema for the database using <code class="literal">mysql</code>
       CLI. For example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mysql powerdns &lt; powerdns-schema.sql</pre></div></li><li class="step"><p>
       Enable <code class="literal">pdns</code> systemd service.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl enable pdns
<code class="prompt user">root # </code>systemctl start pdns</pre></div><p>
       If <code class="literal">pdns</code> is successfully running, you should see the
       following logs by running <code class="literal">journalctl -u pdns</code> command.
      </p><div class="verbatim-wrap"><pre class="screen">Feb 07 01:44:12 d52-54-77-77-01-01 systemd[1]: Started PowerDNS Authoritative Server.
Feb 07 01:44:12 d52-54-77-77-01-01 pdns_server[21285]: Done launching threads, ready to distribute questions</pre></div></li></ol></div></div></section><section class="sect3" id="sec-depl-ostack-designate-use-powerdns-backend" data-id-title="Configure Designate To Use PowerDNS Backend"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">12.1.1.2 </span><span class="title-name">Configure Designate To Use PowerDNS Backend</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-designate-use-powerdns-backend">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
     Configure Designate to use PowerDNS backend by appending the PowerDNS
     servers to <code class="literal">/etc/designate/pools.crowbar.yaml</code> file
     on a Designate worker node.
    </p><div id="id-1.4.5.4.7.14.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      If we are replacing Bind9 backend with PowerDNS backend, make sure to
      remove the <code class="literal">bind9</code> entries from
      <code class="literal">/etc/designate/pools.crowbar.yaml</code>.
     </p><p>
      In HA deployment, there should be multiple PowerDNS entries.
     </p><p>
      Also, make sure the <code class="literal">api_token</code> matches the
      <code class="literal">api-key</code> that was specified in the
      <code class="literal">/etc/pdns/pdns.conf</code> file earlier.
     </p></div><p>
     Append the PowerDNS entries to the end of
     <code class="literal">/etc/designate/pools.crowbar.yaml</code>. For example:
    </p><div class="verbatim-wrap"><pre class="screen">---
- name: default-bind
  description: Default BIND9 Pool
  id: 794ccc2c-d751-44fe-b57f-8894c9f5c842
  attributes: {}
  ns_records:
  - hostname: public-d52-54-77-77-01-01.virtual.cloud.suse.de.
    priority: 1
  - hostname: public-d52-54-77-77-01-02.virtual.cloud.suse.de.
    priority: 1
  nameservers:
  - host: 192.168.124.83
    port: 53
  - host: 192.168.124.81
    port: 53
  also_notifies: []
  targets:
  - type: bind9
    description: BIND9 Server
    masters:
    - host: 192.168.124.83
      port: 5354
    - host: 192.168.124.82
      port: 5354
    - host: 192.168.124.81
      port: 5354
    options:
      host: 192.168.124.83
      port: 53
      rndc_host: 192.168.124.83
      rndc_port: 953
      rndc_key_file: "/etc/designate/rndc.key"
  - type: bind9
    description: BIND9 Server
    masters:
    - host: 192.168.124.83
      port: 5354
    - host: 192.168.124.82
      port: 5354
    - host: 192.168.124.81
      port: 5354
    options:
      host: 192.168.124.81
      port: 53
      rndc_host: 192.168.124.81
      rndc_port: 953
      rndc_key_file: "/etc/designate/rndc.key"
  - type: pdns4
    description: PowerDNS4 DNS Server
    masters:
      - host: 192.168.124.83
        port: 5354
      - host: 192.168.124.82
        port: 5354
      - host: 192.168.124.81
        port: 5354
    options:
      host: 192.168.124.83
      port: 54
      api_endpoint: http://192.168.124.83:8081
      api_token: Sfw234sDFw90z</pre></div><p>
     Update the pools using <code class="literal">designate-manage</code> CLI.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>designate-manage pool update --file /etc/designate/pools.crowbar.yaml</pre></div><p>
     Once Designate sync up with PowerDNS, you should see the domains in the
     PowerDNS database which reflects the zones in Designate.
    </p><div id="id-1.4.5.4.7.14.5.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      It make take a few minutes for Designate to sync with PowerDNS.
     </p></div><p>
     We can verify that the domains are successfully sync up with Designate
     by inpsecting the <code class="literal">domains</code> table in the database.
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mysql powerdns
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 21131
Server version: 10.2.29-MariaDB-log SUSE package

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [powerdns]&gt; select * from domains;
+----+---------+--------------------------------------------------------------+------------+-------+-----------------+---------+
| id | name    | master                                                       | last_check | type  | notified_serial | account |
+----+---------+--------------------------------------------------------------+------------+-------+-----------------+---------+
|  1 | foo.bar | 192.168.124.81:5354 192.168.124.82:5354 192.168.124.83:5354  |       NULL | SLAVE |            NULL |         |
+----+---------+--------------------------------------------------------------+------------+-------+-----------------+---------+
1 row in set (0.00 sec)</pre></div></section></section></section><section class="sect1" id="sec-depl-ostack-pacemaker" data-id-title="Deploying Pacemaker (Optional, HA Setup Only)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.2 </span><span class="title-name">Deploying Pacemaker (Optional, HA Setup Only)</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-pacemaker">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   To make the SUSE <span class="productname">OpenStack</span> Cloud controller functions and the Compute Nodes highly
   available, set up one or more clusters by deploying Pacemaker (see
   <a class="xref" href="#sec-depl-req-ha" title="2.6. High Availability">Section 2.6, “High Availability”</a> for details). Since it is possible (and
   recommended) to deploy more than one cluster, a separate proposal needs to
   be created for each cluster.
  </p><p>
   Deploying Pacemaker is optional. In case you do not want to deploy it, skip
   this section and start the node deployment by deploying the database as
   described in <a class="xref" href="#sec-depl-ostack-db" title="12.3. Deploying the Database">Section 12.3, “Deploying the Database”</a>.
  </p><div id="id-1.4.5.4.8.4" data-id-title="Number of Cluster Nodes" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Number of Cluster Nodes</div><p>
    To set up a cluster, at least two nodes are required.  See <a class="xref" href="#sec-depl-reg-ha-general" title="2.6.5. Cluster Requirements and Recommendations">Section 2.6.5, “Cluster Requirements and Recommendations”</a> for
    more information.
   </p></div><p>
   To create a proposal, go to <span class="guimenu">Barclamps</span> › <span class="guimenu">OpenStack</span> and click <span class="guimenu">Edit</span>
   for the Pacemaker barclamp. A drop-down box where you can enter a name and a
   description for the proposal opens. Click <span class="guimenu">Create</span> to open
   the configuration screen for the proposal.
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/depl_barclamp_pacemaker_proposal.png"><img src="images/depl_barclamp_pacemaker_proposal.png" width="75%" alt="Create Pacemaker Proposal" title="Create Pacemaker Proposal"/></a></div></div><div id="ann-depl-ostack-pacemaker-prop-name" data-id-title="Proposal Name" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Proposal Name</div><p>
    The name you enter for the proposal will be used to generate host names for
    the virtual IP addresses of HAProxy. By default, the names follow this
    scheme:
   </p><table style="border: 0; " class="simplelist"><tr><td><code class="literal">cluster-<em class="replaceable">PROPOSAL_NAME</em>.<em class="replaceable">FQDN</em></code>
    (for the internal name)</td></tr><tr><td><code class="literal">public-cluster-<em class="replaceable">PROPOSAL_NAME</em>.<em class="replaceable">FQDN</em></code>
    (for the public name)</td></tr></table><p>
    For example, when <em class="replaceable">PROPOSAL_NAME</em> is set to
    <code class="literal">data</code>, this results in the following names:
   </p><table style="border: 0; " class="simplelist"><tr><td><code class="literal">cluster-data.example.com</code>
    </td></tr><tr><td><code class="literal">public-cluster-data.example.com</code>
    </td></tr></table><p>
    For requirements regarding SSL encryption and certificates, see
    <a class="xref" href="#sec-depl-req-ssl" title="2.3. SSL Encryption">Section 2.3, “SSL Encryption”</a>.
   </p></div><p>
   The following options are configurable in the Pacemaker configuration
   screen:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.8.9.1"><span class="term">Transport for Communication</span></dt><dd><p>
      Choose a technology used for cluster communication. You can choose
      between <span class="guimenu">Multicast (UDP)</span>, sending a message to multiple
      destinations, or <span class="guimenu">Unicast (UDPU)</span>, sending a message to
      a single destination. By default unicast is used.
     </p></dd><dt id="id-1.4.5.4.8.9.2"><span class="term"><span class="guimenu">Policy when cluster does not have quorum</span>
    </span></dt><dd><p>
      Whenever communication fails between one or more nodes and the rest of
      the cluster a <span class="quote">“<span class="quote">cluster partition</span>”</span> occurs. The nodes of a
      cluster are split in partitions but are still active. They can only
      communicate with nodes in the same partition and are unaware of the
      separated nodes. The cluster partition that has the majority of nodes is
      defined to have <span class="quote">“<span class="quote">quorum</span>”</span>.
     </p><p>
      This configuration option defines what to do with the cluster
      partition(s) that do not have the quorum. See
      <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#sec-conf-hawk2-cluster-config" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#sec-conf-hawk2-cluster-config</a>,
      for details.
     </p><p>
      The recommended setting is to choose <span class="guimenu">Stop</span>. However,
      <span class="guimenu">Ignore</span> is enforced for two-node clusters to ensure
      that the remaining node continues to operate normally in case the other
      node fails. For clusters using shared resources, choosing
      <span class="guimenu">freeze</span> may be used to ensure that these resources
      continue to be available.
     </p></dd><dt id="vle-pacemaker-barcl-stonith"><span class="term">STONITH: Configuration mode for STONITH
    </span></dt><dd><p>
      <span class="quote">“<span class="quote">Misbehaving</span>”</span> nodes in a cluster are shut down to prevent
      them from causing trouble. This mechanism is called STONITH
      (<span class="quote">“<span class="quote">Shoot the other node in the head</span>”</span>). STONITH can be
      configured in a variety of ways, refer to
      <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#cha-ha-fencing" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#cha-ha-fencing</a>
      for details. The following configuration options exist:
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.8.9.3.2.2.1"><span class="term"><span class="guimenu">Configured manually</span>
       </span></dt><dd><p>
         STONITH will not be configured when deploying the barclamp. It needs
         to be configured manually as described in
         <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#cha-ha-fencing" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#cha-ha-fencing</a>.
         For experts only.
        </p></dd><dt id="id-1.4.5.4.8.9.3.2.2.2"><span class="term"><span class="guimenu">Configured with IPMI data from the IPMI barclamp</span>
       </span></dt><dd><p>
         Using this option automatically sets up STONITH with data received
         from the IPMI barclamp. Being able to use this option requires that
         IPMI is configured for all cluster nodes. This should be done by
         default. To check or change the IPMI deployment, go to <span class="guimenu">Barclamps</span> › <span class="guimenu">Crowbar</span> › <span class="guimenu">IPMI</span> › <span class="guimenu">Edit</span>. Also
         make sure the <span class="guimenu">Enable BMC</span> option is set to
         <span class="guimenu">true</span> on this barclamp.
        </p><div id="id-1.4.5.4.8.9.3.2.2.2.2.2" data-id-title="STONITH Devices Must Support IPMI" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: STONITH Devices Must Support IPMI</div><p>
          To configure STONITH with the IPMI data, <span class="emphasis"><em>all</em></span>
          STONITH devices must support IPMI. Problems with this setup may
          occur with IPMI implementations that are not strictly standards
          compliant. In this case it is recommended to set up STONITH with
          STONITH block devices (SBD).
         </p></div></dd><dt id="id-1.4.5.4.8.9.3.2.2.3"><span class="term"><span class="guimenu">Configured with STONITH Block Devices (SBD)</span>
       </span></dt><dd><p>
         This option requires manually setting up shared storage and a watchdog
         on the cluster nodes before applying the proposal. To do so, proceed
         as follows:
        </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
           Prepare the shared storage. The path to the shared storage device
           must be persistent and consistent across all nodes in the cluster.
           The SBD device must not use host-based RAID or cLVM2.
          </p></li><li class="listitem"><p>
           Install the package <code class="systemitem">sbd</code> on
           all cluster nodes.
          </p></li><li class="listitem"><p>
           Initialize the SBD device with by running the following command.
           Make sure to replace
           <code class="filename">/dev/<em class="replaceable">SBD</em></code> with the
           path to the shared storage device.
          </p><div class="verbatim-wrap"><pre class="screen">sbd -d /dev/<em class="replaceable">SBD</em> create</pre></div><p>
           Refer to
           <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#sec-ha-storage-protect-test" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#sec-ha-storage-protect-test</a>
           for details.
          </p></li></ol></div><p>
         In <span class="guimenu">Kernel module for watchdog</span>, specify the
         respective kernel module to be used. Find the most commonly used
         watchdog drivers in the following table:
        </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Hardware</th><th style="border-bottom: 1px solid ; ">Driver</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">HP</td><td style="border-bottom: 1px solid ; "><code class="systemitem">hpwdt</code>
            </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Dell, Fujitsu, Lenovo (Intel TCO)</td><td style="border-bottom: 1px solid ; "><code class="systemitem">iTCO_wdt</code>
            </td></tr><tr><td style="border-right: 1px solid ; ">Generic</td><td><code class="systemitem">softdog</code>
            </td></tr></tbody></table></div><p>
         If your hardware is not listed above, either ask your hardware vendor
         for the right name or check the following directory for a list of
         choices:
         <code class="filename">/lib/modules/<em class="replaceable">KERNEL_VERSION</em>/kernel/drivers/watchdog</code>.
        </p><p>
         Alternatively, list the drivers that have been installed with your
         kernel version:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">rpm</code> -ql kernel-<em class="replaceable">VERSION</em> | <code class="command">grep</code> watchdog</pre></div><p>
         If the nodes need different watchdog modules, leave the text box
         empty.
        </p><p>
         After the shared storage has been set up, specify the path using the
         <span class="quote">“<span class="quote">by-id</span>”</span> notation
         (<code class="filename">/dev/disk/by-id/<em class="replaceable">DEVICE</em></code>).
         It is possible to specify multiple paths as a comma-separated list.
        </p><p>
         Deploying the barclamp will automatically complete the SBD setup on the
         cluster nodes by starting the SBD daemon and configuring the fencing
         resource.
        </p></dd><dt id="id-1.4.5.4.8.9.3.2.2.4"><span class="term"><span class="guimenu">
         Configured with one shared resource for the whole cluster
        </span>
       </span></dt><dd><p>
         All nodes will use the identical configuration. Specify the
         <span class="guimenu">Fencing Agent</span> to use and enter
         <span class="guimenu">Parameters</span> for the agent.
        </p><p>
         To get a list of STONITH devices which are supported by the High
         Availability Extension, run the following command on an already
         installed cluster nodes: <code class="command">stonith -L</code>. The list of
         parameters depends on the respective agent. To view a list of
         parameters use the following command:
        </p><div class="verbatim-wrap"><pre class="screen">stonith -t <em class="replaceable">agent</em> -n</pre></div></dd><dt id="id-1.4.5.4.8.9.3.2.2.5"><span class="term"><span class="guimenu">Configured with one resource per node</span>
       </span></dt><dd><p>
         All nodes in the cluster use the same <span class="guimenu">Fencing
         Agent</span>, but can be configured with different parameters. This
         setup is, for example, required when nodes are in different chassis
         and therefore need different IPMI parameters.
        </p><p>
         To get a list of STONITH devices which are supported by the High
         Availability Extension, run the following command on an already
         installed cluster nodes: <code class="command">stonith -L</code>. The list of
         parameters depends on the respective agent. To view a list of
         parameters use the following command:
        </p><div class="verbatim-wrap"><pre class="screen">stonith -t <em class="replaceable">agent</em> -n</pre></div></dd><dt id="id-1.4.5.4.8.9.3.2.2.6"><span class="term"><span class="guimenu">Configured for nodes running in libvirt</span>
       </span></dt><dd><p>
         Use this setting for completely virtualized test installations. This
         option is not supported.
        </p></dd></dl></div></dd><dt id="var-depl-ostack-pacemaker-corosync-fencing"><span class="term">STONITH: Do not start corosync on boot after fencing</span></dt><dd><p>
      With STONITH, Pacemaker clusters with two nodes may sometimes hit an
      issue known as STONITH deathmatch where each node kills the other one,
      resulting in both nodes rebooting all the time. Another similar issue in
      Pacemaker clusters is the fencing loop, where a reboot caused by
      STONITH will not be enough to fix a node and it will be fenced again
      and again.
     </p><p>
      This setting can be used to limit these issues. When set to
      <span class="guimenu">true</span>, a node that has not been properly shut down or
      rebooted will not start the services for Pacemaker on boot. Instead, the
      node will wait for action from the SUSE <span class="productname">OpenStack</span> Cloud operator. When set to
      <span class="guimenu">false</span>, the services for Pacemaker will always be
      started on boot. The <span class="guimenu">Automatic</span> value is used to have
      the most appropriate value automatically picked: it will be
      <span class="guimenu">true</span> for two-node clusters (to avoid STONITH
      deathmatches), and <span class="guimenu">false</span> otherwise.
     </p><p>
      When a node boots but not starts corosync because of this setting, then
      the node's status is in the <span class="guimenu">Node Dashboard</span> is set to
      "<code class="literal">Problem</code>" (red dot).
      
     </p></dd><dt id="id-1.4.5.4.8.9.5"><span class="term">Mail Notifications: Enable Mail Notifications</span></dt><dd><p>
      Get notified of cluster node failures via e-mail. If set to
      <span class="guimenu">true</span>, you need to specify which <span class="guimenu">SMTP
      Server</span> to use, a prefix for the mails' subject and sender and
      recipient addresses. Note that the SMTP server must be accessible by the
      cluster nodes.
     </p></dd><dt id="id-1.4.5.4.8.9.6"><span class="term"><span class="guimenu">HAProxy: Public name for public virtual IP</span>
    </span></dt><dd><p>
      The public name is the host name that will be used instead of the
      generated public name (see
      <a class="xref" href="#ann-depl-ostack-pacemaker-prop-name" title="Important: Proposal Name">Important: Proposal Name</a>) for the public
      virtual IP address of HAProxy. (This is the case when registering
      public endpoints, for example). Any name specified here needs to be
      resolved by a name server placed outside of the SUSE <span class="productname">OpenStack</span> Cloud network.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.8.10"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_pacemaker.png"><img src="images/depl_barclamp_pacemaker.png" width="75%" alt="The Pacemaker Barclamp" title="The Pacemaker Barclamp"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.1: </span><span class="title-name">The Pacemaker Barclamp </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.8.10">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><p>
   The Pacemaker component consists of the following roles. Deploying the
   <span class="guimenu">hawk-server</span> role is optional:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.8.12.1"><span class="term"><span class="guimenu">pacemaker-cluster-member</span>
    </span></dt><dd><p>
      Deploy this role on all nodes that should become member of the cluster.
     </p></dd><dt id="id-1.4.5.4.8.12.2"><span class="term"><span class="guimenu">hawk-server</span>
    </span></dt><dd><p>
      Deploying this role is optional. If deployed, sets up the Hawk Web
      interface which lets you monitor the status of the cluster. The Web
      interface can be accessed via
      <code class="literal">https://<em class="replaceable">IP-ADDRESS</em>:7630</code>. The
      default hawk credentials are username <code class="literal">hacluster</code>, password
      <code class="literal">crowbar</code>.
      </p><p>
      The password is visible and editable in the <span class="guimenu">Custom</span> view of the Pacemaker barclamp, and also in the <code class="literal">"corosync":</code> section of the
      <span class="guimenu">Raw</span> view.
  </p><p>
      Note that the GUI on SUSE <span class="productname">OpenStack</span> Cloud can only be used to monitor the cluster
      status and not to change its configuration.
     </p><p>
      <span class="guimenu">hawk-server</span> may be deployed on at least one cluster
      node. It is recommended to deploy it on all cluster nodes.
     </p></dd><dt id="id-1.4.5.4.8.12.3"><span class="term"><span class="guimenu">pacemaker-remote</span>
    </span></dt><dd><p>
      Deploy this role on all nodes that should become members of the
      Compute Nodes cluster. They will run as Pacemaker remote nodes that are
      controlled by the cluster, but do not affect quorum. Instead of the
      complete cluster stack, only the <code class="literal">pacemaker-remote</code>
      component will be installed on this nodes.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.8.13"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_pacemaker_node_deployment.png"><img src="images/depl_barclamp_pacemaker_node_deployment.png" width="100%" alt="The Pacemaker Barclamp: Node Deployment Example" title="The Pacemaker Barclamp: Node Deployment Example"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.2: </span><span class="title-name">The Pacemaker Barclamp: Node Deployment Example </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.8.13">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><p>
   After a cluster has been successfully deployed, it is listed under
   <span class="guimenu">Available Clusters</span> in the <span class="guimenu">Deployment</span>
   section and can be used for role deployment like a regular node.
  </p><div id="id-1.4.5.4.8.15" data-id-title="Deploying Roles on Single Cluster Nodes" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Deploying Roles on Single Cluster Nodes</div><p>
    When using clusters, roles from other barclamps must never be deployed to
    single nodes that are already part of a cluster. The only exceptions from
    this rule are the following roles:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      cinder-volume
     </p></li><li class="listitem"><p>
      swift-proxy + swift-dispersion
     </p></li><li class="listitem"><p>
      swift-ring-compute
     </p></li><li class="listitem"><p>
      swift-storage
     </p></li></ul></div></div><div id="id-1.4.5.4.8.16" data-id-title="Service Management on the Cluster" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Service Management on the Cluster</div><p>
    After a role has been deployed on a cluster, its services are managed by
    the HA software. You must <span class="emphasis"><em>never</em></span> manually start or stop
    an HA-managed service, nor configure it to start on boot. Services may only
    be started or stopped by using the cluster management tools Hawk or the crm
    shell. See
    <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#sec-ha-config-basics-resources" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#sec-ha-config-basics-resources</a>
    for more information.
   </p></div><div id="id-1.4.5.4.8.17" data-id-title="Testing the Cluster Setup" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Testing the Cluster Setup</div><p>
    To check whether all cluster resources are running, either use the Hawk
    Web interface or run the command <code class="command">crm_mon</code>
    <code class="option">-1r</code>. If it is not the case, clean up the respective
    resource with <code class="command">crm</code> <code class="option">resource</code>
    <code class="option">cleanup</code> <em class="replaceable">RESOURCE</em> , so it gets
    respawned.
   </p><p>
    Also make sure that STONITH correctly works before continuing with the
    SUSE <span class="productname">OpenStack</span> Cloud setup. This is especially important when having chosen a STONITH
    configuration requiring manual setup. To test if STONITH works, log in to
    a node on the cluster and run the following command:
   </p><div class="verbatim-wrap"><pre class="screen">pkill -9 corosync</pre></div><p>
    In case STONITH is correctly configured, the node will reboot.
   </p><p>
    Before testing on a production cluster, plan a maintenance window in case
    issues should arise.
   </p></div></section><section class="sect1" id="sec-depl-ostack-db" data-id-title="Deploying the Database"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.3 </span><span class="title-name">Deploying the Database</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-db">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   The very first service that needs to be deployed is the
   <span class="guimenu">Database</span>. The database component is using MariaDB and
   is used by all other components. It must be installed on a Control Node. The
   Database can be made highly available by deploying it on a cluster.
  </p><p>
   The only attribute you may change is the maximum number of database
   connections (<span class="guimenu">Global Connection Limit</span>). The default value
   should usually work—only change it for large deployments in case the
   log files show database connection failures.
  </p><div class="figure" id="id-1.4.5.4.9.5"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_database.png"><img src="images/depl_barclamp_database.png" width="75%" alt="The Database Barclamp" title="The Database Barclamp"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.3: </span><span class="title-name">The Database Barclamp </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.9.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><section class="sect2" id="sec-depl-ostack-db-mariadb" data-id-title="Deploying MariaDB"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.3.1 </span><span class="title-name">Deploying MariaDB</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-db-mariadb">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Deploying the database requires the use of MariaDB</p><div id="id-1.4.5.4.9.6.3" data-id-title="MariaDB and HA" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: MariaDB and HA</div><p>
        MariaDB back end features full HA support based on the Galera
        clustering technology. The HA setup requires an odd number of
        nodes. The recommended number of nodes is 3.
      </p></div><section class="sect3" id="sec-depl-ostack-db-mariadb-ssl" data-id-title="SSL Configuration"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">12.3.1.1 </span><span class="title-name">SSL Configuration</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-db-mariadb-ssl">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
        SSL can be enabled with either a stand-alone or cluster deployment.
        The replication traffic between database nodes is not encrypted,
        whilst traffic between the database server(s) and clients are, so
        a separate network for the database servers is recommended.
      </p><p>
        Certificates can be provided, or the barcamp can generate self-signed
        certificates. The certificate filenames are configurable in the
        barclamp, and the directories <code class="literal">/etc/mysql/ssl/certs</code>
        and <code class="literal">/etc/mysql/ssl/private</code> to use the defaults will
        need to be created before the barclamp is applied. The CA certificate
        and the certificate for MariaDB to use both go into
        <code class="literal">/etc/mysql/ssl/certs</code>. The appropriate private key
        for the certificate is placed into the
        <code class="literal">/etc/mysql/ssl/private</code> directory. As long as the
        files are readable when the barclamp is deployed, permissions can be
        tightened after a successful deployment once the appropriate UNIX
        groups exist.
      </p><p>
        The Common Name (CN) for the SSL certificate must be <code class="literal">fully
        qualified server name</code> for single host deployments, and
        cluster-<code class="literal">cluster name</code>.<code class="literal">full domain
        name</code> for cluster deployments.
      </p><div id="id-1.4.5.4.9.6.4.5" data-id-title="Certificate validation errors" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Certificate validation errors</div><p>
          If certificate validation errors are causing issues with deploying
          other barclamps (for example, when creating databases or users) you
          can check the configuration with
          <code class="command">mysql --ssl-verify-server-cert</code> which will perform
          the same verification that Crowbar does when connecting to the
          database server.
        </p></div><p>
        If certificates are supplied, the CA certificate and its full trust
        chain must be in the <code class="literal">ca.pem</code> file. The certificate
        must be trusted by the machine (or all cluster members in a cluster
        deployment), and it must be available on all client machines —
        IE, if the <span class="productname">OpenStack</span> services are deployed on separate machines or
        cluster members they will all require the CA certificate to be in
        <code class="literal">/etc/mysql/ssl/certs</code> as well as trusted by the
        machine.
      </p></section><section class="sect3" id="id-1.4.5.4.9.6.5" data-id-title="MariaDB Configuration Options"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">12.3.1.2 </span><span class="title-name">MariaDB Configuration Options</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.4.9.6.5">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="figure" id="id-1.4.5.4.9.6.5.2"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_database_mariadb.png"><img src="images/depl_barclamp_database_mariadb.png" width="75%" alt="MariaDB Configuration" title="MariaDB Configuration"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.4: </span><span class="title-name">MariaDB Configuration </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.9.6.5.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><p>
      The following configuration settings are available via the <span class="guimenu">Database</span> barclamp
      graphical interface:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.9.6.5.4.1"><span class="term">
          Datadir
        </span></dt><dd><p>
            Path to a directory for storing database data.
          </p></dd><dt id="id-1.4.5.4.9.6.5.4.2"><span class="term">
          Maximum Number of Simultaneous Connections
        </span></dt><dd><p>
            The maximum number of simultaneous client connections.
          </p></dd><dt id="id-1.4.5.4.9.6.5.4.3"><span class="term">
          Number of days after the binary logs can be automatically removed
        </span></dt><dd><p>
            A period after which the binary logs are removed.
          </p></dd><dt id="id-1.4.5.4.9.6.5.4.4"><span class="term">
          Slow Query Logging
        </span></dt><dd><p>
            When enabled, all queries that take longer than usual to execute
            are logged to a separate log file (by default, it's
            <code class="filename">/var/log/mysql/mysql_slow.log</code>). This can be
            useful for debugging.
          </p></dd></dl></div><div id="id-1.4.5.4.9.6.5.5" data-id-title="MariaDB Deployment Restriction" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: MariaDB Deployment Restriction</div><p>
        When MariaDB is used as the database back end, the <span class="guimenu">monasca-server</span>
        role cannot be deployed to the node with the
        <span class="guimenu">database-server</span> role. These two roles cannot
        coexist due to the fact that
        monasca uses its own MariaDB instance.
      </p></div></section></section></section><section class="sect1" id="sec-depl-ostack-rabbit" data-id-title="Deploying RabbitMQ"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.4 </span><span class="title-name">Deploying RabbitMQ</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-rabbit">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   The RabbitMQ messaging system enables services to communicate with the other
   nodes via Advanced Message Queue Protocol (AMQP). Deploying it is mandatory.
   RabbitMQ needs to be installed on a Control Node. RabbitMQ can be made highly
   available by deploying it on a cluster. We recommend not changing the
   default values of the proposal's attributes.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.10.3.1"><span class="term"><span class="guimenu">Virtual Host</span>
    </span></dt><dd><p>
      Name of the default virtual host to be created and used by the RabbitMQ
      server (<code class="literal">default_vhost</code> configuration option in
      <code class="filename">rabbitmq.config</code>).
     </p></dd><dt id="id-1.4.5.4.10.3.2"><span class="term">Port</span></dt><dd><p>
      Port the RabbitMQ server listens on (<code class="literal">tcp_listeners</code>
      configuration option in <code class="filename">rabbitmq.config</code>).
     </p></dd><dt id="id-1.4.5.4.10.3.3"><span class="term">User</span></dt><dd><p>
      RabbitMQ default user (<code class="literal">default_user</code> configuration
      option in <code class="filename">rabbitmq.config</code>).
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.10.4"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_rabbitmq.png"><img src="images/depl_barclamp_rabbitmq.png" width="75%" alt="The RabbitMQ Barclamp" title="The RabbitMQ Barclamp"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.5: </span><span class="title-name">The RabbitMQ Barclamp </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.10.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><section class="sect2" id="sec-depl-ostack-rabbit-ha" data-id-title="HA Setup for RabbitMQ"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.4.1 </span><span class="title-name">HA Setup for RabbitMQ</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-rabbit-ha">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    To make RabbitMQ highly available, deploy it on a cluster instead of a
    single Control Node. This also requires shared storage for the cluster that
    hosts the RabbitMQ data. We recommend using a dedicated cluster to deploy
    RabbitMQ together with the database,
    since both components require shared storage.
   </p><p>
    Deploying RabbitMQ on a cluster makes an additional <span class="guimenu">High
    Availability</span> section available in the
    <span class="guimenu">Attributes</span> section of the proposal. Configure the
    <span class="guimenu">Storage Mode</span> in this section.
   </p></section><section class="sect2" id="sec-depl-ostack-rabbitmq-ssl" data-id-title="SSL Configuration for RabbitMQ"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.4.2 </span><span class="title-name">SSL Configuration for RabbitMQ</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-rabbitmq-ssl">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    The RabbitMQ barclamp supports securing traffic via SSL. This is similar to
    the SSL support in other barclamps, but with these differences:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      RabbitMQ can listen on two ports at the same time, typically port 5672
      for unsecured and port 5671 for secured traffic.
     </p></li><li class="listitem"><p>
      The ceilometer pipeline for <span class="productname">OpenStack</span> swift cannot be passed
      SSL-related parameters. When SSL is enabled for RabbitMQ the ceilometer
      pipeline in swift is turned off, rather than sending it over an
      unsecured channel.
     </p></li></ul></div><p>
The following steps are the fastest way to set up and test a new SSL certificate authority (CA).
</p><div class="procedure" id="pro-rabbitmq-test"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
  In the RabbitMQ barclamp set <span class="guimenu">Enable SSL</span> to <span class="guimenu">true</span>, and <span class="guimenu">Generate (self-signed) certificates (implies insecure)</span>
    to <code class="literal">true</code>, then apply the barclamp. The barclamp will create a new CA, enter the correct settings in <code class="filename">/etc/rabbitmq/rabbitmq.config</code>, and start RabbitMQ.
</p></li><li class="step"><p>
Test your new CA with OpenSSL, substituting the hostname of your control node:</p><div class="verbatim-wrap"><pre class="screen">openssl s_client -connect d52-54-00-59-e5-fd:5671
[...]
Verify return code: 18 (self signed certificate)</pre></div><p>
  This outputs a lot of information, including a copy of the server's public certificate, protocols, ciphers, and the chain of trust.
</p></li><li class="step"><p>
      The last step is to configure client services to use SSL to access the
      RabbitMQ service. (See
      <a class="link" href="https://www.rabbitmq.com/ssl.html" target="_blank">https://www.rabbitmq.com/ssl.html</a> for a complete reference).
     </p></li></ol></div></div><p>
    It is preferable to set up your own CA. The best practice is to use a commercial certificate authority. You may also deploy your own self-signed certificates, provided that your cloud is not publicly-accessible, and only for your internal use. Follow these steps to enable your own CA in RabbitMQ and deploy it to SUSE <span class="productname">OpenStack</span> Cloud:
   </p><div class="procedure" id="pro-rabbitmq-production"><div class="procedure-contents"><ul class="procedure"><li class="step"><p>
      Configure the RabbitMQ barclamp to use the control node's
      certificate authority (CA), if it already has one, or create a CA specifically for RabbitMQ and configure the barclamp to use that. (See <a class="xref" href="#sec-depl-req-ssl" title="2.3. SSL Encryption">Section 2.3, “SSL Encryption”</a>, and the RabbitMQ manual has a detailed howto on creating your CA at <a class="link" href="http://www.rabbitmq.com/ssl.html" target="_blank">http://www.rabbitmq.com/ssl.html</a>, with customizations for .NET and Java clients.)
    </p><div class="figure" id="id-1.4.5.4.10.6.7.1.2"><div class="figure-contents"><div class="mediaobject"><a href="images/rabbitmq-ssl-1.png"><img src="images/rabbitmq-ssl-1.png" width="100%" alt="Example RabbitMQ SSL barclamp configuration" title="Example RabbitMQ SSL barclamp configuration"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.6: </span><span class="title-name">SSL Settings for RabbitMQ Barclamp </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.10.6.7.1.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></li></ul></div></div><p>
    The configuration options in the RabbitMQ barclamp allow tailoring the barclamp to your SSL setup.
</p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.10.6.9.1"><span class="term"><span class="guimenu">Enable SSL</span>
    </span></dt><dd><p>
          Set this to <span class="guimenu">True</span> to expose all of your configuration options.
      </p></dd><dt id="id-1.4.5.4.10.6.9.2"><span class="term"><span class="guimenu">SSL Port</span>
    </span></dt><dd><p>
      RabbitMQ's SSL listening port. The default is 5671.
      </p></dd><dt id="id-1.4.5.4.10.6.9.3"><span class="term"><span class="guimenu">Generate (self-signed) certificates (implies insecure)</span>
    </span></dt><dd><p>
    When this is set to <code class="literal">true</code>, self-signed certificates are automatically generated and copied to the correct locations on the control node, and all other barclamp options are set automatically. This is the fastest way to apply and test the barclamp. Do not use this on production systems. When this is set to <code class="literal">false</code> the remaining options are exposed.
      </p></dd><dt id="id-1.4.5.4.10.6.9.4"><span class="term"><span class="guimenu">SSL Certificate File</span>
    </span></dt><dd><p>
     The location of your public root CA certificate.
      </p></dd><dt id="id-1.4.5.4.10.6.9.5"><span class="term"><span class="guimenu">SSL (Private) Key File</span>
    </span></dt><dd><p>
     The location of your private server key.
      </p></dd><dt id="id-1.4.5.4.10.6.9.6"><span class="term"><span class="guimenu">Require Client Certificate</span>
    </span></dt><dd><p>
          This goes with <span class="guimenu">SSL CA Certificates File</span>. Set to <span class="guimenu">true</span> to require clients to present SSL certificates to RabbitMQ.
      </p></dd><dt id="id-1.4.5.4.10.6.9.7"><span class="term"><span class="guimenu">SSL CA Certificates File</span>
    </span></dt><dd><p>
     Trust client certificates presented by the clients that are signed by other CAs. You'll need to store copies of the CA certificates; see "Trust the Client's Root CA" at <a class="link" href="http://www.rabbitmq.com/ssl.html" target="_blank">http://www.rabbitmq.com/ssl.html</a>.
      </p></dd><dt id="id-1.4.5.4.10.6.9.8"><span class="term"><span class="guimenu">SSL Certificate is insecure (for instance, self-signed)</span>
    </span></dt><dd><p>
      When this is set to <span class="guimenu">false</span>, clients validate the RabbitMQ server certificate with the <span class="guimenu">SSL client CA</span> file.
      </p></dd><dt id="id-1.4.5.4.10.6.9.9"><span class="term"><span class="guimenu">SSL client CA file (used to validate rabbitmq server certificate)</span>
    </span></dt><dd><p>
        Tells clients of RabbitMQ where to find the CA bundle that validates the certificate presented by the RabbitMQ server, when <span class="guimenu">SSL Certificate is insecure (for instance, self-signed)</span> is set to <span class="guimenu">false</span>.
      </p></dd></dl></div></section><section class="sect2" id="sec-depl-ostack-rabbitmq-send-notifications" data-id-title="Configuring Clients to Send Notifications"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.4.3 </span><span class="title-name">Configuring Clients to Send Notifications</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-rabbitmq-send-notifications">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    RabbitMQ has an option called <code class="literal">Configure clients to send
    notifications</code>. It defaults to <code class="literal">false</code>, which
    means no events will be sent. It is required to be set to
    <code class="literal">true</code> for ceilometer, monasca, and any other services
    consuming notifications. When it is set to <code class="literal">true</code>,
    <span class="productname">OpenStack</span> services are configured to submit lifecycle audit events to the
    <code class="literal">notification</code> RabbitMQ queue.
   </p><p>
    This option should only be enabled if an active consumer is configured,
    otherwise events will accumulate on the RabbitMQ server, clogging up CPU,
    memory, and disk storage.
   </p><p>
    Any accumulation can be cleared by running:
   </p><div class="verbatim-wrap"><pre class="screen">$ rabbitmqctl -p /openstack purge_queue notifications.info
$ rabbitmqctl -p /openstack purge_queue notifications.error</pre></div></section></section><section class="sect1" id="sec-depl-ostack-keystone" data-id-title="Deploying keystone"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.5 </span><span class="title-name">Deploying keystone</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-keystone">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   keystone is another core component that is used by all
   other <span class="productname">OpenStack</span> components. It provides authentication and authorization
   services. keystone needs to be installed on a
   Control Node. keystone can be made highly available by deploying it on a
   cluster. You can configure the following parameters of this barclamp:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.11.3.1"><span class="term"><span class="guimenu">Algorithm for Token Generation</span>
    </span></dt><dd><p>
      Set the algorithm used by keystone to generate the tokens. You can
      choose between <code class="literal">Fernet</code> (the default) or
      <code class="literal">UUID</code>. Note that for performance and security reasons
      it is strongly recommended to use <code class="literal">Fernet</code>.
     </p></dd><dt id="id-1.4.5.4.11.3.2"><span class="term"><span class="guimenu">Region Name</span>
    </span></dt><dd><p>
      Allows customizing the region name that crowbar is going to manage.
     </p></dd><dt id="id-1.4.5.4.11.3.3"><span class="term"><span class="guimenu">Default Credentials: Default Tenant</span>
    </span></dt><dd><p>
      Tenant for the users. Do not change the default value of
      <code class="literal">openstack</code>.
     </p></dd><dt id="id-1.4.5.4.11.3.4"><span class="term"><span class="guimenu">
      Default Credentials: Administrator User Name/Password
     </span>
    </span></dt><dd><p>
      User name and password for the administrator.
     </p></dd><dt id="id-1.4.5.4.11.3.5"><span class="term"><span class="guimenu">
      Default Credentials: Create Regular User
     </span>
    </span></dt><dd><p>
      Specify whether a regular user should be created automatically. Not
      recommended in most scenarios, especially in an LDAP environment.
     </p></dd><dt id="id-1.4.5.4.11.3.6"><span class="term"><span class="guimenu">
      Default Credentials: Regular User Username/Password
     </span>
    </span></dt><dd><p>
      User name and password for the regular user. Both the regular user and
      the administrator accounts can be used to log in to the SUSE <span class="productname">OpenStack</span> Cloud Dashboard.
      However, only the administrator can manage keystone users and access.
     </p><div class="figure" id="id-1.4.5.4.11.3.6.2.2"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_keystone.png"><img src="images/depl_barclamp_keystone.png" width="75%" alt="The keystone Barclamp" title="The keystone Barclamp"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.7: </span><span class="title-name">The keystone Barclamp </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.11.3.6.2.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></dd><dt id="sec-depl-ostack-keystone-ssl"><span class="term">SSL Support: Protocol
    </span></dt><dd><p>
      When you use the default value <span class="guimenu">HTTP</span>, public
      communication will not be encrypted. Choose <span class="guimenu">HTTPS</span> to
      use SSL for encryption. See <a class="xref" href="#sec-depl-req-ssl" title="2.3. SSL Encryption">Section 2.3, “SSL Encryption”</a> for
      background information and <a class="xref" href="#sec-depl-inst-nodes-post-ssl" title="11.4.6. Enabling SSL">Section 11.4.6, “Enabling SSL”</a>
      for installation instructions. The following additional configuration
      options will become available when choosing <span class="guimenu">HTTPS</span>:
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.11.3.7.2.2.1"><span class="term"><span class="guimenu">Generate (self-signed) certificates</span>
       </span></dt><dd><p>
         When set to <code class="literal">true</code>, self-signed certificates are
         automatically generated and copied to the correct locations. This
         setting is for testing purposes only and should never be used in
         production environments!
        </p></dd><dt id="id-1.4.5.4.11.3.7.2.2.2"><span class="term"><span class="guimenu">SSL Certificate File</span> / <span class="guimenu">SSL (Private) Key
        File</span>
       </span></dt><dd><p>
         Location of the certificate key pair files.
        </p></dd><dt id="id-1.4.5.4.11.3.7.2.2.3"><span class="term"><span class="guimenu">SSL Certificate is insecure</span>
       </span></dt><dd><p>
         Set this option to <code class="literal">true</code> when using self-signed
         certificates to disable certificate checks. This setting is for
         testing purposes only and should never be used in production
         environments!
        </p></dd><dt id="id-1.4.5.4.11.3.7.2.2.4"><span class="term"><span class="guimenu">SSL CA Certificates File</span>
       </span></dt><dd><p>
         Specify the absolute path to the CA certificate. This field is
         mandatory, and leaving it blank will cause the barclamp to fail. To fix
         this issue, you have to provide the absolute path to the CA
         certificate, restart the <code class="systemitem">apache2</code> service, and
         re-deploy the barclamp.
        </p><p>
         When the certificate is not already trusted by the pre-installed list
         of trusted root certificate authorities, you need to provide a
         certificate bundle that includes the root and all intermediate CAs.
        </p><div class="figure" id="id-1.4.5.4.11.3.7.2.2.4.2.3"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_ssl.png"><img src="images/depl_barclamp_ssl.png" width="75%" alt="The SSL Dialog" title="The SSL Dialog"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.8: </span><span class="title-name">The SSL Dialog </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.11.3.7.2.2.4.2.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></dd></dl></div></dd></dl></div><section class="sect2" id="sec-depl-ostack-keystone-ldap" data-id-title="Authenticating with LDAP"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.5.1 </span><span class="title-name">Authenticating with LDAP</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-keystone-ldap">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
  keystone has the ability to separate
  identity back-ends by domains. SUSE <span class="productname">OpenStack</span> Cloud 9 uses this method for
  authenticating users.
 </p><p>
  The keystone barclamp sets up a MariaDB database by default. Configuring
  an LDAP back-end is done in the <span class="guimenu">Raw</span> view.
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
             Set <span class="guimenu">"domain_specific_drivers": true,</span>
         </p></li><li class="step"><p>
             Then in the <span class="guimenu">"domain_specific_config":</span> section
             configure a map with domain names as keys, and configuration as
             values. In the default proposal the domain name key is
             <span class="guimenu">"ldap_users"</span>, and the keys are the two required
             sections for an LDAP-based identity driver configuration, the <span class="guimenu">[identity]</span> section which sets the driver, and the <span class="guimenu">[ldap]</span> section which sets the LDAP connection options. You may configure multiple domains, each with its own configuration.
             </p></li></ol></div></div><p>
     You may make this available to horizon by setting <span class="guimenu">multi_domain_support</span> to <span class="guimenu">true</span> in the horizon barclamp.
 </p><p>
Users in the LDAP-backed domain have to know the name of the domain in order to authenticate, and must use the  keystone v3 API endpoint. (See the <span class="productname">OpenStack</span> manuals, <a class="link" href="https://docs.openstack.org/keystone/rocky/admin/identity-domain-specific-config.html" target="_blank">Domain-specific Configuration</a> and <a class="link" href="https://docs.openstack.org/keystone/rocky/admin/identity-integrate-with-ldap.html" target="_blank">Integrate Identity with LDAP</a>, for additional details.)
 </p></section><section class="sect2" id="sec-depl-ostack-keystone-ha" data-id-title="HA Setup for keystone"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.5.2 </span><span class="title-name">HA Setup for keystone</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-keystone-ha">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    Making keystone highly available requires no special
    configuration—it is sufficient to deploy it on a cluster.
   </p></section><section class="sect2" id="sec-depl-ostack-keystone-openidc" data-id-title="OpenID Connect Setup for keystone"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.5.3 </span><span class="title-name">OpenID Connect Setup for keystone</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-keystone-openidc">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    keystone supports WebSSO by federating with an external identity
    provider (IdP) using  <a class="link" href="https://github.com/zmartzone/mod_auth_openidc" target="_blank">auth_openidc</a> module.
   </p><p>
    There are two steps to enable this feature:
   </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
       Configure the <code class="literal">"federation"</code> and <code class="literal">"openidc"</code> attributes for the Keystone Barclamp in Crowbar.
      </p></li><li class="listitem"><p>
       Create the Identity Provider, Protocol, and Mapping resource in
       keystone using <span class="guimenu">OpenStack Command Line Tool (CLI)</span>.
      </p></li></ol></div><section class="sect3" id="sec-depl-ostack-keystone-openidc-crowbar" data-id-title="keystone Barclamp Configuration"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">12.5.3.1 </span><span class="title-name">keystone Barclamp Configuration</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-keystone-openidc-crowbar">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    Configurating OpenID Connect is done in the <span class="guimenu">Raw</span> view,
    under the <span class="guimenu">federation</span> section. The global attributes,
    namely <span class="guimenu">trusted_dashboards</span> and <span class="guimenu">websso_keystone_url</span>
    are not specific to OpenID Connect. Rather, they are designed to help
    facilitate WebSSO browser redirects with external IdPs in a complex cloud
    deployment environment.
   </p><p>
    If the cloud deployment does not have any external proxies or load
    balancers, where the public keystone and horizon (Dashboard service)
    endpoints are directly managed by Crowbar, <span class="guimenu">trusted_dashboards</span>
    and <span class="guimenu">websso_keystone_url</span> does not need to be provided.
    However, in a complex cloud deployment where the public Keystone and
    Horizon endpoints are handled by external load balancers or proxies, and
    they are not directly managed by Crowbar, <span class="guimenu">trusted_dashboards</span>
    and <span class="guimenu">websso_keystone_url</span> must be provided and they must
    correctly reflect the external public endpoints.
   </p><p>
    To configure OpenID Connect, edit the attributes in the <span class="guimenu">openidc</span> subsection.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Set <span class="guimenu">"enabled":true</span>
     </p></li><li class="step"><p>
      Provide the name for the <span class="guimenu">identity_provider</span>.
      This must be the same as the identity provider to be created in Keystone
      using the <span class="guimenu">OpenStack Command Line Tool (CLI)</span>.
      For example, if the identity provider is <code class="literal">foo</code>,
      create the identity provider with the name via Openstack CLI (i.e.
      <span class="guimenu">openstack identity provider create foo</span>).
     </p></li><li class="step"><p>
      <span class="guimenu">response_type</span> corresponds to
      <a class="link" href="https://github.com/zmartzone/mod_auth_openidc/blob/master/auth_openidc.conf" target="_blank">auth_openidc OIDCResponseType</a>.
      In most cases, it should be <code class="literal">id_token</code>.
     </p></li><li class="step"><p>
      <span class="guimenu">scope</span> corresponds to
      <a class="link" href="https://github.com/zmartzone/mod_auth_openidc/blob/master/auth_openidc.conf" target="_blank">auth_openidc OIDCScope</a>.
     </p></li><li class="step"><p>
      <span class="guimenu">metadata_url</span> corresponds to
      <a class="link" href="https://github.com/zmartzone/mod_auth_openidc/blob/master/auth_openidc.conf" target="_blank">auth_openidc OIDCProviderMetadataURL</a>.
     </p></li><li class="step"><p>
      <span class="guimenu">client_id</span> corresponds to
      <a class="link" href="https://github.com/zmartzone/mod_auth_openidc/blob/master/auth_openidc.conf" target="_blank">auth_openidc OIDCClientID</a>.
     </p></li><li class="step"><p>
      <span class="guimenu">client_secret</span> corresponds to
      <a class="link" href="https://github.com/zmartzone/mod_auth_openidc/blob/master/auth_openidc.conf" target="_blank">auth_openidc OIDCClientSecret</a>.
     </p></li><li class="step"><p>
      <span class="guimenu">redirect_uri</span> corresponds to
      <a class="link" href="https://github.com/zmartzone/mod_auth_openidc/blob/master/auth_openidc.conf" target="_blank">auth_openidc OIDCRedirectURI</a>.
      In a cloud deployment where all the external endpoints are directly
      managed by Crowbar, this attribute can be left blank as it will be
      auto-populated by Crowbar. However, in a complex cloud deployment where
      the public Keystone endpoint is handled by an external load balancer
      or proxy, this attribute must reflect the external Keystone auth endpoint
      for the OpenID Connect IdP. For example,
      <code class="literal">"https://keystone-public-endpoint.foo.com/v3/OS-FEDERATION/identity_providers/foo/protocols/openid/auth"</code>
     </p><div id="id-1.4.5.4.11.6.5.5.8.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
       Some OpenID Connect IdPs such as Google require the hostname in the
       <span class="guimenu">redirect_uri</span> to be a public FQDN. In that case,
       the hostname in Keystone public endpoint must also be a public FQDN
       and must match the one specified in the <span class="guimenu">redirect_uri</span>.
      </p></div></li></ol></div></div></section><section class="sect3" id="sec-depl-ostack-keystone-openidc-cli" data-id-title="Create Identity Provider, Protocol, and Mapping"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">12.5.3.2 </span><span class="title-name">Create Identity Provider, Protocol, and Mapping</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-keystone-openidc-cli">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    To fully enable OpenID Connect, the <code class="literal">Identity Provider</code>,
    <code class="literal">Protocol</code>, and <code class="literal">Mapping</code> for the given
    IdP must be created in Keystone. This is done by using the
    <span class="guimenu">OpenStack Command Line Tool</span>, on a controller node, and
    using the Keystone <span class="bold"><strong>admin</strong></span> credential.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Login to a controller node as <code class="literal">root</code> user.
     </p></li><li class="step"><p>
      Use the Keystone <span class="bold"><strong>admin</strong></span> credential.
     </p><div class="verbatim-wrap"><pre class="screen">source ~/.openrc</pre></div></li><li class="step"><p>
      Create the <span class="bold"><strong>Identity Provider</strong></span>. For example:
     </p><div class="verbatim-wrap"><pre class="screen">openstack identity provider create foo</pre></div><div id="id-1.4.5.4.11.6.6.3.3.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
       The name of the Identity Provider must be exactly the same as the
        <span class="guimenu">identity_provider</span> attribute given when configuring
        Keystone in the previous section.
      </p></div></li><li class="step"><p>
      Next, create the Mapping for the Identity Provider. Prior to creating the
      Mapping, one must fully grasp the intricacies
      of <a class="link" href="https://docs.openstack.org/keystone/latest/admin/federation/mapping_combinations.html" target="_blank">Mapping Combinations</a>
      as it may have profound security implications if done incorrectly.
      Here's an example of a mapping file.
     </p><div class="verbatim-wrap"><pre class="screen">[
    {
        "local": [
            {
                "user": {
                    "name": "{0}",
                    "email": "{1}",
                    "type": "ephemeral"
                 },
                 "group": {
                    "domain": {
                        "name": "Default"
                    },
                    "name": "openidc_demo"
                }
             }
         ],
         "remote": [
             {
                 "type": "REMOTE_USER"
             },
             {
                 "type": "HTTP_OIDC_EMAIL"
             }

        ]
    }
]</pre></div><p>
      Once the mapping file is created, now create the mapping resource in Keystone. For example:
     </p><div class="verbatim-wrap"><pre class="screen">openstack mapping create --rule oidc_mapping.json oidc_mapping</pre></div></li><li class="step"><p>
      Lastly, create the Protocol for the Identity Provider and its mapping.
      For OpenID Connect, the protocol name must be <span class="bold"><strong>openid</strong></span>. For example:
     </p><div class="verbatim-wrap"><pre class="screen">openstack federation protocol create --identity-provider google --mapping oidc_mapping openid</pre></div></li></ol></div></div></section></section></section><section class="sect1" id="sec-depl-ostack-monasca" data-id-title="Deploying monasca (Optional)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.6 </span><span class="title-name">Deploying monasca (Optional)</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-monasca">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   monasca is an open-source monitoring-as-a-service solution that
   integrates with <span class="productname">OpenStack</span>. monasca is designed for scalability, high
   performance, and fault tolerance.
  </p><p>
   Accessing the <span class="guimenu">Raw</span> interface is not required for
   day-to-day operation. But as not all monasca settings are exposed in the
   barclamp graphical interface (for example, various performance tuneables), it
   is recommended to configure monasca in the <span class="guimenu">Raw</span>
   mode. Below are the options that can be configured via the
   <span class="guimenu">Raw</span> interface of the monasca barclamp.
  </p><div class="figure" id="id-1.4.5.4.12.4"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_monasca_raw.png"><img src="images/depl_barclamp_monasca_raw.png" width="75%" alt="The monasca barclamp Raw Mode" title="The monasca barclamp Raw Mode"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.9: </span><span class="title-name">The monasca barclamp Raw Mode </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.12.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><div class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.12.5"><span class="name"><span class="guimenu">agent: settings for openstack-monasca-agent</span>
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.12.5">#</a></h4></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.12.6.1"><span class="term">keystone</span></dt><dd><p>
      Contains keystone credentials that the agents use to send metrics. Do
      not change these options, as they are configured by Crowbar.
     </p></dd><dt id="id-1.4.5.4.12.6.2"><span class="term">insecure</span></dt><dd><p>
      Specifies whether SSL certificates are verified when communicating with
      keystone. If set to <code class="literal">false</code>, the
      <code class="literal">ca_file</code> option must be specified.
     </p></dd><dt id="id-1.4.5.4.12.6.3"><span class="term">ca_file</span></dt><dd><p>
      Specifies the location of a CA certificate that is used for verifying
      keystone's SSL certificate.
     </p></dd><dt id="id-1.4.5.4.12.6.4"><span class="term">log_dir</span></dt><dd><p>
      Path for storing log files. The specified path must exist. Do not change
      the default <code class="filename">/var/log/monasca-agent</code> path.
     </p></dd><dt id="id-1.4.5.4.12.6.5"><span class="term">log_level</span></dt><dd><p>
      Agent's log level. Limits log messages to the specified level and above.
      The following levels are available: Error, Warning, Info (default), and
      Debug.
     </p></dd><dt id="id-1.4.5.4.12.6.6"><span class="term">check_frequency</span></dt><dd><p>
      Interval in seconds between running agents' checks.
     </p></dd><dt id="id-1.4.5.4.12.6.7"><span class="term">num_collector_threads</span></dt><dd><p>
      Number of simultaneous collector threads to run. This refers to the
      maximum number of different collector plug-ins (for example,
      <code class="literal">http_check</code>) that are allowed to run simultaneously. The
      default value <code class="literal">1</code> means that plug-ins are run
      sequentially.
     </p></dd><dt id="id-1.4.5.4.12.6.8"><span class="term">pool_full_max_retries</span></dt><dd><p>
      If a problem with the results from multiple plug-ins results blocks the
      entire thread pool (as specified by the
      <code class="systemitem">num_collector_threads</code> parameter), the collector
      exits, so it can be restarted by the
      <code class="systemitem">supervisord</code>. The parameter
      <code class="systemitem">pool_full_max_retries</code> specifies when this event
      occurs. The collector exits when the defined number of consecutive
      collection cycles have ended with the thread pool completely full.
     </p></dd><dt id="id-1.4.5.4.12.6.9"><span class="term">plugin_collect_time_warn</span></dt><dd><p>
      Upper limit in seconds for any collection plug-in's run time. A warning
      is logged if a plug-in runs longer than the specified limit.
     </p></dd><dt id="id-1.4.5.4.12.6.10"><span class="term">max_measurement_buffer_size</span></dt><dd><p>
      Maximum number of measurements to buffer locally if the monasca API
      is unreachable. Measurements will be dropped in batches, if the API is
      still unreachable after the specified number of messages are buffered.
      The default <code class="literal">-1</code> value indicates unlimited buffering.
      Note that a large buffer increases the agent's memory usage.
     </p></dd><dt id="id-1.4.5.4.12.6.11"><span class="term">backlog_send_rate</span></dt><dd><p>
      Maximum number of measurements to send when the local measurement buffer
      is flushed.
     </p></dd><dt id="id-1.4.5.4.12.6.12"><span class="term">amplifier</span></dt><dd><p>
      Number of extra dimensions to add to metrics sent to the monasca API.
      This option is intended for load testing purposes only. Do not enable the
      option in production! The default <code class="literal">0</code> value disables the
      addition of dimensions.
     </p></dd></dl></div><div class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.12.7"><span class="name"><span class="guimenu">log_agent: settings for openstack-monasca-log-agent</span>
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.12.7">#</a></h4></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.12.8.1"><span class="term">max_data_size_kb</span></dt><dd><p>
      Maximum payload size in kilobytes for a request sent to the monasca
      log API.
     </p></dd><dt id="id-1.4.5.4.12.8.2"><span class="term">num_of_logs</span></dt><dd><p>
      Maximum number of log entries the log agent sends to the monasca log
      API in a single request. Reducing the number increases performance.
     </p></dd><dt id="id-1.4.5.4.12.8.3"><span class="term">elapsed_time_sec</span></dt><dd><p>
      Time interval in seconds between sending logs to the monasca log API.
     </p></dd><dt id="id-1.4.5.4.12.8.4"><span class="term">delay</span></dt><dd><p>
      Interval in seconds for checking whether
      <code class="literal">elapsed_time_sec</code> has been reached.
     </p></dd><dt id="id-1.4.5.4.12.8.5"><span class="term">keystone</span></dt><dd><p>
      keystone credentials the log agents use to send logs to the monasca
      log API. Do not change this option manually, as it is configured by
      Crowbar.
     </p></dd></dl></div><div class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.12.9"><span class="name"><span class="guimenu">api: Settings for openstack-monasca-api</span>
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.12.9">#</a></h4></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.12.10.1"><span class="term">bind_host</span></dt><dd><p>
      Interfaces <code class="literal">monasca-api</code> listens on. Do not change this
      option, as it is configured by Crowbar.
     </p></dd><dt id="id-1.4.5.4.12.10.2"><span class="term">processes</span></dt><dd><p>
      Number of processes to spawn.
     </p></dd><dt id="id-1.4.5.4.12.10.3"><span class="term">threads</span></dt><dd><p>
      Number of WSGI worker threads to spawn.
     </p></dd><dt id="id-1.4.5.4.12.10.4"><span class="term">log_level</span></dt><dd><p>
      Log level for <code class="systemitem">openstack-monasca-api</code>. Limits log
      messages to the specified level and above. The following levels are
      available: Critical, Error, Warning, Info (default), Debug, and Trace.
     </p></dd></dl></div><div class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.12.11"><span class="name"><span class="guimenu">elasticsearch: server-side settings for elasticsearch</span>
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.12.11">#</a></h4></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.12.12.1"><span class="term">repo_dir</span></dt><dd><p>
      List of directories for storing Elasticsearch snapshots. Must be created
      manually and be writeable by the
      <code class="systemitem">elasticsearch</code> user.
      Must contain at least one entry in order for the snapshot functionality
      to work.
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.12.12.1.2.2.1"><span class="term">heap_size</span></dt><dd><p>
         Sets the heap size. We recommend setting heap size at 50% of the
         available memory, but not more than 31 GB. The default of 4 GB is
         likely too small and should be increased if possible.
        </p></dd><dt id="id-1.4.5.4.12.12.1.2.2.2"><span class="term">limit_memlock</span></dt><dd><p>
         The maximum size that may be locked into memory in bytes
        </p></dd><dt id="id-1.4.5.4.12.12.1.2.2.3"><span class="term">limit_nofile</span></dt><dd><p>
         The maximum number of open file descriptors
        </p></dd><dt id="id-1.4.5.4.12.12.1.2.2.4"><span class="term">limit_nproc</span></dt><dd><p>
         The maximum number of processes
        </p></dd><dt id="id-1.4.5.4.12.12.1.2.2.5"><span class="term">vm_max_map_count</span></dt><dd><p>
         The maximum number of memory map areas a process may have.
        </p></dd></dl></div></dd></dl></div><p>
   For instructions on creating an Elasticsearch snapshot, see
   <span class="intraxref">Book “<em class="citetitle">Operations Guide Crowbar</em>”, Chapter 4 “SUSE <span class="productname">OpenStack</span> Cloud Monitoring”, Section 4.7 “Operation and Maintenance”, Section 4.7.4 “Backup and Recovery”</span>.
  </p><div class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.12.14"><span class="name"><span class="guimenu">elasticsearch_curator: settings for
    elastisearch-curator</span>
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.12.14">#</a></h4></div><p>
   <code class="systemitem">elasticsearch-curator</code> removes old and large
   elasticsearch indices. The settings below determine its behavior.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.12.16.1"><span class="term">delete_after_days</span></dt><dd><p>
      Time threshold for deleting indices. Indices older the specified number
      of days are deleted. This parameter is unset by default, so indices are
      kept indefinitely.
     </p></dd><dt id="id-1.4.5.4.12.16.2"><span class="term">delete_after_size</span></dt><dd><p>
      Maximum size in megabytes of indices. Indices larger than the specified
      size are deleted. This parameter is unset by default, so indices are kept
      irrespective of their size.
     </p></dd><dt id="id-1.4.5.4.12.16.3"><span class="term">delete_exclude_index</span></dt><dd><p>
      List of indices to exclude from
      <code class="systemitem">elasticsearch-curator</code> runs. By default, only the
      <code class="filename">.kibana</code> files are excluded.
     </p></dd></dl></div><div class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.12.17"><span class="name"><span class="guimenu">kafka: tunables for
Kafka</span>
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.12.17">#</a></h4></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.12.18.1"><span class="term">log_retention_hours</span></dt><dd><p>
      Number of hours for retaining log segments in Kafka's on-disk log.
      Messages older than the specified value are dropped.
     </p></dd><dt id="id-1.4.5.4.12.18.2"><span class="term">log_retention_bytes</span></dt><dd><p>
      Maximum size for Kafka's on-disk log in bytes. If the log grows beyond
      this size, the oldest log segments are dropped.
     </p></dd><dt id="id-1.4.5.4.12.18.3"><span class="term">topics</span></dt><dd><p>
      list of topics
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        metrics
       </p></li><li class="listitem"><p>
        events
       </p></li><li class="listitem"><p>
        alarm-state-transitions
       </p></li><li class="listitem"><p>
        alarm-notifications
       </p></li><li class="listitem"><p>
        retry-notifications
       </p></li><li class="listitem"><p>
        60-seconds-notifications
       </p></li><li class="listitem"><p>
        log
       </p></li><li class="listitem"><p>
        transformed-log
       </p></li></ul></div><p>
      The following are options of every topic:
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.12.18.3.2.4.1"><span class="term">replicas</span></dt><dd><p>
         Controls how many servers replicate each message that is written
        </p></dd><dt id="id-1.4.5.4.12.18.3.2.4.2"><span class="term">partitions</span></dt><dd><p>
         Controls how many logs the topic is sharded into
        </p></dd><dt id="id-1.4.5.4.12.18.3.2.4.3"><span class="term">config_options</span></dt><dd><p>
         Map of configuration options is described in the <a class="link" href="https://kafka.apache.org/documentation/#topicconfigs" target="_blank">Apache
         Kafka documentation</a>
        </p></dd></dl></div></dd></dl></div><p>
   These parameters only affect first time installations. Parameters may be
   changed after installation with scripts available from <a class="link" href="https://kafka.apache.org/documentation/#basic_ops" target="_blank">Apache Kafka</a>.
  </p><p>
   Kafka does not support reducing the number of partitions for a topic.
  </p><div class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.12.21"><span class="name"><span class="guimenu">notification:</span>
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.12.21">#</a></h4></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.12.22.1"><span class="term">email_enabled</span></dt><dd><p>
      Enable or disable email alarm notifications.
     </p></dd><dt id="id-1.4.5.4.12.22.2"><span class="term">email_smtp_host</span></dt><dd><p>
      SMTP smarthost for sending alarm notifications.
     </p></dd><dt id="id-1.4.5.4.12.22.3"><span class="term">email_smtp_port</span></dt><dd><p>
      Port for the SMTP smarthost.
     </p></dd><dt id="id-1.4.5.4.12.22.4"><span class="term">email_smtp_user</span></dt><dd><p>
      User name for authenticating against the smarthost.
     </p></dd><dt id="id-1.4.5.4.12.22.5"><span class="term">email_smtp_password</span></dt><dd><p>
      Password for authenticating against the smarthost.
     </p></dd><dt id="id-1.4.5.4.12.22.6"><span class="term">email_smtp_from_address</span></dt><dd><p>
      Sender address for alarm notifications.
     </p></dd></dl></div><div class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.12.23"><span class="name"><span class="guimenu">master: configuration for
monasca-installer on the Crowbar node</span>
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.12.23">#</a></h4></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.12.24.1"><span class="term">influxdb_retention_policy</span></dt><dd><p>
      Number of days to keep metrics records in influxdb.
     </p><p>
      For an overview of all supported values, see
      <a class="link" href="https://docs.influxdata.com/influxdb/v1.1/query_language/database_management/#create-retention-policies-with-create-retention-policy" target="_blank">https://docs.influxdata.com/influxdb/v1.1/query_language/database_management/#create-retention-policies-with-create-retention-policy</a>.
     </p></dd></dl></div><div class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.12.25"><span class="name"><span class="guimenu">monasca: settings for libvirt and Ceph
monitoring</span>
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.12.25">#</a></h4></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.12.26.1"><span class="term">
     monitor_libvirt
    </span></dt><dd><p>
      The global switch for toggling libvirt monitoring. If set to
      <em class="parameter">true</em>, libvirt metrics will be gathered on all
      libvirt based Compute Nodes. This setting is available in the Crowbar UI.
     </p></dd><dt id="id-1.4.5.4.12.26.2"><span class="term">
     monitor_ceph
    </span></dt><dd><p>
      The global switch for toggling Ceph monitoring. If set to
      <em class="parameter">true</em>, Ceph metrics will be gathered on all
      Ceph-based Compute Nodes. This setting is available in Crowbar UI. If the
      Ceph cluster has been set up independently, Crowbar ignores this setting.
     </p></dd><dt id="id-1.4.5.4.12.26.3"><span class="term">
     cache_dir
    </span></dt><dd><p>
      The directory where monasca-agent will locally cache various metadata
      about locally running VMs on each Compute Node.
     </p></dd><dt id="id-1.4.5.4.12.26.4"><span class="term">
     customer_metadata
    </span></dt><dd><p>
      Specifies the list of instance metadata keys to be included as dimensions
      with customer metrics. This is useful for providing more information
      about an instance.
     </p></dd><dt id="id-1.4.5.4.12.26.5"><span class="term">
     disk_collection_period
    </span></dt><dd><p>
      Specifies a minimum interval in seconds for collecting disk metrics.
      Increase this value to reduce I/O load. If the value is lower than the
      global agent collection period (<code class="option">check_frequency</code>), it
      will be ignored in favor of the global collection period.
     </p></dd><dt id="id-1.4.5.4.12.26.6"><span class="term">
     max_ping_concurrency
    </span></dt><dd><p>
      Specifies the number of ping command processes to run concurrently when
      determining whether the VM is reachable. This should be set to a value
      that allows the plugin to finish within the agent's collection period,
      even if there is a networking issue. For example, if the expected number
      of VMs per Compute Node is 40 and each VM has one IP address, then the
      plugin will take at least 40 seconds to do the ping checks in the
      worst-case scenario where all pings fail (assuming the default timeout of
      1 second). Increasing <code class="option">max_ping_concurrency</code> allows the
      plugin to finish faster.
     </p></dd><dt id="id-1.4.5.4.12.26.7"><span class="term">
     metadata
    </span></dt><dd><p>
      Specifies the list of nova side instance metadata keys to be included
      as dimensions with the cross-tenant metrics for the
      <span class="guimenu">monasca</span> project. This is useful for providing more
      information about an instance.
     </p></dd><dt id="id-1.4.5.4.12.26.8"><span class="term">
     nova_refresh
    </span></dt><dd><p>
      Specifies the number of seconds between calls to the nova API to
      refresh the instance cache. This is helpful for updating VM hostname and
      pruning deleted instances from the cache. By default, it is set to 14,400
      seconds (four hours). Set to 0 to refresh every time the Collector runs,
      or to <em class="parameter">None</em> to disable regular refreshes entirely.
      In this case, the instance cache will only be refreshed when a new
      instance is detected.
     </p></dd><dt id="id-1.4.5.4.12.26.9"><span class="term">
     ping_check
    </span></dt><dd><p>
      Includes the entire ping command (without the IP address, which is
      automatically appended) to perform a ping check against instances. The
      <code class="literal">NAMESPACE</code> keyword is automatically replaced with the
      appropriate network namespace for the VM being monitored. Set to
      <em class="parameter">False</em> to disable ping checks.
     </p></dd><dt id="id-1.4.5.4.12.26.10"><span class="term">
     vnic_collection_period
    </span></dt><dd><p>
      Specifies a minimum interval in seconds for collecting disk metrics.
      Increase this value to reduce I/O load. If the value is lower than the
      global agent collection period (<code class="option">check_frequency</code>), it
      will be ignored in favor of the global collection period.
     </p></dd><dt id="id-1.4.5.4.12.26.11"><span class="term">
     vm_cpu_check_enable
    </span></dt><dd><p>
      Toggles the collection of VM CPU metrics. Set to
      <em class="parameter">true</em> to enable.
     </p></dd><dt id="id-1.4.5.4.12.26.12"><span class="term">
     vm_disks_check_enable
    </span></dt><dd><p>
      Toggles the collection of VM disk metrics. Set to
      <em class="parameter">true</em> to enable.
     </p></dd><dt id="id-1.4.5.4.12.26.13"><span class="term">
     vm_extended_disks_check_enable
    </span></dt><dd><p>
      Toggles the collection of extended disk metrics. Set to
      <em class="parameter">true</em> to enable.
     </p></dd><dt id="id-1.4.5.4.12.26.14"><span class="term">
     vm_network_check_enable
    </span></dt><dd><p>
      Toggles the collection of VM network metrics. Set to
      <em class="parameter">true</em> to enable.
     </p></dd><dt id="id-1.4.5.4.12.26.15"><span class="term">
     vm_ping_check_enable
    </span></dt><dd><p>
      Toggles ping checks for checking whether a host is alive. Set to
      <em class="parameter">true</em> to enable.
     </p></dd><dt id="id-1.4.5.4.12.26.16"><span class="term">
     vm_probation
    </span></dt><dd><p>
      Specifies a period of time (in seconds) in which to suspend metrics from
      a newly-created VM. This is to prevent quickly-obsolete metrics in an
      environment with a high amount of instance churn (VMs created and
      destroyed in rapid succession). The default probation length is 300
      seconds (5 minutes). Set to 0 to disable VM probation. In this case,
      metrics are recorded immediately after a VM is created.
     </p></dd><dt id="id-1.4.5.4.12.26.17"><span class="term">
     vnic_collection_period
    </span></dt><dd><p>
      Specifies a minimum interval in seconds for collecting VM network
      metrics. Increase this value to reduce I/O load. If the value is lower
      than the global agent collection period
      (<em class="parameter">check_frequency</em>), it will be ignored in favor of
      the global collection period.
     </p></dd></dl></div><div class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.12.27"><span class="name"><span class="guimenu">Deployment</span>
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.12.27">#</a></h4></div><p>
   The monasca component consists of following roles:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.12.29.1"><span class="term">monasca-server</span></dt><dd><p>
      monasca server-side components that are deployed by Chef.
      Currently, this only creates keystone resources required by monasca,
      such as users, roles, endpoints, etc. The rest is left to the
      Ansible-based <code class="systemitem">monasca-installer</code> run by the
      <code class="systemitem">monasca-master</code> role.
     </p></dd><dt id="id-1.4.5.4.12.29.2"><span class="term">monasca-master</span></dt><dd><p>
      Runs the Ansible-based <code class="systemitem">monasca-installer</code> from
      the Crowbar node. The installer deploys the monasca server-side
      components to the node that has the
      <code class="systemitem">monasca-server</code> role assigned to it. These
      components are <code class="systemitem">openstack-monasca-api</code>, and
      <code class="systemitem">openstack-monasca-log-api</code>, as well as all the
      back-end services they use.
     </p></dd><dt id="id-1.4.5.4.12.29.3"><span class="term">monasca-agent</span></dt><dd><p>
      Deploys <code class="systemitem">openstack-monasca-agent</code> that is
      responsible for sending metrics to <code class="systemitem">monasca-api</code>
      on nodes it is assigned to.
     </p></dd><dt id="id-1.4.5.4.12.29.4"><span class="term">monasca-log-agent</span></dt><dd><p>
      Deploys <code class="systemitem">openstack-monasca-log-agent</code> responsible
      for sending logs to <code class="systemitem">monasca-log-api</code> on nodes it
      is assigned to.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.12.30"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_monasca_node_deployment.png"><img src="images/depl_barclamp_monasca_node_deployment.png" width="75%" alt="The monasca Barclamp: Node Deployment Example" title="The monasca Barclamp: Node Deployment Example"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.10: </span><span class="title-name">The monasca Barclamp: Node Deployment Example </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.12.30">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></section><section class="sect1" id="sec-depl-ostack-swift" data-id-title="Deploying swift (optional)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.7 </span><span class="title-name">Deploying swift (optional)</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-swift">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   swift adds an object storage service to SUSE <span class="productname">OpenStack</span> Cloud for storing single
   files such as images or snapshots. It offers high data security by storing
   the data redundantly on a pool of Storage Nodes—therefore swift
   needs to be installed on at least two dedicated nodes.
  </p><p>
   To properly configure swift it is important to understand how it
   places the data. Data is always stored redundantly within the hierarchy. The
   swift hierarchy in SUSE <span class="productname">OpenStack</span> Cloud is formed out of zones, nodes, hard disks,
   and logical partitions. Zones are physically separated clusters, for example
   different server rooms each with its own power supply and network segment. A
   failure of one zone must not affect another zone. The next level in the
   hierarchy are the individual swift storage nodes (on which
   <span class="guimenu">swift-storage</span> has been deployed), followed by the hard
   disks. Logical partitions come last.
  </p><p>
   swift automatically places three copies of each object on the highest
   hierarchy level possible. If three zones are available, then each copy of
   the object will be placed in a different zone. In a one zone setup with more
   than two nodes, the object copies will each be stored on a different node.
   In a one zone setup with two nodes, the copies will be distributed on
   different hard disks. If no other hierarchy element fits, logical partitions
   are used.
  </p><p>
   The following attributes can be set to configure swift:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.13.6.1"><span class="term"><span class="guimenu">Allow Public Containers</span>
    </span></dt><dd><p>
      Set to <code class="literal">true</code> to enable public access to containers.
     </p></dd><dt id="id-1.4.5.4.13.6.2"><span class="term"><span class="guimenu">Enable Object Versioning</span>
    </span></dt><dd><p>
      If set to true, a copy of the current version is archived each time an
      object is updated.
     </p></dd><dt id="id-1.4.5.4.13.6.3"><span class="term"><span class="guimenu">Zones</span>
    </span></dt><dd><p>
      Number of zones (see above). If you do not have different independent
      installations of storage nodes, set the number of zones to
      <code class="literal">1</code>.
     </p></dd><dt id="id-1.4.5.4.13.6.4"><span class="term"><span class="guimenu">Create 2^X Logical Partitions</span>
    </span></dt><dd><p>
      Partition power. The number entered here is used to compute the number of
      logical partitions to be created in the cluster. The number you enter is
      used as a power of 2 (2^X).
     </p><p>
      We recommend using a minimum of 100 partitions per disk. To measure the
      partition power for your setup, multiply the number of disks from all
      swift nodes by 100, and then round up to the nearest power of two.
      Keep in mind that the first disk of each node is not used by
      swift, but rather for the operating system.
     </p><p><span class="formalpara-title">Example: 10 swift nodes with 5 hard disks each. </span>
       Four hard disks on each node are used for swift, so there is a
       total of forty disks. 40 x 100 = 4000. The nearest power of two, 4096,
       equals 2^12. So the partition power that needs to be entered is
       <code class="literal">12</code>.
      </p><div id="id-1.4.5.4.13.6.4.2.4" data-id-title="Value Cannot be Changed After the Proposal Has Been Deployed" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Value Cannot be Changed After the Proposal Has Been Deployed</div><p>
       Changing the number of logical partition after swift has been
       deployed is not supported. Therefore the value for the partition power
       should be calculated from the maximum number of partitions this cloud
       installation is likely going to need at any point in time.
      </p></div></dd><dt id="id-1.4.5.4.13.6.5"><span class="term"><span class="guimenu">Minimum Hours before Partition is reassigned</span>
    </span></dt><dd><p>
      This option sets the number of hours before a logical partition is
      considered for relocation. <code class="literal">24</code> is the recommended
      value.
     </p></dd><dt id="id-1.4.5.4.13.6.6"><span class="term"><span class="guimenu">Replicas</span>
    </span></dt><dd><p>
      The number of copies generated for each object. The number of replicas
      depends on the number of disks and zones.
     </p></dd><dt id="id-1.4.5.4.13.6.7"><span class="term"><span class="guimenu">Replication interval (in seconds)</span>
    </span></dt><dd><p>
      Time (in seconds) after which to start a new replication process.
     </p></dd><dt id="id-1.4.5.4.13.6.8"><span class="term"><span class="guimenu">Debug</span>
    </span></dt><dd><p>
      Shows debugging output in the log files when set to
      <code class="literal">true</code>.
     </p></dd><dt id="id-1.4.5.4.13.6.9"><span class="term"><span class="guimenu">SSL Support: Protocol</span>
    </span></dt><dd><p>
      Choose whether to encrypt public communication (<span class="guimenu">HTTPS</span>)
      or not (<span class="guimenu">HTTP</span>). If you choose <span class="guimenu">HTTPS</span>,
      you have two options. You can either <span class="guimenu">Generate (self-signed)
      certificates</span> or provide the locations for the certificate key
      pair files. Using self-signed certificates is for testing purposes only
      and should never be used in production environments!
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.13.7"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_swift.png"><img src="images/depl_barclamp_swift.png" width="75%" alt="The swift Barclamp" title="The swift Barclamp"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.11: </span><span class="title-name">The swift Barclamp </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.13.7">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><p>
   Apart from the general configuration described above, the swift
   barclamp lets you also activate and configure <span class="guimenu">Additional
   Middlewares</span>. The features these middlewares provide can be used
   via the swift command line client only. The Ratelimit and S3
   middleware provide for the most interesting features, and we recommend
   enabling other middleware only for specific use-cases.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.13.9.1"><span class="term"><span class="guimenu">S3 Middleware</span>
    </span></dt><dd><p>
      Provides an S3 compatible API on top of swift.
     </p></dd><dt id="id-1.4.5.4.13.9.2"><span class="term"><span class="guimenu">StaticWeb</span>
    </span></dt><dd><p>
      Serve container data as a static Web site with an index file and optional
      file listings. See
      <a class="link" href="http://docs.openstack.org/developer/swift/middleware.html#staticweb" target="_blank">http://docs.openstack.org/developer/swift/middleware.html#staticweb</a>
      for details.
     </p><p>
      This middleware requires setting <span class="guimenu">Allow Public
      Containers</span> to <code class="literal">true</code>.
     </p></dd><dt id="id-1.4.5.4.13.9.3"><span class="term"><span class="guimenu">TempURL</span>
    </span></dt><dd><p>
      Create URLs to provide time-limited access to objects. See
      <a class="link" href="http://docs.openstack.org/developer/swift/middleware.html#tempurl" target="_blank">http://docs.openstack.org/developer/swift/middleware.html#tempurl</a>
      for details.
     </p></dd><dt id="id-1.4.5.4.13.9.4"><span class="term"><span class="guimenu">FormPOST</span>
    </span></dt><dd><p>
      Upload files to a container via Web form. See
      <a class="link" href="http://docs.openstack.org/developer/swift/middleware.html#formpost" target="_blank">http://docs.openstack.org/developer/swift/middleware.html#formpost</a>
      for details.
     </p></dd><dt id="id-1.4.5.4.13.9.5"><span class="term"><span class="guimenu">Bulk</span>
    </span></dt><dd><p>
      Extract TAR archives into a swift account, and delete multiple objects or
      containers with a single request. See
      <a class="link" href="http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.bulk" target="_blank">http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.bulk</a>
      for details.
     </p></dd><dt id="id-1.4.5.4.13.9.6"><span class="term"><span class="guimenu">Cross-domain</span>
    </span></dt><dd><p>
      Interact with the swift API via Flash, Java, and Silverlight from an
      external network. See
      <a class="link" href="http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.crossdomain" target="_blank">http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.crossdomain</a>
      for details.
     </p></dd><dt id="id-1.4.5.4.13.9.7"><span class="term"><span class="guimenu">Domain Remap</span>
    </span></dt><dd><p>
      Translates container and account parts of a domain to path parameters
      that the swift proxy server understands. Can be used to create
      short URLs that are easy to remember, for example by rewriting
      <code class="literal">home.tux.example.com/$ROOT/tux/home/myfile</code>
      to <code class="literal">home.tux.example.com/myfile</code>.
      See
      <a class="link" href="http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.domain_remap" target="_blank">http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.domain_remap</a>
      for details.
     </p></dd><dt id="id-1.4.5.4.13.9.8"><span class="term"><span class="guimenu">Ratelimit</span>
    </span></dt><dd><p>
      Throttle resources such as requests per minute to provide denial of
      service protection. See
      <a class="link" href="http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.ratelimit" target="_blank">http://docs.openstack.org/developer/swift/middleware.html#module-swift.common.middleware.ratelimit</a>
      for details.
     </p></dd></dl></div><p>
   The swift component consists of four different roles. Deploying
   <span class="guimenu">swift-dispersion</span> is optional:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.13.11.1"><span class="term"><span class="guimenu">swift-storage</span>
    </span></dt><dd><p>
      The virtual object storage service. Install this role on all dedicated
      swift Storage Nodes (at least two), but not on any other node.
     </p><div id="id-1.4.5.4.13.11.1.2.2" data-id-title="swift-storage Needs Dedicated Machines" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: swift-storage Needs Dedicated Machines</div><p>
       Never install the swift-storage service on a node that runs other
       <span class="productname">OpenStack</span> components.
      </p></div></dd><dt id="id-1.4.5.4.13.11.2"><span class="term"><span class="guimenu">swift-ring-compute</span>
    </span></dt><dd><p>
      The ring maintains the information about the location of objects,
      replicas, and devices. It can be compared to an index that is used by
      various <span class="productname">OpenStack</span> components to look up the physical location of objects.
      <span class="guimenu">swift-ring-compute</span> must only be installed on a single
      node, preferably a Control Node.
     </p></dd><dt id="id-1.4.5.4.13.11.3"><span class="term"><span class="guimenu">swift-proxy</span>
    </span></dt><dd><p>
      The swift proxy server takes care of routing requests to
      swift. Installing a single instance of
      <span class="guimenu">swift-proxy</span> on a Control Node is recommended. The
      <span class="guimenu">swift-proxy</span> role can be made highly available by
      deploying it on a cluster.
     </p></dd><dt id="id-1.4.5.4.13.11.4"><span class="term"><span class="guimenu">swift-dispersion</span>
    </span></dt><dd><p>
      Deploying <span class="guimenu">swift-dispersion</span> is optional. The
      swift dispersion tools can be used to test the health of the
      cluster. It creates a heap of dummy objects (using 1% of the total space
      available). The state of these objects can be queried using the
      swift-dispersion-report query. <span class="guimenu">swift-dispersion</span> needs
      to be installed on a Control Node.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.13.12"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_swift_node_deployment.png"><img src="images/depl_barclamp_swift_node_deployment.png" width="75%" alt="The swift Barclamp: Node Deployment Example" title="The swift Barclamp: Node Deployment Example"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.12: </span><span class="title-name">The swift Barclamp: Node Deployment Example </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.13.12">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><section class="sect2" id="sec-depl-ostack-swift-ha" data-id-title="HA Setup for swift"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.7.1 </span><span class="title-name">HA Setup for swift</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-swift-ha">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    swift replicates by design, so there is no need for a special HA setup.
    Make sure to fulfill the requirements listed in
    <a class="xref" href="#sec-depl-reg-ha-storage-swift" title="2.6.4.1. swift—Avoiding Points of Failure">Section 2.6.4.1, “swift—Avoiding Points of Failure”</a>.
   </p></section></section><section class="sect1" id="sec-depl-ostack-glance" data-id-title="Deploying glance"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.8 </span><span class="title-name">Deploying glance</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-glance">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   glance provides discovery, registration, and delivery services for virtual
   disk images. An image is needed to start an instance—it is its
   pre-installed root-partition. All images you want to use in your cloud to
   boot instances from, are provided by glance. glance must be deployed onto
   a Control Node. glance can be made highly available by deploying it on a
   cluster.
  </p><p>
   There are a lot of options to configure glance. The most important ones are
   explained below—for a complete reference refer to
   <a class="link" href="https://github.com/crowbar/crowbar-openstack/blob/master/glance.yml" target="_blank">https://github.com/crowbar/crowbar-openstack/blob/master/glance.yml</a>.
  </p><div id="note-glance-api-versions" data-id-title="glance API Versions" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: glance API Versions</div><p>
    As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 7, the glance API v1 is no longer enabled by default.
    Instead, glance API v2 is used by default.
   </p><p>
    If you need to re-enable API v1 for compatibility reasons:
   </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
      Switch to the <span class="guimenu">Raw</span> view of the glance barclamp.
     </p></li><li class="listitem"><p>
      Search for the <code class="literal">enable_v1</code> entry and set it to
      <code class="literal">true</code>:
     </p><div class="verbatim-wrap"><pre class="screen">"enable_v1": true</pre></div><p>
      In new installations, this entry is set to <code class="literal">false</code> by
      default. When upgrading from an older version of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> it is set
      to <code class="literal">true</code> by default.
     </p></li><li class="listitem"><p>
      Apply your changes.
     </p></li></ol></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.14.5.1"><span class="term"><span class="guimenu">Image Storage: Default Storage Store</span>
    </span></dt><dd><p><span class="formalpara-title"><span class="guimenu">File</span>. </span>
       Images are stored in an image file on the Control Node.
      </p><p><span class="formalpara-title"><span class="guimenu">cinder</span>. </span>
       Provides volume block storage to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. Use it to
       store images.
      </p><p><span class="formalpara-title"><span class="guimenu">swift</span>. </span>
       Provides an object storage service to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
      </p><p><span class="formalpara-title"><span class="guimenu">Rados</span>. </span>
       SUSE Enterprise Storage (based on Ceph) provides block storage service to
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
      </p><p><span class="formalpara-title"><span class="guimenu">VMware</span>. </span>
       If you are using VMware as a hypervisor, it is recommended to use
       <span class="guimenu">VMware</span> for storing images. This will make starting
       VMware instances much faster.
      </p><p><span class="formalpara-title"><span class="guimenu">Expose Backend Store Location</span>. </span>
       If this is set to <span class="guimenu">true</span>, the API will communicate the
       direct URl of the image's back-end location to HTTP clients. Set to
       <span class="guimenu">false</span> by default.
      </p><p>
      Depending on the storage back-end, there are additional configuration
      options available:
     </p><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.5.4.14.5.1.2.8"><span class="name"><span class="guimenu">File Store Parameters</span>
     </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.14.5.1.2.8">#</a></h5></div><p>
      Only required if <span class="guimenu">Default Storage Store</span> is set to
      <span class="guimenu">File</span>.
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.14.5.1.2.10.1"><span class="term"><span class="guimenu">Image Store Directory</span>
       </span></dt><dd><p>
         Specify the directory to host the image file. The directory specified
         here can also be an NFS share. See
         <a class="xref" href="#sec-depl-inst-nodes-post-nfs" title="11.4.3. Mounting NFS Shares on a Node">Section 11.4.3, “Mounting NFS Shares on a Node”</a> for more information.
        </p></dd></dl></div><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.5.4.14.5.1.2.11"><span class="name"><span class="guimenu">swift Store Parameters</span>
     </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.14.5.1.2.11">#</a></h5></div><p>
      Only required if <span class="guimenu">Default Storage Store</span> is set to
      <span class="guimenu">swift</span>.
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.14.5.1.2.13.1"><span class="term"><span class="guimenu">swift Container</span>
       </span></dt><dd><p>
         Set the name of the container to use for the images in swift.
        </p></dd></dl></div><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.5.4.14.5.1.2.14"><span class="name"><span class="guimenu">RADOS Store Parameters</span>
     </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.14.5.1.2.14">#</a></h5></div><p>
      Only required if <span class="guimenu">Default Storage Store</span> is set to
      <span class="guimenu">Rados</span>.
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.14.5.1.2.16.1"><span class="term">RADOS User for CephX Authentication</span></dt><dd><p>
         If you are using an external Ceph cluster, specify the user you have
         set up for glance (see <a class="xref" href="#sec-depl-inst-nodes-post-ceph-ext" title="11.4.4. Using an Externally Managed Ceph Cluster">Section 11.4.4, “Using an Externally Managed Ceph Cluster”</a> for more information).
        </p></dd><dt id="id-1.4.5.4.14.5.1.2.16.2"><span class="term">RADOS Pool for glance images</span></dt><dd><p>
         If you are using a SUSE <span class="productname">OpenStack</span> Cloud internal Ceph setup, the pool you specify
         here is created if it does not exist. If you are using an external
         Ceph cluster, specify the pool you have set up for glance (see
         <a class="xref" href="#sec-depl-inst-nodes-post-ceph-ext" title="11.4.4. Using an Externally Managed Ceph Cluster">Section 11.4.4, “Using an Externally Managed Ceph Cluster”</a> for more
         information).
        </p></dd></dl></div><div class="sect4 bridgehead"><h5 class="title" id="id-1.4.5.4.14.5.1.2.17"><span class="name"><span class="guimenu">VMware Store Parameters</span>
     </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.14.5.1.2.17">#</a></h5></div><p>
      Only required if <span class="guimenu">Default Storage Store</span> is set to
      <span class="guimenu">VMware</span>.
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.14.5.1.2.19.1"><span class="term"><span class="guimenu">vCenter Host/IP Address</span>
       </span></dt><dd><p>
         Name or IP address of the vCenter server.
        </p></dd><dt id="id-1.4.5.4.14.5.1.2.19.2"><span class="term"><span class="guimenu">vCenter Username</span> / <span class="guimenu">vCenter
        Password</span>
       </span></dt><dd><p>
         vCenter login credentials.
        </p></dd><dt id="id-1.4.5.4.14.5.1.2.19.3"><span class="term"><span class="guimenu">Datastores for Storing Images</span>
       </span></dt><dd><p>
         A comma-separated list of datastores specified in the format:
         <em class="replaceable">DATACENTER_NAME</em>:<em class="replaceable">DATASTORE_NAME</em>
        </p></dd><dt id="id-1.4.5.4.14.5.1.2.19.4"><span class="term"><span class="guimenu">
         Path on the datastore, where the glance images will be
         stored
        </span>
       </span></dt><dd><p>
         Specify an absolute path here.
        </p></dd></dl></div></dd><dt id="id-1.4.5.4.14.5.2"><span class="term"><span class="guimenu">SSL Support: Protocol</span>
    </span></dt><dd><p>
      Choose whether to encrypt public communication (<span class="guimenu">HTTPS</span>)
      or not (<span class="guimenu">HTTP</span>). If you choose <span class="guimenu">HTTPS</span>,
      refer to <a class="xref" href="#sec-depl-ostack-keystone-ssl">SSL Support: Protocol
    </a> for configuration
      details.
     </p></dd><dt id="id-1.4.5.4.14.5.3"><span class="term"><span class="guimenu">Caching</span>
    </span></dt><dd><p>
      Enable and configure image caching in this section. By default, image
      caching is disabled. You can see this the Raw view of your nova barclamp:
     </p><div class="verbatim-wrap"><pre class="screen">image_cache_manager_interval = -1</pre></div><p>
      This option sets the number of seconds to wait between runs of the image
      cache manager. Disabling it means that the cache manager will not
      automatically remove the unused images from the cache, so if you have
      many glance images and are running out of storage you must manually
      remove the unused images from the cache. We recommend leaving this option
      disabled as it is known to cause issues, especially with shared storage.
      The cache manager may remove images still in use, e.g. when network
      outages cause synchronization problems with compute nodes.
     </p><p>
      If you wish to enable caching, re-enable it in a custom nova
      configuration file, for example
      <code class="filename">/etc/nova/nova.conf.d/500-nova.conf</code>. This sets the
      interval to four minutes:
     </p><div class="verbatim-wrap"><pre class="screen">image_cache_manager_interval = 2400</pre></div><p>
      See <a class="xref" href="#cha-depl-ostack-configs" title="Chapter 14. Configuration Files for OpenStack Services">Chapter 14, <em>Configuration Files for <span class="productname">OpenStack</span> Services</em></a> for more information on
      custom configurations.
     </p><p>
      Learn more about glance's caching feature at
      <a class="link" href="http://docs.openstack.org/developer/glance/cache.html" target="_blank">http://docs.openstack.org/developer/glance/cache.html</a>.
     </p></dd><dt id="id-1.4.5.4.14.5.4"><span class="term"><span class="guimenu">Logging: Verbose Logging</span>
    </span></dt><dd><p>
      Shows debugging output in the log files when set to
      <span class="guimenu">true</span>.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.14.6"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_glance.png"><img src="images/depl_barclamp_glance.png" width="75%" alt="The glance Barclamp" title="The glance Barclamp"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.13: </span><span class="title-name">The glance Barclamp </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.14.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><section class="sect2" id="sec-depl-ostack-glance-ha" data-id-title="HA Setup for glance"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.8.1 </span><span class="title-name">HA Setup for glance</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-glance-ha">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    glance can be made highly available by deploying it on a cluster. We
    strongly recommended doing this for the image data as well. The recommended
    way is to use swift or an external Ceph cluster for the image
    repository. If you are using a directory on the node instead (file storage
    back-end), you should set up shared storage on the cluster for it.
   </p></section></section><section class="sect1" id="sec-depl-ostack-cinder" data-id-title="Deploying cinder"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.9 </span><span class="title-name">Deploying cinder</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-cinder">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   cinder, the successor of <code class="literal">nova-volume</code>, provides
   volume block storage.
   It adds persistent storage to an instance that persists until deleted,
   contrary to ephemeral volumes that only persist while the instance is
   running.
  </p><p>
   cinder can provide volume storage by using different back-ends such
   as local file, one or more local disks, Ceph (RADOS), VMware, or network
   storage solutions from EMC, EqualLogic, Fujitsu, NetApp or Pure Storage.
   Since <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 5, cinder supports using several back-ends
   simultaneously. It is also possible to deploy the same network storage
   back-end multiple times and therefore use different installations at the
   same time.
  </p><p>
   The attributes that can be set to configure cinder depend on the
   back-end. The only general option is <span class="guimenu">SSL Support:
   Protocol</span> (see <a class="xref" href="#sec-depl-ostack-keystone-ssl">SSL Support: Protocol
    </a> for
   configuration details).
  </p><div id="id-1.4.5.4.15.5" data-id-title="Adding or Changing a Back-End" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Adding or Changing a Back-End</div><p>
    When first opening the cinder barclamp, the default
    proposal—<span class="guimenu">Raw Devices</span>—is already available
    for configuration. To optionally add a back-end, go to the section
    <span class="guimenu">Add New cinder Back-End</span> and choose a <span class="guimenu">Type Of
    Volume</span> from the drop-down box. Optionally, specify the
    <span class="guimenu">Name for the Backend</span>. This is recommended when deploying
    the same volume type more than once. Existing back-end configurations
    (including the default one) can be deleted by clicking the trashcan icon if
    no longer needed. Note that you must configure at least one back-end.
   </p></div><div class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.15.6"><span class="name"><span class="guimenu">Raw devices</span> (local disks)
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.15.6">#</a></h3></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.15.7.1"><span class="term"><span class="guimenu">Disk Selection Method</span>
    </span></dt><dd><p>
      Choose whether to use the <span class="guimenu">First Available</span> disk or
      <span class="guimenu">All Available</span> disks. <span class="quote">“<span class="quote">Available disks</span>”</span>
      are all disks currently not used by the system. Note that one disk
      (usually <code class="filename">/dev/sda</code>) of every block storage node is
      already used for the operating system and is not available for
      cinder.
     </p></dd><dt id="id-1.4.5.4.15.7.2"><span class="term"><span class="guimenu">Name of Volume</span>
    </span></dt><dd><p>
      Specify a name for the cinder volume.
     </p></dd></dl></div><div class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.15.8"><span class="name"><span class="guimenu">EMC</span> (EMC² Storage)
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.15.8">#</a></h4></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.15.9.1"><span class="term"><span class="guimenu">IP address of the ECOM server</span> / <span class="guimenu">Port of the ECOM server</span>
    </span></dt><dd><p>
      IP address and Port of the ECOM server.
     </p></dd><dt id="id-1.4.5.4.15.9.2"><span class="term"><span class="guimenu">Username for accessing the ECOM server</span> / <span class="guimenu">Password for accessing the ECOM server</span>
    </span></dt><dd><p>
      Login credentials for the ECOM server.
     </p></dd><dt id="id-1.4.5.4.15.9.3"><span class="term"><span class="guimenu">VMAX port groups to expose volumes managed by this backend</span>
    </span></dt><dd><p>
      VMAX port groups that expose volumes managed by this back-end.
     </p></dd><dt id="id-1.4.5.4.15.9.4"><span class="term"><span class="guimenu">Serial number of the VMAX Array</span>
    </span></dt><dd><p>
      Unique VMAX array serial number.
     </p></dd><dt id="id-1.4.5.4.15.9.5"><span class="term"><span class="guimenu">Pool name within a given array</span>
    </span></dt><dd><p>
      Unique pool name within a given array.
     </p></dd><dt id="id-1.4.5.4.15.9.6"><span class="term"><span class="guimenu">FAST Policy name to be used</span>
    </span></dt><dd><p>
      Name of the FAST Policy to be used. When specified, volumes managed by
      this back-end are managed as under FAST control.
     </p></dd></dl></div><p>
   For more information on the EMC driver refer to the <span class="productname">OpenStack</span> documentation
   at
   <a class="link" href="http://docs.openstack.org/liberty/config-reference/content/emc-vmax-driver.html" target="_blank">http://docs.openstack.org/liberty/config-reference/content/emc-vmax-driver.html</a>.
  </p><div class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.15.11"><span class="name"><span class="guimenu">EqualLogic</span>
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.15.11">#</a></h4></div><p>
   EqualLogic drivers are included as a technology preview and are not
   supported.
  </p><div class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.15.13"><span class="name"><span class="guimenu">Fujitsu ETERNUS DX</span>
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.15.13">#</a></h3></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.15.14.1"><span class="term"><span class="guimenu">Connection Protocol</span>
    </span></dt><dd><p>
      Select the protocol used to connect, either
      <span class="guimenu">FibreChannel</span> or <span class="guimenu">iSCSI</span>.
     </p></dd><dt id="id-1.4.5.4.15.14.2"><span class="term"><span class="guimenu">IP for SMI-S</span> / <span class="guimenu">Port for SMI-S</span>
    </span></dt><dd><p>
      IP address and port of the ETERNUS SMI-S Server.
     </p></dd><dt id="id-1.4.5.4.15.14.3"><span class="term"><span class="guimenu">Username for SMI-S</span> / <span class="guimenu">Password for SMI-S</span>
    </span></dt><dd><p>
      Login credentials for the ETERNUS SMI-S Server.
     </p></dd><dt id="id-1.4.5.4.15.14.4"><span class="term"><span class="guimenu">Snapshot (Thick/RAID Group) Pool Name</span>
    </span></dt><dd><p>
      Storage pool (RAID group) in which the volumes are created. Make sure
      that the RAID group on the server has already been created. If a RAID
      group that does not exist is specified, the RAID group is built from
      unused disk drives. The RAID level is automatically determined by the
      ETERNUS DX Disk storage system.
     </p></dd></dl></div><div class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.15.15"><span class="name"><span class="guimenu">Hitachi HUSVM</span>
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.15.15">#</a></h3></div><p>
   For information on configuring the Hitachi HUSVM back-end, refer to
   <a class="link" href="http://docs.openstack.org/ocata/config-reference/block-storage/drivers/hitachi-storage-volume-driver.html" target="_blank">http://docs.openstack.org/ocata/config-reference/block-storage/drivers/hitachi-storage-volume-driver.html</a>.
  </p><div class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.15.17"><span class="name"><span class="guimenu">NetApp</span>
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.15.17">#</a></h3></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.15.18.1"><span class="term"><span class="guimenu">Storage Family Type</span> / <span class="guimenu">Storage Protocol</span>
    </span></dt><dd><p>
      SUSE <span class="productname">OpenStack</span> Cloud can use <span class="quote">“<span class="quote">Data ONTAP</span>”</span> in <span class="guimenu">7-Mode</span>,
      or in <span class="guimenu">Clustered Mode</span>. In <span class="guimenu">7-Mode</span>
      vFiler will be configured, in <span class="guimenu">Clustered Mode</span> vServer
      will be configured. The <span class="guimenu">Storage Protocol</span> can be set to
      either <span class="guimenu">iSCSI</span> or <span class="guimenu">NFS</span>. Choose the
      driver and the protocol your NetApp is licensed for.
     </p></dd><dt id="id-1.4.5.4.15.18.2"><span class="term"><span class="guimenu">Server host name</span>
    </span></dt><dd><p>
      The management IP address for the 7-Mode storage controller, or the
      cluster management IP address for the clustered Data ONTAP.
     </p></dd><dt id="id-1.4.5.4.15.18.3"><span class="term"><span class="guimenu">Transport Type</span>
    </span></dt><dd><p>
      Transport protocol for communicating with the storage controller or
      clustered Data ONTAP. Supported protocols are HTTP and HTTPS. Choose the
      protocol your NetApp is licensed for.
     </p></dd><dt id="id-1.4.5.4.15.18.4"><span class="term"><span class="guimenu">Server port</span>
    </span></dt><dd><p>
      The port to use for communication. Port 80 is usually used for HTTP, 443
      for HTTPS.
     </p></dd><dt id="id-1.4.5.4.15.18.5"><span class="term"><span class="guimenu">Username for accessing NetApp</span> / <span class="guimenu">Password for Accessing NetApp</span>
    </span></dt><dd><p>
      Login credentials.
     </p></dd><dt id="id-1.4.5.4.15.18.6"><span class="term"><span class="guimenu">
      The vFiler Unit Name for provisioning <span class="productname">OpenStack</span> volumes (netapp_vfiler)
     </span>
    </span></dt><dd><p>
      The vFiler unit to be used for provisioning of <span class="productname">OpenStack</span> volumes. This
      setting is only available in <span class="guimenu">7-Mode</span>.
     </p></dd><dt id="id-1.4.5.4.15.18.7"><span class="term"><span class="guimenu">Restrict provisioning on iSCSI to these volumes (netapp_volume_list)</span>
    </span></dt><dd><p>
      Provide a list of comma-separated volume names to be used for
      provisioning. This setting is only available when using iSCSI as storage
      protocol.
     </p></dd></dl></div><div class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.15.19"><span class="name"><span class="guimenu">NFS</span>
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.15.19">#</a></h3></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.15.20.1"><span class="term"><span class="guimenu">List of NFS Exports</span>
    </span></dt><dd><p>
      A list of available file systems on an NFS server. Enter your NFS mountpoints
      in the <span class="guimenu">List of NFS Exports</span> form in this format: <em class="replaceable">host:mountpoint -o options</em>. For example:
     </p><div class="verbatim-wrap"><pre class="screen">host1:/srv/nfs/share1 /mnt/nfs/share1 -o rsize=8192,wsize=8192,timeo=14,intr</pre></div></dd></dl></div><div class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.15.21"><span class="name"><span class="guimenu">Pure Storage (FlashArray)</span>
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.15.21">#</a></h3></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.15.22.1"><span class="term"><span class="guimenu">IP address of the management VIP</span>
    </span></dt><dd><p>
      IP address of the FlashArray management VIP
     </p></dd><dt id="id-1.4.5.4.15.22.2"><span class="term"><span class="guimenu">API token for the FlashArray</span>
    </span></dt><dd><p>
      API token for access to the FlashArray
     </p></dd><dt id="id-1.4.5.4.15.22.3"><span class="term"><span class="guimenu">iSCSI CHAP authentication enabled</span>
    </span></dt><dd><p>
      Enable or disable iSCSI CHAP authentication
     </p></dd></dl></div><p>
   For more information on the Pure Storage FlashArray driver refer to the <span class="productname">OpenStack</span> documentation
   at
   <a class="link" href="https://docs.openstack.org/ocata/config-reference/block-storage/drivers/pure-storage-driver.html" target="_blank">https://docs.openstack.org/ocata/config-reference/block-storage/drivers/pure-storage-driver.html</a>.
  </p><div class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.15.24"><span class="name"><span class="guimenu">RADOS</span> (Ceph)
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.15.24">#</a></h4></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.15.25.1"><span class="term"><span class="guimenu">Use Ceph Deployed by Crowbar</span>
    </span></dt><dd><p>
      Select <span class="guimenu">false</span>, if you are using an external Ceph cluster (see
      <a class="xref" href="#sec-depl-inst-nodes-post-ceph-ext" title="11.4.4. Using an Externally Managed Ceph Cluster">Section 11.4.4, “Using an Externally Managed Ceph Cluster”</a> for setup
      instructions).
     </p></dd><dt id="id-1.4.5.4.15.25.2"><span class="term"><span class="guimenu">RADOS pool for cinder volumes</span>
    </span></dt><dd><p>
      Name of the pool used to store the cinder volumes.
     </p></dd><dt id="id-1.4.5.4.15.25.3"><span class="term"><span class="guimenu">
      RADOS user (Set Only if Using CephX authentication)
     </span>
    </span></dt><dd><p>
      Ceph user name.
     </p></dd></dl></div><div class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.15.26"><span class="name"><span class="guimenu">VMware Parameters</span>
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.15.26">#</a></h4></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.15.27.1"><span class="term"><span class="guimenu">vCenter Host/IP Address</span>
    </span></dt><dd><p>
      Host name or IP address of the vCenter server.
     </p></dd><dt id="id-1.4.5.4.15.27.2"><span class="term"><span class="guimenu">vCenter Username</span> / <span class="guimenu">vCenter
     Password</span>
    </span></dt><dd><p>
      vCenter login credentials.
     </p></dd><dt id="id-1.4.5.4.15.27.3"><span class="term"><span class="guimenu">vCenter Cluster Names for Volumes</span>
    </span></dt><dd><p>
      Provide a comma-separated list of cluster names.
     </p></dd><dt id="id-1.4.5.4.15.27.4"><span class="term"><span class="guimenu">Folder for Volumes</span>
    </span></dt><dd><p>
      Path to the directory used to store the cinder volumes.
     </p></dd><dt id="id-1.4.5.4.15.27.5"><span class="term"><span class="guimenu">CA file for verifying the vCenter certificate</span>
    </span></dt><dd><p>
      Absolute path to the vCenter CA certificate.
     </p></dd><dt id="id-1.4.5.4.15.27.6"><span class="term"><span class="guimenu">
      vCenter SSL Certificate is insecure (for instance, self-signed)
     </span>
    </span></dt><dd><p>
      Default value: <code class="literal">false</code> (the CA truststore is used for
      verification). Set this option to <code class="literal">true</code> when using
      self-signed certificates to disable certificate checks. This setting is
      for testing purposes only and must not be used in production
      environments!
     </p></dd></dl></div><div class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.15.28"><span class="name"><span class="guimenu">Local file</span>
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.15.28">#</a></h3></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.15.29.1"><span class="term"><span class="guimenu">Volume File Name</span>
    </span></dt><dd><p>
      Absolute path to the file to be used for block storage.
     </p></dd><dt id="id-1.4.5.4.15.29.2"><span class="term"><span class="guimenu">Maximum File Size (GB)</span>
    </span></dt><dd><p>
      Maximum size of the volume file. Make sure not to overcommit the size,
      since it will result in data loss.
     </p></dd><dt id="id-1.4.5.4.15.29.3"><span class="term"><span class="guimenu">Name of Volume</span>
    </span></dt><dd><p>
      Specify a name for the cinder volume.
     </p></dd></dl></div><div id="id-1.4.5.4.15.30" data-id-title="Using Local File for Block Storage" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Using <span class="guimenu">Local File</span> for Block Storage</div><p>
    Using a file for block storage is not recommended for production systems,
    because of performance and data security reasons.
   </p></div><div class="sect3 bridgehead"><h4 class="title" id="id-1.4.5.4.15.31"><span class="name"><span class="guimenu">Other driver</span>
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.15.31">#</a></h4></div><p>
   Lets you manually pick and configure a driver. Only use this option for
   testing purposes, as it is not supported.
  </p><div class="figure" id="id-1.4.5.4.15.33"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_cinder.png"><img src="images/depl_barclamp_cinder.png" width="75%" alt="The cinder Barclamp" title="The cinder Barclamp"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.14: </span><span class="title-name">The cinder Barclamp </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.15.33">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><p>
   The cinder component consists of two different roles:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.15.35.1"><span class="term"><span class="guimenu">cinder-controller</span>
    </span></dt><dd><p>
      The cinder controller provides the scheduler and the API.
      Installing <span class="guimenu">cinder-controller</span> on a Control Node is
      recommended.
     </p></dd><dt id="id-1.4.5.4.15.35.2"><span class="term"><span class="guimenu">cinder-volume</span>
    </span></dt><dd><p>
      The virtual block storage service. It can be installed on a Control Node.
      However, we recommend deploying it on one or more dedicated nodes
      supplied with sufficient networking capacity to handle the increase in
      network traffic.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.15.36"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_cinder_node_deployment.png"><img src="images/depl_barclamp_cinder_node_deployment.png" width="75%" alt="The cinder Barclamp: Node Deployment Example" title="The cinder Barclamp: Node Deployment Example"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.15: </span><span class="title-name">The cinder Barclamp: Node Deployment Example </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.15.36">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><section class="sect2" id="sec-depl-ostack-cinder-ha" data-id-title="HA Setup for cinder"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.9.1 </span><span class="title-name">HA Setup for cinder</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-cinder-ha">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    Both the <span class="guimenu">cinder-controller</span> and the
    <span class="guimenu">cinder-volume</span> role can be deployed on a cluster.
   </p><div id="id-1.4.5.4.15.37.3" data-id-title="Moving cinder-volume to a Cluster" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Moving <span class="guimenu">cinder-volume</span> to a Cluster</div><p>
     If you need to re-deploy <span class="guimenu">cinder-volume</span> role from a
     single machine to a cluster environment, the following will happen:
     Volumes that are currently attached to instances will continue to work,
     but adding volumes to instances will not succeed.
    </p><p>
     To solve this issue, run the following script once on each node that
     belongs to the <span class="guimenu">cinder-volume</span> cluster:
     <code class="filename">/usr/bin/cinder-migrate-volume-names-to-cluster</code>.
    </p><p>
     The script is automatically installed by Crowbar on every machine or
     cluster that has a <span class="guimenu">cinder-volume</span> role applied to it.
    </p></div><p>
    In combination with Ceph or a network storage solution, deploying
    cinder in a cluster minimizes the potential downtime. For
    <span class="guimenu">cinder-volume</span> to be applicable to a cluster, the role
    needs all cinder backends to be configured for non-local
    storage. If you are using local volumes or raw devices in any of your
    volume backends, you cannot apply <span class="guimenu">cinder-volume</span> to a
    cluster.
   </p></section></section><section class="sect1" id="sec-depl-ostack-neutron" data-id-title="Deploying neutron"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.10 </span><span class="title-name">Deploying neutron</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-neutron">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   neutron provides network connectivity between interface devices managed by
   other <span class="productname">OpenStack</span> components (most likely nova). The service works by
   enabling users to create their own networks and then attach interfaces to
   them.
  </p><p>
   neutron must be deployed on a Control Node. You first need to choose a core
   plug-in—<span class="guimenu">ml2</span> or <span class="guimenu">vmware</span>. Depending
   on your choice, more configuration options will become available.
  </p><p>
   The <span class="guimenu">vmware</span> option lets you use an existing VMware NSX
   installation. Using this plugin is not a prerequisite for the VMware vSphere
   hypervisor support. However, it is needed when wanting to have security
   groups supported on VMware compute nodes. For all other scenarios, choose
   <span class="guimenu">ml2</span>.
  </p><p>
   The only global option that can be configured is <span class="guimenu">SSL
   Support</span>. Choose whether to encrypt public communication
   (<span class="guimenu">HTTPS</span>) or not (<span class="guimenu">HTTP</span>). If choosing
   <span class="guimenu">HTTPS</span>, refer to
   <a class="xref" href="#sec-depl-ostack-keystone-ssl">SSL Support: Protocol
    </a> for configuration details.
  </p><div class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.16.6"><span class="name"><span class="guimenu">ml2</span> (Modular Layer 2)
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.16.6">#</a></h3></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.16.7.1"><span class="term"><span class="guimenu">Modular Layer 2 Mechanism Drivers</span>
    </span></dt><dd><p>
      Select which mechanism driver(s) shall be enabled for the ml2 plugin. It
      is possible to select more than one driver by holding the
      <span class="keycap">Ctrl</span> key while clicking. Choices are:
     </p><p><span class="formalpara-title"><span class="guimenu">openvswitch</span>. </span>
       Supports GRE, VLAN and VXLAN networks (to be configured via the
       <span class="guimenu">Modular Layer 2 type drivers</span> setting). VXLAN is the
       default.
      </p><p><span class="formalpara-title"><span class="guimenu">linuxbridge</span>. </span>
       Supports VLANs only. Requires to specify the <span class="guimenu">Maximum Number of
       VLANs</span>.
      </p><p><span class="formalpara-title"><span class="guimenu">cisco_nexus</span>. </span>
       Enables neutron to dynamically adjust the VLAN settings of the ports of
       an existing Cisco Nexus switch when instances are launched. It also
       requires <span class="guimenu">openvswitch</span> which will automatically be
       selected. With <span class="guimenu">Modular Layer 2 type drivers</span>,
       <span class="guimenu">vlan</span> must be added. This option also requires to
       specify the <span class="guimenu">Cisco Switch Credentials</span>. See
       <a class="xref" href="#app-deploy-cisco" title="Appendix A. Using Cisco Nexus Switches with neutron">Appendix A, <em>Using Cisco Nexus Switches with neutron</em></a> for details.
      </p><p><span class="formalpara-title"><span class="guimenu">vmware_dvs</span>. </span>
       vmware_dvs driver makes it possible to use neutron for networking in a
       VMware-based environment. Choosing <span class="guimenu">vmware_dvs</span>,
       automatically selects the required <span class="guimenu">openswitch</span>, <span class="guimenu">vxlan</span>, and
       <span class="guimenu">vlan</span> drivers. In the <span class="guimenu">Raw</span> view,
       it is also possible to configure two additional attributes:
       <span class="guimenu">clean_on_start</span> (clean up the DVS portgroups on the
       target vCenter Servers when neutron-server is restarted) and
       <span class="guimenu">precreate_networks</span> (create DVS portgroups
       corresponding to networks in advance, rather than when virtual machines are attached to these networks).
      </p></dd><dt id="id-1.4.5.4.16.7.2"><span class="term"><span class="guimenu">Use Distributed Virtual Router Setup</span>
    </span></dt><dd><p>
      With the default setup, all intra-Compute Node traffic flows through the
      network Control Node. The same is true for all traffic from floating IPs.
      In large deployments the network Control Node can therefore quickly become
      a bottleneck. When this option is set to <span class="guimenu">true</span>, network
      agents will be installed on all compute nodes. This will de-centralize
      the network traffic, since Compute Nodes will be able to directly
      <span class="quote">“<span class="quote">talk</span>”</span> to each other. Distributed Virtual Routers (DVR)
      require the <span class="guimenu">openvswitch</span> driver and will not work with
      the <span class="guimenu">linuxbridge</span> driver. For details on DVR refer to
      <a class="link" href="https://wiki.openstack.org/wiki/Neutron/DVR" target="_blank">https://wiki.openstack.org/wiki/Neutron/DVR</a>.
     </p></dd><dt id="id-1.4.5.4.16.7.3"><span class="term"><span class="guimenu">Modular Layer 2 Type Drivers</span>
    </span></dt><dd><p>
      This option is only available when having chosen the
      <span class="guimenu">openvswitch</span> or the <span class="guimenu">cisco_nexus</span>
      mechanism drivers. Options are <span class="guimenu">vlan</span>,
      <span class="guimenu">gre</span> and <span class="guimenu">vxlan</span>. It is possible to
      select more than one driver by holding the <span class="keycap">Ctrl</span>
      key while clicking.
     </p><p>
      When multiple type drivers are enabled, you need to select the
      <span class="guimenu">Default Type Driver for Provider Network</span>, that will be
      used for newly created provider networks. This also includes the
      <code class="literal">nova_fixed</code> network, that will be created when applying
      the neutron proposal. When manually creating provider networks with the
      <code class="command">neutron</code> command, the default can be overwritten with
      the <code class="option">--provider:network_type
      <em class="replaceable">type</em></code> switch. You will also need to
      set a <span class="guimenu">Default Type Driver for Tenant Network</span>. It is
      not possible to change this default when manually creating tenant
      networks with the <code class="command">neutron</code> command. The non-default
      type driver will only be used as a fallback.
     </p><p>
      Depending on your choice of the type driver, more configuration options
      become available.
     </p><p><span class="formalpara-title"><span class="guimenu">gre</span>. </span>
       Having chosen <span class="guimenu">gre</span>, you also need to specify the start
       and end of the tunnel ID range.
      </p><p><span class="formalpara-title"><span class="guimenu">vlan</span>. </span>
       The option <span class="guimenu">vlan</span> requires you to specify the
       <span class="guimenu">Maximum number of VLANs</span>.
      </p><p><span class="formalpara-title"><span class="guimenu">vxlan</span>. </span>
       Having chosen <span class="guimenu">vxlan</span>, you also need to specify the
       start and end of the VNI range.
      </p></dd></dl></div><div id="id-1.4.5.4.16.8" data-id-title="Drivers for the VMware Compute Node" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Drivers for the VMware Compute Node</div><p>
    neutron must not be deployed with the <code class="literal">openvswitch with
    gre</code> plug-in.
   </p></div><div class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.16.9"><span class="name"><span class="guimenu">z/VM Configuration</span>
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.16.9">#</a></h3></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.16.10.1"><span class="term">xCAT Host/IP Address</span></dt><dd><p>
      Host name or IP address of the xCAT Management Node.
      
     </p></dd><dt id="id-1.4.5.4.16.10.2"><span class="term">xCAT Username/Password</span></dt><dd><p>
      xCAT login credentials.
     </p></dd><dt id="id-1.4.5.4.16.10.3"><span class="term">rdev list for physnet1 vswitch uplink (if available)</span></dt><dd><p>
      List of rdev addresses that should be connected to this vswitch.
     </p></dd><dt id="id-1.4.5.4.16.10.4"><span class="term">xCAT IP Address on Management Network</span></dt><dd><p>
      IP address of the xCAT management interface.
     </p></dd><dt id="id-1.4.5.4.16.10.5"><span class="term">Net Mask of Management Network</span></dt><dd><p>
      Net mask of the xCAT management interface.
      
     </p></dd></dl></div><div class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.16.11"><span class="name"><span class="guimenu">vmware</span>
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.16.11">#</a></h3></div><p>
   This plug-in requires to configure access to the VMware NSX service.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.16.13.1"><span class="term"><span class="guimenu">VMware NSX User Name/Password</span>
    </span></dt><dd><p>
      Login credentials for the VMware NSX server. The user needs to have
      administrator permissions on the NSX server.
     </p></dd><dt id="id-1.4.5.4.16.13.2"><span class="term"><span class="guimenu">VMware NSX Controllers</span>
    </span></dt><dd><p>
      Enter the IP address and the port number
      (<em class="replaceable">IP-ADDRESS</em>:<em class="replaceable">PORT</em>)
      of the controller API endpoint. If the port number is omitted, port 443
      will be used. You may also enter multiple API endpoints
      (comma-separated), provided they all belong to the same controller
      cluster. When multiple API endpoints are specified, the plugin will load
      balance requests on the various API endpoints.
     </p></dd><dt id="id-1.4.5.4.16.13.3"><span class="term"><span class="guimenu">UUID of the NSX Transport Zone/Gateway Service</span>
    </span></dt><dd><p>
      The UUIDs for the transport zone and the gateway service can be obtained
      from the NSX server. They will be used when networks are created.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.16.14"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_network.png"><img src="images/depl_barclamp_network.png" width="75%" alt="The neutron Barclamp" title="The neutron Barclamp"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.16: </span><span class="title-name">The neutron Barclamp </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.16.14">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><p>
   The neutron component consists of two different roles:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.16.16.1"><span class="term"><span class="guimenu">neutron-server</span>
    </span></dt><dd><p>
      <span class="guimenu">neutron-server</span> provides the scheduler and the API. It
      needs to be installed on a Control Node.
     </p></dd><dt id="id-1.4.5.4.16.16.2"><span class="term"><span class="guimenu">neutron-network</span>
    </span></dt><dd><p>
      This service runs the various agents that manage the network traffic of
      all the cloud instances. It acts as the DHCP and DNS server and as a
      gateway for all cloud instances. It is recommend to deploy this role on a
      dedicated node supplied with sufficient network capacity.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.16.17"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_neutron_node_deployment.png"><img src="images/depl_barclamp_neutron_node_deployment.png" width="75%" alt="The neutron barclamp" title="The neutron barclamp"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.17: </span><span class="title-name">The neutron barclamp </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.16.17">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><section class="sect2" id="sec-depl-ostack-network-infoblox" data-id-title="Using Infoblox IPAM Plug-in"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.10.1 </span><span class="title-name">Using Infoblox IPAM Plug-in</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-network-infoblox">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    In the neutron barclamp, you can enable support for the infoblox IPAM
    plug-in and configure it. For configuration, the
    <code class="literal">infoblox</code> section contains the subsections
    <code class="literal">grids</code> and <code class="literal">grid_defaults</code>.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.16.18.3.1"><span class="term">grids</span></dt><dd><p>
       This subsection must contain at least one entry. For each entry, the
       following parameters are required:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         admin_user_name
        </p></li><li class="listitem"><p>
         admin_password
        </p></li><li class="listitem"><p>
         grid_master_host
        </p></li><li class="listitem"><p>
         grid_master_name
        </p></li><li class="listitem"><p>
         data_center_name
        </p></li></ul></div><p>
       You can also add multiple entries to the <code class="literal">grids</code>
       section. However, the upstream infoblox agent only supports a single
       grid currently.
      </p></dd><dt id="id-1.4.5.4.16.18.3.2"><span class="term">grid_defaults</span></dt><dd><p>
       This subsection contains the default settings that are used for each
       grid (unless you have configured specific settings within the
       <code class="literal">grids</code> section).
      </p></dd></dl></div><p>
    For detailed information on all infoblox-related configuration settings,
    see
    <a class="link" href="https://github.com/openstack/networking-infoblox/blob/master/doc/source/installation.rst" target="_blank">https://github.com/openstack/networking-infoblox/blob/master/doc/source/installation.rst</a>.
   </p><p>
    Currently, all configuration options for infoblox are only available in the
    <code class="literal">raw</code> mode of the neutron barclamp. To enable support for
    the infoblox IPAM plug-in and configure it, proceed as follows:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      <span class="guimenu">Edit</span> the neutron barclamp proposal or create a new
      one.
     </p></li><li class="step"><p>
      Click <span class="guimenu">Raw</span> and search for the following section:
     </p><div class="verbatim-wrap"><pre class="screen">"use_infoblox": false,</pre></div></li><li class="step"><p>
      To enable support for the infoblox IPAM plug-in, change this entry to:
     </p><div class="verbatim-wrap"><pre class="screen">"use_infoblox": true,</pre></div></li><li class="step"><p>
      In the <code class="literal">grids</code> section, configure at least one grid by
      replacing the example values for each parameter with real values.
     </p></li><li class="step"><p>
      If you need specific settings for a grid, add some of the parameters from
      the <code class="literal">grid_defaults</code> section to the respective grid entry
      and adjust their values.
     </p><p>
      Otherwise Crowbar applies the default setting to each grid when you save
      the barclamp proposal.
     </p></li><li class="step"><p>
      Save your changes and apply them.
     </p></li></ol></div></div></section><section class="sect2" id="sec-depl-ostack-network-ha" data-id-title="HA Setup for neutron"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.10.2 </span><span class="title-name">HA Setup for neutron</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-network-ha">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    neutron can be made highly available by deploying
    <span class="guimenu">neutron-server</span> and <span class="guimenu">neutron-network</span> on
    a cluster. While <span class="guimenu">neutron-server</span> may be deployed on a
    cluster shared with other services, it is strongly recommended to use a
    dedicated cluster solely for the <span class="guimenu">neutron-network</span> role.
   </p></section><section class="sect2" id="sec-setup-multi-ext-networks" data-id-title="Setting Up Multiple External Networks"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.10.3 </span><span class="title-name">Setting Up Multiple External Networks</span></span> <a title="Permalink" class="permalink" href="#sec-setup-multi-ext-networks">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   This section shows you how to create external networks on SUSE <span class="productname">OpenStack</span> Cloud.
  </p><section class="sect3" id="sec-config-multi-ext-networks" data-id-title="New Network Configurations"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">12.10.3.1 </span><span class="title-name">New Network Configurations</span></span> <a title="Permalink" class="permalink" href="#sec-config-multi-ext-networks">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      If you have not yet deployed Crowbar, add the following configuration to
      <code class="filename">/etc/crowbar/network.json</code>
      to set up an external network, using the name of your new network, VLAN
      ID, and network addresses. If you have already deployed Crowbar, then add
      this configuration to the <span class="guimenu">Raw</span> view of the Network Barclamp.
     </p><div class="verbatim-wrap"><pre class="screen">"<em class="replaceable">public2</em>": {
          "conduit": "intf1",
          "vlan": <em class="replaceable">600</em>,
          "use_vlan": true,
          "add_bridge": false,
          "subnet": "<em class="replaceable">192.168.135.128</em>",
          "netmask": "<em class="replaceable">255.255.255.128</em>",
          "broadcast": "<em class="replaceable">192.168.135.255</em>",
          "ranges": {
            "host": { "start": "<em class="replaceable">192.168.135.129</em>",
               "end": "<em class="replaceable">192.168.135.254</em>" }
          }
    },</pre></div></li><li class="step"><p>
      Modify the <em class="parameter">additional_external_networks</em> in the
      <span class="guimenu">Raw</span> view of the neutron Barclamp with the name of your
      new external network.
     </p></li><li class="step"><p>
       Apply both barclamps, and it may also be necessary to re-apply the nova
       Barclamp.
   </p></li><li class="step"><p>
      Then follow the steps in the next section to create the new external network.
    </p></li></ol></div></div></section><section class="sect3" id="sec-confignet" data-id-title="Create the New External Network"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">12.10.3.2 </span><span class="title-name">Create the New External Network</span></span> <a title="Permalink" class="permalink" href="#sec-confignet">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    The following steps add the network settings, including IP address pools,
    gateway, routing, and virtual switches to your new network.
   </p><div class="procedure" id="pro-confignet"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Set up interface mapping using either Open vSwitch (OVS) or Linuxbridge.
      For Open vSwitch run the following command:
     </p><div class="verbatim-wrap"><pre class="screen">openstack network create <em class="replaceable">public2</em> --provider:network_type flat \
 --provider:physical_network <em class="replaceable">public2</em> --router:external=True</pre></div><p>
      For Linuxbridge run the following command:
     </p><div class="verbatim-wrap"><pre class="screen"> openstack network create --router:external True --provider:physical_network physnet1 \
 --provider:network_type vlan --provider:segmentation_id <em class="replaceable">600</em></pre></div></li><li class="step"><p>
      If a different network is used then Crowbar will create a new interface
      mapping. Then you can use a flat network:
     </p><div class="verbatim-wrap"><pre class="screen">openstack network create <em class="replaceable">public2</em> --provider:network_type flat \
 --provider:physical_network <em class="replaceable">public2</em> --router:external=True</pre></div></li><li class="step"><p>
      Create a subnet:
     </p><div class="verbatim-wrap"><pre class="screen">openstack subnet create --name <em class="replaceable">public2</em> --allocation-pool \
 start=<em class="replaceable">192.168.135.2</em>,end=<em class="replaceable">192.168.135.127</em> --gateway <em class="replaceable">192.168.135.1</em> <em class="replaceable">public2</em> \
 <em class="replaceable">192.168.135.0/24</em> --enable_dhcp False</pre></div></li><li class="step"><p>
      Create a router, <em class="replaceable">router2</em>:
     </p><div class="verbatim-wrap"><pre class="screen">openstack router create <em class="replaceable">router2</em></pre></div></li><li class="step"><p>
      Connect <em class="replaceable">router2</em> to the new external network:
     </p><div class="verbatim-wrap"><pre class="screen">openstack router set <em class="replaceable">router2</em>  <em class="replaceable">public2</em></pre></div></li><li class="step"><p>
      Create a new private network and connect it to
      <em class="replaceable">router2</em>
     </p><div class="verbatim-wrap"><pre class="screen">openstack network create priv-net
openstack subnet create priv-net --gateway <em class="replaceable">10.10.10.1 10.10.10.0/24</em> \
 --name priv-net-sub
openstack router add subnet <em class="replaceable">router2</em> priv-net-sub</pre></div></li><li class="step"><p>
      Boot a VM on priv-net-sub and set a security group that allows SSH.
     </p></li><li class="step"><p>
      Assign a floating IP address to the VM, this time from network
      <em class="replaceable">public2</em>.
     </p></li><li class="step"><p>
      From the node verify that SSH is working by opening an SSH session to the
      VM.
     </p></li></ol></div></div></section><section class="sect3" id="sec-howbridges" data-id-title="How the Network Bridges are Created"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">12.10.3.3 </span><span class="title-name">How the Network Bridges are Created</span></span> <a title="Permalink" class="permalink" href="#sec-howbridges">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    For OVS, a new bridge will be created by Crowbar, in this case
    <code class="literal">br-public2</code>. In the bridge mapping the new network will
    be assigned to the bridge. The interface specified in
    <code class="filename">/etc/crowbar/network.json</code> (in this case eth0.600) will
    be plugged into <code class="literal">br-public2</code>. The new public network can
    be created in neutron using the new public network name as
    <em class="parameter">provider:physical_network</em>.
   </p><p>
    For Linuxbridge, Crowbar will check the interface associated with
    <em class="replaceable">public2</em>. If this is the same as physnet1 no
    interface mapping will be created. The new public network can be created in
    neutron using physnet1 as physical network and specifying the correct VLAN
    ID:
   </p><div class="verbatim-wrap"><pre class="screen">openstack network create <em class="replaceable">public2</em> --router:external True \
 --provider:physical_network physnet1 --provider:network_type vlan \
 --provider:segmentation_id <em class="replaceable">600</em></pre></div><p>
    A bridge named <code class="varname">brq-NET_ID</code> will be created and the
    interface specified in <code class="filename">/etc/crowbar/network.json</code> will
    be plugged into it. If a new interface is associated in
    <code class="filename">/etc/crowbar/network.json</code> with
    <em class="replaceable">public2</em> then Crowbar will add a new interface
    mapping and the second public network can be created using
    <em class="replaceable">public2</em> as the physical network:
   </p><div class="verbatim-wrap"><pre class="screen">openstack network create <em class="replaceable">public2</em> --provider:network_type flat \
 --provider:physical_network <em class="replaceable">public2</em> --router:external=True</pre></div></section></section></section><section class="sect1" id="sec-depl-ostack-nova" data-id-title="Deploying nova"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.11 </span><span class="title-name">Deploying nova</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-nova">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   nova provides key services for managing the SUSE <span class="productname">OpenStack</span> Cloud, sets up the
   Compute Nodes. SUSE <span class="productname">OpenStack</span> Cloud currently supports KVM and VMware vSphere. The
   unsupported QEMU option is included to enable test setups with virtualized
   nodes. The following attributes can be configured for nova:
   
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.17.3.1"><span class="term"><span class="guimenu">
      Scheduler Options: Virtual RAM to Physical RAM allocation ratio
     </span>
    </span></dt><dd><p>
      Set the <span class="quote">“<span class="quote">overcommit ratio</span>”</span> for RAM for instances on the
      Compute Nodes. A ratio of <code class="literal">1.0</code> means no overcommitment.
      Changing this value is not recommended.
     </p></dd><dt id="id-1.4.5.4.17.3.2"><span class="term"><span class="guimenu">
      Scheduler Options: Virtual CPU to Physical CPU allocation ratio
     </span>
    </span></dt><dd><p>
      Set the <span class="quote">“<span class="quote">overcommit ratio</span>”</span> for CPUs for instances on the
      Compute Nodes. A ratio of <code class="literal">1.0</code> means no overcommitment.
     </p></dd><dt id="id-1.4.5.4.17.3.3"><span class="term"><span class="guimenu">
      Scheduler Options: Virtual Disk to Physical Disk allocation ratio
     </span>
    </span></dt><dd><p>
      Set the <span class="quote">“<span class="quote">overcommit ratio</span>”</span> for virtual disks for instances
      on the Compute Nodes. A ratio of <code class="literal">1.0</code> means no
      overcommitment.
     </p></dd><dt id="id-1.4.5.4.17.3.4"><span class="term"><span class="guimenu">
      Scheduler Options: Reserved Memory for nova-compute hosts (MB)
     </span>
    </span></dt><dd><p>
      Amount of reserved host memory that is not used for allocating VMs by
      <code class="literal">nova-compute</code>.
     </p></dd><dt id="id-1.4.5.4.17.3.5"><span class="term"><span class="guimenu">Live Migration Support: Enable Libvirt Migration</span>
    </span></dt><dd><p>
      Allows to move KVM instances to a different Compute Node
      running the same hypervisor (cross hypervisor migrations are not
      supported). Useful when a Compute Node needs to be shut down or rebooted
      for maintenance or when the load of the Compute Node is very high.
      Instances can be moved while running (Live Migration).
     </p><div id="id-1.4.5.4.17.3.5.2.2" data-id-title="Libvirt Migration and Security" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Libvirt Migration and Security</div><p>
       Enabling the libvirt migration option will open a TCP port on the
       Compute Nodes that allows access to all instances from all machines in
       the admin network. Ensure that only authorized machines have access to
       the admin network when enabling this option.
      </p></div><div id="id-1.4.5.4.17.3.5.2.3" data-id-title="Specifying Network for Live Migration" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Specifying Network for Live Migration</div><p>
        It is possible to change a network to live migrate images. This is done
        in the raw view of the nova barclamp. In the
        <code class="literal">migration</code> section, change the
        <code class="varname">network</code> attribute to the appropriate value (for
        example, <code class="literal">storage</code> for Ceph).
      </p></div></dd><dt id="id-1.4.5.4.17.3.6"><span class="term"><span class="guimenu">KVM Options: Enable Kernel Samepage Merging</span>
    </span></dt><dd><p>
      Kernel SamePage Merging (KSM) is a Linux Kernel feature which merges
      identical memory pages from multiple running processes into one memory
      region. Enabling it optimizes memory usage on the Compute Nodes when using
      the KVM hypervisor at the cost of slightly increasing CPU usage.
     </p></dd><dt id="id-1.4.5.4.17.3.7"><span class="term">SSL Support: Protocol</span></dt><dd><p>
      Choose whether to encrypt public communication (<span class="guimenu">HTTPS</span>)
      or not (<span class="guimenu">HTTP</span>). If choosing
      <span class="guimenu">HTTPS</span>,refer to
      <a class="xref" href="#sec-depl-ostack-keystone-ssl">SSL Support: Protocol
    </a> for configuration details.
     </p></dd><dt id="id-1.4.5.4.17.3.8"><span class="term">VNC Settings: NoVNC Protocol</span></dt><dd><p>
      After having started an instance you can display its VNC console in the
      <span class="productname">OpenStack</span> Dashboard (horizon) via the browser using the noVNC
      implementation. By default this connection is not encrypted and can
      potentially be eavesdropped.
     </p><p>
      Enable encrypted communication for noVNC by choosing
      <span class="guimenu">HTTPS</span> and providing the locations for the certificate
      key pair files.
     </p></dd><dt id="id-1.4.5.4.17.3.9"><span class="term"><span class="guimenu">Logging: Verbose Logging</span>
    </span></dt><dd><p>
      Shows debugging output in the log files when set to
      <span class="guimenu">true</span>.
     </p></dd></dl></div><div id="note-custom-vendor" data-id-title="Custom Vendor Data for Instances" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Custom Vendor Data for Instances</div><p>
    You can pass custom vendor data to all VMs via nova's metadata server.
    For example, information about a custom SMT server can be used by the
    SUSE guest images to automatically configure the repositories for the
    guest.
   </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
      To pass custom vendor data, switch to the <span class="guimenu">Raw</span> view of
      the nova barclamp.
     </p></li><li class="listitem"><p>
      Search for the following section:
     </p><div class="verbatim-wrap"><pre class="screen">"metadata": {
  "vendordata": {
    "json": "{}"
  }
}</pre></div></li><li class="listitem"><p>
      As value of the <code class="literal">json</code> entry, enter valid JSON data. For
      example:
     </p><div class="verbatim-wrap"><pre class="screen">"metadata": {
  "vendordata": {
    "json": "{\"<em class="replaceable">CUSTOM_KEY</em>\": \"<em class="replaceable">CUSTOM_VALUE</em>\"}"
  }
}</pre></div><p>
      The string needs to be escaped because the barclamp file is in JSON
      format, too.
     </p></li></ol></div><p>
    Use the following command to access the custom vendor data from inside a
    VM:
   </p><div class="verbatim-wrap"><pre class="screen">curl -s http://<em class="replaceable">METADATA_SERVER</em>/openstack/latest/vendor_data.json</pre></div><p>
    The IP address of the metadata server is always the same from within a VM.
    For more details, see
    <a class="link" href="https://www.suse.com/communities/blog/vms-get-access-metadata-neutron/" target="_blank">https://www.suse.com/communities/blog/vms-get-access-metadata-neutron/</a>.
   </p></div><div class="figure" id="id-1.4.5.4.17.5"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_nova.png"><img src="images/depl_barclamp_nova.png" width="75%" alt="The nova Barclamp" title="The nova Barclamp"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.18: </span><span class="title-name">The nova Barclamp </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.17.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><p>
   The nova component consists of eight different roles:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.17.7.1"><span class="term"><span class="guimenu">nova-controller</span>
    </span></dt><dd><p>
      Distributing and scheduling the instances is managed by the
      <span class="guimenu">nova-controller</span>. It also provides networking and
      messaging services. <span class="guimenu">nova-controller</span> needs to be
      installed on a Control Node.
     </p></dd><dt id="id-1.4.5.4.17.7.2"><span class="term"><span class="guimenu">nova-compute-kvm</span> /
    <span class="guimenu">nova-compute-qemu</span> /
    <span class="guimenu">nova-compute-vmware</span> /
    </span></dt><dd><p>
      Provides the hypervisors (KVM, QEMU, VMware vSphere, and z/VM)
      and tools needed to manage the instances. Only one hypervisor can be
      deployed on a single compute node. To use different hypervisors in your
      cloud, deploy different hypervisors to different Compute Nodes. A
      <code class="literal">nova-compute-*</code> role needs to be installed on every
      Compute Node. However, not all hypervisors need to be deployed.
     </p><p>
      Each image that will be made available in SUSE <span class="productname">OpenStack</span> Cloud to start an instance
      is bound to a hypervisor. Each hypervisor can be deployed on multiple
      Compute Nodes (except for the VMware vSphere role, see below). In a
      multi-hypervisor deployment you should make sure to deploy the
      <code class="literal">nova-compute-*</code> roles in a way, that enough compute
      power is available for each hypervisor.
     </p><div id="id-1.4.5.4.17.7.2.2.3" data-id-title="Re-assigning Hypervisors" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Re-assigning Hypervisors</div><p>
       Existing <code class="literal">nova-compute-*</code> nodes can be changed in a
       production SUSE <span class="productname">OpenStack</span> Cloud without service interruption. You need to
       <span class="quote">“<span class="quote">evacuate</span>”</span>
       the node, re-assign a new <code class="literal">nova-compute</code> role via the
       nova barclamp and <span class="guimenu">Apply</span> the change.
       <span class="guimenu">nova-compute-vmware</span> can only be deployed on a single
       node.
      </p></div></dd></dl></div><div class="figure" id="id-1.4.5.4.17.8"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_nova_node_deployment.png"><img src="images/depl_barclamp_nova_node_deployment.png" width="75%" alt="The nova Barclamp: Node Deployment Example with Two KVM Nodes" title="The nova Barclamp: Node Deployment Example with Two KVM Nodes"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.19: </span><span class="title-name">The nova Barclamp: Node Deployment Example with Two KVM Nodes </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.17.8">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><p>
    When deploying a <span class="guimenu">nova-compute-vmware</span> node with the
    <span class="guimenu">vmware_dvs</span> ML2 driver enabled in the neutron barclamp, the following
    new attributes are also available in the <span class="guimenu">vcenter</span> section of the
    <span class="guimenu">Raw</span> mode:<span class="guimenu">dvs_name</span> (the name of the
    DVS switch configured on the target vCenter cluster) and
    <span class="guimenu">dvs_security_groups</span> (enable or disable implementing
    security groups through DVS traffic rules).
  </p><p>
    It is important to specify the correct <span class="guimenu">dvs_name</span>
    value, as the barclamp expects the  DVS switch to be preconfigured on the
    target VMware vCenter cluster.
  </p><div id="id-1.4.5.4.17.11" data-id-title="vmware_dvs must be enabled" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: vmware_dvs must be enabled</div><p>
      Deploying <span class="guimenu">nova-compute-vmware</span> nodes will not result in
      a functional cloud setup if the <span class="guimenu">vmware_dvs</span> ML2 plugin
      is not enabled in the neutron barclamp.
     </p></div><section class="sect2" id="sec-depl-ostack-nova-ha" data-id-title="HA Setup for nova"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.11.1 </span><span class="title-name">HA Setup for nova</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-nova-ha">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    Making <span class="guimenu">nova-controller</span> highly available requires no
    special configuration—it is sufficient to deploy it on a cluster.
   </p><p>
    To enable High Availability for Compute Nodes, deploy the following roles to one or more
    clusters with remote nodes:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      nova-compute-kvm
     </p></li><li class="listitem"><p>
      nova-compute-qemu
     </p></li><li class="listitem"><p>
      ec2-api
     </p></li></ul></div><p>
    The cluster to which you deploy the roles above can be completely
    independent of the one to which the role <code class="literal">nova-controller</code>
    is deployed.
   </p><p>
    However, the <code class="literal">nova-controller</code> and
    <code class="literal">ec2-api</code> roles must be deployed the same way (either
    <span class="emphasis"><em>both</em></span> to a cluster or <span class="emphasis"><em>both</em></span> to
    individual nodes. This is due to Crowbar design limitations.
   </p><div id="id-1.4.5.4.17.12.7" data-id-title="Shared Storage" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Shared Storage</div><p>
     It is recommended to use shared storage for the
     <code class="filename">/var/lib/nova/instances</code> directory, to ensure that
     ephemeral disks will be preserved during recovery of VMs from failed
     compute nodes. Without shared storage, any ephemeral disks will be lost,
     and recovery will rebuild the VM from its original image.
    </p><p>
     If an external NFS server is used, enable the following option in the
     nova barclamp proposal: <span class="guimenu">Shared Storage for nova instances has
     been manually configured</span>.
    </p></div></section></section><section class="sect1" id="sec-depl-ostack-dash" data-id-title="Deploying horizon (OpenStack Dashboard)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.12 </span><span class="title-name">Deploying horizon (<span class="productname">OpenStack</span> Dashboard)</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-dash">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   The last component that needs to be deployed is horizon, the <span class="productname">OpenStack</span>
   Dashboard. It provides a Web interface for users to start and stop instances
   and for administrators to manage users, groups, roles, etc. horizon should
   be installed on a Control Node. To make horizon highly available, deploy it
   on a cluster.
  </p><p>
   The following attributes can be configured:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.18.4.1"><span class="term">Session Timeout</span></dt><dd><p>
      Timeout (in minutes) after which a user is been logged out automatically.
      The default value is set to four hours (240 minutes).
     </p><div id="id-1.4.5.4.18.4.1.2.2" data-id-title="Timeouts Larger than Four Hours" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Timeouts Larger than Four Hours</div><p>
       Every horizon session requires a valid keystone token. These tokens
       also have a lifetime of four hours (14400 seconds). Setting the horizon
       session timeout to a value larger than 240 will therefore have no
       effect, and you will receive a warning when applying the barclamp.
      </p><p>
       To successfully apply a timeout larger than four hours, you first need
       to adjust the keystone token expiration accordingly. To do so, open the
       keystone barclamp in <span class="guimenu">Raw</span> mode and adjust the value of
       the key <code class="literal">token_expiration</code>. Note that the value has to
       be provided in <span class="emphasis"><em>seconds</em></span>. When the change is
       successfully applied, you can adjust the horizon session timeout (in
       <span class="emphasis"><em>minutes</em></span>). Note that extending the keystone token
       expiration may cause scalability issues in large and very busy SUSE <span class="productname">OpenStack</span> Cloud
       installations.
      </p></div></dd><dt id="id-1.4.5.4.18.4.2"><span class="term"><span class="guimenu">
      User Password Validation: Regular expression used for password
      validation
     </span>
    </span></dt><dd><p>
      Specify a regular expression with which to check the password. The
      default expression (<code class="literal">.{8,}</code>) tests for a minimum length
      of 8 characters. The string you enter is interpreted as a Python regular
      expression (see
      <a class="link" href="http://docs.python.org/2.7/library/re.html#module-re" target="_blank">http://docs.python.org/2.7/library/re.html#module-re</a>
      for a reference).
     </p></dd><dt id="id-1.4.5.4.18.4.3"><span class="term"><span class="guimenu">
      User Password Validation: Text to display if the password does not pass
      validation
     </span>
    </span></dt><dd><p>
      Error message that will be displayed in case the password validation
      fails.
     </p></dd><dt id="id-1.4.5.4.18.4.4"><span class="term">SSL Support: Protocol</span></dt><dd><p>
      Choose whether to encrypt public communication (<span class="guimenu">HTTPS</span>)
      or not (<span class="guimenu">HTTP</span>). If choosing <span class="guimenu">HTTPS</span>,
      you have two choices. You can either <span class="guimenu">Generate (self-signed)
      certificates</span> or provide the locations for the certificate key
      pair files and,—optionally— the certificate chain file. Using
      self-signed certificates is for testing purposes only and should never be
      used in production environments!
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.18.5"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_nova_dashboard.png"><img src="images/depl_barclamp_nova_dashboard.png" width="75%" alt="The horizon Barclamp" title="The horizon Barclamp"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.20: </span><span class="title-name">The horizon Barclamp </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.18.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><section class="sect2" id="sec-depl-ostack-dash-ha" data-id-title="HA Setup for horizon"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.12.1 </span><span class="title-name">HA Setup for horizon</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-dash-ha">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    Making horizon highly available requires no special configuration—it
    is sufficient to deploy it on a cluster.
   </p></section></section><section class="sect1" id="sec-depl-ostack-heat" data-id-title="Deploying heat (Optional)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.13 </span><span class="title-name">Deploying heat (Optional)</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-heat">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   heat is a template-based orchestration engine that enables you to, for
   example, start workloads requiring multiple servers or to automatically
   restart instances if needed. It also brings auto-scaling to SUSE <span class="productname">OpenStack</span> Cloud by
   automatically starting additional instances if certain criteria are met.
   For more information about heat refer to the <span class="productname">OpenStack</span> documentation at
   <a class="link" href="http://docs.openstack.org/developer/heat/" target="_blank">http://docs.openstack.org/developer/heat/</a>.
  </p><p>
   heat should be deployed on a Control Node. To make heat highly
   available, deploy it on a cluster.
  </p><p>
   The following attributes can be configured for heat:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.19.5.1"><span class="term"><span class="guimenu">Verbose Logging</span>
    </span></dt><dd><p>
      Shows debugging output in the log files when set to
      <span class="guimenu">true</span>.
     </p></dd><dt id="id-1.4.5.4.19.5.2"><span class="term">SSL Support: Protocol</span></dt><dd><p>
      Choose whether to encrypt public communication (<span class="guimenu">HTTPS</span>)
      or not (<span class="guimenu">HTTP</span>). If choosing
      <span class="guimenu">HTTPS</span>, refer to
      <a class="xref" href="#sec-depl-ostack-keystone-ssl">SSL Support: Protocol
    </a> for configuration details.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.19.6"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_heat.png"><img src="images/depl_barclamp_heat.png" width="75%" alt="The heat Barclamp" title="The heat Barclamp"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.21: </span><span class="title-name">The heat Barclamp </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.19.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><section class="sect2" id="sec-depl-ostack-heat-delegated-roles" data-id-title="Enabling Identity Trusts Authorization (Optional)"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.13.1 </span><span class="title-name">Enabling Identity Trusts Authorization (Optional)</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-heat-delegated-roles">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    heat uses keystone Trusts to delegate a subset of user roles to the
    heat engine for deferred operations (see
    <a class="link" href="http://hardysteven.blogspot.de/2014/04/heat-auth-model-updates-part-1-trusts.html" target="_blank">Steve
    Hardy's blog</a> for details). It can either delegate all user roles or
    only those specified in the <code class="literal">trusts_delegated_roles</code>
    setting. Consequently, all roles listed in
    <code class="literal">trusts_delegated_roles</code> need to be assigned to a user,
    otherwise the user will not be able to use heat.
   </p><p>
    The recommended setting for <code class="literal">trusts_delegated_roles</code> is
    <code class="literal">member</code>, since this is the default role most users are
    likely to have. This is also the default setting when installing SUSE <span class="productname">OpenStack</span> Cloud
    from scratch.
   </p><p>
    On installations where this setting is introduced through an upgrade,
    <code class="literal">trusts_delegated_roles</code> will be set to
    <code class="literal">heat_stack_owner</code>. This is a conservative choice to
    prevent breakage in situations where unprivileged users may already have
    been assigned the <code class="literal">heat_stack_owner</code> role to enable them
    to use heat but lack the <code class="literal">member</code> role. As long as you can
    ensure that all users who have the <code class="literal">heat_stack_owner</code> role
    also have the <code class="literal">member</code> role, it is both safe and
    recommended to change trusts_delegated_roles to <code class="literal">member</code>.
    
   </p><div id="id-1.4.5.4.19.7.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
       If the Octavia barclamp is deployed, the <code class="literal">trusts_delegated_roles</code>
       configuration option either needs to be set to an empty value, or the
       <code class="literal">load-balancer_member</code> role needs to be included, otherwise
       it won't be possible to create Octavia load balancers via heat stacks.
       Refer to the <a class="xref" href="#sec-depl-ostack-octavia-migrate-users" title="12.20.3. Migrating Users to Octavia">Section 12.20.3, “Migrating Users to Octavia”</a> section for more
       details on the list of specialized roles employed by Octavia.
       Also note that adding the <code class="literal">load-balancer_member</code> role
       to the <code class="literal">trusts_delegated_roles</code> list has the undesired
       side effect that only users that have this role assigned to them will be
       allowed to access the Heat API, as covered previously in this section.
     </p></div><p>
    To view or change the trusts_delegated_role setting you need to open the
    heat barclamp and click <span class="guimenu">Raw</span> in the
    <span class="guimenu">Attributes</span> section. Search for the
    <code class="literal">trusts_delegated_roles</code> setting and modify the list of
    roles as desired.
   </p><div class="figure" id="id-1.4.5.4.19.7.7"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_heat_raw.png"><img src="images/depl_barclamp_heat_raw.png" width="75%" alt="the heat barclamp: Raw Mode" title="the heat barclamp: Raw Mode"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.22: </span><span class="title-name">the heat barclamp: Raw Mode </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.19.7.7">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><div id="id-1.4.5.4.19.7.8" data-id-title="Empty Value" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Empty Value</div><p>
     An empty value for <code class="literal">trusts_delegated_roles</code> will delegate
     <span class="emphasis"><em>all</em></span> of user roles to heat. This may create a security
     risk for users who are assigned privileged roles, such as
     <code class="literal">admin</code>, because these privileged roles will also be
     delegated to the heat engine when these users create heat stacks.
    </p></div></section><section class="sect2" id="sec-depl-ostack-heat-ha" data-id-title="HA Setup for heat"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.13.2 </span><span class="title-name">HA Setup for heat</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-heat-ha">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    Making heat highly available requires no special configuration—it
    is sufficient to deploy it on a cluster.
   </p></section></section><section class="sect1" id="sec-depl-ostack-ceilometer" data-id-title="Deploying ceilometer (Optional)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.14 </span><span class="title-name">Deploying ceilometer (Optional)</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-ceilometer">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   ceilometer collects CPU and networking data from SUSE <span class="productname">OpenStack</span> Cloud. This data can be
   used by a billing system to enable customer billing. Deploying ceilometer is
   optional. ceilometer agents use monasca database to store collected data.
  </p><p>
   For more information about ceilometer refer to the <span class="productname">OpenStack</span> documentation at
   <a class="link" href="http://docs.openstack.org/developer/ceilometer/" target="_blank">http://docs.openstack.org/developer/ceilometer/</a>.
  </p><div id="id-1.4.5.4.20.4" data-id-title="ceilometer Restrictions" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: ceilometer Restrictions</div><p>
    As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8 data measuring is only supported for
    KVM and Windows instances. Other hypervisors and SUSE <span class="productname">OpenStack</span> Cloud features
    such as object or block storage will not be measured.
   </p></div><p>
   The following attributes can be configured for ceilometer:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.20.6.1"><span class="term">
       Intervals used for <span class="productname">OpenStack</span> Compute, Image, or Block Storage meter updates (in seconds)
     </span></dt><dd><p>
      Specify intervals in seconds after which ceilometer performs updates of
      specified meters.
     </p></dd><dt id="id-1.4.5.4.20.6.2"><span class="term">How long are metering samples kept in the database (in days)
    </span></dt><dd><p>
      Specify how long to keep the metering data. <code class="literal">-1</code> means that samples are
      kept in the database forever.
     </p></dd><dt id="id-1.4.5.4.20.6.3"><span class="term">How long are event samples kept in the database (in days)
    </span></dt><dd><p>
      Specify how long to keep the event data. <code class="literal">-1</code> means that samples are
      kept in the database forever.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.20.7"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_ceilometer.png"><img src="images/depl_barclamp_ceilometer.png" width="75%" alt="The ceilometer Barclamp" title="The ceilometer Barclamp"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.23: </span><span class="title-name">The ceilometer Barclamp </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.20.7">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><p>
   The ceilometer component consists of four different roles:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.20.9.1"><span class="term"><span class="guimenu">ceilometer-server</span>
    </span></dt><dd><p>
      The notification agent.
     </p></dd><dt id="id-1.4.5.4.20.9.2"><span class="term"><span class="guimenu">ceilometer-central</span>
    </span></dt><dd><p>
      The polling agent listens to the message bus to collect data. It needs to
      be deployed on a Control Node. It can be deployed on the same node as
      <span class="guimenu">ceilometer-server</span>.
     </p></dd><dt id="id-1.4.5.4.20.9.3"><span class="term"><span class="guimenu">ceilometer-agent</span>
    </span></dt><dd><p>
      The compute agents collect data from the compute nodes. They need to be
      deployed on all KVM compute nodes in your cloud (other
      hypervisors are currently not supported).
     </p></dd><dt id="id-1.4.5.4.20.9.4"><span class="term"><span class="guimenu">ceilometer-swift-proxy-middleware</span>
    </span></dt><dd><p>
      An agent collecting data from the swift nodes. This role needs to be
      deployed on the same node as swift-proxy.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.20.10"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_ceilometer_deployment.png"><img src="images/depl_barclamp_ceilometer_deployment.png" width="75%" alt="The ceilometer Barclamp: Node Deployment" title="The ceilometer Barclamp: Node Deployment"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.24: </span><span class="title-name">The ceilometer Barclamp: Node Deployment </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.20.10">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><section class="sect2" id="sec-depl-ostack-ceilometer-ha" data-id-title="HA Setup for ceilometer"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.14.1 </span><span class="title-name">HA Setup for ceilometer</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-ceilometer-ha">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    Making ceilometer highly available requires no special
    configuration—it is sufficient to deploy the roles
    <span class="guimenu">ceilometer-server</span> and
    <span class="guimenu">ceilometer-central</span> on a cluster.
   </p></section></section><section class="sect1" id="sec-depl-ostack-manila" data-id-title="Deploying manila"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.15 </span><span class="title-name">Deploying manila</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-manila">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   manila provides coordinated access to shared or distributed file
   systems, similar to what cinder does for block storage. These file
   systems can be shared between instances in SUSE <span class="productname">OpenStack</span> Cloud.
  </p><p>
   manila uses different back-ends. As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8 currently
   supported back-ends include <span class="guimenu">Hitachi HNAS</span>, <span class="guimenu">NetApp
   Driver</span>, and <span class="guimenu">CephFS</span>. Two more back-end options,
   <span class="guimenu">Generic Driver</span> and <span class="guimenu">Other Driver</span> are
   available for testing purposes and are not supported.
  </p><div id="note-limit-cephfs" data-id-title="Limitations for CephFS Back-end" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Limitations for CephFS Back-end</div><p>
    manila uses some CephFS features that are currently
    <span class="emphasis"><em>not</em></span> supported by the SUSE Linux Enterprise Server 12 SP4 CephFS kernel
    client:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      RADOS namespaces
     </p></li><li class="listitem"><p>
      MDS path restrictions
     </p></li><li class="listitem"><p>
      Quotas
     </p></li></ul></div><p>
    As a result, to access CephFS shares provisioned by manila, you must
    use ceph-fuse. For details, see
    <a class="link" href="http://docs.openstack.org/developer/manila/devref/cephfs_native_driver.html" target="_blank">http://docs.openstack.org/developer/manila/devref/cephfs_native_driver.html</a>.
   </p></div><p>
   When first opening the manila barclamp, the default proposal
   <span class="guimenu">Generic Driver</span> is already available for configuration. To
   replace it, first delete it by clicking the trashcan icon and then choose a
   different back-end in the section <span class="guimenu">Add new manila Backend</span>.
   Select a <span class="guimenu">Type of Share</span> and—optionally—provide
   a <span class="guimenu">Name for Backend</span>. Activate the back-end with
   <span class="guimenu">Add Backend</span>. Note that at least one back-end must be
   configured.
  </p><p>
   The attributes that can be set to configure cinder depend on the back-end:
  </p><div class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.21.7"><span class="name"><span class="guimenu">Back-end: Generic</span>
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.21.7">#</a></h3></div><p>
   The generic driver is included as a technology preview and is not supported.
  </p><div class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.21.9"><span class="name"><span class="guimenu">Hitachi HNAS</span>
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.21.9">#</a></h3></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.21.10.1"><span class="term"><span class="guimenu">Specify which EVS this backend is assigned to</span>
    </span></dt><dd><p>
      Provide the name of the Enterprise Virtual Server that the selected
      back-end is assigned to.
      
     </p></dd><dt id="id-1.4.5.4.21.10.2"><span class="term"><span class="guimenu">Specify IP for mounting shares</span>
    </span></dt><dd><p>
      IP address for mounting shares.
      
     </p></dd><dt id="id-1.4.5.4.21.10.3"><span class="term"><span class="guimenu">Specify file-system name for creating shares</span>
    </span></dt><dd><p>
      Provide a file-system name for creating shares.
      
     </p></dd><dt id="id-1.4.5.4.21.10.4"><span class="term"><span class="guimenu">HNAS management interface IP</span>
    </span></dt><dd><p>
      IP address of the HNAS management interface for communication between
      manila controller and HNAS.
     </p></dd><dt id="id-1.4.5.4.21.10.5"><span class="term"><span class="guimenu">HNAS username Base64 String</span>
    </span></dt><dd><p>
      HNAS username Base64 String required to perform tasks like creating
      file-systems and network interfaces.
     </p></dd><dt id="id-1.4.5.4.21.10.6"><span class="term"><span class="guimenu">HNAS user password</span>
    </span></dt><dd><p>
      HNAS user password. Required only if private key is not provided.
      
     </p></dd><dt id="id-1.4.5.4.21.10.7"><span class="term"><span class="guimenu">RSA/DSA private key</span>
    </span></dt><dd><p>
      RSA/DSA private key necessary for connecting to HNAS. Required only if
      password is not provided.
      
     </p></dd><dt id="id-1.4.5.4.21.10.8"><span class="term"><span class="guimenu">The time to wait for stalled HNAS jobs before aborting</span>
    </span></dt><dd><p>
      Time in seconds to wait before aborting stalled HNAS jobs.
     </p></dd></dl></div><div class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.21.11"><span class="name"><span class="guimenu">Back-end: Netapp</span>
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.21.11">#</a></h3></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.21.12.1"><span class="term"><span class="guimenu">Name of the Virtual Storage Server (vserver)</span>
    </span></dt><dd><p>
      Host name of the Virtual Storage Server.
     </p></dd><dt id="id-1.4.5.4.21.12.2"><span class="term"><span class="guimenu">Server Host Name</span>
    </span></dt><dd><p>
      The name or IP address for the storage controller or the cluster.
     </p></dd><dt id="id-1.4.5.4.21.12.3"><span class="term"><span class="guimenu">Server Port</span>
    </span></dt><dd><p>
      The port to use for communication. Port 80 is usually used for HTTP, 443
      for HTTPS.
     </p></dd><dt id="id-1.4.5.4.21.12.4"><span class="term"><span class="guimenu">User name/Password for Accessing NetApp</span>
    </span></dt><dd><p>
      Login credentials.
     </p></dd><dt id="id-1.4.5.4.21.12.5"><span class="term"><span class="guimenu">Transport Type</span>
    </span></dt><dd><p>
      Transport protocol for communicating with the storage controller or
      cluster. Supported protocols are HTTP and HTTPS. Choose the protocol your
      NetApp is licensed for.
     </p></dd></dl></div><div class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.21.13"><span class="name"><span class="guimenu">Back-end: CephFS</span>
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.21.13">#</a></h3></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.21.14.1"><span class="term">Use Ceph deployed by Crowbar</span></dt><dd><p>
      Set to <code class="systemitem">true</code> to use Ceph deployed with Crowbar.
     </p></dd></dl></div><div class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.21.15"><span class="name"><span class="guimenu">Back-end: Manual</span>
  </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.21.15">#</a></h3></div><p>
   Lets you manually pick and configure a driver. Only use this option for
   testing purposes, it is not supported.
  </p><div class="figure" id="id-1.4.5.4.21.17"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_manila.png"><img src="images/depl_barclamp_manila.png" width="75%" alt="The manila Barclamp" title="The manila Barclamp"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.25: </span><span class="title-name">The manila Barclamp </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.21.17">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><p>
   The manila component consists of two different roles:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.21.19.1"><span class="term"><span class="guimenu">manila-server</span>
    </span></dt><dd><p>
      The manila server provides the scheduler and the API. Installing it
      on a Control Node is recommended.
     </p></dd><dt id="id-1.4.5.4.21.19.2"><span class="term"><span class="guimenu">manila-share</span>
    </span></dt><dd><p>
      The shared storage service. It can be installed on a Control Node, but it
      is recommended to deploy it on one or more dedicated nodes supplied with
      sufficient disk space and networking capacity, since it will generate a
      lot of network traffic.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.21.20"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_manila_node_deployment.png"><img src="images/depl_barclamp_manila_node_deployment.png" width="75%" alt="The manila Barclamp: Node Deployment Example" title="The manila Barclamp: Node Deployment Example"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.26: </span><span class="title-name">The manila Barclamp: Node Deployment Example </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.21.20">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><section class="sect2" id="sec-depl-ostack-manila-ha" data-id-title="HA Setup for manila"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.15.1 </span><span class="title-name">HA Setup for manila</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-manila-ha">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    While the <span class="guimenu">manila-server</span> role can be deployed on a
    cluster, deploying <span class="guimenu">manila-share</span> on a cluster is not
    supported. Therefore it is generally recommended to deploy
    <span class="guimenu">manila-share</span> on several nodes—this ensures the
    service continues to be available even when a node fails.
   </p></section></section><section class="sect1" id="sec-depl-ostack-tempest" data-id-title="Deploying Tempest (Optional)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.16 </span><span class="title-name">Deploying Tempest (Optional)</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-tempest">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   Tempest is an integration test suite for SUSE <span class="productname">OpenStack</span> Cloud written in Python. It
   contains multiple integration tests for validating your SUSE <span class="productname">OpenStack</span> Cloud deployment.
   For more information about Tempest refer to the <span class="productname">OpenStack</span> documentation
   at <a class="link" href="http://docs.openstack.org/developer/tempest/" target="_blank">http://docs.openstack.org/developer/tempest/</a>.
  </p><div id="id-1.4.5.4.22.3" data-id-title="Technology Preview" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Technology Preview</div><p>
    Tempest is only included as a technology preview and not supported.
   </p><p>
    Tempest may be used for testing whether the intended setup will run
    without problems. It should not be used in a production environment.
   </p></div><p>
   Tempest should be deployed on a Control Node.
  </p><p>
   The following attributes can be configured for Tempest:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.22.6.1"><span class="term"><span class="guimenu">Choose User name / Password</span>
    </span></dt><dd><p>
      Credentials for a regular user. If the user does not exist, it will be
      created.
     </p></dd><dt id="id-1.4.5.4.22.6.2"><span class="term"><span class="guimenu">Choose Tenant</span>
    </span></dt><dd><p>
      Tenant to be used by Tempest. If it does not exist, it will be
      created. It is safe to stick with the default value.
     </p></dd><dt id="id-1.4.5.4.22.6.3"><span class="term"><span class="guimenu">Choose Tempest Admin User name/Password</span>
    </span></dt><dd><p>
      Credentials for an admin user. If the user does not exist, it will be
      created.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.22.7"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_tempest.png"><img src="images/depl_barclamp_tempest.png" width="75%" alt="The Tempest Barclamp" title="The Tempest Barclamp"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.27: </span><span class="title-name">The Tempest Barclamp </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.22.7">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><div id="id-1.4.5.4.22.8" data-id-title="Running Tests" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Running Tests</div><p>
    To run tests with Tempest, log in to the Control Node on which
    Tempest was deployed. Change into the directory
    <code class="filename">/var/lib/openstack-tempest-test</code>. To get an overview of
    available commands, run:
   </p><div class="verbatim-wrap"><pre class="screen">./tempest --help</pre></div><p>
    To serially invoke a subset of all tests (<span class="quote">“<span class="quote">the gating
    smoketests</span>”</span>) to help validate the working functionality of your
    local cloud instance, run the following command. It will save the output to
    a log file
    <code class="filename">tempest_<em class="replaceable">CURRENT_DATE</em>.log</code>.
   </p><div class="verbatim-wrap"><pre class="screen">./tempest run --smoke --serial 2&gt;&amp;1 \
| tee "tempest_$(date +%Y-%m-%d_%H%M%S).log"</pre></div></div><section class="sect2" id="sec-depl-ostack-tempest-ha" data-id-title="HA Setup for Tempest"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.16.1 </span><span class="title-name">HA Setup for Tempest</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-tempest-ha">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    Tempest cannot be made highly available.
   </p></section></section><section class="sect1" id="sec-depl-ostack-magnum" data-id-title="Deploying Magnum (Optional)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.17 </span><span class="title-name">Deploying Magnum (Optional)</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-magnum">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   Magnum is an <span class="productname">OpenStack</span> project which offers container orchestration
   engines for deploying and managing containers as first class resources in
   <span class="productname">OpenStack</span>.
  </p><p>
   For more information about Magnum, see the <span class="productname">OpenStack</span> documentation at
   <a class="link" href="http://docs.openstack.org/developer/magnum/" target="_blank">http://docs.openstack.org/developer/magnum/</a>.
  </p><p>
   For information on how to deploy a Kubernetes cluster (either from command
   line or from the horizon Dashboard), see the <em class="citetitle">Supplement to <em class="citetitle">Administrator Guide</em> and <em class="citetitle">User Guide</em></em>. It is available
   from <a class="link" href="https://documentation.suse.com/soc/9/" target="_blank">https://documentation.suse.com/soc/9/</a>.
  </p><p>
   The following <span class="guimenu">Attributes</span> can be configured for
   Magnum:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.23.6.1"><span class="term"><span class="guimenu">Trustee Domain</span>: <span class="guimenu">Delegate trust to
    cluster users if required</span></span></dt><dd><p>
      Deploying Kubernetes clusters in a cloud without an Internet connection
      requires the <code class="literal">registry_enabled</code> option in its cluster
      template set to <code class="literal">true</code>. To make this offline scenario
      work, you also need to set the <span class="guimenu">Delegate trust to cluster users if
      required</span> option to <code class="literal">true</code>. This restores the old,
      insecure behavior for clusters with the
      <code class="literal">registry-enabled</code> or <code class="literal">volume_driver=Rexray</code> options enabled.
     </p></dd><dt id="id-1.4.5.4.23.6.2"><span class="term"><span class="guimenu">Trustee Domain</span>: <span class="guimenu">Domain Name</span></span></dt><dd><p>
      Domain name to use for creating trustee for bays.
     </p></dd><dt id="id-1.4.5.4.23.6.3"><span class="term"><span class="guimenu">Logging</span>: <span class="guimenu">Verbose</span></span></dt><dd><p>
      Increases the amount of information that is written to the log files when
      set to <span class="guimenu">true</span>.
     </p></dd><dt id="id-1.4.5.4.23.6.4"><span class="term"><span class="guimenu">Logging</span>: <span class="guimenu">Debug</span></span></dt><dd><p>
      Shows debugging output in the log files when set to <span class="guimenu">true</span>.
     </p></dd><dt id="id-1.4.5.4.23.6.5"><span class="term"><span class="guimenu">Certificate Manager</span>: <span class="guimenu">Plugin</span>
    </span></dt><dd><p>
      To store certificates, either use the <span class="guimenu">barbican</span>
      <span class="productname">OpenStack</span> service, a local directory (<span class="guimenu">Local</span>), or the
      <span class="guimenu">Magnum Database (x590keypair)</span>.
     </p><div id="id-1.4.5.4.23.6.5.2.2" data-id-title="barbican As Certificate Manager" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: barbican As Certificate Manager</div><p>
       If you choose to use barbican for managing certificates, make sure
       that the barbican barclamp is enabled.
      </p></div></dd></dl></div><div class="figure" id="id-1.4.5.4.23.7"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_magnum_attributes.png"><img src="images/depl_barclamp_magnum_attributes.png" width="75%" alt="The Magnum Barclamp" title="The Magnum Barclamp"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.28: </span><span class="title-name">The Magnum Barclamp </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.23.7">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><p>
   The Magnum barclamp consists of the following roles:
   <span class="guimenu">magnum-server</span>. It can either be deployed on a Control Node
   or on a cluster—see <a class="xref" href="#sec-depl-ostack-magnum-ha" title="12.17.1. HA Setup for Magnum">Section 12.17.1, “HA Setup for Magnum”</a>. When
   deploying the role onto a Control Node, additional RAM is required for the
   Magnum server. It is recommended to only deploy the role to a
   Control Node that has 16 GB RAM.
  </p><section class="sect2" id="sec-depl-ostack-magnum-ha" data-id-title="HA Setup for Magnum"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.17.1 </span><span class="title-name">HA Setup for Magnum</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-magnum-ha">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    Making Magnum highly available requires no special configuration. It
    is sufficient to deploy it on a cluster.
   </p></section></section><section class="sect1" id="sec-depl-ostack-barbican" data-id-title="Deploying barbican (Optional)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.18 </span><span class="title-name">Deploying barbican (Optional)</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-barbican">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   barbican is a component designed for storing secrets in a secure and
   standardized manner protected by keystone authentication. Secrets include
   SSL certificates and passwords used by various <span class="productname">OpenStack</span> components.
  </p><p>
   barbican settings can be configured in <code class="literal">Raw</code> mode
   only. To do this, open the barbican barclamp <span class="guimenu">Attribute
   </span>configuration in <span class="guimenu">Raw</span> mode.
  </p><div class="figure" id="id-1.4.5.4.24.4"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_barbican_raw.png"><img src="images/depl_barclamp_barbican_raw.png" width="75%" alt="The barbican Barclamp: Raw Mode" title="The barbican Barclamp: Raw Mode"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.29: </span><span class="title-name">The barbican Barclamp: Raw Mode </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.24.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><p>
   When configuring barbican, pay particular attention to the following
   settings:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <code class="literal">bind_host</code> Bind host for the barbican API service
    </p></li><li class="listitem"><p>
     <code class="literal">bind_port</code> Bind port for the barbican API service
    </p></li><li class="listitem"><p>
     <code class="literal">processes</code> Number of API processes to run in Apache
    </p></li><li class="listitem"><p>
     <code class="literal">ssl</code> Enable or disable SSL
    </p></li><li class="listitem"><p>
     <code class="literal">threads</code> Number of API worker threads
    </p></li><li class="listitem"><p>
     <code class="literal">debug</code> Enable or disable debug logging
    </p></li><li class="listitem"><p>
     <code class="literal">enable_keystone_listener</code> Enable or disable the
     keystone listener services
    </p></li><li class="listitem"><p>
     <code class="literal">kek</code> An encryption key (fixed-length 32-byte
     Base64-encoded value) for barbican's
     <code class="systemitem">simple_crypto</code> plugin. If left unspecified, the
     key will be generated automatically.
    </p><div id="id-1.4.5.4.24.6.8.2" data-id-title="Existing Encryption Key" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Existing Encryption Key</div><p>
      If you plan to restore and use the existing barbican database after
      a full reinstall (including a complete wipe of the Crowbar node), make
      sure to save the specified encryption key beforehand. You will need to
      provide it after the full reinstall in order to access the data in the
      restored barbican database.
     </p></div></li></ul></div><div class="variablelist"><dl class="variablelist"><dt id="sec-depl-ostack-barbican-ssl"><span class="term">SSL Support: Protocol
    </span></dt><dd><p>
      With the default value <span class="guimenu">HTTP</span>, public communication will
      not be encrypted. Choose <span class="guimenu">HTTPS</span> to use SSL for
      encryption. See <a class="xref" href="#sec-depl-req-ssl" title="2.3. SSL Encryption">Section 2.3, “SSL Encryption”</a> for background
      information and <a class="xref" href="#sec-depl-inst-nodes-post-ssl" title="11.4.6. Enabling SSL">Section 11.4.6, “Enabling SSL”</a> for
      installation instructions. The following additional configuration options
      will become available when choosing <span class="guimenu">HTTPS</span>:
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.24.7.1.2.2.1"><span class="term"><span class="guimenu">Generate (self-signed) certificates</span>
       </span></dt><dd><p>
         When set to <code class="literal">true</code>, self-signed certificates are
         automatically generated and copied to the correct locations. This
         setting is for testing purposes only and should never be used in
         production environments!
        </p></dd><dt id="id-1.4.5.4.24.7.1.2.2.2"><span class="term"><span class="guimenu">SSL Certificate File</span> / <span class="guimenu">SSL (Private) Key
        File</span>
       </span></dt><dd><p>
         Location of the certificate key pair files.
        </p></dd><dt id="id-1.4.5.4.24.7.1.2.2.3"><span class="term"><span class="guimenu">SSL Certificate is insecure</span>
       </span></dt><dd><p>
         Set this option to <code class="literal">true</code> when using self-signed
         certificates to disable certificate checks. This setting is for
         testing purposes only and should never be used in production
         environments!
        </p></dd><dt id="id-1.4.5.4.24.7.1.2.2.4"><span class="term"><span class="guimenu">SSL CA Certificates File</span>
       </span></dt><dd><p>
         Specify the absolute path to the CA certificate. This field is
         mandatory, and leaving it blank will cause the barclamp to fail. To fix
         this issue, you have to provide the absolute path to the CA
         certificate, restart the <code class="systemitem">apache2</code> service, and
         re-deploy the barclamp.
        </p><p>
         When the certificate is not already trusted by the pre-installed list
         of trusted root certificate authorities, you need to provide a
         certificate bundle that includes the root and all intermediate CAs.
        </p><div class="figure" id="id-1.4.5.4.24.7.1.2.2.4.2.3"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barbican_ssl.png"><img src="images/depl_barbican_ssl.png" width="75%" alt="The SSL Dialog" title="The SSL Dialog"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.30: </span><span class="title-name">The SSL Dialog </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.24.7.1.2.2.4.2.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></dd></dl></div></dd></dl></div><section class="sect2" id="sec-depl-ostack-barbican-ha" data-id-title="HA Setup for barbican"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.18.1 </span><span class="title-name">HA Setup for barbican</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-barbican-ha">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    To make barbican highly available, assign the
    <span class="guimenu">barbican-controller</span> role to the Controller Cluster.
   </p></section></section><section class="sect1" id="sec-depl-ostack-sahara" data-id-title="Deploying sahara"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.19 </span><span class="title-name">Deploying sahara</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-sahara">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   sahara provides users with simple means to provision data processing
   frameworks (such as Hadoop, Spark, and Storm) on <span class="productname">OpenStack</span>. This is
   accomplished by specifying configuration parameters such as the framework
   version, cluster topology, node hardware details, etc.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.25.3.1"><span class="term">Logging: Verbose</span></dt><dd><p>
      Set to <code class="systemitem">true</code> to increase the amount of
      information written to the log files.
     </p></dd></dl></div><div class="figure" id="id-1.4.5.4.25.4"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_sahara.png"><img src="images/depl_barclamp_sahara.png" width="75%" alt="The sahara Barclamp" title="The sahara Barclamp"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.31: </span><span class="title-name">The sahara Barclamp </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.25.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><section class="sect2" id="sec-depl-ostack-sahara-ha" data-id-title="HA Setup for sahara"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.19.1 </span><span class="title-name">HA Setup for sahara</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-sahara-ha">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    Making sahara highly available requires no special configuration. It is
    sufficient to deploy it on a cluster.
   </p></section></section><section class="sect1" id="sec-depl-ostack-octavia" data-id-title="Deploying Octavia"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.20 </span><span class="title-name">Deploying Octavia</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-octavia">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 9 provides Octavia Load Balancing as a Service
    (LBaaS). It is used to manage a fleet of virtual machines, containers, or
    bare metal servers—collectively known as amphorae — which it spins up on
    demand.
  </p><div id="id-1.4.5.4.26.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      Starting with the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 9 release, we recommend
      running Octavia as a standalone load balancing solution. Neutron LBaaS
      is deprecated in the OpenStack Queens release, and Octavia is its
      replacement. Whenever possible, operators are strongly advised to
      migrate to Octavia.
      For further information on OpenStack Neutron LBaaS deprecation, refer
      to https://wiki.openstack.org/wiki/Neutron/LBaaS/Deprecation.
    </p></div><div id="id-1.4.5.4.26.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
      Deploying the Octavia barclamp does not automatically run all tasks
      required to complete the migration from Neutron LBaaS.
    </p><p>
      Please refer to <a class="xref" href="#sec-depl-ostack-octavia-migrate-users" title="12.20.3. Migrating Users to Octavia">Section 12.20.3, “Migrating Users to Octavia”</a>
      for instructions on migrating existing users to allow them to access
      the Octavia load balancer API after the Octavia barclamp is deployed.
    </p><p>
      Please refer to <a class="xref" href="#sec-depl-ostack-octavia-migrate" title="12.20.4. Migrating Neutron LBaaS Instances to Octavia">Section 12.20.4, “Migrating Neutron LBaaS Instances to Octavia”</a> for
      instructions on migrating existing Neutron LBaaS load balancer instances
      to Octavia and on disabling the deprecated Neutron LBaaS provider
      after the Octavia barclamp is deployed.
    </p></div><p>
    Octavia consists of the following major components:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.26.6.1"><span class="term">amphorae</span></dt><dd><p>
       Amphorae are the individual virtual machines, containers, or bare metal
       servers that accomplish the delivery of load balancing services to
       tenant application environments.
     </p></dd><dt id="id-1.4.5.4.26.6.2"><span class="term">controller</span></dt><dd><p>
       The controller is the brains of Octavia. It consists of five
       sub-components as individual daemons. They can be run on
       separate back-end infrastructure.
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           The API Controller is a subcomponent that
           runs Octavia’s API. It takes API requests, performs simple
           sanitizing on them, and ships them off to the controller worker
           over the Oslo messaging bus.
         </p></li><li class="listitem"><p>
           The controller worker subcomponent takes sanitized API commands
           from the API controller and performs the actions necessary to
           fulfill the API request.
         </p></li><li class="listitem"><p>
           The health manager subcomponent monitors individual amphorae
           to ensure they are up and running, and healthy. It
           also handles failover events if amphorae fail unexpectedly.
         </p></li><li class="listitem"><p>
           The housekeeping manager subcomponent cleans up stale (deleted)
           database records, manages the spares pool, and manages amphora
           certificate rotation.
         </p></li><li class="listitem"><p>
           The driver agent subcomponent receives status and statistics
           updates from provider drivers.
         </p></li></ul></div></dd><dt id="id-1.4.5.4.26.6.3"><span class="term">network</span></dt><dd><p>
       Octavia cannot accomplish what it does without manipulating the
       network environment. Amphorae are spun up with a network
       interface on the <code class="literal">load balancer network</code>. They
       can also plug directly into tenant networks to reach back-end
       pool members, depending on how any given load balancing service
       is deployed by the tenant.
     </p></dd></dl></div><p>The <span class="productname">OpenStack</span> Octavia team has created a glossary of terms used within
   the context of the Octavia project and Neutron LBaaS version 2.
   This glossary is available here: <a class="link" href="https://docs.openstack.org/octavia/rocky/reference/glossary.html" target="_blank">Octavia Glossary</a>.</p><p>In accomplishing its role, Octavia requires OpenStack services
      managed by other barclamps to be already deployed:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Nova - For managing amphora lifecycle and spinning up compute
          resources on demand.
        </p></li><li class="listitem"><p>
          Neutron - For network connectivity between amphorae, tenant
          environments, and external networks.
        </p></li><li class="listitem"><p>
          Barbican - For managing TLS certificates and credentials, when TLS
          session termination is configured on the amphorae.
        </p></li><li class="listitem"><p>
          Keystone - For authentication against the Octavia API, and for
          Octavia to authenticate with other <span class="productname">OpenStack</span> projects.
        </p></li><li class="listitem"><p>
          Glance - For storing the amphora virtual machine image.
        </p></li></ul></div><p>
    The Octavia barclamp component consists of following roles:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.26.11.1"><span class="term">octavia-api</span></dt><dd><p>
          The Octavia API.
        </p></dd><dt id="id-1.4.5.4.26.11.2"><span class="term">octavia-backend</span></dt><dd><p>
        Octavia worker, health-manager and house-keeping.
        </p></dd></dl></div><section class="sect2" id="sec-depl-ostack-octavia-prereqs" data-id-title="Prerequisites"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.20.1 </span><span class="title-name">Prerequisites</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-octavia-prereqs">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
      Before configuring and applying the Octavia barclamp, there are a couple
      of prerequisites that have to be prepared: the Neutron management network
      used by the Octavia control plane services to communicate with Amphorae
      and the certificates needed to secure this communication.
    </p><section class="sect3" id="sec-depl-ostack-octavia-mgmtnet" data-id-title="Management network"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">12.20.1.1 </span><span class="title-name">Management network</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-octavia-mgmtnet">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
        Octavia needs a neutron provider network as a management network that the
        controller uses to communicate with the amphorae. The amphorae
        that Octavia deploys have interfaces and IP addresses on this
        network. It’s important that the subnet deployed on this network
        be sufficiently large to allow for the maximum number of
        amphorae and controllers likely to be deployed throughout the
        lifespan of the cloud installation.
      </p><p>
        To configure the Octavia management network, the network configuration
        must be initialized or updated to include an <code class="literal">octavia</code>
        network entry. The Octavia barclamp uses this information to automatically
        create the neutron provider network used for management traffic.
      </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
          If you have not yet deployed Crowbar, add the following configuration to
          <code class="filename">/etc/crowbar/network.json</code>
          to set up the Octavia management network, using the applicable VLAN
          ID, and network address values. If you have already deployed Crowbar, then add
          this configuration to the <span class="guimenu">Raw</span> view of the Network Barclamp.
         </p><div class="verbatim-wrap"><pre class="screen">"octavia": {
              "conduit": "intf1",
              "vlan": <em class="replaceable">450</em>,
              "use_vlan": true,
              "add_bridge": false,
              "subnet": "<em class="replaceable">172.31.0.0</em>",
              "netmask": "<em class="replaceable">255.255.0.0</em>",
              "broadcast": "<em class="replaceable">172.31.255.255</em>",
              "ranges": {
                "host": { "start": "<em class="replaceable">172.31.0.1</em>",
                   "end": "<em class="replaceable">172.31.0.255</em>" },
                "dhcp": { "start": "<em class="replaceable">172.31.1.1</em>",
                   "end": "<em class="replaceable">172.31.255.254</em>" }
              }
        },</pre></div><div id="id-1.4.5.4.26.12.3.4.1.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
             Care should be taken to ensure the IP subnet doesn't overlap with any of those
             configured for the other networks. The chosen VLAN ID must not be used within the SUSE <span class="productname">OpenStack</span> Cloud
             network and not used by neutron (i.e. if deploying neutron with VLAN support - using the plugins
             linuxbridge or openvswitch plus VLAN - ensure that the VLAN ID doesn't overlap with the range of
             VLAN IDs allocated for the <code class="literal">nova-fixed</code> neutron network).
           </p></div><p>
          The <code class="literal">host</code> range will be used to allocate IP addresses to
          the controller nodes where Octavia services are running, so it needs to
          accommodate the maximum number of controller nodes likely to be deployed
          throughout the lifespan of the cloud installation.
         </p><p>
          The <code class="literal">dhcp</code> range will be reflected in the configuration of the
          actual neutron provider network used for Octavia management traffic and its
          size will determine the maximum number of amphorae and therefore the maximum
          number of load balancer instances that can be running at the same time.
         </p><p>
          See <a class="xref" href="#sec-depl-inst-admserv-post-network" title="7.5. Custom Network Configuration">Section 7.5, “Custom Network Configuration”</a> for detailed instructions
          on how to customize the network configuration.
         </p></li><li class="step"><p>
           If Crowbar is already deployed, it is also necessary to re-apply both the neutron
           Barclamp and the nova Barclamp for the configuration to take effect before applying
           the Octavia Barclamp.
       </p></li></ol></div></div><p>
        Aside from configuring the physical switches to allow VLAN traffic to be correctly forwarded,
        no additional external network configuration is required.
      </p></section><section class="sect3" id="sec-depl-ostack-octavia-certificates" data-id-title="Certificates"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">12.20.1.2 </span><span class="title-name">Certificates</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-octavia-certificates">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div id="id-1.4.5.4.26.12.4.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
          Crowbar will automatically change the filesystem ownership settings for
          these files to match the username and group used by the Octavia
          services, but it is otherwise the responsibility of the cloud
          administrator to ensure that access to these files on the controller
          nodes is properly restricted.
        </p></div><p>
        Octavia administrators set up certificate authorities for the
        two-way TLS authentication used in Octavia for command and
	control of amphorae. For more information, see the 
	<code class="literal">Creating the Certificate Authorities</code> section
	of the <a class="link" href="https://docs.openstack.org/octavia/stein/admin/guides/certificates.html" target="_blank">Octavia
        Certificate Configuration Guide</a> . Note that the <code class="literal">Configuring
	Octavia</code> section of that guide does not apply as the
        barclamp will configure Octavia.
      </p><p>
        The following certificates need to be generated and stored on all
        controller nodes where Octavia is deployed under
        <code class="literal">/etc/octavia/certs</code>, in a relative path matching the
        certificate location attribute values configured in the Octavia barclamp:
      </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.26.12.4.5.1"><span class="term">Server CA certificate</span></dt><dd><p>
           The Certificate Authority (CA) certificate that is used by
           the Octavia controller(s) to sign the generated Amphora server
           certificates. The Octavia control plane services also validate
           the server certificates presented by Amphorae during the TLS handshake
           against this CA certificate.
         </p></dd><dt id="id-1.4.5.4.26.12.4.5.2"><span class="term">Server CA key</span></dt><dd><p>
           The private key associated with the server CA certificate. This key
           must be encrypted with a non-empty passphrase that also needs to
           be provided as a separate barclamp attribute. The private key is
           required alongside the server CA certificate on the Octavia
           controller(s), to sign the generated Amphora server certificates.
         </p></dd><dt id="id-1.4.5.4.26.12.4.5.3"><span class="term">Passphrase</span></dt><dd><p>
           The passphrase used to encrypt the server CA key.
         </p></dd><dt id="id-1.4.5.4.26.12.4.5.4"><span class="term">Client CA certificate</span></dt><dd><p>
           The CA certificate used to sign the client certificates installed on
           the Octavia controller nodes and presented by Octavia control plane
           services during the TLS handshake. This CA certificate is stored on the
           Amphorae, which use it to validate the client certificate presented by
           the Octavia control plane services during the TLS handshake.
           The same CA certificate may be used for both client and server roles,
           but this is perceived as a security weakness and recommended against,
           as a server certificate from an amphora could be used to impersonate a
           controller.
         </p></dd><dt id="id-1.4.5.4.26.12.4.5.5"><span class="term">Client certificate concat key</span></dt><dd><p>
           The client certificate, signed with the client CA certificate, bundled
           together with the client certificate key, that is presented by
           the Octavia control plane services during the TLS handshake.
         </p></dd></dl></div><p>
        All Octavia barclamp attributes listed above, with the exception of the
        pasphrase are paths relative to <code class="literal">/etc/octavia/certs</code>.
        The required certificates must be present in their corresponding locations
        on all controller nodes where the Octavia barclamp will be deployed.
      </p></section></section><section class="sect2" id="sec-depl-ostack-octavia-raw-mode" data-id-title="Barclmap raw mode"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.20.2 </span><span class="title-name">Barclmap raw mode</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-octavia-raw-mode">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
      If a user wants to be able to debug or get access to an amphora,
      you can provide an SSH keyname to the barclamp via the
      <code class="literal">raw mode</code>. This is a keyname to a key that has
      been uploaded to openstack. For example:
    </p><div class="verbatim-wrap"><pre class="screen">      openstack keypair create --public-key /etc/octavia/.ssh/id_rsa_amphora.pub octavia_key</pre></div><p>
      Note that the keypair has to be owned by the octavia user.
    </p></section><section class="sect2" id="sec-depl-ostack-octavia-migrate-users" data-id-title="Migrating Users to Octavia"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.20.3 </span><span class="title-name">Migrating Users to Octavia</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-octavia-migrate-users">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div id="id-1.4.5.4.26.14.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
        This behaviour is not backwards compatible with the legacy Neutron LBaaS
        API policy, as non-admin OpenStack users will not be allowed to run
        <code class="literal">openstack loadbalancer</code> CLI commands or use
        the load balancer horizon dashboard unless their accounts are explicitly
        reconfigured to be associated with one or more of these roles.
      </p></div><div id="id-1.4.5.4.26.14.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
        Please follow the instructions documented under <a class="xref" href="#sec-depl-ostack-heat-delegated-roles" title="12.13.1. Enabling Identity Trusts Authorization (Optional)">Section 12.13.1, “Enabling Identity Trusts Authorization (Optional)”</a>
        on updating the trusts roles in the heat barclamp configuration. This is
        required to configure heat to use the correct roles when communicating with
        the Octavia API and manage load balancers.
      </p></div><p>
      Octavia employs a set of specialized roles to control access to the
      load balancer API:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.26.14.5.1"><span class="term">load-balancer_observer</span></dt><dd><p>
         User has access to load-balancer read-only APIs.
       </p></dd><dt id="id-1.4.5.4.26.14.5.2"><span class="term">load-balancer_global_observer</span></dt><dd><p>
         User has access to load-balancer read-only APIs including resources
         owned by others.
       </p></dd><dt id="id-1.4.5.4.26.14.5.3"><span class="term">load-balancer_member</span></dt><dd><p>
         User has access to load-balancer read and write APIs.
       </p></dd><dt id="id-1.4.5.4.26.14.5.4"><span class="term">load-balancer_quota_admin</span></dt><dd><p>
         User is considered an admin for quota APIs only.
       </p></dd><dt id="id-1.4.5.4.26.14.5.5"><span class="term">load-balancer_admin</span></dt><dd><p>
         User is considered an admin for all load-balancer APIs including
         resources owned by others.
       </p></dd></dl></div></section><section class="sect2" id="sec-depl-ostack-octavia-migrate" data-id-title="Migrating Neutron LBaaS Instances to Octavia"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.20.4 </span><span class="title-name">Migrating Neutron LBaaS Instances to Octavia</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-octavia-migrate">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div id="id-1.4.5.4.26.15.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
        Disabling LBaaS or switching the LBaaS provider in the Neutron barclamp
        to Octavia is not possible while there are load balancers still running
        under the previous Neutron LBaaS provider and will result in a Neutron
        barclamp redeployment failure. To avoid this, ensure that load balancer
        instances that are running under the old provider are either migrated
        or deleted.
      </p></div><p>
      The migration procedure documented in this section is only relevant if
      LBaaS was already enabled in the Neutron barclamp, with either the HAProxy
      or H5 provider configured, before Octavia was deployed.
      The procedure should be followed by operators to migrate and/or delete all
      load balancer instances using the Neutron LBaaS provider that are still
      active, and concluded the switch to Octavia by reconfiguring or disabling
      the deprecated Neutron LBaaS feature.
    </p><p>
      Octavia is a replacement for the Neutron LBaaS feature, that is deprecated
      in the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 9 release. However, deploying the
      Octavia barclamp does not automatically disable the legacy Neutron LBaaS
      provider, if one is already configured in the Neutron barclamp.
    </p><p>
      Both Octavia and Neutron LBaaS need to be enabled at the same time,
      to facilitate the load balancer migration process. This way, operators
      have a migration path they can use to gradually decommission Neutron LBaaS
      load balancers that use the HAProxy or F5 provider and replace them with
      Octavia load balancers.
    </p><p>
      With Octavia deployed and Neutron LBaaS enabled, both load balancer
      providers can be used simultaneously:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          The (deprecated) <code class="literal">neutron lbaas-...</code> CLI
          commands can be used to manage load balancer instances using the
          legacy Neutron LBaaS provider configured in the Neutron barclamp.
          Note that the legacy Neutron LBaaS instances will not be visible in
          the load balancer horizon dashboard.
        </p></li><li class="listitem"><p>
          The <code class="literal">openstack loadbalancer</code> CLI commands as well
          as the load balancer horizon dashboard can be used to manage Octavia
          load balancers. Also note that OpenStack users are required to have
          special roles associated with their projects to be able to access
          the Octavia API, as covered in
          <a class="xref" href="#sec-depl-ostack-octavia-migrate-users" title="12.20.3. Migrating Users to Octavia">Section 12.20.3, “Migrating Users to Octavia”</a>.
        </p></li></ul></div><div id="id-1.4.5.4.26.15.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
        (Optional): to prevent regular users from creating or changing the
        configuration of currently running legacy Neutron LBaaS load balancer
        instances during the migration process, the neutron API policy should be
        temporarily changed to prevent these operations. For this purpose, a
        <code class="literal">neutron-lbaas.json</code> file can be created in the
        <code class="literal">/etc/neutron/policy.d</code> folder on all neutron-server
        nodes (no service restart required):
      </p><div class="verbatim-wrap"><pre class="screen">mkdir /etc/neutron/policy.d
cat &gt; /etc/neutron/policy.d/neutron-lbaas.json &lt;&lt;EOF
{
  "context_is_admin": "role:admin",
  "context_is_advsvc": "role:advsvc",
  "default": "rule:admin_or_owner",
  "create_loadbalancer": "rule:admin_only",
  "update_loadbalancer": "rule:admin_only",
  "get_loadbalancer": "!",
  "delete_loadbalancer": "rule:admin_only",
  "create_listener": "rule:admin_only",
  "get_listener": "",
  "delete_listener": "rule:admin_only",
  "update_listener": "rule:admin_only",
  "create_pool": "rule:admin_only",
  "get_pool": "",
  "delete_pool": "rule:admin_only",
  "update_pool": "rule:admin_only",
  "create_healthmonitor": "rule:admin_only",
  "get_healthmonitor": "",
  "update_healthmonitor": "rule:admin_only",
  "delete_healthmonitor": "rule:admin_only",
  "create_pool_member": "rule:admin_only",
  "get_pool_member": "",
  "update_pool_member": "rule:admin_only",
  "delete_pool_member": "rule:admin_only"
}
EOF
chown -R root:neutron /etc/neutron/policy.d
chmod 640 /etc/neutron/policy.d/neutron-lbaas.json</pre></div><p>
	If users need to create or change the configuration of currently running
	legacy Neutron LBaaS load balancer instances during the migration process,
	Create a <code class="filename">neutron-lbaas.json</code> file in the
        <code class="filename">/etc/neutron/policy.d</code> folder on all neutron-server nodes.
	The <code class="filename">neutron-lbaas.json</code> file should be empty, then
	restart the neutron service via <code class="command">systemctl restart openstack-neutron.service</code>
	on all neutron-server nodes.
      </p></div><p>
      With all of the above in check, the actual migration process consists of
      replacing Neutron LBaaS instances with Octavia instances. There are many
      different ways to accomplish this, depending on the size and purpose of
      the cloud deployment, the number of load balancers that need to be
      migrated, the project and user configuration etc. This section only gives
      a few pointers and recommendations on how to approach this tasks, but
      the actual execution needs to be attuned to each particular situation.
    </p><p>
      Migrating a single load balancer instance is generally comprised of these
      steps:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Use the <code class="literal">neutron lbaas-...</code> CLI to
          retrieve information about the load balancer configuration,
          including the complete set of related listener, pool, member
          and health monitor instances
        </p></li><li class="listitem"><p>
          Use the <code class="literal">openstack loadbalancer</code> CLI or the
          load balancer horizon dashboard to create an Octavia load
          balancer and its associated listener, pool, member and health
          monitor instances to accurately match the project and Neutron LBaaS
          load balancer configuration extracted during the previous step.
          Note that the Octavia load balancer instance and the Neutron
          LBaaS instance cannot share the same VIP address value if
          both instances are running at the same time. This could be a
          problem, if the load balancer VIP address is accessed directly
          (i.e. as opposed to being accessed via a floating IP).
          In this case, the legacy load balancer instance needs to be
          deleted first, which incurs a longer interruption in service
          availability.
        </p></li><li class="listitem"><p>
          Once the Octavia instance is up and running, if a floating IP
          is associated with the Neutron LBaaS load balancer VIP
          address, re-associate the floating IP with the Octavia load
          balancer VIP address. Using a floating IP has the advantage
          that the migration can be performed with minimal downtime.
          If the load balancer VIP address needs to be accessed directly
          (e.g. from another VM attached to the same Neutron network or
          router), then all the remote affected services need to be
          reconfigured to use the new VIP address.
        </p></li><li class="listitem"><p>
          The two load balancer instances can continue to run in parallel,
          while the operator or owner verifies the Octavia load balancer
          operation. If any problems occur, the change can be reverted by
          undoing the actions performed during the previous step. If a
          floating IP is involved, this could be as simple as switching it
          back to the Neutron LBaaS load balancer instance.
        </p></li><li class="listitem"><p>
          When it's safe, delete the Neutron LBaaS load balancer instance,
          along with all its related listener, pool, member and health
          monitor instances.
        </p></li></ul></div><p>
      Depending on the number of load balancer instances that need to
      be migrated and the complexity of the overall setup that they are
      integrated into, the migration may be performed by the cloud
      operators, the owners themselves, or a combination of both. It is
      generally recommended that the load balancer owners have some involvement
      in this process or at least be notified of this migration procedure,
      because the load balancer migration is not an entirely seamless operation.
      One or more of the load balancer configuration attributes listed below may
      change during the migration and there may be other operational components,
      managed by OpenStack or otherwise (e.g. OpenStack heat stacks,
      configuration management scripts, database entries or non-persistent
      application states, etc.), that only the owner(s) may be aware
      of:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          The load balancer UUID value, along with the UUID values of every
          other related object (listeners, pools, members etc.). Even though the
          name values may be preserved by the migration, the UUID values will be
          different.
        </p></li><li class="listitem"><p>
          The load balancer VIP address will change during a non-disruptive
          migration. This is especially relevant if there is no floating IP
          associated with the previous VIP address.
        </p></li></ul></div><p>
      When the load balancer migration is complete, the Neutron LBaaS provider
      can either be switched to Octavia or turned off entirely in the Neutron
      barclamp, to finalize the migration process.
    </p><p>
      The only advantage of having Octavia configured as the Neutron LBaaS
      provider is that it continues to allow users to manage Octavia load
      balancers via the deprecated <code class="literal">neutron lbaas-...</code> CLI, but
      it is otherwise recommended to disable LBaaS in the Neutron barclamp.
    </p></section></section><section class="sect1" id="sec-depl-ostack-ironic" data-id-title="Deploying ironic (optional)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.21 </span><span class="title-name">Deploying ironic (optional)</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-ironic">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
  Ironic is the <span class="productname">OpenStack</span> bare metal service for provisioning physical
  machines. Refer to the <span class="productname">OpenStack</span> <a class="link" href="https://docs.openstack.org/ironic/latest/" target="_blank">developer and admin
  manual</a> for information on drivers, and administering ironic.
 </p><p>
  Deploying the ironic barclamp is done in five steps:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
Set options in the Custom view of the barclamp.
</p></li><li class="listitem"><p>
List the <code class="literal">enabled_drivers</code> in the Raw view.
</p></li><li class="listitem"><p>
Configure the ironic network in <code class="filename">network.json</code>.
</p></li><li class="listitem"><p>
Apply the barclamp to a Control Node.
</p></li><li class="listitem"><p>
          Apply the <span class="guimenu">nova-compute-ironic</span> role to the same node
          you applied the ironic barclamp to, in place of the other
          <span class="guimenu">nova-compute-*</span> roles.
</p></li></ul></div><section class="sect2" id="sec-depl-ostack-ironic-custom-view" data-id-title="Custom View Options"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.21.1 </span><span class="title-name">Custom View Options</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-ironic-custom-view">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
        Currently, there are two options in the Custom view of the barclamp.
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.27.5.3.1"><span class="term"><span class="guimenu">Enable automated node cleaning</span>
    </span></dt><dd><p>
         Node cleaning prepares the node to accept a new workload. When you set this to <span class="guimenu">true</span>,
         ironic collects a list of cleaning steps from the Power, Deploy, Management, and RAID
         interfaces of the driver assigned to the node. ironic automatically prioritizes and
         executes the cleaning steps, and changes the state of the node to "cleaning". When cleaning
         is complete the state becomes "available". After a new workload is assigned to the machine
         its state changes to "active".
     </p><p>
        <span class="guimenu">false</span> disables automatic cleaning, and you must configure and apply
        node cleaning manually. This requires the admin to create and prioritize the cleaning steps,
        and to set up a cleaning network. Apply manual cleaning when you have long-running or
        destructive tasks that you wish to monitor and control more closely.
        (See <a class="link" href="https://docs.openstack.org/ironic/latest/admin/cleaning.html" target="_blank">Node Cleaning</a>.)
        </p></dd><dt id="id-1.4.5.4.27.5.3.2"><span class="term"><span class="guimenu">SSL Support: Protocol</span>
    </span></dt><dd><p>
         SSL support is not yet enabled, so the only option is <span class="guimenu">HTTP</span>.
        </p></dd></dl></div><div class="figure" id="id-1.4.5.4.27.5.4"><div class="figure-contents"><div class="mediaobject"><a href="images/ironic-1.png"><img src="images/ironic-1.png" width="75%" alt="The ironic barclamp Custom view" title="The ironic barclamp Custom view"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 12.32: </span><span class="title-name">The ironic barclamp Custom view </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.27.5.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></section><section class="sect2" id="sec-depl-ostack-ironic-drivers" data-id-title="ironic Drivers"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.21.2 </span><span class="title-name">ironic Drivers</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-ironic-drivers">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    You must enter the Raw view of barclamp and specify a list of drivers to load during service initialization.
    <code class="literal">pxe_ipmitool</code> is the recommended default ironic driver. It uses the
    Intelligent Platform Management Interface (IPMI) to control the power state
    of your bare metal machines, creates the appropriate PXE configurations
    to start them, and then performs the steps to provision and configure the machines.</p><div class="verbatim-wrap"><pre class="screen">"enabled_drivers": ["pxe_ipmitool"],</pre></div><p>
     See <a class="link" href="https://docs.openstack.org/ironic/latest/admin/drivers.html" target="_blank">ironic
     Drivers</a> for more information.
    </p></section><section class="sect2" id="id-1.4.5.4.27.7" data-id-title="Example ironic Network Configuration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.21.3 </span><span class="title-name">Example ironic Network Configuration</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.4.27.7">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
          This is a complete ironic <code class="filename">network.json</code> example, using
          the default <code class="filename">network.json</code>, followed by a diff that shows
          the ironic-specific configurations.</p><div class="example" id="ex-ironic-network-json" data-id-title="Example network.json"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 12.1: </span><span class="title-name">Example network.json </span></span><a title="Permalink" class="permalink" href="#ex-ironic-network-json">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">{
  "start_up_delay": 30,
  "enable_rx_offloading": true,
  "enable_tx_offloading": true,
  "mode": "single",
  "teaming": {
    "mode": 1
  },
  "interface_map": [
    {
      "bus_order": [
        "0000:00/0000:00:01",
        "0000:00/0000:00:03"
      ],
      "pattern": "PowerEdge R610"
    },
    {
      "bus_order": [
        "0000:00/0000:00:01.1/0000:01:00.0",
        "0000:00/0000:00:01.1/0000.01:00.1",
        "0000:00/0000:00:01.0/0000:02:00.0",
        "0000:00/0000:00:01.0/0000:02:00.1"
      ],
      "pattern": "PowerEdge R620"
    },
    {
      "bus_order": [
        "0000:00/0000:00:01",
        "0000:00/0000:00:03"
      ],
      "pattern": "PowerEdge R710"
    },
    {
      "bus_order": [
        "0000:00/0000:00:04",
        "0000:00/0000:00:02"
      ],
      "pattern": "PowerEdge C6145"
    },
    {
      "bus_order": [
        "0000:00/0000:00:03.0/0000:01:00.0",
        "0000:00/0000:00:03.0/0000:01:00.1",
        "0000:00/0000:00:1c.4/0000:06:00.0",
        "0000:00/0000:00:1c.4/0000:06:00.1"
      ],
      "pattern": "PowerEdge R730xd"
    },
    {
      "bus_order": [
        "0000:00/0000:00:1c",
        "0000:00/0000:00:07",
        "0000:00/0000:00:09",
        "0000:00/0000:00:01"
      ],
      "pattern": "PowerEdge C2100"
    },
    {
      "bus_order": [
        "0000:00/0000:00:01",
        "0000:00/0000:00:03",
        "0000:00/0000:00:07"
      ],
      "pattern": "C6100"
    },
    {
      "bus_order": [
        "0000:00/0000:00:01",
        "0000:00/0000:00:02"
      ],
      "pattern": "product"
    }
  ],
  "conduit_map": [
    {
      "conduit_list": {
        "intf0": {
          "if_list": [
            "1g1",
            "1g2"
          ]
        },
        "intf1": {
          "if_list": [
            "1g1",
            "1g2"
          ]
        },
        "intf2": {
          "if_list": [
            "1g1",
            "1g2"
          ]
        },
        "intf3": {
          "if_list": [
            "1g1",
            "1g2"
          ]
        }
      },
      "pattern": "team/.*/.*"
    },
    {
      "conduit_list": {
        "intf0": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf1": {
          "if_list": [
            "?1g2"
          ]
        },
        "intf2": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf3": {
          "if_list": [
            "?1g2"
          ]
        }
      },
      "pattern": "dual/.*/.*"
    },
    {
      "conduit_list": {
        "intf0": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf1": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf2": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf3": {
          "if_list": [
            "?1g2"
          ]
        }
      },
      "pattern": "single/.*/.*ironic.*"
    },
    {
      "conduit_list": {
        "intf0": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf1": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf2": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf3": {
          "if_list": [
            "?1g1"
          ]
        }
      },
      "pattern": "single/.*/.*"
    },
    {
      "conduit_list": {
        "intf0": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf1": {
          "if_list": [
            "1g1"
          ]
        },
        "intf2": {
          "if_list": [
            "1g1"
          ]
        },
        "intf3": {
          "if_list": [
            "1g1"
          ]
        }
      },
      "pattern": ".*/.*/.*"
    },
    {
      "conduit_list": {
        "intf0": {
          "if_list": [
            "1g1"
          ]
        },
        "intf1": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf2": {
          "if_list": [
            "?1g1"
          ]
        },
        "intf3": {
          "if_list": [
            "?1g1"
          ]
        }
      },
      "pattern": "mode/1g_adpt_count/role"
    }
  ],
  "networks": {
    "ironic": {
      "conduit": "intf3",
      "vlan": 100,
      "use_vlan": false,
      "add_bridge": false,
      "add_ovs_bridge": false,
      "bridge_name": "br-ironic",
      "subnet": "192.168.128.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.128.255",
      "router": "192.168.128.1",
      "router_pref": 50,
      "ranges": {
        "admin": {
          "start": "192.168.128.10",
          "end": "192.168.128.11"
        },
        "dhcp": {
          "start": "192.168.128.21",
          "end": "192.168.128.254"
        }
      },
      "mtu": 1500
    },
    "storage": {
      "conduit": "intf1",
      "vlan": 200,
      "use_vlan": true,
      "add_bridge": false,
      "mtu": 1500,
      "subnet": "192.168.125.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.125.255",
      "ranges": {
        "host": {
          "start": "192.168.125.10",
          "end": "192.168.125.239"
        }
      }
    },
    "public": {
      "conduit": "intf1",
      "vlan": 300,
      "use_vlan": true,
      "add_bridge": false,
      "subnet": "192.168.122.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.122.255",
      "router": "192.168.122.1",
      "router_pref": 5,
      "ranges": {
        "host": {
          "start": "192.168.122.2",
          "end": "192.168.122.127"
        }
      },
      "mtu": 1500
    },
    "nova_fixed": {
      "conduit": "intf1",
      "vlan": 500,
      "use_vlan": true,
      "add_bridge": false,
      "add_ovs_bridge": false,
      "bridge_name": "br-fixed",
      "subnet": "192.168.123.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.123.255",
      "router": "192.168.123.1",
      "router_pref": 20,
      "ranges": {
        "dhcp": {
          "start": "192.168.123.1",
          "end": "192.168.123.254"
        }
      },
      "mtu": 1500
    },
    "nova_floating": {
      "conduit": "intf1",
      "vlan": 300,
      "use_vlan": true,
      "add_bridge": false,
      "add_ovs_bridge": false,
      "bridge_name": "br-public",
      "subnet": "192.168.122.128",
      "netmask": "255.255.255.128",
      "broadcast": "192.168.122.255",
      "ranges": {
        "host": {
          "start": "192.168.122.129",
          "end": "192.168.122.254"
        }
      },
      "mtu": 1500
    },
    "bmc": {
      "conduit": "bmc",
      "vlan": 100,
      "use_vlan": false,
      "add_bridge": false,
      "subnet": "192.168.124.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.124.255",
      "ranges": {
        "host": {
          "start": "192.168.124.162",
          "end": "192.168.124.240"
        }
      },
      "router": "192.168.124.1"
    },
    "bmc_vlan": {
      "conduit": "intf2",
      "vlan": 100,
      "use_vlan": true,
      "add_bridge": false,
      "subnet": "192.168.124.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.124.255",
      "ranges": {
        "host": {
          "start": "192.168.124.161",
          "end": "192.168.124.161"
        }
      }
    },
    "os_sdn": {
      "conduit": "intf1",
      "vlan": 400,
      "use_vlan": true,
      "add_bridge": false,
      "mtu": 1500,
      "subnet": "192.168.130.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.130.255",
      "ranges": {
        "host": {
          "start": "192.168.130.10",
          "end": "192.168.130.254"
        }
      }
    },
    "admin": {
      "conduit": "intf0",
      "vlan": 100,
      "use_vlan": false,
      "add_bridge": false,
      "mtu": 1500,
      "subnet": "192.168.124.0",
      "netmask": "255.255.255.0",
      "broadcast": "192.168.124.255",
      "router": "192.168.124.1",
      "router_pref": 10,
      "ranges": {
        "admin": {
          "start": "192.168.124.10",
          "end": "192.168.124.11"
        },
        "dhcp": {
          "start": "192.168.124.21",
          "end": "192.168.124.80"
        },
        "host": {
          "start": "192.168.124.81",
          "end": "192.168.124.160"
        },
        "switch": {
          "start": "192.168.124.241",
          "end": "192.168.124.250"
        }
      }
    }
  }
}</pre></div></div></div><div class="complex-example"><div class="example" id="ex-ironic-network-json-diff" data-id-title="Diff of ironic Configuration"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 12.2: </span><span class="title-name">Diff of ironic Configuration </span></span><a title="Permalink" class="permalink" href="#ex-ironic-network-json-diff">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="example-contents"><p>
      This diff should help you separate the ironic items from the default
      <code class="filename">network.json</code>.
</p><div class="verbatim-wrap"><pre class="screen">--- network.json        2017-06-07 09:22:38.614557114 +0200
+++ ironic_network.json 2017-06-05 12:01:15.927028019 +0200
@@ -91,6 +91,12 @@
             "1g1",
             "1g2"
           ]
+        },
+        "intf3": {
+          "if_list": [
+            "1g1",
+            "1g2"
+          ]
         }
       },
       "pattern": "team/.*/.*"
@@ -111,6 +117,11 @@
           "if_list": [
             "?1g1"
           ]
+        },
+        "intf3": {
+          "if_list": [
+            "?1g2"
+          ]
         }
       },
       "pattern": "dual/.*/.*"
@@ -131,6 +142,36 @@
           "if_list": [
             "?1g1"
           ]
+        },
+        "intf3": {
+          "if_list": [
+            "?1g2"
+          ]
+        }
+      },
+      "pattern": "single/.*/.*ironic.*"
+    },
+    {
+      "conduit_list": {
+        "intf0": {
+          "if_list": [
+            "?1g1"
+          ]
+        },
+        "intf1": {
+          "if_list": [
+            "?1g1"
+          ]
+        },
+        "intf2": {
+          "if_list": [
+            "?1g1"
+          ]
+        },
+        "intf3": {
+          "if_list": [
+            "?1g1"
+          ]
         }
       },
       "pattern": "single/.*/.*"
@@ -151,6 +192,11 @@
           "if_list": [
             "1g1"
           ]
+        },
+        "intf3": {
+          "if_list": [
+            "1g1"
+          ]
         }
       },
       "pattern": ".*/.*/.*"
@@ -171,12 +217,41 @@
           "if_list": [
             "?1g1"
           ]
+        },
+        "intf3": {
+          "if_list": [
+            "?1g1"
+          ]
         }
       },
       "pattern": "mode/1g_adpt_count/role"
     }
   ],
   "networks": {
+    "ironic": {
+      "conduit": "intf3",
+      "vlan": 100,
+      "use_vlan": false,
+      "add_bridge": false,
+      "add_ovs_bridge": false,
+      "bridge_name": "br-ironic",
+      "subnet": "192.168.128.0",
+      "netmask": "255.255.255.0",
+      "broadcast": "192.168.128.255",
+      "router": "192.168.128.1",
+      "router_pref": 50,
+      "ranges": {
+        "admin": {
+          "start": "192.168.128.10",
+          "end": "192.168.128.11"
+        },
+        "dhcp": {
+          "start": "192.168.128.21",
+          "end": "192.168.128.254"
+        }
+      },
+      "mtu": 1500
+    },
     "storage": {
       "conduit": "intf1",
       "vlan": 200,</pre></div></div></div></div></section></section><section class="sect1" id="sec-depl-ostack-final" data-id-title="How to Proceed"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.22 </span><span class="title-name">How to Proceed</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-final">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   With a successful deployment of the <span class="productname">OpenStack</span> Dashboard, the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>
   installation is finished. To be able to test your setup by starting an
   instance one last step remains to be done—uploading an image to the
   glance component. Refer to the <em class="citetitle">Supplement to <em class="citetitle">Administrator Guide</em> and <em class="citetitle">User Guide</em></em>, chapter <em class="citetitle">Manage
   images</em>

   for instructions. Images for SUSE <span class="productname">OpenStack</span> Cloud can be built in SUSE Studio. Refer to
   the <em class="citetitle">Supplement to <em class="citetitle">Administrator Guide</em> and <em class="citetitle">User Guide</em></em>, section <em class="citetitle">Building Images with
   SUSE Studio</em>.
  </p><p>
   Now you can hand over to the cloud administrator to set up users, roles,
   flavors, etc.—refer to the <em class="citetitle">Administrator Guide</em> for details. The default
   credentials for the <span class="productname">OpenStack</span> Dashboard are user name <code class="literal">admin</code>
   and password <code class="literal">crowbar</code>.
  </p></section><section class="sect1" id="crow-ses-integration" data-id-title="SUSE Enterprise Storage integration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.23 </span><span class="title-name">SUSE Enterprise Storage integration</span></span> <a title="Permalink" class="permalink" href="#crow-ses-integration">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> supports integration with SUSE Enterprise Storage (SES), enabling Ceph block
    storage as well as image storage services in SUSE <span class="productname">OpenStack</span> Cloud.
   </p><div class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.29.3"><span class="name">Enabling SES Integration
   </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.29.3">#</a></h3></div><p>
    To enable SES integration on Crowbar, an SES configuration file must be
    uploaded to Crowbar. SES integration functionality is included in the
    <code class="literal">crowbar-core</code> package and can be used with the Crowbar UI
    or CLI (<code class="literal">crowbarctl</code>). The SES configuration file
    describes various aspects of the Ceph environment, and keyrings for each
    user and pool created in the Ceph environment for SUSE <span class="productname">OpenStack</span> Cloud Crowbar services.
   </p><div class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.29.5"><span class="name">SES 7 Configuration
   </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.29.5">#</a></h3></div><p>The following instructions detail integrating SUSE Enterprise Storage 7.0 with SUSE <span class="productname">OpenStack</span> Cloud.</p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Create the osd pools on the SUSE Enterprise Storage admin node (the names provided here are examples)</p><div class="verbatim-wrap"><pre class="screen">ceph osd pool create ses-cloud-volumes 16 &amp;&amp; \
ceph osd pool create ses-cloud-backups 16 &amp;&amp; \
ceph osd pool create ses-cloud-images 16 &amp;&amp;\
ceph osd pool create ses-cloud-vms 16</pre></div></li><li class="step"><p>Enable the osd pools</p><div class="verbatim-wrap"><pre class="screen">ceph osd pool application enable ses-cloud-volumes rbd &amp;&amp; \
ceph osd pool application enable ses-cloud-backups rbd &amp;&amp; \
ceph osd pool application enable ses-cloud-images rbd &amp;&amp; \
ceph osd pool application enable ses-cloud-vms rbd</pre></div></li><li class="step"><p>Configure permissions on the SUSE <span class="productname">OpenStack</span> Cloud Crowbar admin node</p><div class="verbatim-wrap"><pre class="screen">ceph-authtool -C /etc/ceph/ceph.client.ses-cinder.keyring --name client.ses-cinder --add-key $(ceph-authtool --gen-print-key) --cap mon "allow r" --cap osd "allow class-read object_prefix rbd_children, allow rwx pool=ses-cloud-volumes, allow rwx pool=ses-cloud-vms, allow rwx pool=ses-cloud-images"
ceph-authtool -C /etc/ceph/ceph.client.ses-cinder-backup.keyring --name client.ses-cinder-backup --add-key $(ceph-authtool --gen-print-key) --cap mon "allow r" --cap osd "allow class-read object_prefix rbd_children, allow rwx pool=ses-cloud-cinder-backups"
ceph-authtool -C /etc/ceph/ceph.client.ses-glance.keyring --name client.ses-glance --add-key $(ceph-authtool --gen-print-key) --cap mon "allow r" --cap osd "allow class-read object_prefix rbd_children, allow rwx pool=ses-cloud-images"</pre></div></li><li class="step"><p>Import the updated keyrings into Ceph</p><div class="verbatim-wrap"><pre class="screen">ceph auth import -i /etc/ceph/ceph.client.ses-cinder-backup.keyring &amp;&amp; \
ceph auth import -i /etc/ceph/ceph.client.ses-cinder.keyring &amp;&amp; \
ceph auth import -i /etc/ceph/ceph.client.ses-glance.keyring</pre></div></li></ol></div></div><div class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.29.8"><span class="name">SES 6, 5.5, 5 Configuration
   </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.29.8">#</a></h3></div><p>
    For SES deployments that are version 5.5 or 6, a Salt runner is used
    to create all the users and pools. It also generates a YAML
    configuration that is needed to integrate with SUSE <span class="productname">OpenStack</span> Cloud. The integration
    runner creates separate users for cinder, cinder backup
    (not used by Crowbar currently) and glance. Both the cinder and
    nova services have the same user, because cinder needs
    access to create objects that nova uses.
   </p><div id="id-1.4.5.4.29.10" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>Support for SUSE Enterprise Storage 5 and 5.5 is deprecated. The documentation for
         integrating these versions is included for customers who may not
         yet have upgraded to newer versions of SUSE Enterprise Storage . These versions
         are no longer officially supported.
     </p></div><p>
    Configure SES 6, 5.5, or 5 with the following steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Login as <code class="literal">root</code> and run the SES 5.5 Salt runner on the
      Salt admin host.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>salt-run --out=yaml openstack.integrate prefix=mycloud</pre></div><p>
      The prefix parameter allows pools to be created with the specified
      prefix. By using different prefix parameters, multiple cloud deployments
      can support different users and pools on the same SES deployment.
     </p></li><li class="step"><p>
      YAML output is created with content similar to the following
      example, and can be redirected to a file using the redirect
      operator <code class="literal">&gt;</code> or using the additional
      parameter <code class="literal">--out-file=&lt;filename&gt;</code>:
     </p><div class="verbatim-wrap"><pre class="screen">ceph_conf:
     cluster_network: 10.84.56.0/21
     fsid: d5d7c7cb-5858-3218-a36f-d028df7b0673
     mon_host: 10.84.56.8, 10.84.56.9, 10.84.56.7
     mon_initial_members: ses-osd1, ses-osd2, ses-osd3
     public_network: 10.84.56.0/21
cinder:
     key: ABCDEFGaxefEMxAAW4zp2My/5HjoST2Y87654321==
     rbd_store_pool: mycloud-cinder
     rbd_store_user: cinder
cinder-backup:
     key: AQBb8hdbrY2bNRAAqJC2ZzR5Q4yrionh7V5PkQ==
     rbd_store_pool: mycloud-backups
     rbd_store_user: cinder-backup
glance:
     key: AQD9eYRachg1NxAAiT6Hw/xYDA1vwSWLItLpgA==
     rbd_store_pool: mycloud-glance
     rbd_store_user: glance
nova:
     rbd_store_pool: mycloud-nova
radosgw_urls:
     - http://10.84.56.7:80/swift/v1
     - http://10.84.56.8:80/swift/v1</pre></div></li><li class="step"><p>
      Upload the generated YAML file to Crowbar using the UI or
      <code class="literal">crowbarctl</code> CLI.
     </p></li><li class="step"><p>
      If the Salt runner is not available, you must manually create pools and
      users to allow SUSE <span class="productname">OpenStack</span> Cloud services to use the SES/Ceph cluster. Pools and
      users must be created for cinder, nova, and
      glance. Instructions for creating and managing pools, users and keyrings
      can be found in the <a class="link" href="https://documentation.suse.com/ses/6/" target="_blank">
      SUSE Enterprise Storage</a> Administration Guide in the Key Management
      section.
     </p><p>
      After the required pools and users are set up on the SUSE Enterprise Storage/Ceph
      cluster, create an SES configuration file in YAML format (using the
      example template above). Upload this file to Crowbar using the UI or
      <code class="literal">crowbarctl</code> CLI.
     </p></li><li class="step"><p>
      As indicated above, the SES configuration file can be uploaded to Crowbar
      using the UI or <code class="literal">crowbarctl</code> CLI.
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        From the main Crowbar UI, the upload page is under
        <span class="guimenu">Utilities</span> › <span class="guimenu">SUSE Enterprise
        Storage</span>.
       </p><p>
        If a configuration is already stored in Crowbar, it will be visible in
        the upload page. A newly uploaded configuration will replace existing
        one. The new configuration will be applied to the cloud on the next
        <code class="literal">chef-client</code> run. There is no need to reapply
        proposals.
       </p><p>
        Configurations can also be deleted from Crowbar. After deleting a
        configuration, you must manually update and reapply all proposals that
        used SES integration.
       </p></li><li class="listitem"><p>
        With the <code class="literal">crowbarctl</code> CLI, the command <code class="command">crowbarctl ses
        upload <em class="replaceable">FILE</em></code> accepts a path to the
        SES configuration file.
       </p></li></ul></div></li></ol></div></div><div class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.29.13"><span class="name">Cloud Service Configuration</span><a title="Permalink" class="permalink" href="#id-1.4.5.4.29.13">#</a></h3></div><p>
    SES integration with SUSE <span class="productname">OpenStack</span> Cloud services is implemented with relevant Barclamps
    and installed with the <code class="literal">crowbar-openstack</code> package.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.29.15.1"><span class="term">glance</span></dt><dd><p>
       Set <code class="literal">Use SES Configuration</code> to <code class="literal">true</code>
       under <code class="literal">RADOS Store Parameters</code>. The glance barclamp
       pulls the uploaded SES configuration from Crowbar when applying the
       glance proposal and on <code class="literal">chef-client</code> runs. If the SES
       configuration is uploaded before the glance proposal is created,
       <code class="literal">Use SES Configuration</code> is enabled automatically
       upon proposal creation.
      </p></dd><dt id="id-1.4.5.4.29.15.2"><span class="term">cinder</span></dt><dd><p>
       Create a new RADOS backend and set <code class="literal">Use SES
       Configuration</code> to <code class="literal">true</code>. The cinder
       barclamp pulls the uploaded SES configuration from Crowbar when applying the
       cinder proposal and on <code class="literal">chef-client</code> runs. If
       the SES configuration was uploaded before the cinder proposal
       was created, a <code class="literal">ses-ceph</code> RADOS backend is created
       automatically on proposal creation with <code class="literal">Use SES
       Configuration</code> already enabled.
      </p></dd><dt id="id-1.4.5.4.29.15.3"><span class="term">nova</span></dt><dd><p>
       To connect with volumes stores in SES, nova uses the configuration
       from the cinder barclamp.
       For ephemeral storage, nova re-uses the <code class="literal">rbd_store_user</code>
       and <code class="literal">key</code> from cinder but has a separate <code class="literal">rbd_store_pool</code>
       defined in the SES configuration. Ephemeral storage on SES can be
       enabled or disabled by setting <code class="option">Use Ceph RBD Ephemeral Backend</code>
       in nova proposal. In new deployments it is enabled by default.
       In existing ones it is disabled for compatibility reasons.
      </p></dd></dl></div><div class="sect2 bridgehead"><h3 class="title" id="id-1.4.5.4.29.16"><span class="name"><span class="productname">RADOS Gateway</span> Integration</span><a title="Permalink" class="permalink" href="#id-1.4.5.4.29.16">#</a></h3></div><p>
      Besides block storage, the SES cluster can also be used as a swift
      replacement for object storage. If <code class="literal">radosgw_urls</code> section is present
      in uploaded SES configuration, first of the URLs is registered
      in the keystone catalog as the "swift"/"object-store" service. Some
      configuration is needed on SES side to fully integrate with keystone
      auth.
      If SES integration is enabled on a cloud with swift deployed,
      SES object storage service will get higher priority by default. To
      override this and use swift for object storage instead, remove
      <code class="literal">radosgw_urls</code> section from the SES configuration file and re-upload it
      to Crowbar. Re-apply swift proposal or wait for next periodic
      chef-client run to make changes effective.
    </p></section><section class="sect1" id="sec-depl-services" data-id-title="Roles and Services in SUSE OpenStack Cloud Crowbar"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.24 </span><span class="title-name">Roles and Services in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span></span></span> <a title="Permalink" class="permalink" href="#sec-depl-services">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   The following table lists all roles (as defined in the barclamps), and their
   associated services. As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8, this list is work in
   progress. Services can be manually started and stopped with the commands
   <code class="command">systemctl start <em class="replaceable">SERVICE</em></code> and
   <code class="command">systemctl stop <em class="replaceable">SERVICE</em></code>.
  </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Role
       </p>
      </th><th style="border-bottom: 1px solid ; ">
       <p>
        Service
       </p>
      </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        ceilometer-agent
       </p>
      </td><td style="border-bottom: 1px solid ; "><code class="systemitem">
       openstack-ceilometer-agent-compute
      </code>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " rowspan="2">
       <p>
        ceilometer-central
       </p>
       <p>
        ceilometer-server
       </p>
       <p>
        ceilometer-swift-proxy-middleware
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem"> openstack-ceilometer-agent-notification
        </code>
       </p>
      </td></tr><tr><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem"> openstack-ceilometer-agent-central </code>
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " rowspan="2">
       <p>
        cinder-controller
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-cinder-api</code>
       </p>
      </td></tr><tr><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-cinder-scheduler</code>
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        cinder-volume
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-cinder-volume</code>
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        database-server
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">postgresql</code>
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " rowspan="2">
       <p>
        glance-server
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-glance-api</code>
       </p>
      </td></tr><tr><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-glance-registry</code>
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " rowspan="4">
       <p>
        heat-server
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-heat-api-cfn</code>
       </p>
      </td></tr><tr><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-heat-api-cloudwatch</code>
       </p>
      </td></tr><tr><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-heat-api</code>
       </p>
      </td></tr><tr><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-heat-engine</code>
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        horizon
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">apache2</code>
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        keystone-server
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-keystone</code>
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " rowspan="2">
       <p>
        manila-server
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-manila-api</code>
       </p>
      </td></tr><tr><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-manila-scheduler</code>
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        manila-share
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-manila-share</code>
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        neutron-server
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-neutron</code>
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " rowspan="2">
       <p>
        nova-compute-*
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-nova-compute</code>
       </p>
      </td></tr><tr><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem"> openstack-neutron-openvswitch-agent
        </code> (when neutron is deployed with openvswitch)
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " rowspan="6">
       <p>
        nova-controller
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-nova-api</code>
       </p>
      </td></tr><tr><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-nova-cert</code>
       </p>
      </td></tr><tr><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-nova-conductor</code>
       </p>
      </td></tr><tr><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-nova-novncproxy</code>
       </p>
      </td></tr><tr><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-nova-objectstore</code>
       </p>
      </td></tr><tr><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-nova-scheduler</code>
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        rabbitmq-server
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">rabbitmq-server</code>
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        swift-dispersion
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        none
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        swift-proxy
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-swift-proxy</code>
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        swift-ring-compute
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        none
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; " rowspan="14">
       <p>
        swift-storage
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-swift-account-auditor</code>
       </p>
      </td></tr><tr><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-swift-account-reaper</code>
       </p>
      </td></tr><tr><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-swift-account-replicator</code>
       </p>
      </td></tr><tr><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-swift-account</code>
       </p>
      </td></tr><tr><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-swift-container-auditor</code>
       </p>
      </td></tr><tr><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-swift-container-replicator</code>
       </p>
      </td></tr><tr><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-swift-container-sync</code>
       </p>
      </td></tr><tr><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-swift-container-updater</code>
       </p>
      </td></tr><tr><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-swift-container</code>
       </p>
      </td></tr><tr><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-swift-object-auditor</code>
       </p>
      </td></tr><tr><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-swift-object-expirer</code>
       </p>
      </td></tr><tr><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-swift-object-replicator</code>
       </p>
      </td></tr><tr><td style="border-bottom: 1px solid ; ">
       <p>
        <code class="systemitem">openstack-swift-object-updater</code>
       </p>
      </td></tr><tr><td>
       <p>
        <code class="systemitem">openstack-swift-object</code>
       </p>
      </td></tr></tbody></table></div></section><section class="sect1" id="sec-deploy-crowbatch-description" data-id-title="Crowbar Batch Command"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.25 </span><span class="title-name">Crowbar Batch Command</span></span> <a title="Permalink" class="permalink" href="#sec-deploy-crowbatch-description">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   This is the documentation for the <code class="command">crowbar batch</code>
   subcommand.
  </p><p>
   <code class="command">crowbar batch</code> provides a quick way of creating, updating,
   and applying Crowbar proposals. It can be used to:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Accurately capture the configuration of an existing Crowbar environment.
    </p></li><li class="listitem"><p>
     Drive Crowbar to build a complete new environment from scratch.
    </p></li><li class="listitem"><p>
     Capture one SUSE <span class="productname">OpenStack</span> Cloud environment and then reproduce it on another set of
     hardware (provided hardware and network configuration match to an
     appropriate extent).
    </p></li><li class="listitem"><p>
     Automatically update existing proposals.
    </p></li></ul></div><p>
   As the name suggests, <code class="command">crowbar batch</code> is intended to be run
   in <span class="quote">“<span class="quote">batch mode</span>”</span> that is mostly unattended. It has two modes of
   operation:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.31.6.1"><span class="term">crowbar batch export
    </span></dt><dd><p>
      Exports a YAML file which describes existing proposals and how their
      parameters deviate from the default proposal values for that barclamp.
     </p></dd><dt id="id-1.4.5.4.31.6.2"><span class="term">crowbar batch build
    </span></dt><dd><p>
      Imports a YAML file in the same format as above. Uses it to build new
      proposals if they do not yet exist. Updates the existing proposals so
      that their parameters match those given in the YAML file.
     </p></dd></dl></div><section class="sect2" id="sec-deploy-crowbatch-yaml" data-id-title="YAML file format"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.25.1 </span><span class="title-name">YAML file format</span></span> <a title="Permalink" class="permalink" href="#sec-deploy-crowbatch-yaml">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    Here is an example YAML file. At the top-level there is a proposals array,
    each entry of which is a hash representing a proposal:
   </p><div class="verbatim-wrap"><pre class="screen">proposals:
- barclamp: provisioner
  # Proposal name defaults to 'default'.
  attributes:
    shell_prompt: USER@ALIAS:CWD SUFFIX
- barclamp: database
  # Default attributes are good enough, so we just need to assign
  # nodes to roles:
  deployment:
    elements:
      database-server:
        - "@@controller1@@"
- barclamp: rabbitmq
  deployment:
    elements:
      rabbitmq-server:
        - "@@controller1@@"</pre></div><div id="id-1.4.5.4.31.7.4" data-id-title="Reserved Indicators in YAML" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Reserved Indicators in YAML</div><p>
     Note that the characters <code class="literal">@</code> and <code class="literal">`</code> are
     reserved indicators in YAML. They can appear anywhere in a string
     <span class="emphasis"><em>except at the beginning</em></span>. Therefore a string such as
     <code class="literal">@@controller1@@</code> needs to be quoted using double quotes.
    </p></div></section><section class="sect2" id="sec-deploy-crowbatch-yaml-attributes" data-id-title="Top-level proposal attributes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.25.2 </span><span class="title-name">Top-level proposal attributes</span></span> <a title="Permalink" class="permalink" href="#sec-deploy-crowbatch-yaml-attributes">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.31.8.2.1"><span class="term">barclamp
     </span></dt><dd><p>
       Name of the barclamp for this proposal (required).
      </p></dd><dt id="id-1.4.5.4.31.8.2.2"><span class="term">name
     </span></dt><dd><p>
       Name of this proposal (optional; default is <code class="literal">default</code>).
       In <code class="command">build</code> mode, if the proposal does not already
       exist, it will be created.
      </p></dd><dt id="id-1.4.5.4.31.8.2.3"><span class="term">attributes
     </span></dt><dd><p>
       An optional nested hash containing any attributes for this proposal
       which deviate from the defaults for the barclamp.
      </p><p>
       In <code class="command">export</code> mode, any attributes set to the default
       values are excluded to keep the YAML as short and readable as possible.
      </p><p>
       In <code class="command">build</code> mode, these attributes are deep-merged with
       the current values for the proposal. If the proposal did not already
       exist, batch build will create it first. The attributes are merged with
       the default values for the barclamp's proposal.
      </p></dd><dt id="id-1.4.5.4.31.8.2.4"><span class="term">wipe_attributes
     </span></dt><dd><p>
       An optional array of paths to nested attributes which should be removed
       from the proposal.
      </p><p>
       Each path is a period-delimited sequence of attributes; for example
       <code class="literal">pacemaker.stonith.sbd.nodes</code> would remove all SBD
       nodes from the proposal if it already exists. If a path segment contains
       a period, it should be escaped with a backslash, for example
       <code class="literal">segment-one.segment\.two.segment_three</code>.
      </p><p>
       This removal occurs before the deep merge described above.

       For example, think of a YAML file which includes a Pacemaker barclamp
       proposal where the <code class="literal">wipe_attributes</code> entry contains
       <code class="literal">pacemaker.stonith.sbd.nodes</code>. A batch build with this
       YAML file ensures that only SBD nodes listed in the <code class="literal">attributes
       sibling</code> hash are used at the end of the run. In contrast,
       without the <code class="literal">wipe_attributes</code> entry, the given SBD
       nodes would be appended to any SBD nodes already defined in the
       proposal.
      </p></dd><dt id="id-1.4.5.4.31.8.2.5"><span class="term">deployment
     </span></dt><dd><p>
       A nested hash defining how and where this proposal should be deployed.
      </p><p>
       In <code class="command">build</code> mode, this hash is deep-merged in the same
       way as the attributes hash, except that the array of elements for each
       Chef role is reset to the empty list before the deep merge. This
       behavior may change in the future.
      </p></dd></dl></div></section><section class="sect2" id="sec-deploy-crowbatch-yaml-subst" data-id-title="Node Alias Substitutions"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.25.3 </span><span class="title-name">Node Alias Substitutions</span></span> <a title="Permalink" class="permalink" href="#sec-deploy-crowbatch-yaml-subst">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    A string like <code class="literal">@@<em class="replaceable">node</em>@@</code> (where
    <em class="replaceable">node</em> is a node alias) will be substituted for
    the name of that node, no matter where the string appears in the YAML file.
    For example, if <code class="literal">controller1</code> is a Crowbar alias for node
    <code class="literal">d52-54-02-77-77-02.mycloud.com</code>, then
    <code class="literal">@@controller1@@</code> will be substituted for that host name.
    This allows YAML files to be reused across environments.
   </p></section><section class="sect2" id="sec-deploy-crowbatch-options" data-id-title="Options"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.25.4 </span><span class="title-name">Options</span></span> <a title="Permalink" class="permalink" href="#sec-deploy-crowbatch-options">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    In addition to the standard options available to every
    <code class="command">crowbar</code> subcommand (run <code class="command">crowbar batch
    --help</code> for a full list), there are some extra options
    specifically for <code class="command">crowbar batch</code>:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.31.10.3.1"><span class="term">--include &lt;barclamp[.proposal]&gt;
    </span></dt><dd><p>
       Only include the barclamp / proposals given.
      </p><p>
       This option can be repeated multiple times. The inclusion value can
       either be the name of a barclamp (for example,
       <code class="literal">pacemaker</code>) or a specifically named proposal within
       the barclamp (for example, <code class="literal">pacemaker.network_cluster</code>).
      </p><p>
       If it is specified, then only the barclamp / proposals specified are
       included in the build or export operation, and all others are ignored.
      </p></dd><dt id="id-1.4.5.4.31.10.3.2"><span class="term">--exclude &lt;barclamp[.proposal]&gt;
    </span></dt><dd><p>
       This option can be repeated multiple times. The exclusion value is the
       same format as for <code class="option">--include</code>. The barclamps / proposals
       specified are excluded from the build or export operation.
      </p></dd><dt id="id-1.4.5.4.31.10.3.3"><span class="term">--timeout &lt;seconds&gt;
    </span></dt><dd><p>
       Change the timeout for Crowbar API calls.
      </p><p>
       As Chef's run lists grow, some of the later <span class="productname">OpenStack</span> barclamp proposals
       (for example nova, horizon, or heat) can take over 5 or even 10
       minutes to apply. Therefore you may need to increase this timeout to 900
       seconds in some circumstances.
      </p></dd></dl></div></section></section></section><section class="chapter" id="sec-deploy-policy-json" data-id-title="Limiting Users Access Rights"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">13 </span><span class="title-name">Limiting Users' Access Rights</span></span> <a title="Permalink" class="permalink" href="#sec-deploy-policy-json">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>To limit users' access rights (or to define more fine-grained access
  rights), you can use Role Based Access Control (RBAC, only available with
  keystone v3). In the example below, we will create a
  new role (<code class="literal">ProjectAdmin</code>). It allows users with this role to
  add and remove other users to the <code class="literal">member</code> role on the same
  project.
  </p><p>To create a new role that can be assigned to a user-project pair, the
  following basic steps are needed:</p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Create a custom <code class="filename">policy.json</code> file for the
    keystone component. On the node where the <code class="filename">keystone-server</code> role is
    deployed, copy the file to
    <code class="filename">/etc/keystone/<em class="replaceable">CUSTOM_</em>policy.json</code>.
    For details, see <a class="xref" href="#sec-deploy-policy-json-edit" title="13.1. Editing policy.json">Section 13.1, “Editing <code class="filename">policy.json</code>”</a>.
   </p></li><li class="step"><p>Create a custom <code class="filename">keystone_policy.json</code> file for the
    horizon component. On the node where the
     <code class="literal">nova_dashboard-server</code> role is deployed, copy the custom
    <code class="filename">keystone_policy.json</code> file to
    <code class="filename">/srv/www/openstack-dashboard/openstack_dashboard/conf/</code>
    (default directory for policy files in horizon). For details, see
     <a class="xref" href="#sec-deploy-keystone-policy-json-edit" title="13.2. Editing keystone_policy.json">Section 13.2, “Editing <code class="filename">keystone_policy.json</code>”</a>.</p></li><li class="step"><p>Make the keystone component aware of the
      <code class="filename"><em class="replaceable">CUSTOM_</em>policy.json</code> file by
    editing and reapplying the <span class="guimenu">keystone</span> barclamp. For details,
    see <a class="xref" href="#sec-deploy-policy-json-keystone" title="13.3. Adjusting the keystone Barclamp Proposal">Section 13.3, “Adjusting the <span class="guimenu">keystone</span> Barclamp
   Proposal”</a>.</p></li><li class="step"><p>Make the horizon component aware of the
     <code class="filename">keystone_policy.json</code> file by editing and reapplying
    the <span class="guimenu">horizon</span> barclamp. For details, see
    <a class="xref" href="#sec-deploy-policy-json-horizon" title="13.4. Adjusting the horizon Barclamp Proposal">Section 13.4, “Adjusting the <span class="guimenu">horizon</span> Barclamp
   Proposal”</a>.</p></li></ol></div></div><section class="sect1" id="sec-deploy-policy-json-edit" data-id-title="Editing policy.json"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">13.1 </span><span class="title-name">Editing <code class="filename">policy.json</code></span></span> <a title="Permalink" class="permalink" href="#sec-deploy-policy-json-edit">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p> The <code class="filename">policy.json</code> file is located in
    <code class="filename">/etc/keystone/</code> on the node where the
    <code class="filename">keystone-server</code> role is deployed. </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Copy <code class="filename">/etc/keystone/policy.json</code> and save it under
     a different name, for example
      <code class="filename"><em class="replaceable">CUSTOM_</em>policy.json</code>. </p><div id="id-1.4.5.5.6.3.1.2" data-id-title="Use Different File Name" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Use Different File Name</div><p>If you use the same name as the original file, your custom file will
      be overwritten by the next package update.</p></div></li><li class="step"><p>To add the new role, enter the following two lines at the beginning of
     the file:</p><div class="verbatim-wrap"><pre class="screen">{
  "subadmin": "role:ProjectAdmin",
  "projectadmin": "rule:subadmin and project_id:%(target.project.id)s",
  [...]</pre></div></li><li class="step"><p>Adjust the other rules in the file accordingly:</p><div class="verbatim-wrap"><pre class="screen">  "identity:get_domain": "rule:admin_required or rule:subadmin",
  [...]
  "identity:get_project": "rule:admin_required or rule:projectadmin",
  [...]
  "identity:list_user_projects": "rule:admin_or_owner or rule:projectadmin",
  [...]
  "identity:update_project": "rule:admin_required or rule:projectadmin",
  [...]
  "identity:get_user": "rule:admin_required or rule:projectadmin",
  "identity:list_users": "rule:admin_required or rule:subadmin",
  [...]
  "identity:list_groups": "rule:admin_required or rule:subadmin",
  [...]
  "identity:list_roles": "rule:admin_required or rule:subadmin",
  [...]
  "identity:list_grants": "rule:admin_required or (rule:subadmin and project_id:%(target.project.id)s)",
  "identity:create_grant": "rule:admin_required or (rule:subadmin and project_id:%(target.project.id)s and 'member':%(target.role.name)s)",
  "identity:revoke_grant": "rule:admin_required or (rule:subadmin and project_id:%(target.project.id)s and 'member':%(target.role.name)s)",
  [...]
  "identity:list_role_assignments": "rule:admin_required or rule:subadmin",</pre></div></li><li class="step"><p>
     Save the changes.
    </p></li><li class="step"><p>
     On the node where the <code class="filename">keystone-server</code> role is
     deployed, copy the file to
     <code class="filename">/etc/keystone/<em class="replaceable">CUSTOM_</em>policy.json</code>.
     Usually, the <code class="filename">keystone-server</code> role is deployed to a Control Node
     (or to a cluster, if you use a High Availability setup).
    </p></li></ol></div></div></section><section class="sect1" id="sec-deploy-keystone-policy-json-edit" data-id-title="Editing keystone_policy.json"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">13.2 </span><span class="title-name">Editing <code class="filename">keystone_policy.json</code></span></span> <a title="Permalink" class="permalink" href="#sec-deploy-keystone-policy-json-edit">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>By default, the <code class="filename">keystone_policy.json</code> file is
   located in
    <code class="filename">/srv/www/openstack-dashboard/openstack_dashboard/conf/</code>
   on the node where the <code class="literal">nova_dashboard-server</code> role is
   deployed. It is similar (but not identical) to
    <code class="filename">policy.json</code> and defines which actions the user with a
   certain role is allowed to execute in horizon. If the user is not
   allowed to execute a certain action, the <span class="productname">OpenStack</span> Dashboard will show an
   error message.</p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Copy
      <code class="filename">/srv/www/openstack-dashboard/openstack_dashboard/conf/keystone_policy.json</code>
     and save it under a different name, for example
       <code class="filename"><em class="replaceable">CUSTOM_</em>keystone_policy.json</code>. </p><div id="id-1.4.5.5.7.3.1.2" data-id-title="Use Different File Name" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Use Different File Name</div><p>If you use the same name as the original file, your custom file will
      be overwritten by the next package update.</p></div></li><li class="step"><p>To add the new role, enter the following two lines at the beginning of
     the file:</p><div class="verbatim-wrap"><pre class="screen">{
  "subadmin": "role:ProjectAdmin",
  "projectadmin": "rule:subadmin and project_id:%(target.project.id)s",
  [...]</pre></div></li><li class="step"><p>Adjust the other rules in the file accordingly:</p><div class="verbatim-wrap"><pre class="screen">  "identity:get_project": "rule:admin_required or rule:projectadmin",
  [...]
  "identity:list_user_projects": "rule:admin_or_owner or rule:projectadmin",
  [...]
  "identity:get_user": "rule:admin_required or rule:projectadmin",
  "identity:list_users": "rule:admin_required or rule:subadmin",
  [...]
  "identity:list_roles": "rule:admin_required or rule:subadmin",
  [...]
  "identity:list_role_assignments": "rule:admin_required or rule:subadmin",</pre></div></li><li class="step"><p>Save the changes and copy the file to
       <code class="filename">/srv/www/openstack-dashboard/openstack_dashboard/conf/<em class="replaceable">CUSTOM_</em>keystone_policy.json</code>
     on the node where the <code class="literal">nova_dashboard-server</code> role is
     deployed.</p></li></ol></div></div></section><section class="sect1" id="sec-deploy-policy-json-keystone" data-id-title="Adjusting the keystone Barclamp Proposal"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">13.3 </span><span class="title-name">Adjusting the <span class="guimenu">keystone</span> Barclamp
   Proposal</span></span> <a title="Permalink" class="permalink" href="#sec-deploy-policy-json-keystone">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Log in to the Crowbar Web interface.</p></li><li class="step"><p>Select <span class="guimenu">Barclamps</span> › <span class="guimenu">All barclamps</span>.</p></li><li class="step"><p>Go to the <span class="guimenu">keystone</span> barclamp and click
      <span class="guimenu">Edit</span>.</p></li><li class="step"><p>In the <span class="guimenu">Attributes</span> section, click
     <span class="guimenu">Raw</span>. This shows the complete configuration file
     and allows you to edit it directly.</p></li><li class="step"><p>Adjust the <code class="literal">policy_file</code> parameter to point to the
       <code class="filename"><em class="replaceable">CUSTOM_</em>policy.json</code> file.
     For example:</p><div class="verbatim-wrap"><pre class="screen">{
  [...]
  "policy_file": "mypolicy.json",</pre></div></li><li class="step"><p>
     <span class="guimenu">Save</span> and <span class="guimenu">Apply</span> the changes to the
     keystone barclamp.</p></li></ol></div></div></section><section class="sect1" id="sec-deploy-policy-json-horizon" data-id-title="Adjusting the horizon Barclamp Proposal"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">13.4 </span><span class="title-name">Adjusting the <span class="guimenu">horizon</span> Barclamp
   Proposal</span></span> <a title="Permalink" class="permalink" href="#sec-deploy-policy-json-horizon">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Log in to the Crowbar Web interface.</p></li><li class="step"><p>Select <span class="guimenu">Barclamps</span> › <span class="guimenu">All barclamps</span>.</p></li><li class="step"><p>Go to the <span class="guimenu">horizon</span> barclamp and click
      <span class="guimenu">Edit</span>.</p></li><li class="step"><p>In the <span class="guimenu">Attributes</span> section, click
      <span class="guimenu">Raw</span>. This shows the complete configuration file
     and allows you to edit it directly.</p></li><li class="step"><p>If needed, adjust the <code class="literal">policy_file_path</code> parameter to
     point to the directory where you copied the newly added
       <code class="filename"><em class="replaceable">CUSTOM_</em>keystone_policy.json</code>
     file. By default, its value is an empty string—this means that
     the default directory will be used.
    </p></li><li class="step"><p>Enter the new file's name as value of the <code class="literal">identity</code>
     parameter within the <code class="literal">policy_file</code> section (<a class="xref" href="#co-horizon-barcl-policy"><span class="callout">1</span></a>):</p><div class="verbatim-wrap"><pre class="screen">{
  "policy_file_path": "",
  "policy_file": {
    "identity": "mykeystone_policy.json", <span class="callout" id="co-horizon-barcl-policy">1</span>
    "compute": "nova_policy.json",
    "volume": "cinder_policy.json",
    "image": "glance_policy.json",
    "orchestration": "heat_policy.json",
    "network": "neutron_policy.json",
    "telemetry": "ceilometer_policy.json"</pre></div></li><li class="step"><p>
     <span class="guimenu">Save</span> and <span class="guimenu">Apply</span> the changes to the
     horizon barclamp.</p></li></ol></div></div></section><section class="sect1" id="sec-deploy-policy-json-admin" data-id-title="Pre-Installed Service Admin Role Components"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">13.5 </span><span class="title-name">Pre-Installed Service Admin Role Components</span></span> <a title="Permalink" class="permalink" href="#sec-deploy-policy-json-admin">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   The following are the roles defined in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. These roles serve as a
   way to group common administrative needs at the <span class="productname">OpenStack</span> service
   level. Each role represents administrative privilege into each
   service. Multiple roles can be assigned to a user. You can assign a Service
   Admin Role to a user once you have determined that the user is authorized to
   perform administrative actions and access resources in that service.
  </p><p>
   The main components of Service Administrator Roles are:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <code class="literal">nova_admin</code> role in the identity service (keystone) and
     support in <code class="filename">nova policy.json</code>. Assign this role to
     users whose job function it is to perform <code class="literal">nova-compute</code>-related
     administrative tasks.
    </p></li><li class="listitem"><p>
     <code class="literal">neutron_admin</code> role in the identity service and support
     in <code class="filename">neutron policy.json</code>. Assign this role to users
     whose job function it is to perform neutron networking-related
     administrative tasks.
    </p></li><li class="listitem"><p>
     <code class="literal">cinder_admin</code> role in the identity service and support
     in <code class="filename">cinder policy.json</code>. Assign this role to users
     whose job function it is to perform cinder storage-related administrative
     tasks.
    </p></li><li class="listitem"><p>
     <code class="literal">glance_admin</code> role in the identity service and support
     in <code class="filename">glance policy.json</code>. Assign this role to users
     whose job function it is to perform cinder storage-related administrative
     tasks.
    </p><div id="id-1.4.5.5.10.4.4.2" data-id-title="Changing glance_policy.json may Introduce a Security Issue" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: 
      Changing <code class="filename">glance_policy.json</code> may Introduce a
      Security Issue
     </div><p>
      The <span class="productname">OpenStack</span> Security Note OSSN-0075 <a class="link" href="https://wiki.openstack.org/wiki/OSSN/OSSN-0075" target="_blank">https://wiki.openstack.org/wiki/OSSN/OSSN-0075</a> describes a
      scenario where a malicious tenant is able to reuse deleted glance image
      IDs to share malicious images with other tenants in a manner that is
      undetectable to the victim tenant.
     </p><p>
      The default policy <code class="filename">glance_policy.json</code> that is
      shipped with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> prevents this by ensuring only admins can
      deactivate/reactivate images:
     </p><div class="verbatim-wrap"><pre class="screen">"deactivate": "role:admin"
"reactivate": "role:admin"</pre></div><p>
      SUSE suggests these settings should <span class="emphasis"><em>not</em></span> be
      changed. If you do change them please refer to the OSSN-0075 <a class="link" href="https://wiki.openstack.org/wiki/OSSN/OSSN-0075" target="_blank">https://wiki.openstack.org/wiki/OSSN/OSSN-0075</a> for details
      on the exact scope of the security issue.
      </p></div></li></ul></div></section></section><section class="chapter" id="cha-depl-ostack-configs" data-id-title="Configuration Files for OpenStack Services"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">14 </span><span class="title-name">Configuration Files for <span class="productname">OpenStack</span> Services</span></span> <a title="Permalink" class="permalink" href="#cha-depl-ostack-configs">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><div><div class="abstract"><p>
    Typically, each <span class="productname">OpenStack</span> component comes with a configuration file, for
    example: <code class="filename">/etc/nova/nova.conf</code>.
   </p><p>
    These configuration files can still be used. However, to configure an
    <span class="productname">OpenStack</span> component and its different components and roles, it is now
    preferred to add custom configuration file snippets to a
    <code class="filename"><em class="replaceable">SERVICE</em>.conf.d/</code> directory
    instead.

   </p></div></div></div></div><section class="sect1" id="id-1.4.5.6.2" data-id-title="Default Configuration Files"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">14.1 </span><span class="title-name">Default Configuration Files</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.6.2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   By default, a configuration snippet with a basic configuration for each
   <span class="productname">OpenStack</span> component is available in the following directory:
  </p><div class="verbatim-wrap"><pre class="screen">/etc/<em class="replaceable">SERVICE</em>/<em class="replaceable">SERVICE</em>.conf.d/010-<em class="replaceable">SERVICE</em>.conf</pre></div><p>
   For example: <code class="filename">/etc/nova/nova.conf.d/010-nova.conf</code>
  </p><p>
   Those files should not be modified.
  </p></section><section class="sect1" id="id-1.4.5.6.3" data-id-title="Custom Configuration Files"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">14.2 </span><span class="title-name">Custom Configuration Files</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.6.3">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   To adjust or overwrite settings for the respective <span class="productname">OpenStack</span> component, add a
   custom configuration file to the same directory,
   <code class="filename">/etc/<em class="replaceable">SERVICE</em>/<em class="replaceable">SERVICE</em>.conf.d/</code>.
  </p><p>
   The same applies if you want to configure individual components or roles of
   an <span class="productname">OpenStack</span> component, such as <code class="literal">nova-api</code> or
   <code class="literal">nova-compute</code>, for example. But in this case, add your
   custom configuration file to the following directory:
  </p><div class="verbatim-wrap"><pre class="screen">/etc/<em class="replaceable">SERVICE</em>/<em class="replaceable">ROLE</em>.conf.d/</pre></div><p>
   For example: <code class="filename">/etc/nova/nova-api.conf.d/</code>
  </p><p>
   All custom configuration file must follow the rules listed in
   <a class="xref" href="#sec-depl-ostack-configs-custom-naming" title="14.3. Naming Conventions for Custom Configuration Files">Section 14.3, “Naming Conventions for Custom Configuration Files”</a>.
  </p></section><section class="sect1" id="sec-depl-ostack-configs-custom-naming" data-id-title="Naming Conventions for Custom Configuration Files"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">14.3 </span><span class="title-name">Naming Conventions for Custom Configuration Files</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-configs-custom-naming">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   Use the following rules for any configuration files you add:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The file name must start with a 3-digit number and a dash. For example:
     <code class="filename">/etc/nova/nova.conf.d/500-nova.conf</code>
    </p></li><li class="listitem"><p>
     The file must have the following file name extension:
     <code class="literal">.conf</code>
    </p></li><li class="listitem"><p>
     For configuration management systems (for example: Crowbar, Salt), use
     numbers between <code class="literal">100</code> and <code class="literal">499</code>.
    </p></li><li class="listitem"><p>
     To override settings written by the configuration management system, use
     numbers starting from <code class="literal">500</code>. They have higher priority.
    </p></li></ul></div></section><section class="sect1" id="sec-depl-ostack-configs-custom-order" data-id-title="Processing Order of Configuration Files"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">14.4 </span><span class="title-name">Processing Order of Configuration Files</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-configs-custom-order">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   The configuration files are processed in the following order:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <code class="filename">/etc/<em class="replaceable">SERVICE</em>/<em class="replaceable">SERVICE</em>.conf</code>
    </p></li><li class="listitem"><p>
     <code class="filename">/etc/<em class="replaceable">SERVICE</em>/<em class="replaceable">SERVICE</em>.conf.d/*.conf</code>
     (in dictionary order)
    </p></li><li class="listitem"><p>
     <code class="filename">/etc/<em class="replaceable">SERVICE</em>/<em class="replaceable">ROLE</em>.conf.d/*.conf</code>
     (in dictionary order)
    </p></li></ul></div><p>
   If conflicting values are set for the same parameter, the last configured
   value overwrites all previous ones. In particular, values defined in
  </p><div class="verbatim-wrap"><pre class="screen">/etc/<em class="replaceable">SERVICE</em>/<em class="replaceable">SERVICE</em>.conf.d/<em class="replaceable">XXX</em>-<em class="replaceable">SERVICE</em>.conf</pre></div><p>
   overwrite configuration values in
  </p><div class="verbatim-wrap"><pre class="screen">/etc/<em class="replaceable">SERVICE</em>/<em class="replaceable">SERVICE</em>.conf</pre></div></section><section class="sect1" id="sec-depl-ostack-configs-custom-restart" data-id-title="Restarting with New or Changed Configuration Files"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">14.5 </span><span class="title-name">Restarting with New or Changed Configuration Files</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-configs-custom-restart">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   After making changes to configuration files, you must restart the service(s)
   where the configuration changes should apply.
  </p><p>
   For example: to restart the nova service(s) with a new configuration,
   run the following command:
  </p><div class="verbatim-wrap"><pre class="screen">systemctl restart openstack-nova-compute</pre></div></section><section class="sect1" id="sec-depl-ostack-configs-custom-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">14.6 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-depl-ostack-configs-custom-more">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   For details, also see
   <code class="filename">/etc/<em class="replaceable">SERVICE</em>/README.config</code>.
  </p></section></section><section class="chapter" id="install-heat-templates" data-id-title="Installing SUSE CaaS Platform heat Templates"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">15 </span><span class="title-name">Installing SUSE CaaS Platform heat Templates</span></span> <a title="Permalink" class="permalink" href="#install-heat-templates">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
  This chapter describes how to install SUSE CaaS Platform heat template on
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
 </p><section class="sect1" id="sec-heat-templates-install" data-id-title="SUSE CaaS Platform heat Installation Procedure"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.1 </span><span class="title-name">SUSE CaaS Platform heat Installation Procedure</span></span> <a title="Permalink" class="permalink" href="#sec-heat-templates-install">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="procedure" id="id-1.4.5.7.3.2" data-id-title="Preparation"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 15.1: </span><span class="title-name">Preparation </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.7.3.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Download the latest SUSE CaaS Platform for <span class="productname">OpenStack</span> image (for example,
     <code class="filename">SUSE-CaaS-Platform-3.0-OpenStack-Cloud.x86_64-1.0.0-GM.qcow2</code>)
     from <a class="link" href="https://download.suse.com" target="_blank">https://download.suse.com</a>.
    </p></li><li class="step"><p>
     Upload the image to glance:
    </p><div class="verbatim-wrap"><pre class="screen">openstack image create --public --disk-format qcow2 --container-format \
bare --file SUSE-CaaS-Platform-3.0-OpenStack-Cloud.x86_64-1.0.0-GM.qcow2 \
CaaSP-3</pre></div></li><li class="step"><p>
     Install the <span class="package">caasp-openstack-heat-templates</span> package on a
     machine with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> repositories:
    </p><div class="verbatim-wrap"><pre class="screen">zypper in caasp-openstack-heat-templates</pre></div><p>
     The installed templates are located in
     <code class="filename">/usr/share/caasp-openstack-heat-templates</code>.
    </p><p>
     Alternatively, you can get official heat templates by cloning the
     appropriate Git repository:
    </p><div class="verbatim-wrap"><pre class="screen">git clone https://github.com/SUSE/caasp-openstack-heat-templates</pre></div></li></ol></div></div><div class="procedure" id="id-1.4.5.7.3.3" data-id-title="Installing Templates via horizon"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 15.2: </span><span class="title-name">Installing Templates via horizon </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.7.3.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     In horizon, go to
     <span class="guimenu">Project</span> › <span class="guimenu">Stacks</span> › <span class="guimenu">Launch
     Stack</span>.
    </p></li><li class="step"><p>
     Select <span class="guimenu">File</span> from the <span class="guimenu">Template Source</span>
     drop-down box and upload the <code class="filename">caasp-stack.yaml</code> file.
    </p></li><li class="step"><p>
     In the <span class="guimenu">Launch Stack</span> dialog, provide the required
     information (stack name, password, flavor size, external network of your
     environment, etc.).
    </p></li><li class="step"><p>
     Click <span class="guimenu">Launch</span> to launch the stack. This creates all
     required resources for running SUSE CaaS Platform in an <span class="productname">OpenStack</span> environment. The
     stack creates one Admin Node, one Master Node, and server worker nodes as
     specified.
    </p></li></ol></div></div><div class="procedure" id="id-1.4.5.7.3.4" data-id-title="Install Templates from the Command Line"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 15.3: </span><span class="title-name">Install Templates from the Command Line </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.7.3.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Specify the appropriate flavor and network settings in the
     <code class="filename">caasp-environment.yaml</code> file.
    </p></li><li class="step"><p>
     Create a stack in heat by passing the template, environment file, and
     parameters:
    </p><div class="verbatim-wrap"><pre class="screen">openstack stack create -t caasp-stack.yaml -e caasp-environment.yaml \
--parameter image=CaaSP-3 caasp-stack</pre></div></li></ol></div></div><div class="procedure" id="id-1.4.5.7.3.5" data-id-title="Accessing Velum SUSE CaaS Platform dashboard"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 15.4: </span><span class="title-name">Accessing Velum SUSE CaaS Platform dashboard </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.7.3.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     After the stack has been created, the Velum SUSE CaaS Platform dashboard runs on the Admin Node.
     You can access it using the Admin Node's floating IP address.
    </p></li><li class="step"><p>
     Create an account and follow the steps in the Velum SUSE CaaS Platform dashboard to complete the
     SUSE CaaS Platform installation.
    </p></li></ol></div></div><p>
   When you have successfully accessed the admin node web interface via the
   floating IP, follow the instructions at <a class="link" href="https://documentation.suse.com/suse-caasp/3/single-html/caasp-deployment/" target="_blank">https://documentation.suse.com/suse-caasp/3/single-html/caasp-deployment/</a> to
   continue the setup of SUSE CaaS Platform.
  </p></section><section class="sect1" id="id-1.4.5.7.4" data-id-title="Installing SUSE CaaS Platform with Multiple Masters"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.2 </span><span class="title-name">Installing SUSE CaaS Platform with Multiple Masters</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.7.4">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div id="id-1.4.5.7.4.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    A heat stack with load balancing and multiple master nodes can only be
    created from the command line, because horizon does not have support for nested
    heat templates.
   </p></div><p>
   Install the <span class="package">caasp-openstack-heat-templates</span> package on a
   machine with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> repositories:
  </p><div class="verbatim-wrap"><pre class="screen">zypper in caasp-openstack-heat-templates</pre></div><p>
   The installed templates are located in
   <code class="filename">/usr/share/caasp-openstack-heat-templates</code>.
  </p><p>
   A working load balancer is needed in your SUSE <span class="productname">OpenStack</span> Cloud deployment. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>
   uses HAProxy.
  </p><p>
   Verify that load balancing with HAProxy is working correctly
   in your <span class="productname">OpenStack</span> installation by creating a load balancer manually and
   checking that the <code class="literal">provisioning_status</code> changes to
   <code class="literal">Active</code>.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack loadbalancer show
&lt;<em class="replaceable">LOAD_BALANCER_ID</em>&gt;</pre></div><p>
   HAProxy is the default load balancer provider in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
  </p><p>
   The steps below can be used to set up a network, subnet, router, security
   and IPs for a test <code class="literal">lb_net1</code> network with
   <code class="literal">lb_subnet1</code> subnet.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack network create lb_net1
  <code class="prompt user">tux &gt; </code>openstack subnet create --name lb_subnet1 lb_net1 \
--subnet-range 172.29.0.0/24 --gateway 172.29.0.2
<code class="prompt user">tux &gt; </code>openstack router create lb_router1
<code class="prompt user">tux &gt; </code>openstack router add subnet lb_router1 lb_subnet1
<code class="prompt user">tux &gt; </code>openstack router set lb_router1 --external-gateway ext-net
<code class="prompt user">tux &gt; </code>openstack network list</pre></div><div class="procedure" id="id-1.4.5.7.4.12" data-id-title="Steps to Install SUSE CaaS Platform with Multiple Masters"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 15.5: </span><span class="title-name">Steps to Install SUSE CaaS Platform with Multiple Masters </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.7.4.12">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Specify the appropriate flavor and network settings in the
     <code class="filename">caasp-multi-master-environment.yaml</code> file.
    </p></li><li class="step"><p>
     Set <code class="literal">master_count</code> to the desired number in the
     <code class="filename">caasp-multi-master-environment.yaml</code> file. The master
     count must be set to an odd number of nodes.
    </p><div class="verbatim-wrap"><pre class="screen">master_count: 3</pre></div></li><li class="step"><p>
     Create a stack in heat by passing the template, environment file, and
     parameters:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack stack create -t caasp-multi-master-stack.yaml \
-e caasp-multi-master-environment.yaml --parameter image=CaaSP-3 caasp-multi-master-stack</pre></div></li><li class="step"><p>
     Find the floating IP address of the load balancer. This is necessary for
     accessing the Velum SUSE CaaS Platform dashboard.
    </p><ol type="a" class="substeps"><li class="step"><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack loadbalancer list --provider</pre></div></li><li class="step"><p>
       From the output, copy the <code class="literal">id</code> and enter it in the
       following command as shown in the following example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack loadbalancer show id</pre></div><div class="verbatim-wrap"><pre class="screen">+---------------------+------------------------------------------------+
| Field               | Value                                          |
+---------------------+------------------------------------------------+
| admin_state_up      | True                                           |
| description         |                                                |
| id                  | 0d973d80-1c79-40a4-881b-42d111ee9625           |
| listeners           | {"id": "c9a34b63-a1c8-4a57-be22-75264769132d"} |
|                     | {"id": "4fa2dae0-126b-4eb0-899f-b2b6f5aab461"} |
| name                | caasp-stack-master_lb-bhr66gtrx3ue             |
| operating_status    | ONLINE                                         |
| pools               | {"id": "8c011309-150c-4252-bb04-6550920e0059"} |
|                     | {"id": "c5f55af7-0a25-4dfa-a088-79e548041929"} |
| provider            | haproxy                                        |
| provisioning_status | ACTIVE                                         |
| tenant_id           | fd7ffc07400642b1b05dbef647deb4c1               |
| vip_address         | 172.28.0.6                                     |
| vip_port_id         | 53ad27ba-1ae0-4cd7-b798-c96b53373e8b           |
| vip_subnet_id       | 87d18a53-ad0c-4d71-b82a-050c229b710a           |
+---------------------+------------------------------------------------+</pre></div></li><li class="step"><p>
       Search the floating IP list for <code class="literal">vip_address</code>
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack floating ip list | grep 172.28.0.6
| d636f3...481b0c | fd7ff...deb4c1 | 172.28.0.6  | 10.84.65.37  | 53ad2...373e8b |</pre></div></li><li class="step"><p>
       The load balancer floating ip address is 10.84.65.37
      </p></li></ol></li></ol></div></div><p>
   <span class="bold"><strong>Accessing the Velum SUSE CaaS Platform Dashboard</strong></span>
  </p><p>
   After the stack has been created, the Velum SUSE CaaS Platform dashboard runs on the
   admin node. You can access it using the floating IP address of the admin
   node.
  </p><p>
   Create an account and follow the steps in the Velum SUSE CaaS Platform dashboard to
   complete the SUSE CaaS Platform installation.
  </p><p>
   SUSE CaaS Platform Admin Node Install: Screen 1
  </p><p>
   If you plan to manage your containers using Helm or Airship (this is common),
   check the box labeled <code class="literal">Install Tiller (Helm's server component)</code>.
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/caasp_1.png"><img src="images/caasp_1.png" width="90%" alt="Image" title="Image"/></a></div></div><p>
   SUSE CaaS Platform Admin Node Install: Screen 2
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/caasp_2.png"><img src="images/caasp_2.png" width="90%" alt="Image" title="Image"/></a></div></div><p>
   SUSE CaaS Platform Admin Node Install: Screen 3
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/caasp_3.png"><img src="images/caasp_3.png" width="90%" alt="Image" title="Image"/></a></div></div><p>
   SUSE CaaS Platform Admin Node Install: Screen 4
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/caasp_4.png"><img src="images/caasp_4.png" width="90%" alt="Image" title="Image"/></a></div></div><p>
   SUSE CaaS Platform Admin Node Install: Screen 5
  </p><p>
   Set External Kubernetes API to
   <em class="replaceable">LOADBALANCER_FLOATING_IP</em>, External Dashboard FQDN
   to <em class="replaceable">ADMIN_NODE_FLOATING_IP</em>
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/caasp_5.png"><img src="images/caasp_5.png" width="90%" alt="Image" title="Image"/></a></div></div><p>
   SUSE CaaS Platform Admin Node Install: Screen 6
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/caasp_6.png"><img src="images/caasp_6.png" width="90%" alt="Image" title="Image"/></a></div></div><p>
   SUSE CaaS Platform Admin Node Install: Screen 7
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/caasp_7.png"><img src="images/caasp_7.png" width="90%" alt="Image" title="Image"/></a></div></div></section><section class="sect1" id="id-1.4.5.7.5" data-id-title="Enabling the Cloud Provider Integration (CPI) Feature"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.3 </span><span class="title-name">Enabling the Cloud Provider Integration (CPI) Feature</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.7.5">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   When deploying a CaaaSP cluster using SUSE CaaS Platform <span class="productname">OpenStack</span> heat
   templates, the following CPI parameters can be set in
   <code class="filename">caasp-environment.yaml</code> or
   <code class="filename">caasp-multi-master-environment.yaml</code>.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.7.5.3.1"><span class="term">cpi_auth_url</span></dt><dd><p>
      The URL of the keystone API used to authenticate the user. This value
      can be found on <span class="productname">OpenStack</span> Dashboard under
      <span class="guimenu">Access and Security</span> › <span class="guimenu">API
      Access</span> › <span class="guimenu">Credentials</span> (for
      example, https://api.keystone.example.net:5000/)
     </p></dd><dt id="id-1.4.5.7.5.3.2"><span class="term">cpi_domain_name</span></dt><dd><p>
      Name of the domain the user belongs to.
     </p></dd><dt id="id-1.4.5.7.5.3.3"><span class="term">cpi_tenant_name</span></dt><dd><p>
      Name of the project the user belongs to. This is the project in which
      SUSE CaaS Platform resources are created.
     </p></dd><dt id="id-1.4.5.7.5.3.4"><span class="term">cpi_region</span></dt><dd><p>
      Name of the region to use when running a multi-region <span class="productname">OpenStack</span>
      cloud. The region is a general division of an <span class="productname">OpenStack</span> deployment.
     </p></dd><dt id="id-1.4.5.7.5.3.5"><span class="term">cpi_username</span></dt><dd><p>
      Username of a valid user that has been set in keystone. Default: admin
     </p></dd><dt id="id-1.4.5.7.5.3.6"><span class="term">cpi_password</span></dt><dd><p>
      Password of a valid user that has been set in keystone.
     </p></dd><dt id="id-1.4.5.7.5.3.7"><span class="term">cpi_monitor_max_retries</span></dt><dd><p>
      neutron load balancer monitoring max retries. Default: 3
     </p></dd><dt id="id-1.4.5.7.5.3.8"><span class="term">cpi_bs_version</span></dt><dd><p>
      cinder Block Storage API version. Possible values are v1, v2 , v3 or
      auto. Default: <code class="literal">auto</code>
     </p></dd><dt id="id-1.4.5.7.5.3.9"><span class="term">cpi_ignore_volume_az</span></dt><dd><p>
      Ignore cinder and nova availability zones. Default: <code class="literal">true</code>
     </p></dd><dt id="id-1.4.5.7.5.3.10"><span class="term">dns_nameserver</span></dt><dd><p>
      Set this to the IP address of a DNS nameserver accessible by the SUSE CaaS Platform
      cluster.
     </p></dd></dl></div><p>
   Immediately after the SUSE CaaS Platform cluster comes online, and before bootstrapping,
   install the latest SUSE CaaS Platform 3.0 Maintenance Update using the following steps.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Register the SUSE CaaS Platform nodes for Maintenance Updates by following the
     instructions in <a class="xref" href="#sec-heat-templates-register" title="15.4. Register SUSE CaaS Platform Cluster for Software Updates">Section 15.4, “Register SUSE CaaS Platform Cluster for Software Updates”</a>.
    </p></li><li class="step"><p>
     On each of the SUSE CaaS Platform nodes, install the latest Maintenance Update:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo transactional-update</pre></div><p>
     Verify that the Velum image packages were updated:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper se --detail velum-image
i | sles12-velum-image | package    | 3.1.7-3.27.3  | x86_64 | update_caasp</pre></div><p>
     Reboot the node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo reboot</pre></div></li><li class="step"><p>
     Finally, when preparing to bootstrap using the SUSE CaaS Platform web interface,
     upload a valid trust certificate that can validate a certificate
     presented by keystone at the specified
     <code class="literal">keystone_auth_url</code> in the <code class="literal">System-wide
     certificate</code> section of Velum. If the SSL certificate provided
     by keystone cannot be verified, bootstrapping fails with the error
     <code class="literal">x509: certificate signed by unknown authority</code>.
    </p><div id="id-1.4.5.7.5.5.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      If your <span class="productname">OpenStack</span> endpoints operate on the Internet, or if the SSL
      certificates in use have been signed by a public authority, no action
      should be needed to enable secure communication with them.
     </p><p>
      If your <span class="productname">OpenStack</span> services operate in a private network using SSL
      certificates signed by an organizational certificate authority, provide
      that CA certificate as the system-wide certificate.
     </p><p>
      If your <span class="productname">OpenStack</span> service SSL infrastructure was self-signed during the
      installation of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 9 (as is done by default), its CA certificate
      (with the file extension <code class="literal">.pem</code>) can be retrieved from
      the admin node in the <code class="filename">/etc/ssl/certs/</code> directory. The
      filename should match the node name of your primary controller
      node. Download this file and provide it as the system-wide
      certificate.</p></div><p>
     The CPI configuration settings match the values provided
     via the <code class="filename">caasp-environment.yaml</code> or
     <code class="filename">caasp-multi-master-environment.yaml</code> files.
     Verify that they are correct before proceeding.
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/cpi_1.png"><img src="images/cpi_1.png" width="90%" alt="Image" title="Image"/></a></div></div></li></ol></div></div></section><section class="sect1" id="sec-heat-templates-register" data-id-title="Register SUSE CaaS Platform Cluster for Software Updates"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.4 </span><span class="title-name">Register SUSE CaaS Platform Cluster for Software Updates</span></span> <a title="Permalink" class="permalink" href="#sec-heat-templates-register">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   Software updates are published for all registered users of SUSE CaaS Platform, and
   should always be enabled upon deploying a new cluster.
  </p><p>
   These steps may be performed on cluster nodes one at a time, or in parallel,
   making SSH connections as the <code class="literal">root</code> user with the password
   that was established in your
   <code class="filename">/usr/share/caasp-openstack-heat-templates/caasp-environment.yaml</code> file.
  </p><div id="id-1.4.5.7.6.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    If using a private SMT server for registration, use its hostname or IP
    address when running the commands below. Otherwise, use
    <code class="literal">scc.suse.com</code> to connect to SUSE's public registration
    server.
   </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     If this node was previously registered, deactivate its current registration:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo SUSEConnect -d
<code class="prompt user">tux &gt; </code>sudo SUSEConnect --cleanup</pre></div></li><li class="step"><p>
     If you are registering with a private SMT server, install its SSL
     certificate or the related organizational CA in order to perform SMT
     operations securely.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo curl <em class="replaceable">SMT_SERVER</em>/smt.crt \
  -o /etc/pki/trust/anchors/registration-server.pem
<code class="prompt user">tux &gt; </code>sudo update-ca-certificates</pre></div></li><li class="step"><p>
     Establish the new system registration:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo SUSEConnect --write-config --url https://<em class="replaceable">SMT_SERVER</em> \
  -r <em class="replaceable">REGISTRATION_CODE</em> -e <em class="replaceable">EMAIL_ADDRESS</em></pre></div><p>
     The same registration code may be used for all the nodes in your cluster.
    </p></li><li class="step"><p>
     Test the registration and look for a status of <code class="literal">Registered</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo SUSEConnect --status-text</pre></div></li></ol></div></div></section><section class="sect1" id="id-1.4.5.7.7" data-id-title="More Information about SUSE CaaS Platform"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.5 </span><span class="title-name">More Information about SUSE CaaS Platform</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.7.7">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   More information about the SUSE CaaS Platform is available at <a class="link" href="https://documentation.suse.com/suse-caasp/3/single-html/caasp-deployment/" target="_blank">https://documentation.suse.com/suse-caasp/3/single-html/caasp-deployment/</a>
  </p></section></section><section class="chapter" id="install-caasp-terraform" data-id-title="Installing SUSE CaaS Platform v4 using terraform"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">16 </span><span class="title-name">Installing SUSE CaaS Platform v4 using terraform</span></span> <a title="Permalink" class="permalink" href="#install-caasp-terraform">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><section class="sect1" id="id-1.4.5.8.2" data-id-title="CaaSP v4 deployment on SOC using terraform."><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">16.1 </span><span class="title-name">CaaSP v4 deployment on SOC using terraform.</span></span> <a title="Permalink" class="permalink" href="#id-1.4.5.8.2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   More information about the SUSE CaaS Platform v4 is available at <a class="link" href="https://documentation.suse.com/suse-caasp/4.0/html/caasp-deployment/_deployment_instructions.html#_deployment_on_suse_openstack_cloud" target="_blank">https://documentation.suse.com/suse-caasp/4.0/html/caasp-deployment/_deployment_instructions.html#_deployment_on_suse_openstack_cloud</a>
  </p><div id="id-1.4.5.8.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
   For SOC deployments that support Octavia, set export OS_USE_OCTAVIA=true in the downloaded openstack rc file in order for the load balancing API requests to the octavia service instead of the networking service.
  </p></div></section></section></div><div class="part" id="part-depl-nostack" data-id-title="Setting Up Non-OpenStack Services"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">Part IV </span><span class="title-name">Setting Up Non-<span class="productname">OpenStack</span> Services </span></span><a title="Permalink" class="permalink" href="#part-depl-nostack">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-depl-nostack"><span class="title-number">17 </span><span class="title-name">Deploying the Non-<span class="productname">OpenStack</span> Components</span></a></span></li><dd class="toc-abstract"><p>
    In addition to <span class="productname">OpenStack</span> barclamps, SUSE <span class="productname">OpenStack</span> Cloud includes several components that can be configured using the appropriate  Crowbar barclamps.
   </p></dd></ul></div><section class="chapter" id="cha-depl-nostack" data-id-title="Deploying the Non-OpenStack Components"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">17 </span><span class="title-name">Deploying the Non-<span class="productname">OpenStack</span> Components</span></span> <a title="Permalink" class="permalink" href="#cha-depl-nostack">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    In addition to <span class="productname">OpenStack</span> barclamps, SUSE <span class="productname">OpenStack</span> Cloud includes several components that can be configured using the appropriate  Crowbar barclamps.
   </p><section class="sect1" id="sec-depl-nostack-crowbar-tuning" data-id-title="Tuning the Crowbar Service"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">17.1 </span><span class="title-name">Tuning the Crowbar Service</span></span> <a title="Permalink" class="permalink" href="#sec-depl-nostack-crowbar-tuning">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    Crowbar is a self-referential barclamp used for enabling other barclamps. By
    creating a Crowbar proposal, you can modify the default number of threads
    and workers. This way, you can scale the admin
    server according to the actual usage or the number of available cores of
    the admin node.
  </p><div class="figure" id="id-1.4.6.2.4.3"><div class="figure-contents"><div class="mediaobject"><a href="images/crowbar_tuning_raw.png"><img src="images/crowbar_tuning_raw.png" width="75%" alt="The Crowbar barclamp: Raw Mode" title="The Crowbar barclamp: Raw Mode"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 17.1: </span><span class="title-name">The Crowbar barclamp: Raw Mode </span></span><a title="Permalink" class="permalink" href="#id-1.4.6.2.4.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div><p>
    To change the default settings, create a Crowbar proposal and switch to the
    <span class="guimenu">Raw</span> view. Adjust then the
    <code class="systemitem">workers</code> and <code class="systemitem">threads</code>
    values. The number of threads should be set to the number of available
    cores. The default number of workers should be increased to 3 if the
    graphical interface becomes slow. Save and apply the changes using the
    appropriate buttons.
  </p></section><section class="sect1" id="sec-depl-nostack-ntp" data-id-title="Configuring the NTP Service"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">17.2 </span><span class="title-name">Configuring the NTP Service</span></span> <a title="Permalink" class="permalink" href="#sec-depl-nostack-ntp">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    The NTP service is responsible for keeping the clocks in your cloud servers
    in sync. Among other things, synchronized clocks ensure that the
    chef-client works properly. It also makes it easier to read logs from
    different nodes by correlating timestamps in them. The NTP component is deployed on the Administration Server automatically using the default settings. The NTP barclamp can be used to specify IP addresses of the external NTP servers and assign specific roles to the desired nodes. The following parameter can be configured using the NTP barclamp:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.6.2.5.3.1"><span class="term">External servers</span></dt><dd><p>
	  A comma-separated list of IP addresses of external NTP servers.
	</p></dd></dl></div><p>
    The NTP service consists of two different roles:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.6.2.5.5.1"><span class="term"><span class="guimenu">ntp-server</span>
    </span></dt><dd><p>
      A node that acts as an NTP server for NTP clients in your cloud. There
      can be more than one node with the ntp-server role in your cloud. In
      this scenario, the NTP server nodes can communicate with each other and the specified external servers to
      keep their time in sync.
     </p></dd><dt id="id-1.4.6.2.5.5.2"><span class="term"><span class="guimenu">ntp-client</span>
    </span></dt><dd><p>
      The <code class="systemitem">ntp-client</code> role can be assigned to any
      node. Nodes with the ntp-client role assigned to them keep their time in
      sync using NTP servers in your cloud.
     </p></dd></dl></div></section><section class="sect1" id="sec-depl-nostack-salt" data-id-title="Installing and using Salt"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">17.3 </span><span class="title-name">Installing and using Salt</span></span> <a title="Permalink" class="permalink" href="#sec-depl-nostack-salt">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    Crowbar can setup Salt on the admin node to be able to use
    <code class="command">salt-ssh</code> from the admin node.
  </p><div id="id-1.4.6.2.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
        Salt is not replacing Chef. It can be used in parallel to Chef
        to automate tasks on the nodes.
      </p></div><div id="id-1.4.6.2.6.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
        Only <code class="command">salt-ssh</code> can currently be used. Installing the
        Salt barclamp does not setup a full Salt stack with Salt Master and
        Salt Minions.
      </p></div><p>
    To be able to apply the Salt proposal, the
    <a class="link" href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-deployment/#sec-yast-install-addons" target="_blank">https://documentation.suse.com/sles/15-SP1/single-html/SLES-deployment/#sec-yast-install-addons</a>
    must be available on the node where the <code class="literal">salt-ssh</code> role will be applied
    (usually the admin node).
    After the Module is available, the barclamp can be applied.
  </p><p>
    From the node where the <code class="literal">salt-ssh</code> role is applied,
    Salt can be tested with:
  </p><div class="verbatim-wrap"><pre class="screen"># salt-ssh '*' test.ping
crowbar:
    True
storage1:
    True
controller:
    True</pre></div><p>
    To list the available nodes visible to <code class="literal">salt-ssh</code>, do:
  </p><div class="verbatim-wrap"><pre class="screen"># salt-ssh -H
/etc/salt/roster:
    ----------
    controller:
        192.168.192.81
    crowbar:
        192.168.192.10
    storage1:
        192.168.192.82</pre></div></section></section></div><div class="part" id="part-depl-troubleshooting" data-id-title="Troubleshooting and Support"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">Part V </span><span class="title-name">Troubleshooting and Support </span></span><a title="Permalink" class="permalink" href="#part-depl-troubleshooting">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-depl-trouble"><span class="title-number">18 </span><span class="title-name">Troubleshooting and Support</span></a></span></li><dd class="toc-abstract"><p>
  Find solutions for the most common pitfalls and technical details on how
  to create a support request for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> here.
 </p></dd></ul></div><section class="chapter" id="cha-depl-trouble" data-id-title="Troubleshooting and Support"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">18 </span><span class="title-name">Troubleshooting and Support</span></span> <a title="Permalink" class="permalink" href="#cha-depl-trouble">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
  Find solutions for the most common pitfalls and technical details on how
  to create a support request for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> here.
 </p><section class="sect1" id="sec-depl-trouble-faq" data-id-title="FAQ"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.1 </span><span class="title-name">FAQ</span></span> <a title="Permalink" class="permalink" href="#sec-depl-trouble-faq">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   If your problem is not mentioned here, checking the log files on either
   the Administration Server or the <span class="productname">OpenStack</span> nodes may help. A list of log files
   is available at <span class="intraxref">Book “<em class="citetitle">Operations Guide Crowbar</em>”, Chapter 5 “Log Management”, Section 5.4 “Log Files”</span>.
  </p><div class="qandaset" id="id-1.4.7.2.4.3"><div class="qandadiv-title-wrap"><h4 class="qandadiv-title" id="sec-depl-trouble-faq-admin">1. Admin Node Deployment</h4></div><div class="qandadiv"><div class="free-id" id="id-1.4.7.2.4.3.1.2"> </div><dl class="qandaentry"><dt class="question" id="id-1.4.7.2.4.3.1.2.1"><strong>Q:</strong>
       What to do if the initial SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation on the Administration Server fails?
      </dt><dd class="answer" id="id-1.4.7.2.4.3.1.2.2"><p>
       Check the installation routine's log file at
       <code class="filename">/var/log/crowbar/install.log</code> for error messages.
      </p></dd></dl><div class="free-id" id="id-1.4.7.2.4.3.1.3"> </div><dl class="qandaentry"><dt class="question" id="id-1.4.7.2.4.3.1.3.1"><strong>Q:</strong>

       What to do if the initial SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation on the Administration Server fails while
       deploying the IPMI/BMC network?
      </dt><dd class="answer" id="id-1.4.7.2.4.3.1.3.2"><p>
       As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8, it is assumed that each
       machine can be accessed directly via IPMI/BMC. However, this is not
       the case on certain blade hardware, where several nodes are accessed
       via a common adapter. Such a hardware setup causes an error on
       deploying the IPMI/BMC network. You need to disable the IPMI
       deployment running the following command:
      </p><div class="verbatim-wrap"><pre class="screen">/opt/dell/bin/json-edit -r -a "attributes.ipmi.bmc_enable" \
-v "false" /opt/dell/chef/data_bags/crowbar/bc-template-ipmi.json</pre></div><p>
       Re-run the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation after having disabled the IPMI deployment.
      </p></dd></dl><div class="free-id" id="id-1.4.7.2.4.3.1.4"> </div><dl class="qandaentry"><dt class="question" id="id-1.4.7.2.4.3.1.4.1"><strong>Q:</strong>
       Why am I not able to reach the Administration Server from outside the admin
       network via the bastion network?
      </dt><dd class="answer" id="id-1.4.7.2.4.3.1.4.2"><p>
       If <code class="command">route</code> <code class="option">-n</code> shows no gateway for
       the bastion network, check the value of the following entries in
       <code class="filename">/etc/crowbar/network.json</code>:
       <code class="literal">"router_pref":</code> and <code class="literal">"router_pref":</code>.
       Make sure the value for the bastion network's
       <code class="literal">"router_pref":</code> is set to a
       <span class="emphasis"><em>lower</em></span> value than <code class="literal">"router_pref":</code>
       for the admin network.
      </p><p>
       If the router preference is set correctly, <code class="command">route</code>
       <code class="option">-n</code> shows a gateway for the bastion network. In case
       the Administration Server is still not accessible via its admin network
       address (for example,
       <code class="systemitem">192.168.124.10</code>), you need
       to disable route verification (<code class="literal">rp_filter</code>). Do so
       by running the following command on the Administration Server:
      </p><div class="verbatim-wrap"><pre class="screen">echo 0 &gt; /proc/sys/net/ipv4/conf/all/rp_filter</pre></div><p>
       If this setting solves the problem, make it permanent by editing
       <code class="filename">/etc/sysctl.conf</code> and setting the value for
       <code class="literal">net.ipv4.conf.all.rp_filter</code> to
       <code class="literal">0</code>.
      </p></dd></dl><div class="free-id" id="id-1.4.7.2.4.3.1.5"> </div><dl class="qandaentry"><dt class="question" id="id-1.4.7.2.4.3.1.5.1"><strong>Q:</strong>
       Can I change the host name of the Administration Server?
      </dt><dd class="answer" id="id-1.4.7.2.4.3.1.5.2"><p>
       No, after you have run the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation you cannot change the host name.
       Services like Crowbar, Chef, and the RabbitMQ will fail after changing
       the host name.
      </p></dd></dl><div class="free-id" id="id-1.4.7.2.4.3.1.6"> </div><dl class="qandaentry"><dt class="question" id="id-1.4.7.2.4.3.1.6.1"><strong>Q:</strong>
       What to do when browsing the Chef Web UI gives a
       <code class="literal">Tampered with cookie</code> error?
      </dt><dd class="answer" id="id-1.4.7.2.4.3.1.6.2"><p>
       You probably have an old cookie in your browser from a previous
       Chef installation on the same IP address. Remove the cookie named
       <code class="literal">_chef_server_session_id</code> and try again.
      </p></dd></dl><div class="free-id" id="q-depl-trouble-faq-admin-custom-repos"> </div><dl class="qandaentry"><dt class="question" id="id-1.4.7.2.4.3.1.7.1"><strong>Q:</strong>
       How to make custom software repositories from an external server (for
       example a remote SMT or SUSE Manager server) available for the nodes?
      </dt><dd class="answer" id="id-1.4.7.2.4.3.1.7.2"><p>
       Custom repositories need to be added using the YaST Crowbar
       module:
      </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
         Start the YaST Crowbar module and switch to the
         <span class="guimenu">Repositories</span> tab: <span class="guimenu">YaST</span> › <span class="guimenu">Miscellaneous</span> › <span class="guimenu">Crowbar</span> › <span class="guimenu">Repositories</span>.
        </p></li><li class="step"><p>
         Choose <span class="guimenu">Add Repositories</span>
        </p></li><li class="step"><p>
         Enter the following data:
        </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.7.2.4.3.1.7.2.2.3.2.1"><span class="term"><span class="guimenu">Name</span>
          </span></dt><dd><p>
            A unique name to identify the repository.
           </p></dd><dt id="id-1.4.7.2.4.3.1.7.2.2.3.2.2"><span class="term"><span class="guimenu">URL</span>
          </span></dt><dd><p>
            Link or path to the repository.
           </p></dd><dt id="id-1.4.7.2.4.3.1.7.2.2.3.2.3"><span class="term"><span class="guimenu">Ask On Error</span>
          </span></dt><dd><p>
            Access errors to a repository are silently ignored by default.
            To ensure that you get notified of these errors, set the
            <code class="literal">Ask On Error</code> flag.
           </p></dd><dt id="id-1.4.7.2.4.3.1.7.2.2.3.2.4"><span class="term"><span class="guimenu">Target Platform/Architecture</span>
          </span></dt><dd><p>
            Currently only repositories for SUSE Linux Enterprise Server 12 SP4 on
            the x86-64 architecture are supported. Make
            sure to select both options.
           </p></dd></dl></div></li><li class="step"><p>
         Save your settings selecting <span class="guimenu">OK</span>.
        </p></li></ol></div></div></dd></dl></div><div class="qandadiv-title-wrap"><h4 class="qandadiv-title" id="sec-depl-trouble-faq-ostack">2. <span class="productname">OpenStack</span> Node Deployment</h4></div><div class="qandadiv"><div class="free-id" id="var-depl-trouble-faq-ostack-login"> </div><dl class="qandaentry"><dt class="question" id="id-1.4.7.2.4.3.2.2.1"><strong>Q:</strong>
       How can I log in to a node as <code class="systemitem">root</code>?
      </dt><dd class="answer" id="id-1.4.7.2.4.3.2.2.2"><p>
       By default you cannot directly log in to a node as <code class="systemitem">root</code>,
       because the nodes were set up without a <code class="systemitem">root</code> password. You
       can only log in via SSH from the Administration Server. You should be able to
       log in to a node with <code class="command">ssh root@<em class="replaceable">NAME</em></code>
       where <em class="replaceable">NAME</em>
       is the name (alias) of the node.
      </p><p>
       If name resolution does not work, go to the Crowbar Web interface
       and open the <span class="guimenu">Node Dashboard</span>. Click the name of the
       node and look for its <span class="guimenu">admin (eth0)</span> <span class="guimenu">IP
       Address</span>. Log in to that IP address via SSH as user
       <code class="systemitem">root</code>.
      </p></dd></dl><div class="free-id" id="id-1.4.7.2.4.3.2.3"> </div><dl class="qandaentry"><dt class="question" id="id-1.4.7.2.4.3.2.3.1"><strong>Q:</strong>
       What to do if a node refuses to boot or boots into a previous
       installation?
      </dt><dd class="answer" id="id-1.4.7.2.4.3.2.3.2"><p>
       Make sure to change the boot order in the BIOS of the node, so that
       the first boot option is to boot from the network/boot using PXE.
      </p></dd></dl><div class="free-id" id="id-1.4.7.2.4.3.2.4"> </div><dl class="qandaentry"><dt class="question" id="id-1.4.7.2.4.3.2.4.1"><strong>Q:</strong>
       What to do if a node hangs during hardware discovery after the very
       first boot using PXE into the <span class="quote">“<span class="quote">SLEShammer</span>”</span> image?
      </dt><dd class="answer" id="id-1.4.7.2.4.3.2.4.2"><p>
       The <code class="systemitem">root</code> login is enabled at a very early state in discovery
       mode, so chances are high that you can log in for debugging purposes as
       described in <a class="xref" href="#var-depl-trouble-faq-ostack-login" title="Q:"><em>
       How can I log in to a node as root?
      </em></a>.  If
       logging in as <code class="systemitem">root</code> does not work, you need to set the <code class="systemitem">root</code>
       password manually. This can either be done by setting the password via
       the Kernel command line as explained in <a class="xref" href="#qa-depl-trouble-faq-ostack-rootpw" title="Q:"><em>
       How to provide Kernel Parameters for the SLEShammer Discovery Image?
      </em></a>, or by creating a hook as
       explained below:
      </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
         Create a directory on the Administration Server named
         <code class="filename">/updates/discovering-pre</code>
        </p><div class="verbatim-wrap"><pre class="screen">mkdir /updates/discovering-pre</pre></div></li><li class="step"><p>
         Create a hook script <code class="filename">setpw.hook</code> in the
         directory created in the previous step:
        </p><div class="verbatim-wrap"><pre class="screen">cat &gt; /updates/discovering-pre/setpw.hook &lt;&lt;EOF
#!/bin/sh
echo "root:linux" | chpasswd
EOF</pre></div></li><li class="step"><p>
         Make the script executable:
        </p><div class="verbatim-wrap"><pre class="screen">chmod a+x  /updates/discovering-pre/setpw.hook</pre></div></li></ol></div></div><p>
       If you are still cannot log in, you very likely hit a bug in the
       discovery image. Report it at
       <a class="link" href="http://bugzilla.suse.com/" target="_blank">http://bugzilla.suse.com/</a>.
      </p></dd></dl><div class="free-id" id="qa-depl-trouble-faq-ostack-rootpw"> </div><dl class="qandaentry"><dt class="question" id="id-1.4.7.2.4.3.2.5.1"><strong>Q:</strong>
       How to provide Kernel Parameters for the SLEShammer Discovery Image?
      </dt><dd class="answer" id="id-1.4.7.2.4.3.2.5.2"><p>
       Kernel Parameters for the SLEShammer Discovery Image can be provided
       via the Provisioner barclamp. The following example shows how to set a
       <code class="systemitem">root</code> password:
      </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
         Open a browser and point it to the Crowbar Web interface available on
         the Administration Server, for example
         <code class="literal">http://192.168.124.10/</code>. Log in as user <code class="systemitem">crowbar</code>. The password is
         <code class="literal">crowbar</code> by default, if you have not changed it.
        </p></li><li class="step"><p>
         Open <span class="guimenu">Barclamps</span> › <span class="guimenu">Crowbar</span> and click
         <span class="guimenu">Edit</span> in the <span class="guimenu">Provisioner</span> row.
        </p></li><li class="step"><p>
         Click <span class="guimenu">Raw</span> in the <span class="guimenu">Attributes</span>
         section and add the Kernel parameter(s) to the <code class="literal">"discovery":
         { "append": "" }</code> line, for example;
        </p><div class="verbatim-wrap"><pre class="screen">  "discovery": {
    "append": "DISCOVERY_ROOT_PASSWORD=<em class="replaceable">PASSWORD</em>"
  },</pre></div></li><li class="step"><p>
         <span class="guimenu">Apply</span> the proposal without changing the
         assignments in the <span class="guimenu">Deployment</span> section.
        </p></li></ol></div></div></dd></dl><div class="free-id" id="id-1.4.7.2.4.3.2.6"> </div><dl class="qandaentry"><dt class="question" id="id-1.4.7.2.4.3.2.6.1"><strong>Q:</strong>
       What to do when a deployed node fails to boot using PXE with the
       following error message: <span class="quote">“<span class="quote">Could not find kernel image:
       ../suse-12.2/install/boot/x86_64/loader/linux</span>”</span>?
      </dt><dd class="answer" id="id-1.4.7.2.4.3.2.6.2"><p>
       The installation repository on the
       Administration Server at <code class="filename">/srv/tftpboot/suse-12.3/install</code>
       has not been set up correctly to contain the SUSE Linux Enterprise Server 12 SP4
       installation media. Review the instructions at
       <a class="xref" href="#sec-depl-adm-conf-repos-product" title="5.1. Copying the Product Media Repositories">Section 5.1, “Copying the Product Media Repositories”</a>.
      </p></dd></dl><div class="free-id" id="id-1.4.7.2.4.3.2.7"> </div><dl class="qandaentry"><dt class="question" id="id-1.4.7.2.4.3.2.7.1"><strong>Q:</strong>
       Why does my deployed node hang at <code class="literal">Unpacking
       initramfs</code> during boot when using PXE?
      </dt><dd class="answer" id="id-1.4.7.2.4.3.2.7.2"><p>
       The node probably does not have enough RAM. You need at least 4 GB
       RAM for the deployment process to work.
      </p></dd></dl><div class="free-id" id="q-depl-trouble-faq-ostack-problem"> </div><dl class="qandaentry"><dt class="question" id="id-1.4.7.2.4.3.2.8.1"><strong>Q:</strong>
       What to do if a node is reported to be in the state
       <code class="literal">Problem</code>? What to do if a node hangs at
       <span class="quote">“<span class="quote">Executing AutoYast script:
           /usr/sbin/crowbar_join --setup</span>”</span> after the installation is finished?
      </dt><dd class="answer" id="id-1.4.7.2.4.3.2.8.2"><p>
       Be patient—the AutoYaST script may take a while to finish. If
       it really hangs, log in to the node as <code class="systemitem">root</code> (see
       <a class="xref" href="#var-depl-trouble-faq-ostack-login" title="Q:"><em>
       How can I log in to a node as root?
      </em></a> for details).
       Check for error messages at the end of
       <code class="filename">/var/log/crowbar/crowbar_join/chef.log</code>. Fix the
       errors and restart the AutoYaST script by running the following command:
      </p><div class="verbatim-wrap"><pre class="screen">crowbar_join --start</pre></div><p>
       If successful, the node will be listed in state
       <code class="literal">Ready</code>, when the script has finished.
      </p><p>
       In cases where the initial --setup wasn't able to complete successfully,
       you can rerun that once after the previous issue is solved.
      </p><p>
       If that does not help or if the log does not provide useful
       information, proceed as follows:
      </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
         Log in to the Administration Server and run the following command:
        </p><div class="verbatim-wrap"><pre class="screen">crowbar crowbar transition $<em class="replaceable">NODE</em></pre></div><p>
         <em class="replaceable">NODE</em> needs to be replaced by the alias
         name you have given to the node when having installed it. Note that
         this name needs to be prefixed with <code class="literal">$</code>.
        </p></li><li class="step"><p>
         Log in to the node and run <code class="command">chef-client</code>.
        </p></li><li class="step"><p>
         Check the output of the command for failures and error messages and
         try to fix the cause of these messages.
        </p></li><li class="step"><p>
         Reboot the node.
        </p></li></ol></div></div><p>
       If the node is in a state where login in from the Administration Server is not
       possible, you need to create a <code class="systemitem">root</code> password for it as
       described in <a class="xref" href="#var-depl-inst-nodes-prep-root-login">Direct <code class="systemitem">root</code> Login</a>.
       Now re-install the node by going to the node on the Crowbar Web
       interface and clicking <span class="guimenu">Reinstall</span>. After having
       been re-installed, the node will hang again, but now you can log in
       and check the log files to find the cause.
      </p></dd></dl><div class="free-id" id="id-1.4.7.2.4.3.2.9"> </div><dl class="qandaentry"><dt class="question" id="id-1.4.7.2.4.3.2.9.1"><strong>Q:</strong>
       Where to find more information when applying a barclamp proposal
       fails?
      </dt><dd class="answer" id="id-1.4.7.2.4.3.2.9.2"><p>
       Check the Chef client log files on the Administration Server located at
       <code class="filename">/var/log/crowbar/chef-client/d*.log</code>. Further
       information is available from the Chef client log files located
       on the node(s) affected by the proposal
       (<code class="filename">/var/log/chef/client.log</code>), and from the log
       files of the service that failed to be deployed. Additional
       information may be gained from the Crowbar Web UI log files on the
       Administration Server. For a list of log file locations refer to
       <span class="intraxref">Book “<em class="citetitle">Operations Guide Crowbar</em>”, Chapter 5 “Log Management”, Section 5.4 “Log Files”</span>.
      </p></dd></dl><div class="free-id" id="id-1.4.7.2.4.3.2.10"> </div><dl class="qandaentry"><dt class="question" id="id-1.4.7.2.4.3.2.10.1"><strong>Q:</strong>
       How to Prevent the Administration Server from Installing the <span class="productname">OpenStack</span> Nodes
       (Disable PXE and DNS Services)?
      </dt><dd class="answer" id="id-1.4.7.2.4.3.2.10.2"><p>
       By default, the <span class="productname">OpenStack</span> nodes are installed by booting a discovery
       image from the Administration Server using PXE. They are allocated and then boot
       via PXE into an automatic installation (see <a class="xref" href="#sec-depl-inst-nodes-install" title="11.2. Node Installation">Section 11.2, “Node Installation”</a> for details). To
       install the <span class="productname">OpenStack</span> nodes manually or with a custom provisioning tool,
       you need to disable the PXE boot service and the DNS service on the
       Administration Server.
      </p><p>
       As a consequence you also need to provide an external DNS server. Such
       a server needs to comply with the following requirements:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          It needs to handle all domain-to-IP requests for SUSE <span class="productname">OpenStack</span> Cloud.
        </p></li><li class="listitem"><p>
         It needs to handle all IP-to-domain requests for SUSE <span class="productname">OpenStack</span> Cloud.
        </p></li><li class="listitem"><p>
         It needs to forward unknown requests to other DNS servers.
        </p></li></ul></div><p>
       To disable the PXE and DNS services when setting up the Administration Server,
       proceed as follows:
      </p><div class="procedure" id="id-1.4.7.2.4.3.2.10.2.5" data-id-title="Disabling PXE/DNS when Setting Up the Administration Server"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 18.1: </span><span class="title-name">Disabling PXE/DNS when Setting Up the Administration Server </span></span><a title="Permalink" class="permalink" href="#id-1.4.7.2.4.3.2.10.2.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="procedure-contents"><p>
        The following steps need to be performed <span class="emphasis"><em>before</em></span>
        starting the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation.
       </p><ol class="procedure" type="1"><li class="step"><p>
         Create the file <code class="filename">/etc/crowbar/dns.json</code> with the
         following content:
        </p><div class="verbatim-wrap"><pre class="screen">{
  "attributes": {
    "dns": {
      "nameservers": [ "<em class="replaceable">DNS_SERVER</em>", "<em class="replaceable">DNS_SERVER2</em>" ],
      "auto_assign_server": false
    }
  }
}</pre></div><p>
         Replace <em class="replaceable">DNS_SERVER</em> and
         <em class="replaceable">DNS_SERVER2</em> with the IP address(es) of the
         external DNS server(s). Specifying more than one server is optional.
        </p></li><li class="step"><p>
         Create the file <code class="filename">/etc/crowbar/provisioner.json</code>
         with the following content:
        </p><div class="verbatim-wrap"><pre class="screen">{
  "attributes": {
    "provisioner": {
      "enable_pxe": false
    }
  }
}</pre></div></li><li class="step"><p>
         If these files are present when the SUSE <span class="productname">OpenStack</span> Cloud Crowbar installation is started, the
         Administration Server will be set up using external DNS services and no PXE boot
         server.
        </p></li></ol></div></div><p>
       If you already have deployed SUSE <span class="productname">OpenStack</span> Cloud, proceed as follows to
       disable the DNS and PXE services on the Administration Server:
      </p><div class="procedure" id="id-1.4.7.2.4.3.2.10.2.7" data-id-title="Disabling PXE/DNS on an Administration Server Running Crowbar"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 18.2: </span><span class="title-name">Disabling PXE/DNS on an Administration Server Running Crowbar </span></span><a title="Permalink" class="permalink" href="#id-1.4.7.2.4.3.2.10.2.7">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
         Open a browser and point it to the Crowbar Web interface available on
         the Administration Server, for example
         <code class="literal">http://192.168.124.10/</code>. Log in as user <code class="systemitem">crowbar</code>. The password is
         <code class="literal">crowbar</code> by default, if you have not changed it.
        </p></li><li class="step"><p>
         Open <span class="guimenu">Barclamps</span> › <span class="guimenu">Crowbar</span> and click
         <span class="guimenu">Edit</span> in the <span class="guimenu">Provisioner</span> row.
        </p></li><li class="step"><p>
         Click <span class="guimenu">Raw</span> in the <span class="guimenu">Attributes</span>
         section and change the value for <span class="guimenu">enable_pxe</span> to
         <code class="literal">false</code>:
        </p><div class="verbatim-wrap"><pre class="screen">"enable_pxe": false,</pre></div></li><li class="step"><p>
         <span class="guimenu">Apply</span> the proposal without changing the
         assignments in the <span class="guimenu">Deployment</span> section.
        </p></li><li class="step"><p>
         Change to the <span class="guimenu">DNS</span> barclamp via <span class="guimenu">Barclamps</span> › <span class="guimenu">Crowbar</span>
         and click <span class="guimenu">Edit</span> in the <span class="guimenu">DNS</span> row.
        </p></li><li class="step"><p>
         Click <span class="guimenu">Raw</span> in the <span class="guimenu">Attributes</span>
         section. Change the value for <span class="guimenu">auto_assign_server</span> to <code class="literal">false</code> and
         add the address(es) for the external name server(s):
        </p><div class="verbatim-wrap"><pre class="screen">"auto_assign_server": false,
"nameservers": [
  "<em class="replaceable">DNS_SERVER</em>",
  "<em class="replaceable">DNS_SERVER2</em>"
],</pre></div><p>
         Replace <em class="replaceable">DNS_SERVER</em> and
         <em class="replaceable">DNS_SERVER2</em> with the IP address(es) of the
         external DNS server(s). Specifying more than one server is optional.
        </p></li><li class="step"><p>
         <span class="guimenu">Save</span> your changes, but do not apply them, yet!
        </p></li><li class="step"><p>
         In the <span class="guimenu">Deployment</span> section of the barclamp remove
         all nodes from the <span class="guimenu">dns-server</span> role, but do not
         change the assignments for the <span class="guimenu">dns-client</span> role.
        </p></li><li class="step"><p>
         <span class="guimenu">Apply</span> the barclamp.
        </p></li><li class="step"><p>
         When the DNS barclamp has been successfully applied, log in to the
         Administration Server and stop the DNS service:
        </p><div class="verbatim-wrap"><pre class="screen">systemctl stop named</pre></div></li></ol></div></div><p>
       Now that the PXE and DNS services are disabled you can install
       SUSE Linux Enterprise Server 12 SP4 on the <span class="productname">OpenStack</span> nodes. When a node is ready, add it to the
       pool of nodes as described in <a class="xref" href="#sec-depl-inst-nodes-install-external" title="11.3. Converting Existing SUSE Linux Enterprise Server 12 SP4 Machines Into SUSE OpenStack Cloud Nodes">Section 11.3, “Converting Existing SUSE Linux Enterprise Server 12 SP4 Machines Into SUSE <span class="productname">OpenStack</span> Cloud Nodes”</a>.
      </p></dd></dl><div class="free-id" id="id-1.4.7.2.4.3.2.11"> </div><dl class="qandaentry"><dt class="question" id="id-1.4.7.2.4.3.2.11.1"><strong>Q:</strong>
       I have installed a new hard disk on a node that was already deployed.
       Why is it ignored by Crowbar?
      </dt><dd class="answer" id="id-1.4.7.2.4.3.2.11.2"><p>
       When adding a new hard disk to a node that has already been deployed,
       it can take up to 15 minutes before the new disk is detected.
      </p></dd></dl><div class="free-id" id="id-1.4.7.2.4.3.2.12"> </div><dl class="qandaentry"><dt class="question" id="id-1.4.7.2.4.3.2.12.1"><strong>Q:</strong>
       How to install additional packages (for example a driver) when nodes
       are deployed?
      </dt><dd class="answer" id="id-1.4.7.2.4.3.2.12.2"><p>
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> offers the possibility to install additional packages that
       are not part of the default scope of packages installed on the <span class="productname">OpenStack</span>
       nodes. This is for example required if your hardware is only supported
       by a third party driver. It is also useful if your setup requires to
       install additional tools that would otherwise need to be installed
       manually.
      </p><p>
       Prerequisite for using this feature is that the packages are available
       in a repository known on the Administration Server. Refer to <a class="xref" href="#q-depl-trouble-faq-admin-custom-repos" title="Q:"><em>How to make custom software repositories from an external server (for example a remote SMT or SUSE M..?</em></a> for details, if the
       packages you want to install are not part of the repositories already
       configured.
      </p><p>
       To add packages for installation on node deployment, proceed as
       follows:
      </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
         Open a browser and point it to the Crowbar Web interface on the Administration Server, for
         example <code class="literal">http://192.168.124.10/</code>. Log in as user
         <code class="systemitem">crowbar</code>. The password is
         <code class="literal">crowbar</code> by default, if you have not changed it
         during the installation.
        </p></li><li class="step"><p>
         Go to <span class="guimenu">Barclamps</span> › <span class="guimenu">Crowbar</span> and click the
         <span class="guimenu">Edit</span> button for <span class="guimenu">Provisioner</span>.
        </p></li><li class="step"><p>
         Next click <span class="guimenu">Raw</span> in the
         <span class="guimenu">Attributes</span> page to open an editable view of the
         provisioner configuration.
        </p></li><li class="step"><p>
         Add the following JSON code <span class="emphasis"><em>before</em></span> the last
         closing curly bracket (replace the <em class="replaceable">PACKAGE</em>
         placeholders with real package names):
        </p><div class="verbatim-wrap"><pre class="screen">         "packages": {
    "suse-12.2": ["<em class="replaceable">PACKAGE_1</em>", "<em class="replaceable">PACKAGE_2</em>"],
  }</pre></div></li></ol></div></div><p>
       Note that these packages will get installed on all <span class="productname">OpenStack</span> nodes. If
       the change to the Provisioner barclamp is made after nodes have already
       been deployed, the packages will be installed on the affected nodes
       with the next run of Chef or <code class="command">crowbar-register</code>. Package
       names will be validated against the package naming guidelines to
       prevent script-injection.
      </p></dd></dl></div><div class="qandadiv-title-wrap"><h4 class="qandadiv-title" id="sec-depl-trouble-faq-misc">3. Miscellaneous</h4></div><div class="qandadiv"><div class="free-id" id="sec-depl-trouble-faq-misc-keystone-pw"> </div><dl class="qandaentry"><dt class="question" id="id-1.4.7.2.4.3.3.2.1"><strong>Q:</strong>
       How to change the keystone credentials after the keystone barclamp has
       been deployed?
      </dt><dd class="answer" id="id-1.4.7.2.4.3.3.2.2"><p>
       To change the credentials for the keystone administrator (<code class="systemitem">admin</code>) or the regular user (<code class="systemitem">crowbar</code> by default), proceed as follows:
      </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
         Log in to the Control Node on which keystone is deployed as user
         <code class="systemitem">root</code> via the Administration Server.
        </p></li><li class="step"><p>
         In a shell, source the <span class="productname">OpenStack</span> RC file for the project that you want
         to upload an image to. For details, refer to <a class="link" href="http://docs.openstack.org/user-guide/common/cli_set_environment_variables_using_openstack_rc.html" target="_blank">Set
         environment variables using the <span class="productname">OpenStack</span> RC file</a> in the
         <span class="productname">OpenStack</span> documentation.
        </p></li><li class="step"><p>
         Enter the following command to change the
         <em class="replaceable">PASSWORD</em> for the administrator or the
         regular user (<em class="replaceable">USER</em>):
        </p><div class="verbatim-wrap"><pre class="screen"> keystone-manage bootstrap --bootstrap-password <em class="replaceable">PASSWORD</em> \
--bootstrap-username <em class="replaceable">USER</em></pre></div><p>
         For a complete list of command line options, run
         <code class="command">keystone-manage bootstrap --help</code>. Make sure to
         start the command with a <span class="keycap">Space</span> to make sure the
         password does not appear in the command history.
        </p></li><li class="step"><p>
         Access the keystone barclamp on the Crowbar Web interface by going to
         <span class="guimenu">Barclamps</span> › <span class="guimenu">OpenStack</span> and click
         <span class="guimenu">Edit</span> for the keystone barclamp.
        </p></li><li class="step"><p>
         Enter the new password for the same user you specified on the command
         line before.
        </p></li><li class="step"><p>
         Activate the change by clicking <span class="guimenu">Apply</span>. When the
         proposal has been re-applied, the password has changed and can be
         used.
        </p></li></ol></div></div></dd></dl><div class="free-id" id="sec-depl-trouble-faq-misc-change-openstack-config"> </div><dl class="qandaentry"><dt class="question" id="id-1.4.7.2.4.3.3.3.1"><strong>Q:</strong>
       How to add or change a configuration value for an <span class="productname">OpenStack</span> service?
      </dt><dd class="answer" id="id-1.4.7.2.4.3.3.3.2"><p>
       See <a class="xref" href="#cha-depl-ostack-configs" title="Chapter 14. Configuration Files for OpenStack Services">Chapter 14, <em>Configuration Files for <span class="productname">OpenStack</span> Services</em></a>.
      </p></dd></dl></div></div></section><section class="sect1" id="sec-depl-trouble-support" data-id-title="Support"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.2 </span><span class="title-name">Support</span></span> <a title="Permalink" class="permalink" href="#sec-depl-trouble-support">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   
   Before contacting support to help you with a problem on SUSE <span class="productname">OpenStack</span> Cloud, it is
   strongly recommended that you gather as much information about your
   system and the problem as possible. For this purpose, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>
   ships with a tool called <code class="command">supportconfig</code>. It gathers
   system information such as the current kernel version being used, the
   hardware, RPM database, partitions, and other items.
   <code class="command">supportconfig</code> also collects the most important log
   files, making it easier for the supporters to identify and solve your
   problem.
  </p><p>
   It is recommended to always run <code class="command">supportconfig</code> on the
   Administration Server and on the Control Node(s). If a Compute Node or a
   Storage Node is part of the problem, run
   <code class="command">supportconfig</code> on the affected node as well. For
   details on how to run <code class="command">supportconfig</code>, see
   <a class="link" href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-admin/#cha-adm-support" target="_blank">https://documentation.suse.com/sles/15-SP1/single-html/SLES-admin/#cha-adm-support</a>.
  </p><section class="sect2" id="sec-depl-trouble-support-ptf" data-id-title="Applying PTFs (Program Temporary Fixes) Provided by the SUSE L3 Support"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.2.1 </span><span class="title-name">
    Applying PTFs (Program Temporary Fixes) Provided by the SUSE L3 Support
   </span></span> <a title="Permalink" class="permalink" href="#sec-depl-trouble-support-ptf">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
    Under certain circumstances, the SUSE support may provide temporary
    fixes, the so-called PTFs, to customers with an L3 support contract.
    These PTFs are provided as RPM packages. To make them available on all
    nodes in SUSE <span class="productname">OpenStack</span> Cloud, proceed as follows.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Download the packages from the location provided by the SUSE L3
      Support to a temporary location on the Administration Server.
     </p></li><li class="step"><p>
      Move the packages from the temporary download location to the
      following directories on the Administration Server:
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.7.2.5.4.3.2.2.1"><span class="term">
        <span class="quote">“<span class="quote">noarch</span>”</span> packages (<code class="filename">*.noarch.rpm</code>):
       </span></dt><dd><table style="border: 0; " class="simplelist"><tr><td>
          <code class="filename">/srv/tftpboot/suse-12.4/x86_64/repos/PTF/rpm/noarch/</code>
         </td></tr><tr><td>
          <code class="filename">/srv/tftpboot/suse-12.4/s390x/repos/PTF/rpm/noarch/</code>
         </td></tr></table></dd><dt id="id-1.4.7.2.5.4.3.2.2.2"><span class="term">
        <span class="quote">“<span class="quote">x86_64</span>”</span> packages (<code class="filename">*.x86_64.rpm</code>)
       </span></dt><dd><p>
         <code class="filename">/srv/tftpboot/suse-12.4/x86_64/repos/PTF/rpm/x86_64/</code>
        </p></dd><dt id="id-1.4.7.2.5.4.3.2.2.3"><span class="term">
        <span class="quote">“<span class="quote">s390x</span>”</span> packages (<code class="filename">*.s390x.rpm</code>)
       </span></dt><dd><p>
         <code class="filename">/srv/tftpboot/suse-12.4/s390x/repos/PTF/rpm/s390x/</code>
        </p></dd></dl></div></li><li class="step"><p>
      Create or update the repository metadata:
     </p><div class="verbatim-wrap"><pre class="screen">createrepo-cloud-ptf</pre></div></li><li class="step"><p>
      The repositories are now set up and are available for all nodes in
      SUSE <span class="productname">OpenStack</span> Cloud except for the Administration Server. In case the PTF also contains
      packages to be installed on the Administration Server, make the
      repository available on the Administration Server as well:
     </p><div class="verbatim-wrap"><pre class="screen">zypper ar -f /srv/tftpboot/suse-12.4/x86_64/repos/PTF PTF</pre></div></li><li class="step"><p>
      To deploy the updates, proceed as described in
      <a class="xref" href="#sec-depl-inst-nodes-post-updater" title="11.4.1. Deploying Node Updates with the Updater Barclamp">Section 11.4.1, “Deploying Node Updates with the Updater Barclamp”</a>. Alternatively, run
      <code class="command">zypper up</code> manually on each node.
     </p></li></ol></div></div></section></section></section></div><section class="appendix" id="app-deploy-cisco" data-id-title="Using Cisco Nexus Switches with neutron"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">A </span><span class="title-name">Using Cisco Nexus Switches with neutron</span></span> <a title="Permalink" class="permalink" href="#app-deploy-cisco">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><section class="sect1" id="app-deploy-cisco-requirements" data-id-title="Requirements"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">A.1 </span><span class="title-name">Requirements</span></span> <a title="Permalink" class="permalink" href="#app-deploy-cisco-requirements">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
   The following requirements must be met to use Cisco Nexus switches with
   neutron:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Cisco Nexus series 3000, 5000 or 7000
    </p></li><li class="listitem"><p>
     All Compute Nodes must be equipped with at least two network cards.
    </p></li><li class="listitem"><p>
     The switch needs to have the XML management interface enabled. SSH
     access to the management interface must be enabled (refer to the
     switch's documentation for details).
    </p></li><li class="listitem"><p>
     Enable VLAN trunking for all neutron managed VLANs on the switch
     port to which the controller node running neutron is connected to.
    </p></li><li class="listitem"><p>
     Before deploying neutron, check if VLAN configurations for neutron managed
     VLANs already exist on the switch (for example, from a previous SUSE <span class="productname">OpenStack</span> Cloud
     deployment). If yes, delete them via the switch's management interface prior to
     deploying neutron.
    </p></li><li class="listitem"><p>
     When using the Cisco plugin, neutron reconfigures the VLAN trunk
     configuration on all ports used for the <code class="literal">nova-fixed</code>
     traffic (the traffic between the instances). This requires to
     configure separate network interfaces exclusively used by
     <code class="literal">nova-fixed</code>. This can be achieved by adjusting
     <code class="filename">/etc/crowbar/network.json</code> (refer to
     <a class="xref" href="#sec-depl-inst-admserv-post-network" title="7.5. Custom Network Configuration">Section 7.5, “Custom Network Configuration”</a>). The following example shows
     an appropriate configuration for dual mode, where
     <span class="guimenu">nova-fixed</span> has been mapped to conduit
     <span class="guimenu">intf1</span> and all other networks to other conduits.
     Configuration attributes not relevant in this context have been
     replaced with <code class="literal">...</code>.
    </p><div class="example" id="id-1.4.8.4.3.6.2" data-id-title="Exclusively Mapping nova-fixed to conduit intf1 in dual mode"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example A.1: </span><span class="title-name">Exclusively Mapping <span class="guimenu">nova-fixed</span> to conduit <span class="guimenu">intf1</span> in dual mode </span></span><a title="Permalink" class="permalink" href="#id-1.4.8.4.3.6.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">{
   "attributes" : {
      "network" : {
         "conduit_map" : [
            <em class="replaceable">...</em>
         ],
         "mode" : "single",
         "networks" : {
            "nova_fixed" : {
              <em class="replaceable">...</em>,
              "conduit" : "intf1"
            },
            "nova_floating" : {
              <em class="replaceable">...</em>,
              "conduit" : "intf0"
            },
            "public" : {
              <em class="replaceable">...</em>,
              "conduit" : "intf0"
            },
            "storage" : {
              <em class="replaceable">...</em>,
              "conduit" : "intf0"
            },
            "os_sdn" : {
              <em class="replaceable">...</em>,
              "conduit" : "intf0"
            },
            "admin" : {
              <em class="replaceable">...</em>,
              "conduit" : "intf0"
            },
            "bmc" : {
              <em class="replaceable">...</em>,
              "conduit" : "bmc"
            },
            "bmc_vlan" : {
              <em class="replaceable">...</em>,
              "conduit" : "intf2"
            },
         },
         <em class="replaceable">...</em>,
      },
   }
}</pre></div></div></div></li><li class="listitem"><p>
     Make a note of all switch ports to which the interfaces using the
     <code class="literal">nova-fixed</code> network on the Compute Nodes are
     connected. This information will be needed when deploying neutron.
    </p></li></ul></div></section><section class="sect1" id="app-deploy-cisco-deploy" data-id-title="Deploying neutron with the Cisco Plugin"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">A.2 </span><span class="title-name">Deploying neutron with the Cisco Plugin</span></span> <a title="Permalink" class="permalink" href="#app-deploy-cisco-deploy">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Create a neutron barclamp proposal in the Crowbar Web
     interface.
    </p></li><li class="step"><p>
     As the <span class="guimenu">Plugin</span>, select <code class="literal">ml2</code>.
    </p></li><li class="step"><p>
     As <span class="guimenu">Modular Layer 2 mechanism drive</span>, select
     <code class="literal">cisco_nexus</code>.
    </p></li><li class="step"><p>
     In <span class="guimenu">Modular Layer2 type drivers</span>, select
     <code class="literal">vlan</code>.
    </p></li><li class="step"><p>
     In the <span class="guimenu">Cisco Switch Credentials</span> table, enter the
     <span class="guimenu">IP Address</span>, the SSH
     <span class="guimenu">Port</span> number and the login credentials for the
     switch's management interface. If you have multiple switches, open a
     new row in the table by clicking <span class="guimenu">Add</span> and enter the
     data for another switch.
    </p><div class="figure" id="id-1.4.8.5.2.5.2"><div class="figure-contents"><div class="mediaobject"><a href="images/depl_barclamp_network_cisco.png"><img src="images/depl_barclamp_network_cisco.png" width="75%" alt="The neutron barclamp: Cisco Plugin" title="The neutron barclamp: Cisco Plugin"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure A.1: </span><span class="title-name">The neutron barclamp: Cisco Plugin </span></span><a title="Permalink" class="permalink" href="#id-1.4.8.5.2.5.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></li><li class="step"><p>
     Choose whether to encrypt public communication
     (<span class="guimenu">HTTPS</span>) or not (<span class="guimenu">HTTP</span>). If
     choosing <span class="guimenu">HTTPS</span>, refer to
     <a class="xref" href="#sec-depl-ostack-keystone-ssl">SSL Support: Protocol
    </a> for configuration
     details.
    </p></li><li class="step"><p>
     Choose a node for deployment and <span class="guimenu">Apply</span> the proposal.
    </p></li><li class="step"><p>
     Deploy nova (see <a class="xref" href="#sec-depl-ostack-nova" title="12.11. Deploying nova">Section 12.11, “Deploying nova”</a>),
     horizon (see <a class="xref" href="#sec-depl-ostack-dash" title="12.12. Deploying horizon (OpenStack Dashboard)">Section 12.12, “Deploying horizon (<span class="productname">OpenStack</span> Dashboard)”</a> and all other
     remaining barclamps.
    </p></li><li class="step"><p>
     When all barclamps have been deployed, return to the neutron
     barclamp by choosing <span class="guimenu">Barclamps</span> › <span class="guimenu">OpenStack</span> › <span class="guimenu">neutron</span> › <span class="guimenu">Edit</span>. The proposal now contains an
     additional table named <span class="guimenu">Assign Switch Ports</span>, listing
     all Compute Nodes.
    </p><p>
     For each Compute Node enter the switch it is connected to and the port
     number from the notes you took earlier. The values need to be entered
     like the following: <code class="literal">1/13</code> or
     <code class="literal">Eth1/20</code>.
    </p></li><li class="step"><p>
     When you have entered the data for all Compute Nodes, re-apply the
     proposal.
    </p><div id="id-1.4.8.5.2.10.2" data-id-title="Deploying Additional Compute Nodes" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Deploying Additional Compute Nodes</div><p>
      Whenever you deploy additional Compute Nodes to an active SUSE <span class="productname">OpenStack</span> Cloud
      deployment using the Cisco plugin with neutron, update
      the neutron barclamp proposal by entering their port data as
      described in the previous step.
     </p></div></li></ol></div></div><div id="id-1.4.8.5.3" data-id-title="Verifying the Setup" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Verifying the Setup</div><p>
    To verify if neutron was correctly deployed, do the following:
   </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
      Launch an instance (refer to the <em class="citetitle">User Guide</em>, chapter
      <em class="citetitle">Launch and manage instances</em>

      for instructions).
     </p></li><li class="listitem"><p>
      Find out which VLAN was assigned to the network by running the command
      <code class="command">openstack network show fixed</code>. The result lists a
      <span class="guimenu">segmentation_id</span> matching the VLAN.
     </p></li><li class="listitem"><p>
      Log in to the switch's management interface and list the VLAN
      configuration. If the setup was deployed correctly, the port of the
      Compute Node the instance is running on, is in trunk mode for the
      matching VLAN.
     </p></li></ol></div></div></section></section><section class="appendix" id="app-deploy-docupdates" data-id-title="Documentation Updates"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">B </span><span class="title-name">Documentation Updates</span></span> <a title="Permalink" class="permalink" href="#app-deploy-docupdates">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>
  This chapter lists content changes for this document since the release of
  <span class="phrase"><span class="phrase">SUSE® <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8.0.
 </p><p>
  This manual was updated on the following dates:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <a class="xref" href="#sec-deploy-docupdates-c8-gm" title="B.1. April 2018 (Initial Release SUSE OpenStack Cloud Crowbar 8)">Section B.1, “April 2018 (Initial Release <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8)”</a>
   </p></li></ul></div><section class="sect1" id="sec-deploy-docupdates-c8-gm" data-id-title="April 2018 (Initial Release SUSE OpenStack Cloud Crowbar 8)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">B.1 </span><span class="title-name">April 2018 (Initial Release <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8)</span></span> <a title="Permalink" class="permalink" href="#sec-deploy-docupdates-c8-gm">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.9.6.2.1"><span class="term">Bugfixes</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        In <a class="xref" href="#sec-depl-ostack-glance" title="12.8. Deploying glance">Section 12.8, “Deploying glance”</a>, corrected the name of
        an example nova configuration file for custom settings (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1077947" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1077947</a>).
       </p></li><li class="listitem"><p>
        In <a class="xref" href="#sec-depl-ostack-glance" title="12.8. Deploying glance">Section 12.8, “Deploying glance”</a>, updated the entries of a
        drop-down box (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1073333" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1073333</a>).
       </p></li><li class="listitem"><p>
        Numerous small fixes and corrections throughout the document (<a class="link" href="http://bugzilla.suse.com/show_bug.cgi?id=1073508" target="_blank">http://bugzilla.suse.com/show_bug.cgi?id=1073508</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1073516" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1073516</a>).
       </p></li></ul></div></dd></dl></div></section></section><section class="glossary"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number"> </span><span class="title-name">Glossary of Terminology and Product Names</span></span> <a title="Permalink" class="permalink" href="#gl-cloud">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="line"/><dl><dt id="gloss-act-act"><span><span class="glossterm">Active/Active</span> <a title="Permalink" class="permalink" href="#gloss-act-act">#</a></span></dt><dd class="glossdef"><p>
    A concept of how services are running on nodes in a High Availability cluster. In
    an active/active setup, both the main and redundant systems are managed
    concurrently. If a failure of services occurs, the redundant system is
    already online, and can take over until the main system is fixed and
    brought back online.
   </p></dd><dt id="gloss-act-pass"><span><span class="glossterm">Active/Passive</span> <a title="Permalink" class="permalink" href="#gloss-act-pass">#</a></span></dt><dd class="glossdef"><p>
    A concept of how services are running on nodes in a High Availability cluster. In
    an active/passive setup, one or more services are running on an active
    cluster node, whereas the passive node stands by. If the active node fails then the services are transferred to the passive node.
   </p></dd><dt id="id-1.4.10.5"><span><span class="glossterm">Administration Server</span> <a title="Permalink" class="permalink" href="#id-1.4.10.5">#</a></span></dt><dd class="glossdef"><p>
    Also called Crowbar Administration Node. Manages all other nodes. It
    assigns IP addresses to them, boots them using PXE, configures them, and
    provides them the necessary software for their roles. To provide these
    services, the Administration Server runs Crowbar, Chef, DHCP, TFTP, NTP, and other
    services.
   </p></dd><dt id="id-1.4.10.6"><span><span class="glossterm">AMI (Amazon Machine Image)</span> <a title="Permalink" class="permalink" href="#id-1.4.10.6">#</a></span></dt><dd class="glossdef"><p>
    A virtual machine that can be created and customized by a user. AMIs can
    be identified by an ID prefixed with <code class="literal">ami-</code>.
   </p></dd><dt id="gloss-az"><span><span class="glossterm">Availability Zone</span> <a title="Permalink" class="permalink" href="#gloss-az">#</a></span></dt><dd class="glossdef"><p>
    An <span class="productname">OpenStack</span> method of partitioning clouds. It enables you to arrange
    <span class="productname">OpenStack</span> Compute hosts into logical groups. The groups typically have
    physical isolation and redundancy from other availability zones, for
    example, by using separate power supply or network equipment for each
    zone. When users provision resources, they can specify from which
    availability zone their instance should be created. This allows cloud
    consumers to ensure that their application resources are spread across
    disparate machines to achieve high availability if the hardware fails.
    Since the Grizzly release, availability zones are implemented via host
    aggregates.

   </p></dd><dt id="id-1.4.10.8"><span><span class="glossterm">AWS (Amazon Web Services)</span> <a title="Permalink" class="permalink" href="#id-1.4.10.8">#</a></span></dt><dd class="glossdef"><p>
    A collection of remote computing services (including Amazon EC2, Amazon
    S3, and others) that together make up Amazon's cloud computing platform.
   </p></dd><dt id="id-1.4.10.9"><span><span class="glossterm">Barclamp</span> <a title="Permalink" class="permalink" href="#id-1.4.10.9">#</a></span></dt><dd class="glossdef"><p>
    A set of Chef cookbooks, templates, and other logic. Used to apply
    a particular Chef role to individual nodes or a set of nodes.

   </p></dd><dt id="id-1.4.10.10"><span><span class="glossterm">ceilometer</span> <a title="Permalink" class="permalink" href="#id-1.4.10.10">#</a></span></dt><dd class="glossdef"><p>
    Code name for <a class="xref" href="#gloss-ceilo" title="Telemetry">Telemetry</a>.
   </p></dd><dt id="id-1.4.10.11"><span><span class="glossterm">Cell</span> <a title="Permalink" class="permalink" href="#id-1.4.10.11">#</a></span></dt><dd class="glossdef"><p>
    Cells provide a new way to scale Compute deployments. This includes the
    ability to have compute clusters (cells) in different geographic
    locations all under the same Compute API. This allows for a single API
    server being used to control access to multiple cloud installations.
    Cells provide logical partitioning of Compute resources in a
    child/parent relationship.
   </p></dd><dt id="id-1.4.10.13"><span><span class="glossterm">Ceph</span> <a title="Permalink" class="permalink" href="#id-1.4.10.13">#</a></span></dt><dd class="glossdef"><p>
    A massively scalable, open source, distributed storage system. It
    consists of an object store, a block store, and a POSIX-compliant
    distributed file system.
   </p></dd><dt id="id-1.4.10.12"><span><span class="glossterm">Chef</span> <a title="Permalink" class="permalink" href="#id-1.4.10.12">#</a></span></dt><dd class="glossdef"><p>
    An automated configuration management platform for deployment of your
    entire cloud infrastructure. The Chef server manages many of the
    software packages and allows the easy changing of nodes.
   </p></dd><dt id="id-1.4.10.14"><span><span class="glossterm">cinder</span> <a title="Permalink" class="permalink" href="#id-1.4.10.14">#</a></span></dt><dd class="glossdef"><p>
    Code name for <a class="xref" href="#gloss-cinder" title="OpenStack Block Storage"><span class="productname">OpenStack</span> Block Storage</a>.
   </p></dd><dt id="id-1.4.10.20"><span><span class="glossterm">cloud-init</span> <a title="Permalink" class="permalink" href="#id-1.4.10.20">#</a></span></dt><dd class="glossdef"><p>
    A package commonly installed in virtual machine images. It uses the SSH
    public key to initialize an instance after boot.
   </p></dd><dt id="id-1.4.10.15"><span><span class="glossterm">Cluster</span> <a title="Permalink" class="permalink" href="#id-1.4.10.15">#</a></span></dt><dd class="glossdef"><p>
    A set of connected computers that work together. In many respects (and
    from the outside) they can be viewed as a single system. Clusters can be
    further categorized depending on their purpose, for example: High Availability
    clusters, high-performance clusters, or load-balancing clusters.
   </p></dd><dt id="gloss-partition"><span><span class="glossterm">Cluster Partition</span> <a title="Permalink" class="permalink" href="#gloss-partition">#</a></span></dt><dd class="glossdef"><p>
    Whenever communication fails between one or more nodes and the rest of
    the cluster, a cluster partition occurs: The nodes of a cluster are
    split into partitions but still active. They can only communicate with
    nodes in the same partition and are unaware of the separated nodes. As
    the loss of the nodes on the other partition cannot be confirmed, a
    <a class="xref" href="#gloss-splitbrain" title="Split Brain">Split Brain</a> scenario develops.
   </p></dd><dt id="gloss-crm"><span><span class="glossterm">Cluster Resource Manager</span> <a title="Permalink" class="permalink" href="#gloss-crm">#</a></span></dt><dd class="glossdef"><p>
    The main management entity in a High Availability cluster responsible for
    coordinating all non-local interactions. The
    <a class="xref" href="#gloss-hasi" title="SUSE Linux Enterprise High Availability Extension">SUSE Linux Enterprise High Availability Extension</a> uses Pacemaker as CRM. Each node of the
    cluster has its own CRM instance. The instance running on the
    <a class="xref" href="#gloss-dc" title="Designated Coordinator (DC)">Designated Coordinator (DC)</a> is the one elected to relay decisions to the
    other non-local CRMs and to process their input.
   </p></dd><dt id="id-1.4.10.21"><span><span class="glossterm">Compute Node</span> <a title="Permalink" class="permalink" href="#id-1.4.10.21">#</a></span></dt><dd class="glossdef"><p>
    Node within a SUSE <span class="productname">OpenStack</span> Cloud. A physical server running a Hypervisor. A
    Compute Node is a host for guest virtual machines that are deployed in
    the cloud. It starts virtual machines on demand using
    <code class="literal">nova-compute</code>. To split virtual machine load across
    more than one server, a cloud should contain multiple Compute Nodes.
   </p></dd><dt id="id-1.4.10.18"><span><span class="glossterm">Container</span> <a title="Permalink" class="permalink" href="#id-1.4.10.18">#</a></span></dt><dd class="glossdef"><p>
    A container is a storage compartment for data. It can be thought of as a
    directory, only that it cannot be nested.
   </p></dd><dt id="id-1.4.10.22"><span><span class="glossterm">Control Node</span> <a title="Permalink" class="permalink" href="#id-1.4.10.22">#</a></span></dt><dd class="glossdef"><p>
    Node within a SUSE <span class="productname">OpenStack</span> Cloud. The Control Node is configured through the
    Administration Server and registers with the Administration Server for all required
    software. Hosts the <span class="productname">OpenStack</span> API endpoints and the <span class="productname">OpenStack</span>
    scheduler and runs the <code class="literal">nova</code> services—except
    for <code class="literal">nova-compute</code>, which is run on the Compute Nodes.
    The Control Node coordinates everything about cloud virtual machines:
    like a central communication center it receives all requests (for
    example, if a user wants to start or stop a virtual machine). It
    communicates with the Compute Nodes to coordinate fulfillment of the
    request. A cloud can contain multiple Control Nodes.
   </p></dd><dt id="id-1.4.10.23"><span><span class="glossterm">Cookbook</span> <a title="Permalink" class="permalink" href="#id-1.4.10.23">#</a></span></dt><dd class="glossdef"><p>
    A collection of Chef recipes which deploy a software stack or
    functionality. The unit of distribution for Chef.
   </p></dd><dt id="id-1.4.10.19"><span><span class="glossterm">Corosync</span> <a title="Permalink" class="permalink" href="#id-1.4.10.19">#</a></span></dt><dd class="glossdef"><p>
    The messaging/infrastructure layer used in a High Availability cluster that is set
    up with SUSE Linux Enterprise High Availability Extension. For example, the cluster communication
    channels are defined in
    <code class="filename">/etc/corosync/corosync.conf</code>.
   </p></dd><dt id="id-1.4.10.24"><span><span class="glossterm">Crowbar</span> <a title="Permalink" class="permalink" href="#id-1.4.10.24">#</a></span></dt><dd class="glossdef"><p>
    Bare-metal installer and an extension of Chef server. The primary
    function of Crowbar is to get new hardware into a state where it can
    be managed by Chef. That means: Setting up BIOS and RAID, network,
    installing a basic operating system, and setting up services like DNS,
    NTP, and DHCP. The Crowbar server manages all nodes, supplying
    configuration of hardware and software.
   </p></dd><dt id="gloss-dc"><span><span class="glossterm">Designated Coordinator (DC)</span> <a title="Permalink" class="permalink" href="#gloss-dc">#</a></span></dt><dd class="glossdef"><p>
    One <a class="xref" href="#gloss-crm" title="Cluster Resource Manager">Cluster Resource Manager</a> in a High Availability cluster is elected as the
    Designated Coordinator (DC). The DC is the only entity in the cluster
    that can decide that a cluster-wide change needs to be performed. For
    example, fencing a node or moving resources around. After a membership change,
    the DC is elected from all nodes in the cluster.
   </p></dd><dt id="id-1.4.10.26"><span><span class="glossterm">DRBD (Distributed Replicated Block Device)</span> <a title="Permalink" class="permalink" href="#id-1.4.10.26">#</a></span></dt><dd class="glossdef"><p>
    DRBD is a block device designed for building high availability
    clusters. The whole block device is mirrored via a dedicated network and
    is seen as a network RAID-1.
   </p></dd><dt id="id-1.4.10.27"><span><span class="glossterm">EBS (Amazon Elastic Block Store)</span> <a title="Permalink" class="permalink" href="#id-1.4.10.27">#</a></span></dt><dd class="glossdef"><p>
    Block-level storage volumes for use with Amazon EC2 instances. Similar
    to <span class="productname">OpenStack</span> cinder.
   </p></dd><dt id="id-1.4.10.28"><span><span class="glossterm">EC2 (Amazon Elastic Compute Cloud)</span> <a title="Permalink" class="permalink" href="#id-1.4.10.28">#</a></span></dt><dd class="glossdef"><p>
    A public cloud run by Amazon. It provides similar functionality to
    <span class="productname">OpenStack</span> Compute.
   </p></dd><dt id="id-1.4.10.29"><span><span class="glossterm">Ephemeral Disk</span> <a title="Permalink" class="permalink" href="#id-1.4.10.29">#</a></span></dt><dd class="glossdef"><p>
    Ephemeral disks offer machine local disk storage linked to the life
    cycle of a virtual machine instance. When a virtual machine is
    terminated, all data on the ephemeral disk is lost. Ephemeral disks are
    not included in any snapshots.
   </p></dd><dt id="gloss-failover"><span><span class="glossterm">Failover</span> <a title="Permalink" class="permalink" href="#gloss-failover">#</a></span></dt><dd class="glossdef"><p>
    Occurs when a resource fails on a cluster node (or the node itself
    fails) and the affected resources are started on another node.
   </p></dd><dt id="gloss-fencing"><span><span class="glossterm">Fencing</span> <a title="Permalink" class="permalink" href="#gloss-fencing">#</a></span></dt><dd class="glossdef"><p>
    Describes the concept of preventing access to a shared resource by
    isolated or failing cluster members. Should a cluster node fail, it will
    be shut down or reset to prevent it from causing trouble. The resources
    running on the cluster node will be moved away to another node. This
    way, resources are locked out of a node whose status is uncertain.
   </p></dd><dt id="gloss-IP-fixed"><span><span class="glossterm">Fixed IP Address</span> <a title="Permalink" class="permalink" href="#gloss-IP-fixed">#</a></span></dt><dd class="glossdef"><p>
    When an instance is launched, it is automatically assigned a fixed
    (private) IP address, which stays the same until the instance is
    explicitly terminated. Private IP addresses are used for communication
    between instances.
   </p></dd><dt id="gloss-flavor"><span><span class="glossterm">Flavor</span> <a title="Permalink" class="permalink" href="#gloss-flavor">#</a></span></dt><dd class="glossdef"><p>
    The compute, memory, and storage capacity of <code class="literal">nova</code>
    computing instances (in terms of virtual CPUs, RAM, etc.). Flavors can
    be thought of as <span class="quote">“<span class="quote">templates</span>”</span> for the amount of cloud
    resources that are assigned to an instance.
   </p></dd><dt id="gloss-IP-float"><span><span class="glossterm">Floating IP Address</span> <a title="Permalink" class="permalink" href="#gloss-IP-float">#</a></span></dt><dd class="glossdef"><p>
    An IP address that a Compute project can associate with a virtual
    machine. A pool of floating IP addresses is available in <span class="productname">OpenStack</span> Compute,
    as configured by the cloud operator. After a floating IP address has
    been assigned to an instance, the instance can be reached from outside
    the cloud by this public IP address. Floating IP addresses can be
    dynamically disassociated and associated with other instances.
   </p></dd><dt id="id-1.4.10.35"><span><span class="glossterm">glance</span> <a title="Permalink" class="permalink" href="#id-1.4.10.35">#</a></span></dt><dd class="glossdef"><p>
    Code name for <a class="xref" href="#gloss-glance" title="OpenStack Image"><span class="productname">OpenStack</span> Image</a>.
   </p></dd><dt id="id-1.4.10.36"><span><span class="glossterm">Guest Operating System</span> <a title="Permalink" class="permalink" href="#id-1.4.10.36">#</a></span></dt><dd class="glossdef"><p>
    An instance of an operating system installed on a virtual machine.
   </p></dd><dt id="id-1.4.10.37"><span><span class="glossterm">heat</span> <a title="Permalink" class="permalink" href="#id-1.4.10.37">#</a></span></dt><dd class="glossdef"><p>
    Code name for <a class="xref" href="#gloss-heat" title="Orchestration">Orchestration</a>.
   </p></dd><dt id="id-1.4.10.38"><span><span class="glossterm">High Availability Cluster</span> <a title="Permalink" class="permalink" href="#id-1.4.10.38">#</a></span></dt><dd class="glossdef"><p>
    High Availability clusters seek to minimize two things: system downtime and data
    loss. System downtime occurs when a user-facing service is unavailable
    beyond a specified maximum amount of time. System downtime and data
    loss (data is accidentally destroyed) can occur not only in case of a single
    failure. There are also cases of cascading failures,
    where a single failure deteriorates into a series of consequential
    failures.
   </p></dd><dt id="id-1.4.10.39"><span><span class="glossterm">horizon</span> <a title="Permalink" class="permalink" href="#id-1.4.10.39">#</a></span></dt><dd class="glossdef"><p>
    Code name for <a class="xref" href="#gloss-horizon" title="OpenStack Dashboard"><span class="productname">OpenStack</span> Dashboard</a>.
   </p></dd><dt id="id-1.4.10.40"><span><span class="glossterm">Host</span> <a title="Permalink" class="permalink" href="#id-1.4.10.40">#</a></span></dt><dd class="glossdef"><p>
    A physical computer.
   </p></dd><dt id="gloss-host-aggr"><span><span class="glossterm">Host Aggregate</span> <a title="Permalink" class="permalink" href="#gloss-host-aggr">#</a></span></dt><dd class="glossdef"><p>
    An <span class="productname">OpenStack</span> method of grouping hosts via a common set of metadata. It
    enables you to tag groups of hosts with certain capabilities or
    characteristics. A characteristic could be related to physical location,
    allowing creation or further partitioning of availability zones. It could
    also be related to performance (for example, indicating the
    availability of SSD storage) or anything else that the cloud
    administrators deem appropriate. A host can be in more than one host
    aggregate.
   </p></dd><dt id="id-1.4.10.42"><span><span class="glossterm">Hybrid Cloud</span> <a title="Permalink" class="permalink" href="#id-1.4.10.42">#</a></span></dt><dd class="glossdef"><p>
    One of several deployment models for a cloud infrastructure. A
    composition of both public and private clouds that remain unique
    entities, but are bound together by standardized technology for enabling
    data and application portability. Integrating SUSE Studio and
    SUSE Manager with SUSE <span class="productname">OpenStack</span> Cloud delivers a platform and tools with which to
    enable enterprise hybrid clouds.
    
   </p></dd><dt id="id-1.4.10.43"><span><span class="glossterm">Hypervisor</span> <a title="Permalink" class="permalink" href="#id-1.4.10.43">#</a></span></dt><dd class="glossdef"><p>
    A piece of computer software, firmware or hardware that creates and runs
    virtual machines. It arbitrates and controls access of the virtual
    machines to the underlying hardware.
   </p></dd><dt id="id-1.4.10.44"><span><span class="glossterm">IaaS (Infrastructure-as-a-Service)</span> <a title="Permalink" class="permalink" href="#id-1.4.10.44">#</a></span></dt><dd class="glossdef"><p>
    A service model of cloud computing where processing, storage, networks,
    and other fundamental computing resources are rented over the Internet.
    It allows the customer to deploy and run arbitrary software, including
    operating systems and applications. The customer has control over
    operating systems, storage, and deployed applications but does not
    control the underlying cloud infrastructure. Housing and maintaining it
    is in the responsibility of the service provider.
   </p></dd><dt id="id-1.4.10.45"><span><span class="glossterm">Image</span> <a title="Permalink" class="permalink" href="#id-1.4.10.45">#</a></span></dt><dd class="glossdef"><p>
    A file that contains a complete Linux virtual machine.
   </p><p>
    In the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> context, images are virtual disk images that
    represent the contents and structure of a storage medium or device
    (such as a hard disk), in a single file. Images are used as a template from which
    a virtual machine can be started. For starting a virtual machine,
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> always uses a copy of the image.
   </p><p>
    Images have both content and metadata; the latter are also called
    image properties.
   </p></dd><dt id="id-1.4.10.46"><span><span class="glossterm">Instance</span> <a title="Permalink" class="permalink" href="#id-1.4.10.46">#</a></span></dt><dd class="glossdef"><p>
    A virtual machine that runs inside the cloud.
   </p></dd><dt id="gloss-instsnap"><span><span class="glossterm">Instance Snapshot</span> <a title="Permalink" class="permalink" href="#gloss-instsnap">#</a></span></dt><dd class="glossdef"><p>
    A point-in-time copy of an instance. It preserves the disk state of a
    running instance and can be used to launch a new instance or to create a
    new image based upon the snapshot.
   </p></dd><dt id="id-1.4.10.48"><span><span class="glossterm">Keypair</span> <a title="Permalink" class="permalink" href="#id-1.4.10.48">#</a></span></dt><dd class="glossdef"><p>
    <span class="productname">OpenStack</span> Compute injects SSH keypair credentials that are injected
    into images when they are launched.
   </p></dd><dt id="id-1.4.10.49"><span><span class="glossterm">keystone</span> <a title="Permalink" class="permalink" href="#id-1.4.10.49">#</a></span></dt><dd class="glossdef"><p>
    Code name for <a class="xref" href="#gloss-keystone" title="OpenStack Identity"><span class="productname">OpenStack</span> Identity</a>.
   </p></dd><dt id="id-1.4.10.50"><span><span class="glossterm">libvirt</span> <a title="Permalink" class="permalink" href="#id-1.4.10.50">#</a></span></dt><dd class="glossdef"><p>
    Virtualization API library. Used by <span class="productname">OpenStack</span> to interact with many of
    its supported hypervisors.
   </p></dd><dt id="id-1.4.10.51"><span><span class="glossterm">Linux Bridge</span> <a title="Permalink" class="permalink" href="#id-1.4.10.51">#</a></span></dt><dd class="glossdef"><p>
    A software allowing multiple virtual machines to share a single physical
    NIC within <span class="productname">OpenStack</span> Compute. It behaves like a hub: You can connect
    multiple (physical or virtual) network interface devices to it. Any
    Ethernet frames that come in from one interface attached to the bridge
    is transmitted to all other devices.
   </p></dd><dt id="id-1.4.10.52"><span><span class="glossterm">Logical Volume (LV)</span> <a title="Permalink" class="permalink" href="#id-1.4.10.52">#</a></span></dt><dd class="glossdef"><p>
    Acts as a virtual disk partition. After creating a
    <a class="xref" href="#gloss-vg" title="Volume Group (VG)">Volume Group (VG)</a>, logical volumes can be created in that
    volume group. Logical volumes can be used as raw block devices, swap
    devices, or for creating a (mountable) file system like disk partitions.
   </p></dd><dt id="id-1.4.10.53"><span><span class="glossterm">Migration</span> <a title="Permalink" class="permalink" href="#id-1.4.10.53">#</a></span></dt><dd class="glossdef"><p>
    The process of moving a virtual machine instance from one Compute Node
    to another. This process can only be executed by cloud administrators.
   </p></dd><dt id="id-1.4.10.54"><span><span class="glossterm">Multicast</span> <a title="Permalink" class="permalink" href="#id-1.4.10.54">#</a></span></dt><dd class="glossdef"><p>
    A technology used for a one-to-many communication within a network that
    can be used for cluster communication. Corosync supports both
    multicast and unicast.
   </p></dd><dt id="id-1.4.10.55"><span><span class="glossterm">Network</span> <a title="Permalink" class="permalink" href="#id-1.4.10.55">#</a></span></dt><dd class="glossdef"><p>
    In the <span class="productname">OpenStack</span> Networking API: An isolated L2 network segment
    (similar to a VLAN). It forms the basis for describing the L2 network
    topology in a given <span class="productname">OpenStack</span> Networking deployment.
   </p></dd><dt id="id-1.4.10.56"><span><span class="glossterm">neutron</span> <a title="Permalink" class="permalink" href="#id-1.4.10.56">#</a></span></dt><dd class="glossdef"><p>
    Code name for <a class="xref" href="#gloss-neutron" title="OpenStack Networking"><span class="productname">OpenStack</span> Networking</a>.
   </p></dd><dt id="id-1.4.10.57"><span><span class="glossterm">Node</span> <a title="Permalink" class="permalink" href="#id-1.4.10.57">#</a></span></dt><dd class="glossdef"><p>
    A (physical) server that is managed by Crowbar.
   </p></dd><dt id="id-1.4.10.58"><span><span class="glossterm">nova</span> <a title="Permalink" class="permalink" href="#id-1.4.10.58">#</a></span></dt><dd class="glossdef"><p>
    Code name for <a class="xref" href="#gloss-nova" title="OpenStack Compute"><span class="productname">OpenStack</span> Compute</a>.
   </p></dd><dt id="id-1.4.10.59"><span><span class="glossterm">Object</span> <a title="Permalink" class="permalink" href="#id-1.4.10.59">#</a></span></dt><dd class="glossdef"><p>
    Basic storage entity in <span class="productname">OpenStack</span> Object Storage, representing a file
    that your store there. When you upload data to <span class="productname">OpenStack</span>
    Object Storage, the data is neither compressed nor encrypted, it is
    stored as-is.
   </p></dd><dt id="id-1.4.10.60"><span><span class="glossterm">Open vBridge</span> <a title="Permalink" class="permalink" href="#id-1.4.10.60">#</a></span></dt><dd class="glossdef"><p>
    A virtual networking device. It behaves like a virtual switch: network
    interface devices connect to its ports. The ports can be configured
    similar to a physical switch's port, including VLAN configurations.
   </p></dd><dt id="id-1.4.10.61"><span><span class="glossterm"><span class="productname">OpenStack</span></span> <a title="Permalink" class="permalink" href="#id-1.4.10.61">#</a></span></dt><dd class="glossdef"><p>
    A collection of open source software to build and manage public and
    private clouds. Its components are designed to work together to provide
    Infrastructure as a Service and massively scalable cloud computing
    software.
   </p><p>
    At the same time, <span class="productname">OpenStack</span> is also a community and a project.
   </p></dd><dt id="gloss-cinder"><span><span class="glossterm"><span class="productname">OpenStack</span> Block Storage</span> <a title="Permalink" class="permalink" href="#gloss-cinder">#</a></span></dt><dd class="glossdef"><p>
    One of the core <span class="productname">OpenStack</span> components and services (code name:
    <code class="literal">cinder</code>). It provides persistent block
    level storage devices for use <span class="productname">OpenStack</span> compute instances. The block
    storage system manages the creation, attaching and detaching of the
    block devices to servers.
    
    Prior to the <span class="productname">OpenStack</span> Grizzly release, the service was part of
    <code class="literal">nova-volume</code> (block service).

   </p></dd><dt id="gloss-nova"><span><span class="glossterm"><span class="productname">OpenStack</span> Compute</span> <a title="Permalink" class="permalink" href="#gloss-nova">#</a></span></dt><dd class="glossdef"><p>
    One of the core <span class="productname">OpenStack</span> components and services (code name:
    <code class="literal">nova</code>). It is a cloud computing fabric
    controller and as such, the main part of an IaaS system. It provides
    virtual machines on demand.
   </p></dd><dt id="gloss-horizon"><span><span class="glossterm"><span class="productname">OpenStack</span> Dashboard</span> <a title="Permalink" class="permalink" href="#gloss-horizon">#</a></span></dt><dd class="glossdef"><p>
    One of the core <span class="productname">OpenStack</span> components or services (code name:
    <code class="literal">horizon</code>). It provides a modular Web interface for
    <span class="productname">OpenStack</span> services and allows end users and administrators to interact
    with each <span class="productname">OpenStack</span> service through the service's API.
   </p></dd><dt id="gloss-keystone"><span><span class="glossterm"><span class="productname">OpenStack</span> Identity</span> <a title="Permalink" class="permalink" href="#gloss-keystone">#</a></span></dt><dd class="glossdef"><p>
    One of the core <span class="productname">OpenStack</span> components or services (code name:
    <code class="literal">keystone</code>). It provides authentication and
    authorization for all <span class="productname">OpenStack</span> services.
   </p></dd><dt id="gloss-glance"><span><span class="glossterm"><span class="productname">OpenStack</span> Image</span> <a title="Permalink" class="permalink" href="#gloss-glance">#</a></span></dt><dd class="glossdef"><p>
    One of the core <span class="productname">OpenStack</span> components or services (code name:
    <code class="literal">glance</code>). It provides discovery, registration, and
    delivery services for virtual disk images.
   </p></dd><dt id="gloss-neutron"><span><span class="glossterm"><span class="productname">OpenStack</span> Networking</span> <a title="Permalink" class="permalink" href="#gloss-neutron">#</a></span></dt><dd class="glossdef"><p>
    One of the core <span class="productname">OpenStack</span> components or services (code name:
    <code class="literal">neutron</code>). It provides <span class="quote">“<span class="quote">network connectivity
    as a service</span>”</span> between interface devices (for example, vNICs)
    managed by other <span class="productname">OpenStack</span> services (for example, Compute). Allows
    users to create their own networks and attach interfaces to them.
   </p></dd><dt id="gloss-swift"><span><span class="glossterm"><span class="productname">OpenStack</span> Object Storage</span> <a title="Permalink" class="permalink" href="#gloss-swift">#</a></span></dt><dd class="glossdef"><p>
    One of the core <span class="productname">OpenStack</span> components or services (code name:
    <code class="literal">swift</code>). Allows to store and retrieve files
    while providing built-in redundancy and fail-over. Can be used for
    backing up and archiving data, streaming data to a user's Web browser,
    or developing new applications with data storage integration.
   </p></dd><dt id="id-1.4.10.70"><span><span class="glossterm"><span class="productname">OpenStack</span> Service</span> <a title="Permalink" class="permalink" href="#id-1.4.10.70">#</a></span></dt><dd class="glossdef"><p>
    A collection of Linux services (or daemons) that work together to
    provide core functionality within the <span class="productname">OpenStack</span> project. This can be storing
    objects, providing virtual servers, or authentication and authorization.
    All services have code names, which are also used in configuration files,
    and command line programs.
   </p></dd><dt id="gloss-heat"><span><span class="glossterm">Orchestration</span> <a title="Permalink" class="permalink" href="#gloss-heat">#</a></span></dt><dd class="glossdef"><p>
    
    A module (code name: <code class="literal">heat</code>) to orchestrate
    multiple composite cloud applications using file-based or Web-based
    templates. It contains both a user interface and an API and describes
    your cloud deployment in a declarative language. The module is an
    integrated project of <span class="productname">OpenStack</span> as of the Havana release.
   </p></dd><dt id="id-1.4.10.72"><span><span class="glossterm">PaaS (Platform-as-a-Service)</span> <a title="Permalink" class="permalink" href="#id-1.4.10.72">#</a></span></dt><dd class="glossdef"><p>
    A service model of cloud computing where a computing platform and
    cloud-based application development tools are rented over the Internet.
    The customer controls software deployment and configuration settings,
    but not the underlying cloud infrastructure including network, servers,
    operating systems, or storage.
   </p></dd><dt id="id-1.4.10.73"><span><span class="glossterm">Pacemaker</span> <a title="Permalink" class="permalink" href="#id-1.4.10.73">#</a></span></dt><dd class="glossdef"><p>
    An open source cluster resource manager used in SUSE Linux Enterprise High Availability Extension.
   </p></dd><dt id="id-1.4.10.74"><span><span class="glossterm">Port</span> <a title="Permalink" class="permalink" href="#id-1.4.10.74">#</a></span></dt><dd class="glossdef"><p>
    In the <span class="productname">OpenStack</span> Networking API: An attachment port to an L2
    <span class="productname">OpenStack</span> Networking network.

   </p></dd><dt id="id-1.4.10.77"><span><span class="glossterm">Private Cloud</span> <a title="Permalink" class="permalink" href="#id-1.4.10.77">#</a></span></dt><dd class="glossdef"><p>
    One of several deployment models for a cloud infrastructure. The
    infrastructure is operated exclusively for a single organization and may
    exist on or off premises. The cloud is owned and managed by the
    organization itself, by a third party or a combination of both.
   </p></dd><dt id="id-1.4.10.78"><span><span class="glossterm">Private IP Address</span> <a title="Permalink" class="permalink" href="#id-1.4.10.78">#</a></span></dt><dd class="glossdef"><p>
    See <a class="xref" href="#gloss-IP-fixed" title="Fixed IP Address">Fixed IP Address</a>.
   </p></dd><dt id="gloss-project"><span><span class="glossterm">Project</span> <a title="Permalink" class="permalink" href="#gloss-project">#</a></span></dt><dd class="glossdef"><p>
    A concept in <span class="productname">OpenStack</span> Identity. Used to identify a group, an
    organization, or a project (or more generically, an individual customer
    environment in the cloud). Also called <code class="literal">tenant</code>. The
    term <code class="literal">tenant</code> is primarily used in the <span class="productname">OpenStack</span>
    command line tools.

   </p></dd><dt id="gloss-proposal"><span><span class="glossterm">Proposal</span> <a title="Permalink" class="permalink" href="#gloss-proposal">#</a></span></dt><dd class="glossdef"><p>
    Special configuration for a barclamp. It includes barclamp-specific
    settings, and a list of nodes to which the proposal should be applied.
   </p></dd><dt id="id-1.4.10.79"><span><span class="glossterm">Public Cloud</span> <a title="Permalink" class="permalink" href="#id-1.4.10.79">#</a></span></dt><dd class="glossdef"><p>
    One of several deployment models for a cloud infrastructure. The cloud
    infrastructure is designed for use by the general public and exists on
    the premises of the cloud provider. Services like applications, storage,
    and other resources are made available to the general public for free or
    are offered on a pay-per-use model. The infrastructure is owned and
    managed by a business, academic or government organization, or some
    combination of these.
   </p></dd><dt id="id-1.4.10.80"><span><span class="glossterm">Public IP Address</span> <a title="Permalink" class="permalink" href="#id-1.4.10.80">#</a></span></dt><dd class="glossdef"><p>
    See <a class="xref" href="#gloss-IP-float" title="Floating IP Address">Floating IP Address</a>.
   </p></dd><dt id="id-1.4.10.81"><span><span class="glossterm">qcow (QEMU Copy on Write)</span> <a title="Permalink" class="permalink" href="#id-1.4.10.81">#</a></span></dt><dd class="glossdef"><p>
    A disk image format supported by the QEMU virtual machine manager. A
    <code class="literal">qcow2</code> image helps to optimize disk space. It
    consumes disk space only when contents are written on it and grows as
    data is added.
   </p><p>
    <code class="literal">qcow2</code> is a more recent version of the
    <code class="literal">qcow</code> format where a read-only base image is used, and
    all writes are stored to the <code class="literal">qcow2</code> image.
   </p></dd><dt id="gloss-quorum"><span><span class="glossterm">Quorum</span> <a title="Permalink" class="permalink" href="#gloss-quorum">#</a></span></dt><dd class="glossdef"><p>
    In a cluster, a <a class="xref" href="#gloss-partition" title="Cluster Partition">Cluster Partition</a> is defined to have
    quorum (is <span class="quote">“<span class="quote">quorate</span>”</span>) if it has the majority of nodes (or
    votes). Quorum distinguishes exactly one partition. It is part of the
    algorithm to prevent several disconnected partitions or nodes from
    proceeding and causing data and service corruption
    (<a class="xref" href="#gloss-splitbrain" title="Split Brain">Split Brain</a>). Quorum is a prerequisite for
    <a class="xref" href="#gloss-fencing" title="Fencing">Fencing</a>, which then ensures that quorum is
    indeed unique.
   </p></dd><dt id="id-1.4.10.83"><span><span class="glossterm">Quota</span> <a title="Permalink" class="permalink" href="#id-1.4.10.83">#</a></span></dt><dd class="glossdef"><p>
    Restriction of resources to prevent overconsumption within a cloud. In
    <span class="productname">OpenStack</span>, quotas are defined per project and contain multiple
    parameters, such as amount of RAM, number of instances, or number of
    floating IP addresses.
   </p></dd><dt id="id-1.4.10.84"><span><span class="glossterm">RC File  (openrc.sh)</span> <a title="Permalink" class="permalink" href="#id-1.4.10.84">#</a></span></dt><dd class="glossdef"><p>
    Environment file needed for the <span class="productname">OpenStack</span> command line tools. The RC
    file is project-specific and contains the credentials used by
    <span class="productname">OpenStack</span> Compute, Image, and Identity services.
   </p></dd><dt id="id-1.4.10.85"><span><span class="glossterm">Recipe</span> <a title="Permalink" class="permalink" href="#id-1.4.10.85">#</a></span></dt><dd class="glossdef"><p>
    A group of Chef scripts and templates. Recipes are used by
    Chef to deploy a unit of functionality.
   </p></dd><dt id="id-1.4.10.86"><span><span class="glossterm">Region</span> <a title="Permalink" class="permalink" href="#id-1.4.10.86">#</a></span></dt><dd class="glossdef"><p>
    
    An <span class="productname">OpenStack</span> method of aggregating clouds. Regions are a robust way to
    share some infrastructure between <span class="productname">OpenStack</span> compute installations,
    while allowing for a high degree of failure tolerance. Regions have a
    separate API endpoint per installation.
   </p></dd><dt id="gloss-rsc"><span><span class="glossterm">Resource</span> <a title="Permalink" class="permalink" href="#gloss-rsc">#</a></span></dt><dd class="glossdef"><p>
    In a High Availability context: Any type of service or application that is known
    to the cluster resource manager. Examples include an IP address, a file
    system, or a database.
   </p></dd><dt id="gloss-ra"><span><span class="glossterm">Resource Agent (RA)</span> <a title="Permalink" class="permalink" href="#gloss-ra">#</a></span></dt><dd class="glossdef"><p>
    A script acting as a proxy to manage a resource in a High Availability cluster.
    For example, it can start, stop or monitor a resource.
   </p></dd><dt id="id-1.4.10.89"><span><span class="glossterm">Role</span> <a title="Permalink" class="permalink" href="#id-1.4.10.89">#</a></span></dt><dd class="glossdef"><p>
    In the Crowbar/Chef context: an instance of a
    <a class="xref" href="#gloss-proposal" title="Proposal">Proposal</a> that is active on a node.
   </p><p>
    In the <a class="xref" href="#gloss-keystone" title="OpenStack Identity"><span class="productname">OpenStack</span> Identity</a> context: concept of controlling
    the actions or set of operations that a user is allowed to perform. A
    role includes a set of rights and privileges. A user assuming that role
    inherits those rights and privileges.
   </p></dd><dt id="id-1.4.10.90"><span><span class="glossterm">S3 (Amazon Simple Storage Service)</span> <a title="Permalink" class="permalink" href="#id-1.4.10.90">#</a></span></dt><dd class="glossdef"><p>
    An object storage by Amazon that can be used to store and retrieve data
    on the Web. Similar in function to <span class="productname">OpenStack</span> Object Storage. It can act
    as a back-end store for glance images.
   </p></dd><dt id="id-1.4.10.91"><span><span class="glossterm">SaaS (Software-as-a-Service)</span> <a title="Permalink" class="permalink" href="#id-1.4.10.91">#</a></span></dt><dd class="glossdef"><p>
    A service model of cloud computing where applications are hosted by a
    service provider and made available to customers remotely as a Web-based
    service.
   </p></dd><dt id="id-1.4.10.92"><span><span class="glossterm">SBD (STONITH Block Device)</span> <a title="Permalink" class="permalink" href="#id-1.4.10.92">#</a></span></dt><dd class="glossdef"><p>
    In an environment where all nodes of a High Availability cluster have access to
    shared storage, a small partition is used for disk-based fencing.
   </p></dd><dt id="id-1.4.10.93"><span><span class="glossterm">Security Group</span> <a title="Permalink" class="permalink" href="#id-1.4.10.93">#</a></span></dt><dd class="glossdef"><p>
    Concept in <span class="productname">OpenStack</span> Networking. A security group is a container for
    security group rules. Security group rules allow to specify the type of
    traffic and direction (ingress/egress) that is allowed to pass through a
    port.
   </p></dd><dt id="gloss-galera-sequence-number"><span><span class="glossterm">Sequence number (seqno)</span> <a title="Permalink" class="permalink" href="#gloss-galera-sequence-number">#</a></span></dt><dd class="glossdef"><p>
      A term from a MariaDB Galera Cluster which is used for replication.
      It's a 64-bit signed integer that the node uses to denote the position of
      a given transaction in the sequence.
   </p></dd><dt id="gloss-spof"><span><span class="glossterm">Single Point of Failure (SPOF)</span> <a title="Permalink" class="permalink" href="#gloss-spof">#</a></span></dt><dd class="glossdef"><p>
    An individual piece of equipment or software which will cause system
    downtime or data loss if it fails. To eliminate single points of
    failure, High Availability systems seek to provide redundancy for crucial pieces
    of equipment or software.
   </p></dd><dt id="gloss-hammer"><span><span class="glossterm">SLEShammer</span> <a title="Permalink" class="permalink" href="#gloss-hammer">#</a></span></dt><dd class="glossdef"><p>
    When you first boot a node in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> via PXE, it is booted with the SLEShammer image. This performs the initial hardware discovery, and registers the node with Crowbar.
    After you allocate the node, it is rebooted with a regular SLES installation image.
   </p></dd><dt id="id-1.4.10.96"><span><span class="glossterm">Snapshot</span> <a title="Permalink" class="permalink" href="#id-1.4.10.96">#</a></span></dt><dd class="glossdef"><p>
    See <a class="xref" href="#gloss-volsnap" title="Volume Snapshot">Volume Snapshot</a> or <a class="xref" href="#gloss-instsnap" title="Instance Snapshot">Instance Snapshot</a>.
   </p></dd><dt id="gloss-splitbrain"><span><span class="glossterm">Split Brain</span> <a title="Permalink" class="permalink" href="#gloss-splitbrain">#</a></span></dt><dd class="glossdef"><p>
    Also known as a <span class="quote">“<span class="quote">partitioned cluster</span>”</span> scenario. Either
    through a software or hardware failure, the cluster nodes are divided
    into two or more groups that do not know of each other.
    <a class="xref" href="#gloss-stonith" title="STONITH">STONITH</a> prevents a split brain situation from
    badly affecting the entire cluster.
   </p></dd><dt id="gloss-stateful"><span><span class="glossterm">Stateful Service</span> <a title="Permalink" class="permalink" href="#gloss-stateful">#</a></span></dt><dd class="glossdef"><p>
    A service where subsequent requests to the service depend on the results
    of the first request.
   </p></dd><dt id="gloss-stateless"><span><span class="glossterm">Stateless Service</span> <a title="Permalink" class="permalink" href="#gloss-stateless">#</a></span></dt><dd class="glossdef"><p>
    A service that provides a response after your request, and then requires
    no further attention.

   </p></dd><dt id="gloss-stonith"><span><span class="glossterm">STONITH</span> <a title="Permalink" class="permalink" href="#gloss-stonith">#</a></span></dt><dd class="glossdef"><p>
    The acronym for <span class="quote">“<span class="quote">Shoot the other node in the head</span>”</span>. It
    refers to the fencing mechanism that shuts down a misbehaving node to
    prevent it from causing trouble in a cluster.
   </p></dd><dt id="id-1.4.10.101"><span><span class="glossterm">Storage Node</span> <a title="Permalink" class="permalink" href="#id-1.4.10.101">#</a></span></dt><dd class="glossdef"><p>
    Node within a SUSE <span class="productname">OpenStack</span> Cloud. Acts as the controller for cloud-based
    storage. A cloud can contain multiple Storage Nodes.
   </p></dd><dt id="id-1.4.10.102"><span><span class="glossterm">Subnet</span> <a title="Permalink" class="permalink" href="#id-1.4.10.102">#</a></span></dt><dd class="glossdef"><p>
    In the <span class="productname">OpenStack</span> Networking API: A block of IP addresses and other
    network configuration (for example, a default gateway, DNS servers) that
    can be associated with an <span class="productname">OpenStack</span> Networking network. Each subnet
    represents an IPv4 or IPv6 address block. Multiple subnets can be
    associated with a network, if necessary.
   </p></dd><dt id="gloss-hasi"><span><span class="glossterm">SUSE Linux Enterprise High Availability Extension</span> <a title="Permalink" class="permalink" href="#gloss-hasi">#</a></span></dt><dd class="glossdef"><p>
    An integrated suite of open source clustering technologies that enables
    you to implement highly available physical and virtual Linux clusters.
   </p></dd><dt id="id-1.4.10.103"><span><span class="glossterm">SUSE <span class="productname">OpenStack</span> Cloud Administrator</span> <a title="Permalink" class="permalink" href="#id-1.4.10.103">#</a></span></dt><dd class="glossdef"><p>
    User role in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. Manages projects, users, images, flavors,
    and quotas within <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
   </p></dd><dt id="id-1.4.10.104"><span><span class="glossterm">SUSE <span class="productname">OpenStack</span> Cloud Dashboard</span> <a title="Permalink" class="permalink" href="#id-1.4.10.104">#</a></span></dt><dd class="glossdef"><p>
    The <span class="phrase"><span class="phrase">SUSE® <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Dashboard is a Web interface that enables cloud
    administrators and users to manage various <span class="productname">OpenStack</span> services. It is based
    on <span class="productname">OpenStack</span> Dashboard (also known under its codename
    <code class="literal">horizon</code>).
   </p></dd><dt id="id-1.4.10.105"><span><span class="glossterm">SUSE <span class="productname">OpenStack</span> Cloud Operator</span> <a title="Permalink" class="permalink" href="#id-1.4.10.105">#</a></span></dt><dd class="glossdef"><p>
    User role in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. Installs and deploys <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
   </p></dd><dt id="id-1.4.10.106"><span><span class="glossterm">SUSE <span class="productname">OpenStack</span> Cloud User</span> <a title="Permalink" class="permalink" href="#id-1.4.10.106">#</a></span></dt><dd class="glossdef"><p>
    User role in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>. End user who launches and manages
    instances, can create snapshots, and use volumes for persistent storage
    within <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
   </p></dd><dt id="id-1.4.10.108"><span><span class="glossterm">swift</span> <a title="Permalink" class="permalink" href="#id-1.4.10.108">#</a></span></dt><dd class="glossdef"><p>
    Code name for <a class="xref" href="#gloss-swift" title="OpenStack Object Storage"><span class="productname">OpenStack</span> Object Storage</a>.
   </p></dd><dt id="id-1.4.10.109"><span><span class="glossterm">TAP Device</span> <a title="Permalink" class="permalink" href="#id-1.4.10.109">#</a></span></dt><dd class="glossdef"><p>
    A virtual networking device. A TAP device, such as
    <code class="literal">vnet0</code> is how hypervisors such as KVM and
    Xen implement a virtual network interface card (vNIC). An Ethernet
    frame sent to a TAP device is received by the guest operating system.
    The tap option connects the network stack of the guest operating system
    to a TAP network device on the host.
   </p></dd><dt id="gloss-ceilo"><span><span class="glossterm">Telemetry</span> <a title="Permalink" class="permalink" href="#gloss-ceilo">#</a></span></dt><dd class="glossdef"><p>
    A module (code name: <code class="literal">ceilometer</code>) for metering
    <span class="productname">OpenStack</span>-based clouds. The project aims to provide a unique point of
    contact across all <span class="productname">OpenStack</span> core components for acquiring metrics.
    The metrics can then be consumed by other components such as customer billing.
    The module is an integrated project of <span class="productname">OpenStack</span> as of the Havana
    release.
   </p></dd><dt id="id-1.4.10.111"><span><span class="glossterm">Tenant</span> <a title="Permalink" class="permalink" href="#id-1.4.10.111">#</a></span></dt><dd class="glossdef"><p>
    See <a class="xref" href="#gloss-project" title="Project">Project</a>.
   </p></dd><dt id="id-1.4.10.112"><span><span class="glossterm">Unicast</span> <a title="Permalink" class="permalink" href="#id-1.4.10.112">#</a></span></dt><dd class="glossdef"><p>
    A technology for sending messages to a single network destination.
    Corosync supports both multicast and unicast. In Corosync,
    unicast is implemented as UDP-unicast (UDPU).
   </p></dd><dt id="id-1.4.10.113"><span><span class="glossterm">User</span> <a title="Permalink" class="permalink" href="#id-1.4.10.113">#</a></span></dt><dd class="glossdef"><p>
    In the <span class="productname">OpenStack</span> context, a digital representation of a person,
    system, or service who uses <span class="productname">OpenStack</span> cloud services. Users can be
    directly assigned to a particular project and behave as if they are
    contained in that project.
   </p></dd><dt id="id-1.4.10.114"><span><span class="glossterm">Veth Pair</span> <a title="Permalink" class="permalink" href="#id-1.4.10.114">#</a></span></dt><dd class="glossdef"><p>
    A virtual networking device.
    
    The acronym veth stands for virtual Ethernet interface. A veth is a pair
    of virtual network interfaces correctly

    directly together. An Ethernet frame sent to one end of a veth pair is
    received by the other end of a veth pair. <span class="productname">OpenStack</span> Networking uses
    veth pairs as virtual patch cables to make connections between virtual
    bridges.
   </p></dd><dt id="id-1.4.10.115"><span><span class="glossterm">VLAN</span> <a title="Permalink" class="permalink" href="#id-1.4.10.115">#</a></span></dt><dd class="glossdef"><p>
    A physical method for network virtualization. VLANs allow to create
    virtual networks across a distributed network. Disparate hosts
    (on independent networks) appear as if they were part of the same
    broadcast domain.
   </p></dd><dt id="id-1.4.10.116"><span><span class="glossterm">VM (Virtual Machine)</span> <a title="Permalink" class="permalink" href="#id-1.4.10.116">#</a></span></dt><dd class="glossdef"><p>
    An operating system instance that runs on top of a hypervisor. Multiple
    virtual machines can run on the same physical host at the same time.
   </p></dd><dt id="id-1.4.10.117"><span><span class="glossterm">vNIC </span> <a title="Permalink" class="permalink" href="#id-1.4.10.117">#</a></span></dt><dd class="glossdef"><p>
    Virtual network interface card.
   </p></dd><dt id="id-1.4.10.118"><span><span class="glossterm">Volume</span> <a title="Permalink" class="permalink" href="#id-1.4.10.118">#</a></span></dt><dd class="glossdef"><p>
    Detachable block storage device. Unlike a SAN, it can only be attached
    to one instance at a time.
   </p></dd><dt id="gloss-vg"><span><span class="glossterm">Volume Group (VG)</span> <a title="Permalink" class="permalink" href="#gloss-vg">#</a></span></dt><dd class="glossdef"><p>
    A virtual disk consisting of aggregated physical volumes. Volume groups
    can be logically partitioned into logical volumes.
   </p></dd><dt id="gloss-volsnap"><span><span class="glossterm">Volume Snapshot</span> <a title="Permalink" class="permalink" href="#gloss-volsnap">#</a></span></dt><dd class="glossdef"><p>
    A point-in-time copy of an <span class="productname">OpenStack</span> storage volume. Used to back up
    volumes.
   </p></dd><dt id="id-1.4.10.121"><span><span class="glossterm">vSwitch (Virtual Switch)</span> <a title="Permalink" class="permalink" href="#id-1.4.10.121">#</a></span></dt><dd class="glossdef"><p>
    
    A software that runs on a host or node and provides the features and
    functions of a hardware-based network switch.
   </p></dd><dt id="id-1.4.10.122"><span><span class="glossterm">Zone</span> <a title="Permalink" class="permalink" href="#id-1.4.10.122">#</a></span></dt><dd class="glossdef"><p>
    
    A logical grouping of Compute services and virtual machine hosts.
   </p></dd></dl></section></section></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>