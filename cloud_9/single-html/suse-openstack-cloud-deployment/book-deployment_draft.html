<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Deployment Guide using Cloud Lifecycle Manager | SUSE OpenStack Cloud 9</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DAPS 3.0.0 (https://opensuse.github.io/daps) using SUSE XSL Stylesheets 2.0.17 (based on DocBook XSL Stylesheets 1.79.2)" /><meta name="product-name" content="SUSE OpenStack Cloud" /><meta name="product-number" content="9" /><meta name="book-title" content="Deployment Guide using Cloud Lifecycle Manager" /><meta name="description" content="" /><meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi" /><meta name="tracker-type" content="bsc" /><meta name="tracker-bsc-component" content="Documentation" /><meta name="tracker-bsc-product" content="SUSE OpenStack Cloud 9" />
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css"></link>');
}
else {
  document.write('<link rel="stylesheet" type="text/css" href="static/css/fonts-onlylocal.css"></link>');
}

</script><noscript><link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft single offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a></div><div id="_outer-wrap"><div id="_white-bg" style="background-color: #FABEBE;"><div id="_header"><div id="_logo"><img src="static/images/logo.png" alt="Logo" /></div><div class="crumbs inactive"><a class="single-crumb" href="#book-deployment" accesskey="c"><span class="single-contents-icon"></span>Deployment Guide using Cloud Lifecycle Manager</a><div class="bubble-corner active-contents"></div></div><div class="clearme"></div></div></div><div id="_fixed-header-wrap" style="background-color: #FABEBE;" class="inactive"><div id="_fixed-header"><div class="crumbs inactive"><a class="single-crumb" href="#book-deployment" accesskey="c"><span class="single-contents-icon"></span>Show Contents: Deployment Guide using Cloud Lifecycle Manager</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="clearme"></div></div><div class="clearme"></div></div><div class="active-contents bubble"><div class="bubble-container"><div id="_bubble-toc"><ol><li class="inactive"><a href="#planning-index"><span class="number">I </span><span class="name">Planning an Installation using Cloud Lifecycle Manager</span></a><ol><li class="inactive"><a href="#register-suse-overview"><span class="number">1 </span><span class="name">Registering SLES</span></a></li><li class="inactive"><a href="#min-hardware"><span class="number">2 </span><span class="name">Hardware and Software Support Matrix</span></a></li><li class="inactive"><a href="#idg-planning-planning-recommended-hardware-minimums-xml-1"><span class="number">3 </span><span class="name">Recommended Hardware Minimums for the Example Configurations</span></a></li><li class="inactive"><a href="#HP3-0HA"><span class="number">4 </span><span class="name">High Availability</span></a></li></ol></li><li class="inactive"><a href="#architecture"><span class="number">II </span><span class="name">Cloud Lifecycle Manager Overview</span></a><ol><li class="inactive"><a href="#cha-input-model-intro-concept"><span class="number">5 </span><span class="name">Input Model</span></a></li><li class="inactive"><a href="#configurationobjects"><span class="number">6 </span><span class="name">Configuration Objects</span></a></li><li class="inactive"><a href="#othertopics"><span class="number">7 </span><span class="name">Other Topics</span></a></li><li class="inactive"><a href="#cpinfofiles"><span class="number">8 </span><span class="name">Configuration Processor Information Files</span></a></li><li class="inactive"><a href="#example-configurations"><span class="number">9 </span><span class="name">Example Configurations</span></a></li><li class="inactive"><a href="#modify-compute-input-model"><span class="number">10 </span><span class="name">Modifying Example Configurations for Compute Nodes</span></a></li><li class="inactive"><a href="#modify-input-model"><span class="number">11 </span><span class="name">Modifying Example Configurations for Object Storage using Swift</span></a></li><li class="inactive"><a href="#alternative-configurations"><span class="number">12 </span><span class="name">Alternative Configurations</span></a></li></ol></li><li class="inactive"><a href="#preinstall"><span class="number">III </span><span class="name">Pre-Installation</span></a><ol><li class="inactive"><a href="#preinstall-overview"><span class="number">13 </span><span class="name">Overview</span></a></li><li class="inactive"><a href="#preinstall-checklist"><span class="number">14 </span><span class="name">Pre-Installation Checklist</span></a></li><li class="inactive"><a href="#cha-depl-dep-inst"><span class="number">15 </span><span class="name">Installing the Cloud Lifecycle Manager server</span></a></li><li class="inactive"><a href="#app-deploy-smt-lcm"><span class="number">16 </span><span class="name">Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</span></a></li><li class="inactive"><a href="#cha-depl-repo-conf-lcm"><span class="number">17 </span><span class="name">Software Repository Setup</span></a></li><li class="inactive"><a href="#multipath-boot-from-san"><span class="number">18 </span><span class="name">Boot from SAN and Multipath Configuration</span></a></li></ol></li><li class="inactive"><a href="#cloudinstallation"><span class="number">IV </span><span class="name">Cloud Installation</span></a><ol><li class="inactive"><a href="#cloudinstallation-overview"><span class="number">19 </span><span class="name">Overview</span></a></li><li class="inactive"><a href="#preparing-standalone"><span class="number">20 </span><span class="name">Preparing for Stand-Alone Deployment</span></a></li><li class="inactive"><a href="#install-gui"><span class="number">21 </span><span class="name">Installing with the Install UI</span></a></li><li class="inactive"><a href="#using-git"><span class="number">22 </span><span class="name">Using Git for Configuration Management</span></a></li><li class="inactive"><a href="#install-standalone"><span class="number">23 </span><span class="name">Installing a Stand-Alone Cloud Lifecycle Manager</span></a></li><li class="inactive"><a href="#install-kvm"><span class="number">24 </span><span class="name">Installing Mid-scale and Entry-scale KVM</span></a></li><li class="inactive"><a href="#DesignateInstallOverview"><span class="number">25 </span><span class="name">DNS Service Installation Overview</span></a></li><li class="inactive"><a href="#MagnumOverview"><span class="number">26 </span><span class="name">Magnum Overview</span></a></li><li class="inactive"><a href="#install-esx-ovsvapp"><span class="number">27 </span><span class="name">Installing ESX Computes and OVSvAPP</span></a></li><li class="inactive"><a href="#integrate-nsx-vsphere"><span class="number">28 </span><span class="name">Integrating NSX for vSphere</span></a></li><li class="inactive"><a href="#install-ironic-overview"><span class="number">29 </span><span class="name">Installing Baremetal (Ironic)</span></a></li><li class="inactive"><a href="#install-swift"><span class="number">30 </span><span class="name">Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Swift Only</span></a></li><li class="inactive"><a href="#install-sles-compute"><span class="number">31 </span><span class="name">Installing SLES Compute</span></a></li><li class="inactive"><a href="#install-ardana-manila"><span class="number">32 </span><span class="name">Installing manila and Creating manila Shares</span></a></li><li class="inactive"><a href="#install-heat-templates"><span class="number">33 </span><span class="name">Installing SUSE CaaS Platform heat Templates</span></a></li><li class="inactive"><a href="#install-caasp-terraform"><span class="number">34 </span><span class="name">Installing SUSE CaaS Platform v4 using terraform</span></a></li><li class="inactive"><a href="#integrations"><span class="number">35 </span><span class="name">Integrations</span></a></li><li class="inactive"><a href="#troubleshooting-installation"><span class="number">36 </span><span class="name">Troubleshooting the Installation</span></a></li><li class="inactive"><a href="#esx-troubleshooting-installation"><span class="number">37 </span><span class="name">Troubleshooting the ESX</span></a></li></ol></li><li class="inactive"><a href="#post-install"><span class="number">V </span><span class="name">Post-Installation</span></a><ol><li class="inactive"><a href="#cloud-verification"><span class="number">38 </span><span class="name">Post Installation Tasks</span></a></li><li class="inactive"><a href="#ui-verification"><span class="number">39 </span><span class="name">UI Verification</span></a></li><li class="inactive"><a href="#install-openstack-clients"><span class="number">40 </span><span class="name">Installing OpenStack Clients</span></a></li><li class="inactive"><a href="#tls30"><span class="number">41 </span><span class="name">Configuring Transport Layer Security (TLS)</span></a></li><li class="inactive"><a href="#config-availability-zones"><span class="number">42 </span><span class="name">Configuring Availability Zones</span></a></li><li class="inactive"><a href="#OctaviaInstall"><span class="number">43 </span><span class="name">Configuring Load Balancer as a Service</span></a></li><li class="inactive"><a href="#postinstall-checklist"><span class="number">44 </span><span class="name">Other Common Post-Installation Tasks</span></a></li></ol></li><li class="inactive"><a href="#cha-inst-trouble"><span class="number">VI </span><span class="name">Support</span></a><ol><li class="inactive"><a href="#sec-depl-trouble-faq"><span class="number">45 </span><span class="name">FAQ</span></a></li><li class="inactive"><a href="#sec-installation-trouble-support"><span class="number">46 </span><span class="name">Support</span></a></li><li class="inactive"><a href="#inst-support-ptf"><span class="number">47 </span><span class="name">
    Applying PTFs (Program Temporary Fixes) Provided by SUSE L3 Support
   </span></a></li><li class="inactive"><a href="#inst-support-ptf-test"><span class="number">48 </span><span class="name">
    Testing PTFs (Program Temporary Fixes) on a Single Node
   </span></a></li></ol></li></ol></div><div class="clearme"></div></div></div></div><div id="_toc-bubble-wrap"></div><div id="_content" class="draft "><div class="documentation"><div xml:lang="en" class="book" id="book-deployment" lang="en"><div class="titlepage"><div><h6 class="version-info"><span class="productname "><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <span class="productnumber ">9</span></h6><div><h1 class="title"><em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em> <a title="Permalink" class="permalink" href="#book-deployment">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/book_cloud_deploy_clm.xml" title="Edit the source file for this section">Edit source</a></h1></div><div class="date"><span class="imprint-label">Publication Date: </span>10/22/2020</div></div></div><div class="toc"><dl><dt><span class="part"><a href="#planning-index"><span class="number">I </span><span class="name">Planning an Installation using Cloud Lifecycle Manager</span></a></span></dt><dd><dl><dt><span class="chapter"><a href="#register-suse-overview"><span class="number">1 </span><span class="name">Registering SLES</span></a></span></dt><dd><dl><dt><span class="section"><a href="#register-suse-installation"><span class="number">1.1 </span><span class="name">Registering SLES during the Installation</span></a></span></dt><dt><span class="section"><a href="#register-suse-already-installed"><span class="number">1.2 </span><span class="name">Registering SLES from the Installed System</span></a></span></dt><dt><span class="section"><a href="#register-suse-automated"><span class="number">1.3 </span><span class="name">Registering SLES during Automated Deployment</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#min-hardware"><span class="number">2 </span><span class="name">Hardware and Software Support Matrix</span></a></span></dt><dd><dl><dt><span class="section"><a href="#hw-support-openstackvers"><span class="number">2.1 </span><span class="name">OpenStack Version Information</span></a></span></dt><dt><span class="section"><a href="#hw-support-hardwareconfig"><span class="number">2.2 </span><span class="name">Supported Hardware Configurations</span></a></span></dt><dt><span class="section"><a href="#core-noncore-openstack"><span class="number">2.3 </span><span class="name">Support for Core and Non-Core OpenStack Features</span></a></span></dt><dt><span class="section"><a href="#hw-support-scaling"><span class="number">2.4 </span><span class="name">Cloud Scaling</span></a></span></dt><dt><span class="section"><a href="#hw-support-software"><span class="number">2.5 </span><span class="name">Supported Software</span></a></span></dt><dt><span class="section"><a href="#hw-support-perfnotes"><span class="number">2.6 </span><span class="name">Notes About Performance</span></a></span></dt><dt><span class="section"><a href="#hw-support-kvmguestos"><span class="number">2.7 </span><span class="name">KVM Guest OS Support</span></a></span></dt><dt><span class="section"><a href="#hw-support-esxguestos"><span class="number">2.8 </span><span class="name">ESX Guest OS Support</span></a></span></dt><dt><span class="section"><a href="#hw-support-ironicguestos"><span class="number">2.9 </span><span class="name">Ironic Guest OS Support</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#idg-planning-planning-recommended-hardware-minimums-xml-1"><span class="number">3 </span><span class="name">Recommended Hardware Minimums for the Example Configurations</span></a></span></dt><dd><dl><dt><span class="section"><a href="#rec-min-entryscale-kvm"><span class="number">3.1 </span><span class="name">Recommended Hardware Minimums for an Entry-scale KVM</span></a></span></dt><dt><span class="section"><a href="#rec-min-entryscale-esx-kvm"><span class="number">3.2 </span><span class="name">Recommended Hardware Minimums for an Entry-scale ESX KVM Model</span></a></span></dt><dt><span class="section"><a href="#rec-min-entryscale-esx-kvm-mml"><span class="number">3.3 </span><span class="name">Recommended Hardware Minimums for an Entry-scale ESX, KVM with Dedicated Cluster for Metering, Monitoring, and Logging</span></a></span></dt><dt><span class="section"><a href="#rec-min-ironic"><span class="number">3.4 </span><span class="name">Recommended Hardware Minimums for an Ironic Flat Network Model</span></a></span></dt><dt><span class="section"><a href="#rec-min-swift"><span class="number">3.5 </span><span class="name">Recommended Hardware Minimums for an Entry-scale Swift Model</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#HP3-0HA"><span class="number">4 </span><span class="name">High Availability</span></a></span></dt><dd><dl><dt><span class="section"><a href="#concepts-overview"><span class="number">4.1 </span><span class="name">High Availability Concepts Overview</span></a></span></dt><dt><span class="section"><a href="#highly-available-cloud-infrastructure"><span class="number">4.2 </span><span class="name">Highly Available Cloud Infrastructure</span></a></span></dt><dt><span class="section"><a href="#high-availablity-controllers"><span class="number">4.3 </span><span class="name">High Availability of Controllers</span></a></span></dt><dt><span class="section"><a href="#CVR"><span class="number">4.4 </span><span class="name">High Availability Routing - Centralized</span></a></span></dt><dt><span class="section"><a href="#availability-zones"><span class="number">4.5 </span><span class="name">Availability Zones</span></a></span></dt><dt><span class="section"><a href="#compute-kvm"><span class="number">4.6 </span><span class="name">Compute with KVM</span></a></span></dt><dt><span class="section"><a href="#nova-availability-zones"><span class="number">4.7 </span><span class="name">Nova Availability Zones</span></a></span></dt><dt><span class="section"><a href="#compute-esx"><span class="number">4.8 </span><span class="name">Compute with ESX Hypervisor</span></a></span></dt><dt><span class="section"><a href="#cinder-availability-zones"><span class="number">4.9 </span><span class="name">cinder Availability Zones</span></a></span></dt><dt><span class="section"><a href="#object-storage-swift"><span class="number">4.10 </span><span class="name">Object Storage with Swift</span></a></span></dt><dt><span class="section"><a href="#highly-available-app-workloads"><span class="number">4.11 </span><span class="name">Highly Available Cloud Applications and Workloads</span></a></span></dt><dt><span class="section"><a href="#what-not-ha"><span class="number">4.12 </span><span class="name">What is not Highly Available?</span></a></span></dt><dt><span class="section"><a href="#more-information"><span class="number">4.13 </span><span class="name">More Information</span></a></span></dt></dl></dd></dl></dd><dt><span class="part"><a href="#architecture"><span class="number">II </span><span class="name">Cloud Lifecycle Manager Overview</span></a></span></dt><dd><dl><dt><span class="chapter"><a href="#cha-input-model-intro-concept"><span class="number">5 </span><span class="name">Input Model</span></a></span></dt><dd><dl><dt><span class="section"><a href="#input-model-introduction"><span class="number">5.1 </span><span class="name">Introduction to the Input Model</span></a></span></dt><dt><span class="section"><a href="#concepts"><span class="number">5.2 </span><span class="name">Concepts</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#configurationobjects"><span class="number">6 </span><span class="name">Configuration Objects</span></a></span></dt><dd><dl><dt><span class="section"><a href="#configobj-cloud"><span class="number">6.1 </span><span class="name">Cloud Configuration</span></a></span></dt><dt><span class="section"><a href="#configobj-controlplane"><span class="number">6.2 </span><span class="name">Control Plane</span></a></span></dt><dt><span class="section"><a href="#configobj-load-balancers"><span class="number">6.3 </span><span class="name">Load Balancers</span></a></span></dt><dt><span class="section"><a href="#configobj-regions"><span class="number">6.4 </span><span class="name">Regions</span></a></span></dt><dt><span class="section"><a href="#configobj-servers"><span class="number">6.5 </span><span class="name">Servers</span></a></span></dt><dt><span class="section"><a href="#configobj-servergroups"><span class="number">6.6 </span><span class="name">Server Groups</span></a></span></dt><dt><span class="section"><a href="#configobj-serverroles"><span class="number">6.7 </span><span class="name">Server Roles</span></a></span></dt><dt><span class="section"><a href="#configobj-diskmodels"><span class="number">6.8 </span><span class="name">
  Disk Models</span></a></span></dt><dt><span class="section"><a href="#configobj-memorymodels"><span class="number">6.9 </span><span class="name">Memory Models</span></a></span></dt><dt><span class="section"><a href="#configobj-cpumodels"><span class="number">6.10 </span><span class="name">
  CPU Models</span></a></span></dt><dt><span class="section"><a href="#configobj-interfacemodels"><span class="number">6.11 </span><span class="name">Interface Models</span></a></span></dt><dt><span class="section"><a href="#configobj-nicmappings"><span class="number">6.12 </span><span class="name">NIC Mappings</span></a></span></dt><dt><span class="section"><a href="#configobj-networkgroups"><span class="number">6.13 </span><span class="name">Network Groups</span></a></span></dt><dt><span class="section"><a href="#configobj-networks"><span class="number">6.14 </span><span class="name">Networks</span></a></span></dt><dt><span class="section"><a href="#configobj-firewallrules"><span class="number">6.15 </span><span class="name">Firewall Rules</span></a></span></dt><dt><span class="section"><a href="#configobj-configurationdata"><span class="number">6.16 </span><span class="name">Configuration Data</span></a></span></dt><dt><span class="section"><a href="#passthrough"><span class="number">6.17 </span><span class="name">Pass Through</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#othertopics"><span class="number">7 </span><span class="name">Other Topics</span></a></span></dt><dd><dl><dt><span class="section"><a href="#services-components"><span class="number">7.1 </span><span class="name">Services and Service Components</span></a></span></dt><dt><span class="section"><a href="#namegeneration"><span class="number">7.2 </span><span class="name">Name Generation</span></a></span></dt><dt><span class="section"><a href="#persisteddata"><span class="number">7.3 </span><span class="name">Persisted Data</span></a></span></dt><dt><span class="section"><a href="#serverallocation"><span class="number">7.4 </span><span class="name">Server Allocation</span></a></span></dt><dt><span class="section"><a href="#servernetworkselection"><span class="number">7.5 </span><span class="name">Server Network Selection</span></a></span></dt><dt><span class="section"><a href="#networkroutevalidation"><span class="number">7.6 </span><span class="name">Network Route Validation</span></a></span></dt><dt><span class="section"><a href="#configneutronprovidervlans"><span class="number">7.7 </span><span class="name">Configuring neutron Provider VLANs</span></a></span></dt><dt><span class="section"><a href="#standalonedeployer"><span class="number">7.8 </span><span class="name">Standalone Cloud Lifecycle Manager</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#cpinfofiles"><span class="number">8 </span><span class="name">Configuration Processor Information Files</span></a></span></dt><dd><dl><dt><span class="section"><a href="#address-info-yml"><span class="number">8.1 </span><span class="name">address_info.yml</span></a></span></dt><dt><span class="section"><a href="#firewall-info-yml"><span class="number">8.2 </span><span class="name">firewall_info.yml</span></a></span></dt><dt><span class="section"><a href="#route-info-yml"><span class="number">8.3 </span><span class="name">route_info.yml</span></a></span></dt><dt><span class="section"><a href="#server-info-yml"><span class="number">8.4 </span><span class="name">server_info.yml</span></a></span></dt><dt><span class="section"><a href="#service-info-yml"><span class="number">8.5 </span><span class="name">service_info.yml</span></a></span></dt><dt><span class="section"><a href="#control-plane-topology-yml"><span class="number">8.6 </span><span class="name">control_plane_topology.yml</span></a></span></dt><dt><span class="section"><a href="#network-topology-yml"><span class="number">8.7 </span><span class="name">network_topology.yml</span></a></span></dt><dt><span class="section"><a href="#region-topology-yml"><span class="number">8.8 </span><span class="name">region_topology.yml</span></a></span></dt><dt><span class="section"><a href="#service-topology-yml"><span class="number">8.9 </span><span class="name">service_topology.yml</span></a></span></dt><dt><span class="section"><a href="#private-data-metadata-ccp-yml"><span class="number">8.10 </span><span class="name">private_data_metadata_ccp.yml</span></a></span></dt><dt><span class="section"><a href="#password-change-yml"><span class="number">8.11 </span><span class="name">password_change.yml</span></a></span></dt><dt><span class="section"><a href="#explain-txt"><span class="number">8.12 </span><span class="name">explain.txt</span></a></span></dt><dt><span class="section"><a href="#clouddiagram-txt"><span class="number">8.13 </span><span class="name">CloudDiagram.txt</span></a></span></dt><dt><span class="section"><a href="#html-representation"><span class="number">8.14 </span><span class="name">HTML Representation</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#example-configurations"><span class="number">9 </span><span class="name">Example Configurations</span></a></span></dt><dd><dl><dt><span class="section"><a href="#example-configs"><span class="number">9.1 </span><span class="name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Example Configurations</span></a></span></dt><dt><span class="section"><a href="#alternative"><span class="number">9.2 </span><span class="name">Alternative Configurations</span></a></span></dt><dt><span class="section"><a href="#kvm-examples"><span class="number">9.3 </span><span class="name">KVM Examples</span></a></span></dt><dt><span class="section"><a href="#esx-examples"><span class="number">9.4 </span><span class="name">ESX Examples</span></a></span></dt><dt><span class="section"><a href="#swift-examples"><span class="number">9.5 </span><span class="name">Swift Examples</span></a></span></dt><dt><span class="section"><a href="#ironic-examples"><span class="number">9.6 </span><span class="name">Ironic Examples</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#modify-compute-input-model"><span class="number">10 </span><span class="name">Modifying Example Configurations for Compute Nodes</span></a></span></dt><dd><dl><dt><span class="section"><a href="#sles-compute-model"><span class="number">10.1 </span><span class="name">SLES Compute Nodes</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#modify-input-model"><span class="number">11 </span><span class="name">Modifying Example Configurations for Object Storage using Swift</span></a></span></dt><dd><dl><dt><span class="section"><a href="#objectstorage-overview"><span class="number">11.1 </span><span class="name">Object Storage using swift Overview</span></a></span></dt><dt><span class="section"><a href="#topic-r3k-v2c-jt"><span class="number">11.2 </span><span class="name">Allocating Proxy, Account, and Container (PAC) Servers for Object Storage</span></a></span></dt><dt><span class="section"><a href="#topic-tq1-xt5-dt"><span class="number">11.3 </span><span class="name">Allocating Object Servers</span></a></span></dt><dt><span class="section"><a href="#topic-uh2-td1-kt"><span class="number">11.4 </span><span class="name">Creating Roles for swift Nodes</span></a></span></dt><dt><span class="section"><a href="#allocating-disk-drives"><span class="number">11.5 </span><span class="name">Allocating Disk Drives for Object Storage</span></a></span></dt><dt><span class="section"><a href="#topic-d1s-hht-tt"><span class="number">11.6 </span><span class="name">Swift Requirements for Device Group Drives</span></a></span></dt><dt><span class="section"><a href="#topic-rvj-21c-jt"><span class="number">11.7 </span><span class="name">Creating a Swift Proxy, Account, and Container (PAC) Cluster</span></a></span></dt><dt><span class="section"><a href="#topic-jzk-q1c-jt"><span class="number">11.8 </span><span class="name">Creating Object Server Resource Nodes</span></a></span></dt><dt><span class="section"><a href="#topic-pcj-hzv-dt"><span class="number">11.9 </span><span class="name">Understanding Swift Network and Service Requirements</span></a></span></dt><dt><span class="section"><a href="#ring-specification"><span class="number">11.10 </span><span class="name">Understanding Swift Ring Specifications</span></a></span></dt><dt><span class="section"><a href="#swift-storage-policies"><span class="number">11.11 </span><span class="name">Designing Storage Policies</span></a></span></dt><dt><span class="section"><a href="#designing-swift-zones"><span class="number">11.12 </span><span class="name">Designing Swift Zones</span></a></span></dt><dt><span class="section"><a href="#topic-rdf-hkp-rt"><span class="number">11.13 </span><span class="name">Customizing Swift Service Configuration Files</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#alternative-configurations"><span class="number">12 </span><span class="name">Alternative Configurations</span></a></span></dt><dd><dl><dt><span class="section"><a href="#standalone-deployer"><span class="number">12.1 </span><span class="name">Using a Dedicated Cloud Lifecycle Manager Node</span></a></span></dt><dt><span class="section"><a href="#without-dvr"><span class="number">12.2 </span><span class="name">Configuring <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> without DVR</span></a></span></dt><dt><span class="section"><a href="#without-l3agent"><span class="number">12.3 </span><span class="name">Configuring <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> with Provider VLANs and Physical Routers Only</span></a></span></dt><dt><span class="section"><a href="#twosystems"><span class="number">12.4 </span><span class="name">Considerations When Installing Two Systems on One Subnet</span></a></span></dt></dl></dd></dl></dd><dt><span class="part"><a href="#preinstall"><span class="number">III </span><span class="name">Pre-Installation</span></a></span></dt><dd><dl><dt><span class="chapter"><a href="#preinstall-overview"><span class="number">13 </span><span class="name">Overview</span></a></span></dt><dt><span class="chapter"><a href="#preinstall-checklist"><span class="number">14 </span><span class="name">Pre-Installation Checklist</span></a></span></dt><dd><dl><dt><span class="section"><a href="#id-1.3.5.3.4"><span class="number">14.1 </span><span class="name">BIOS and IPMI Settings</span></a></span></dt><dt><span class="section"><a href="#id-1.3.5.3.5"><span class="number">14.2 </span><span class="name">Network Setup and Configuration</span></a></span></dt><dt><span class="section"><a href="#id-1.3.5.3.6"><span class="number">14.3 </span><span class="name">Cloud Lifecycle Manager</span></a></span></dt><dt><span class="section"><a href="#id-1.3.5.3.7"><span class="number">14.4 </span><span class="name">Information for the <code class="filename">nic_mappings.yml</code> Input File</span></a></span></dt><dt><span class="section"><a href="#id-1.3.5.3.8"><span class="number">14.5 </span><span class="name">Control Plane</span></a></span></dt><dt><span class="section"><a href="#id-1.3.5.3.9"><span class="number">14.6 </span><span class="name">Compute Hosts</span></a></span></dt><dt><span class="section"><a href="#id-1.3.5.3.10"><span class="number">14.7 </span><span class="name">Storage Hosts</span></a></span></dt><dt><span class="section"><a href="#id-1.3.5.3.11"><span class="number">14.8 </span><span class="name">Additional Comments</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#cha-depl-dep-inst"><span class="number">15 </span><span class="name">Installing the Cloud Lifecycle Manager server</span></a></span></dt><dd><dl><dt><span class="sect1"><a href="#sec-depl-adm-inst-online-update"><span class="number">15.1 </span><span class="name">Registration and Online Updates</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-adm-inst-os"><span class="number">15.2 </span><span class="name">Starting the Operating System Installation</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-adm-inst-partitioning"><span class="number">15.3 </span><span class="name">Partitioning</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-adm-inst-user"><span class="number">15.4 </span><span class="name">Creating a User</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-adm-inst-settings"><span class="number">15.5 </span><span class="name">Installation Settings</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#app-deploy-smt-lcm"><span class="number">16 </span><span class="name">Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</span></a></span></dt><dd><dl><dt><span class="sect1"><a href="#app-deploy-smt-install"><span class="number">16.1 </span><span class="name">SMT Installation</span></a></span></dt><dt><span class="sect1"><a href="#clm-app-deploy-smt-config"><span class="number">16.2 </span><span class="name">SMT Configuration</span></a></span></dt><dt><span class="sect1"><a href="#app-deploy-smt-repos"><span class="number">16.3 </span><span class="name">Setting up Repository Mirroring on the SMT Server</span></a></span></dt><dt><span class="sect1"><a href="#app-deploy-smt-info"><span class="number">16.4 </span><span class="name">For More Information</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#cha-depl-repo-conf-lcm"><span class="number">17 </span><span class="name">Software Repository Setup</span></a></span></dt><dd><dl><dt><span class="sect1"><a href="#sec-depl-adm-conf-repos-product"><span class="number">17.1 </span><span class="name">Copying the Product Media Repositories</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-adm-conf-repos-scc"><span class="number">17.2 </span><span class="name">Update and Pool Repositories</span></a></span></dt><dt><span class="sect1"><a href="#sec-deploy-repo-locations"><span class="number">17.3 </span><span class="name">Repository Locations</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#multipath-boot-from-san"><span class="number">18 </span><span class="name">Boot from SAN and Multipath Configuration</span></a></span></dt><dd><dl><dt><span class="section"><a href="#multipath-overview"><span class="number">18.1 </span><span class="name">Introduction</span></a></span></dt><dt><span class="section"><a href="#id-1.3.5.7.3"><span class="number">18.2 </span><span class="name">Install Phase Configuration</span></a></span></dt></dl></dd></dl></dd><dt><span class="part"><a href="#cloudinstallation"><span class="number">IV </span><span class="name">Cloud Installation</span></a></span></dt><dd><dl><dt><span class="chapter"><a href="#cloudinstallation-overview"><span class="number">19 </span><span class="name">Overview</span></a></span></dt><dt><span class="chapter"><a href="#preparing-standalone"><span class="number">20 </span><span class="name">Preparing for Stand-Alone Deployment</span></a></span></dt><dd><dl><dt><span class="section"><a href="#id-1.3.6.3.2"><span class="number">20.1 </span><span class="name">Cloud Lifecycle Manager Installation Alternatives</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.3.3"><span class="number">20.2 </span><span class="name">Installing a Stand-Alone Deployer</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#install-gui"><span class="number">21 </span><span class="name">Installing with the Install UI</span></a></span></dt><dd><dl><dt><span class="section"><a href="#id-1.3.6.4.8"><span class="number">21.1 </span><span class="name">Prepare for Cloud Installation</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.4.9"><span class="number">21.2 </span><span class="name">Preparing to Run the Install UI</span></a></span></dt><dt><span class="section"><a href="#create-csv-file"><span class="number">21.3 </span><span class="name">Optional: Creating a CSV File to Import Server Data</span></a></span></dt><dt><span class="section"><a href="#discover-servers"><span class="number">21.4 </span><span class="name">Optional: Importing Certificates for SUSE Manager and HPE OneView</span></a></span></dt><dt><span class="section"><a href="#running-install-ui"><span class="number">21.5 </span><span class="name">Running the Install UI</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#using-git"><span class="number">22 </span><span class="name">Using Git for Configuration Management</span></a></span></dt><dd><dl><dt><span class="section"><a href="#id-1.3.6.5.4"><span class="number">22.1 </span><span class="name">Initialization on a new deployment</span></a></span></dt><dt><span class="section"><a href="#updating-configuration-including-default-config"><span class="number">22.2 </span><span class="name">Updating any configuration, including the default configuration</span></a></span></dt><dt><span class="section"><a href="#git-merge"><span class="number">22.3 </span><span class="name">Resolving Git merge conflicts</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#install-standalone"><span class="number">23 </span><span class="name">Installing a Stand-Alone Cloud Lifecycle Manager</span></a></span></dt><dd><dl><dt><span class="section"><a href="#id-1.3.6.6.2"><span class="number">23.1 </span><span class="name">Important Notes</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.6.3"><span class="number">23.2 </span><span class="name">Prepare for Cloud Installation</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.6.4"><span class="number">23.3 </span><span class="name">Configuring Your Environment</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.6.5"><span class="number">23.4 </span><span class="name">Running the Configuration Processor</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.6.6"><span class="number">23.5 </span><span class="name">Configuring TLS</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.6.7"><span class="number">23.6 </span><span class="name">Deploying the Cloud</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.6.8"><span class="number">23.7 </span><span class="name">Installing <span class="productname">OpenStack</span> Assets on the Stand-alone Deployer</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.6.9"><span class="number">23.8 </span><span class="name">Post-Installation Verification and Administration</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#install-kvm"><span class="number">24 </span><span class="name">Installing Mid-scale and Entry-scale KVM</span></a></span></dt><dd><dl><dt><span class="section"><a href="#sec-kvm-important-notes"><span class="number">24.1 </span><span class="name">Important Notes</span></a></span></dt><dt><span class="section"><a href="#sec-kvm-prereqs"><span class="number">24.2 </span><span class="name">Prepare for Cloud Installation</span></a></span></dt><dt><span class="section"><a href="#sec-kvm-configuration"><span class="number">24.3 </span><span class="name">Configuring Your Environment</span></a></span></dt><dt><span class="section"><a href="#sec-kvm-provision"><span class="number">24.4 </span><span class="name">Provisioning Your Baremetal Nodes</span></a></span></dt><dt><span class="section"><a href="#sec-kvm-config-processor"><span class="number">24.5 </span><span class="name">Running the Configuration Processor</span></a></span></dt><dt><span class="section"><a href="#sec-kvm-security"><span class="number">24.6 </span><span class="name">Configuring TLS</span></a></span></dt><dt><span class="section"><a href="#sec-kvm-deploy"><span class="number">24.7 </span><span class="name">Deploying the Cloud</span></a></span></dt><dt><span class="section"><a href="#sec-kvm-configure-backend"><span class="number">24.8 </span><span class="name">Configuring a Block Storage Backend</span></a></span></dt><dt><span class="section"><a href="#sec-kvm-post-installation"><span class="number">24.9 </span><span class="name">Post-Installation Verification and Administration</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#DesignateInstallOverview"><span class="number">25 </span><span class="name">DNS Service Installation Overview</span></a></span></dt><dd><dl><dt><span class="section"><a href="#DesignateBIND"><span class="number">25.1 </span><span class="name">Installing the DNS Service with BIND</span></a></span></dt><dt><span class="section"><a href="#DesignatePowerDNS"><span class="number">25.2 </span><span class="name">Install the DNS Service with PowerDNS</span></a></span></dt><dt><span class="section"><a href="#DNS-NS"><span class="number">25.3 </span><span class="name">Configure DNS Domain and NS Records</span></a></span></dt><dt><span class="section"><a href="#DNS-MIGRATE"><span class="number">25.4 </span><span class="name">Migrate Zone/Pool to Worker/Producer after Upgrade</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#MagnumOverview"><span class="number">26 </span><span class="name">Magnum Overview</span></a></span></dt><dd><dl><dt><span class="section"><a href="#MagnumArchitecture"><span class="number">26.1 </span><span class="name">Magnum Architecture</span></a></span></dt><dt><span class="section"><a href="#MagnumInstall"><span class="number">26.2 </span><span class="name">Install the Magnum Service</span></a></span></dt><dt><span class="section"><a href="#MagnumIntegrateDNS"><span class="number">26.3 </span><span class="name">Integrate Magnum with the DNS Service</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#install-esx-ovsvapp"><span class="number">27 </span><span class="name">Installing ESX Computes and OVSvAPP</span></a></span></dt><dd><dl><dt><span class="section"><a href="#sec-ironic-prereqs"><span class="number">27.1 </span><span class="name">Prepare for Cloud Installation</span></a></span></dt><dt><span class="section"><a href="#sec-ironic-setup-deployer"><span class="number">27.2 </span><span class="name">Setting Up the Cloud Lifecycle Manager</span></a></span></dt><dt><span class="section"><a href="#esxi-overview"><span class="number">27.3 </span><span class="name">Overview of ESXi and OVSvApp</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.10.6"><span class="number">27.4 </span><span class="name">VM Appliances Used in OVSvApp Implementation</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.10.7"><span class="number">27.5 </span><span class="name">Prerequisites for Installing ESXi and Managing with vCenter</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.10.8"><span class="number">27.6 </span><span class="name">ESXi/vCenter System Requirements</span></a></span></dt><dt><span class="section"><a href="#create-esx-cluster"><span class="number">27.7 </span><span class="name">Creating an ESX Cluster</span></a></span></dt><dt><span class="section"><a href="#config-dvs-pg"><span class="number">27.8 </span><span class="name">Configuring the Required Distributed vSwitches and Port Groups</span></a></span></dt><dt><span class="section"><a href="#create-vapp-template"><span class="number">27.9 </span><span class="name">Create a SUSE-based Virtual Appliance Template in vCenter</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.10.12"><span class="number">27.10 </span><span class="name">ESX Network Model Requirements</span></a></span></dt><dt><span class="section"><a href="#create-vms-vapp-template"><span class="number">27.11 </span><span class="name">Creating and Configuring Virtual Machines Based on Virtual Appliance
 Template</span></a></span></dt><dt><span class="section"><a href="#collect-vcenter-credentials"><span class="number">27.12 </span><span class="name">Collect vCenter Credentials and UUID</span></a></span></dt><dt><span class="section"><a href="#edit-input-models"><span class="number">27.13 </span><span class="name">Edit Input Models to Add and Configure Virtual Appliances</span></a></span></dt><dt><span class="section"><a href="#run-config-processor"><span class="number">27.14 </span><span class="name">Running the Configuration Processor With Applied Changes</span></a></span></dt><dt><span class="section"><a href="#test-esx-environment"><span class="number">27.15 </span><span class="name">Test the ESX-OVSvApp Environment</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#integrate-nsx-vsphere"><span class="number">28 </span><span class="name">Integrating NSX for vSphere</span></a></span></dt><dd><dl><dt><span class="section"><a href="#nsx-vsphere-vm"><span class="number">28.1 </span><span class="name">Integrating with NSX for vSphere</span></a></span></dt><dt><span class="section"><a href="#nsx-vsphere-baremetal"><span class="number">28.2 </span><span class="name">Integrating with NSX for vSphere on Baremetal</span></a></span></dt><dt><span class="section"><a href="#nsx-verification"><span class="number">28.3 </span><span class="name">Verifying the NSX-v Functionality After Integration</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#install-ironic-overview"><span class="number">29 </span><span class="name">Installing Baremetal (Ironic)</span></a></span></dt><dd><dl><dt><span class="section"><a href="#install-ironic"><span class="number">29.1 </span><span class="name">Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Ironic Flat Network</span></a></span></dt><dt><span class="section"><a href="#ironic-multi-control-plane"><span class="number">29.2 </span><span class="name">ironic in Multiple Control Plane</span></a></span></dt><dt><span class="section"><a href="#ironic-provisioning"><span class="number">29.3 </span><span class="name">Provisioning Bare-Metal Nodes with Flat Network Model</span></a></span></dt><dt><span class="section"><a href="#ironic-provisioning-multi-tenancy"><span class="number">29.4 </span><span class="name">Provisioning Baremetal Nodes with Multi-Tenancy</span></a></span></dt><dt><span class="section"><a href="#ironic-system-details"><span class="number">29.5 </span><span class="name">View Ironic System Details</span></a></span></dt><dt><span class="section"><a href="#ironic-toubleshooting"><span class="number">29.6 </span><span class="name">Troubleshooting ironic Installation</span></a></span></dt><dt><span class="section"><a href="#ironic-node-cleaning"><span class="number">29.7 </span><span class="name">Node Cleaning</span></a></span></dt><dt><span class="section"><a href="#ironic-oneview"><span class="number">29.8 </span><span class="name">Ironic and HPE OneView</span></a></span></dt><dt><span class="section"><a href="#ironic-raid-config"><span class="number">29.9 </span><span class="name">RAID Configuration for Ironic</span></a></span></dt><dt><span class="section"><a href="#ironic-audit-support"><span class="number">29.10 </span><span class="name">Audit Support for Ironic</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#install-swift"><span class="number">30 </span><span class="name">Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Swift Only</span></a></span></dt><dd><dl><dt><span class="section"><a href="#sec-swift-important-notes"><span class="number">30.1 </span><span class="name">Important Notes</span></a></span></dt><dt><span class="section"><a href="#sec-swift-prereqs"><span class="number">30.2 </span><span class="name">Prepare for Cloud Installation</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.13.5"><span class="number">30.3 </span><span class="name">Configure Your Environment</span></a></span></dt><dt><span class="section"><a href="#sec-swift-provision"><span class="number">30.4 </span><span class="name">Provisioning Your Baremetal Nodes</span></a></span></dt><dt><span class="section"><a href="#sec-swift-config-processor"><span class="number">30.5 </span><span class="name">Running the Configuration Processor</span></a></span></dt><dt><span class="section"><a href="#sec-swift-deploy"><span class="number">30.6 </span><span class="name">Deploying the Cloud</span></a></span></dt><dt><span class="section"><a href="#sec-swift-post-installation"><span class="number">30.7 </span><span class="name">Post-Installation Verification and Administration</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#install-sles-compute"><span class="number">31 </span><span class="name">Installing SLES Compute</span></a></span></dt><dd><dl><dt><span class="section"><a href="#sles-overview"><span class="number">31.1 </span><span class="name">SLES Compute Node Installation Overview</span></a></span></dt><dt><span class="section"><a href="#sles-support"><span class="number">31.2 </span><span class="name">SLES Support</span></a></span></dt><dt><span class="section"><a href="#install-sles"><span class="number">31.3 </span><span class="name">Using the Cloud Lifecycle Manager to Deploy SLES Compute Nodes</span></a></span></dt><dt><span class="section"><a href="#provisioning-sles"><span class="number">31.4 </span><span class="name">Provisioning SLES Yourself</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#install-ardana-manila"><span class="number">32 </span><span class="name">Installing manila and Creating manila Shares</span></a></span></dt><dd><dl><dt><span class="section"><a href="#id-1.3.6.15.2"><span class="number">32.1 </span><span class="name">Installing manila</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.15.3"><span class="number">32.2 </span><span class="name">Adding manila to an Existing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Environment</span></a></span></dt><dt><span class="section"><a href="#configure-manila-backend"><span class="number">32.3 </span><span class="name">Configure manila Backend</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.15.5"><span class="number">32.4 </span><span class="name">Creating manila Shares</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.15.6"><span class="number">32.5 </span><span class="name">Troubleshooting</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#install-heat-templates"><span class="number">33 </span><span class="name">Installing SUSE CaaS Platform heat Templates</span></a></span></dt><dd><dl><dt><span class="section"><a href="#sec-heat-templates-install"><span class="number">33.1 </span><span class="name">SUSE CaaS Platform heat Installation Procedure</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.16.4"><span class="number">33.2 </span><span class="name">Installing SUSE CaaS Platform with Multiple Masters</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.16.5"><span class="number">33.3 </span><span class="name">Deploy SUSE CaaS Platform Stack Using heat SUSE CaaS Platform Playbook</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.16.6"><span class="number">33.4 </span><span class="name">Deploy SUSE CaaS Platform Cluster with Multiple Masters Using heat CaasP
  Playbook</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.16.7"><span class="number">33.5 </span><span class="name">SUSE CaaS Platform <span class="productname">OpenStack</span> Image for heat SUSE CaaS Platform Playbook</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.16.8"><span class="number">33.6 </span><span class="name">Enabling the Cloud Provider Integration (CPI) Feature</span></a></span></dt><dt><span class="section"><a href="#sec-heat-templates-register"><span class="number">33.7 </span><span class="name">Register SUSE CaaS Platform Cluster for Software Updates</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#install-caasp-terraform"><span class="number">34 </span><span class="name">Installing SUSE CaaS Platform v4 using terraform</span></a></span></dt><dd><dl><dt><span class="section"><a href="#id-1.3.6.17.2"><span class="number">34.1 </span><span class="name">CaaSP v4 deployment on SOC using terraform.</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#integrations"><span class="number">35 </span><span class="name">Integrations</span></a></span></dt><dd><dl><dt><span class="section"><a href="#config-3par"><span class="number">35.1 </span><span class="name">Configuring for 3PAR Block Storage Backend</span></a></span></dt><dt><span class="section"><a href="#ironic-oneview-integration"><span class="number">35.2 </span><span class="name">Ironic HPE OneView Integration</span></a></span></dt><dt><span class="section"><a href="#ses-integration"><span class="number">35.3 </span><span class="name">SUSE Enterprise Storage Integration</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#troubleshooting-installation"><span class="number">36 </span><span class="name">Troubleshooting the Installation</span></a></span></dt><dd><dl><dt><span class="section"><a href="#sec-trouble-deployer-setup"><span class="number">36.1 </span><span class="name">Issues during Cloud Lifecycle Manager Setup</span></a></span></dt><dt><span class="section"><a href="#sec-trouble-config-processor"><span class="number">36.2 </span><span class="name">Issues while Updating Configuration Files</span></a></span></dt><dt><span class="section"><a href="#sec-trouble-deploy-cloud"><span class="number">36.3 </span><span class="name">Issues while Deploying the Cloud</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#esx-troubleshooting-installation"><span class="number">37 </span><span class="name">Troubleshooting the ESX</span></a></span></dt><dd><dl><dt><span class="section"><a href="#id-1.3.6.20.3"><span class="number">37.1 </span><span class="name">Issue: ardana-service.service is not running</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.20.4"><span class="number">37.2 </span><span class="name">Issue: ESX Cluster shows UNKNOWN in Operations Console</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.20.5"><span class="number">37.3 </span><span class="name">Issue: Unable to view the VM console in Horizon UI</span></a></span></dt></dl></dd></dl></dd><dt><span class="part"><a href="#post-install"><span class="number">V </span><span class="name">Post-Installation</span></a></span></dt><dd><dl><dt><span class="chapter"><a href="#cloud-verification"><span class="number">38 </span><span class="name">Post Installation Tasks</span></a></span></dt><dd><dl><dt><span class="section"><a href="#api-verification"><span class="number">38.1 </span><span class="name">API Verification</span></a></span></dt><dt><span class="section"><a href="#sec-verify-block-storage-swift"><span class="number">38.2 </span><span class="name">Verify the Object Storage (swift) Operations</span></a></span></dt><dt><span class="section"><a href="#upload-image"><span class="number">38.3 </span><span class="name">Uploading an Image for Use</span></a></span></dt><dt><span class="section"><a href="#create-extnet"><span class="number">38.4 </span><span class="name">Creating an External Network</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#ui-verification"><span class="number">39 </span><span class="name">UI Verification</span></a></span></dt><dd><dl><dt><span class="section"><a href="#sec-verify-block-storage-volume"><span class="number">39.1 </span><span class="name">Verifying Your Block Storage Backend</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#install-openstack-clients"><span class="number">40 </span><span class="name">Installing OpenStack Clients</span></a></span></dt><dt><span class="chapter"><a href="#tls30"><span class="number">41 </span><span class="name">Configuring Transport Layer Security (TLS)</span></a></span></dt><dd><dl><dt><span class="section"><a href="#id-1.3.7.5.11"><span class="number">41.1 </span><span class="name">Configuring TLS in the input model</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.5.12"><span class="number">41.2 </span><span class="name">User-provided certificates and trust chains</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.5.13"><span class="number">41.3 </span><span class="name">Edit the input model to include your certificate files</span></a></span></dt><dt><span class="section"><a href="#sec-generate-certificate"><span class="number">41.4 </span><span class="name">Generate a self-signed CA</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.5.15"><span class="number">41.5 </span><span class="name">Generate a certificate signing request</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.5.16"><span class="number">41.6 </span><span class="name">Generate a server certificate</span></a></span></dt><dt><span class="section"><a href="#sec-upload-toclm"><span class="number">41.7 </span><span class="name">Upload to the Cloud Lifecycle Manager</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.5.18"><span class="number">41.8 </span><span class="name">Configuring the cipher suite</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.5.19"><span class="number">41.9 </span><span class="name">Testing</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.5.20"><span class="number">41.10 </span><span class="name">Verifying that the trust chain is correctly deployed</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.5.21"><span class="number">41.11 </span><span class="name">Turning TLS on or off</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#config-availability-zones"><span class="number">42 </span><span class="name">Configuring Availability Zones</span></a></span></dt><dt><span class="chapter"><a href="#OctaviaInstall"><span class="number">43 </span><span class="name">Configuring Load Balancer as a Service</span></a></span></dt><dd><dl><dt><span class="section"><a href="#id-1.3.7.7.6"><span class="number">43.1 </span><span class="name">Summary</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.7.7"><span class="number">43.2 </span><span class="name">Prerequisites</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.7.8"><span class="number">43.3 </span><span class="name">Octavia Load Balancing Provider</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.7.9"><span class="number">43.4 </span><span class="name">Prerequisite Setup</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.7.10"><span class="number">43.5 </span><span class="name">Create Load Balancers</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.7.11"><span class="number">43.6 </span><span class="name">Create Floating IPs for Load Balancer</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.7.12"><span class="number">43.7 </span><span class="name">Testing the Octavia Load Balancer</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#postinstall-checklist"><span class="number">44 </span><span class="name">Other Common Post-Installation Tasks</span></a></span></dt><dd><dl><dt><span class="section"><a href="#id-1.3.7.8.2"><span class="number">44.1 </span><span class="name">Determining Your User Credentials</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.8.3"><span class="number">44.2 </span><span class="name">Configure your Cloud Lifecycle Manager to use the command-line tools</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.8.4"><span class="number">44.3 </span><span class="name">Protect home directory</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.8.5"><span class="number">44.4 </span><span class="name">Back up Your SSH Keys</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.8.6"><span class="number">44.5 </span><span class="name">Retrieving Service Endpoints</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.8.7"><span class="number">44.6 </span><span class="name">Other Common Post-Installation Tasks</span></a></span></dt></dl></dd></dl></dd><dt><span class="part"><a href="#cha-inst-trouble"><span class="number">VI </span><span class="name">Support</span></a></span></dt><dd><dl><dt><span class="chapter"><a href="#sec-depl-trouble-faq"><span class="number">45 </span><span class="name">FAQ</span></a></span></dt><dt><span class="chapter"><a href="#sec-installation-trouble-support"><span class="number">46 </span><span class="name">Support</span></a></span></dt><dt><span class="chapter"><a href="#inst-support-ptf"><span class="number">47 </span><span class="name">
    Applying PTFs (Program Temporary Fixes) Provided by SUSE L3 Support
   </span></a></span></dt><dt><span class="chapter"><a href="#inst-support-ptf-test"><span class="number">48 </span><span class="name">
    Testing PTFs (Program Temporary Fixes) on a Single Node
   </span></a></span></dt></dl></dd></dl></div><div class="list-of-figures"><div class="toc-title">List of Figures</div><dl><dt><span class="figure"><a href="#ControlPlane1"><span class="number">4.1 </span><span class="name">HA Architecture</span></a></span></dt><dt><span class="figure"><a href="#Layer3HA"><span class="number">4.2 </span><span class="name">Layer-3 HA</span></a></span></dt><dt><span class="figure"><a href="#DeploymentZones"><span class="number">4.3 </span><span class="name">Availability Zones</span></a></span></dt><dt><span class="figure"><a href="#multi-tenancy"><span class="number">9.1 </span><span class="name">Entry-scale Cloud with Ironic Muti-Tenancy</span></a></span></dt><dt><span class="figure"><a href="#ui-deploy-successful"><span class="number">21.1 </span><span class="name">Cloud Deployment Successful</span></a></span></dt><dt><span class="figure"><a href="#magnum-service-arch-diagram"><span class="number">26.1 </span><span class="name">Service Architecture Diagram for Kubernetes</span></a></span></dt><dt><span class="figure"><a href="#id-1.3.6.12.4.3.8"><span class="number">29.1 </span><span class="name">Architecture of Multiple Control Plane with ironic</span></a></span></dt></dl></div><div class="list-of-tables"><div class="toc-title">List of Tables</div><dl><dt><span class="table"><a href="#neutron-networks-vxlan"><span class="number">6.1 </span><span class="name">neutron.networks.vxlan</span></a></span></dt><dt><span class="table"><a href="#neutron-networks-vlan"><span class="number">6.2 </span><span class="name">neutron.networks.vlan</span></a></span></dt><dt><span class="table"><a href="#neutron-networks-flat"><span class="number">6.3 </span><span class="name">neutron.networks.flat</span></a></span></dt><dt><span class="table"><a href="#neutron-l3-agent"><span class="number">6.4 </span><span class="name">neutron.l3_agent.external_network_bridge</span></a></span></dt><dt><span class="table"><a href="#cp-1"><span class="number">14.1 </span><span class="name">Control Plane 1</span></a></span></dt><dt><span class="table"><a href="#cp-2"><span class="number">14.2 </span><span class="name">Control Plane 2</span></a></span></dt><dt><span class="table"><a href="#cp-3"><span class="number">14.3 </span><span class="name">Control Plane 3</span></a></span></dt><dt><span class="table"><a href="#id-1.3.5.6.5.4"><span class="number">17.1 </span><span class="name">Local Product Repositories for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span></a></span></dt><dt><span class="table"><a href="#tab-smt-repos-local"><span class="number">17.2 </span><span class="name">SMT Repositories Hosted on the Cloud Lifecycle Manager</span></a></span></dt><dt><span class="table"><a href="#tab-depl-adm-conf-local-repos"><span class="number">17.3 </span><span class="name">Repository Locations on the Cloud Lifecycle Manager server</span></a></span></dt><dt><span class="table"><a href="#DNSBackendTable"><span class="number">25.1 </span><span class="name">DNS Backends</span></a></span></dt><dt><span class="table"><a href="#table-ebc-x5v-jz"><span class="number">26.1 </span><span class="name">Data</span></a></span></dt><dt><span class="table"><a href="#table-fst-gxv-jz"><span class="number">26.2 </span><span class="name">Interfaces</span></a></span></dt><dt><span class="table"><a href="#security-groups-table"><span class="number">26.3 </span><span class="name">Security Groups</span></a></span></dt><dt><span class="table"><a href="#network-ports-table"><span class="number">26.4 </span><span class="name">Network Ports</span></a></span></dt><dt><span class="table"><a href="#nsx-hw-reqs-vm"><span class="number">28.1 </span><span class="name">NSX Hardware Requirements for Virtual Machine Integration</span></a></span></dt><dt><span class="table"><a href="#nsx-hw-reqs-bm"><span class="number">28.2 </span><span class="name">NSX Hardware Requirements for Baremetal Integration</span></a></span></dt><dt><span class="table"><a href="#nsx-interface-reqs"><span class="number">28.3 </span><span class="name">NSX Interface Requirements</span></a></span></dt></dl></div><div class="list-of-examples"><div class="toc-title">List of Examples</div><dl><dt><span class="example"><a href="#id-1.3.4.9.10.3.12"><span class="number">11.1 </span><span class="name">
    <span class="bold">PACO</span> - proxy, account, container,
    and object run on the same node type.
   </span></a></span></dt><dt><span class="example"><a href="#id-1.3.4.9.10.3.13"><span class="number">11.2 </span><span class="name">
    <span class="bold">PAC</span> - proxy, account, and
    container run on the same node type.
   </span></a></span></dt><dt><span class="example"><a href="#id-1.3.4.9.10.3.14"><span class="number">11.3 </span><span class="name"><span class="bold">OBJ</span> - Dedicated object
    server</span></a></span></dt><dt><span class="example"><a href="#id-1.3.6.18.5.7.20"><span class="number">35.1 </span><span class="name">ses_config.yml Example</span></a></span></dt><dt><span class="example"><a href="#sec-tls-private-metadata"><span class="number">41.1 </span><span class="name">Certificate request file</span></a></span></dt></dl></div><div><div class="legalnotice" id="id-1.3.2.1"><p>
  Copyright © 2006–
2020

  SUSE LLC and contributors. All rights reserved.
 </p><p>
  Except where otherwise noted, this document is licensed under
  <span class="bold"><strong>Creative Commons Attribution 3.0 License
  </strong></span>: 
   <a class="link" href="https://creativecommons.org/licenses/by/3.0/legalcode" target="_blank">https://creativecommons.org/licenses/by/3.0/legalcode</a>.
 </p><p>
  For SUSE trademarks, see
  <a class="link" href="https://www.suse.com/company/legal/" target="_blank">https://www.suse.com/company/legal/</a>. All other
  third-party trademarks are the property of their respective owners. Trademark
  symbols (®, ™ etc.) denote trademarks of SUSE and its affiliates.
  Asterisks (*) denote third-party trademarks.
 </p><p>
  All information found in this book has been compiled with utmost attention
  to detail. However, this does not guarantee complete accuracy. Neither
  SUSE LLC, its affiliates, the authors nor the translators shall be held
  liable for possible errors or the consequences thereof.
 </p></div></div><div class="part" id="planning-index"><div class="titlepage"><div><div><h1 class="title"><span class="number">Part I </span><span class="name">Planning an Installation using Cloud Lifecycle Manager </span><a title="Permalink" class="permalink" href="#planning-index">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-planning_index.xml" title="Edit the source file for this section">Edit source</a></h1></div></div></div><div class="toc"><dl><dt><span class="chapter"><a href="#register-suse-overview"><span class="number">1 </span><span class="name">Registering SLES</span></a></span></dt><dd class="toc-abstract"><p>To get technical support and product updates, you need to register and activate your SUSE product with the SUSE Customer Center. It is recommended to register during the installation, since this will enable you to install the system with the latest updates and patches available. However, if you are …</p></dd><dt><span class="chapter"><a href="#min-hardware"><span class="number">2 </span><span class="name">Hardware and Software Support Matrix</span></a></span></dt><dd class="toc-abstract"><p>
  This document lists the details about the supported hardware and software for
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9
 </p></dd><dt><span class="chapter"><a href="#idg-planning-planning-recommended-hardware-minimums-xml-1"><span class="number">3 </span><span class="name">Recommended Hardware Minimums for the Example Configurations</span></a></span></dt><dd class="toc-abstract"><p>These recommended minimums are based on example configurations included with the installation models (see Chapter 9, Example Configurations). They are suitable only for demo environments. For production systems you will want to consider your capacity and performance requirements when making decision…</p></dd><dt><span class="chapter"><a href="#HP3-0HA"><span class="number">4 </span><span class="name">High Availability</span></a></span></dt><dd class="toc-abstract"><p>
    This chapter covers High Availability concepts overview and cloud
    infrastructure.
   </p></dd></dl></div><div class="chapter " id="register-suse-overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Registering SLES</span> <a title="Permalink" class="permalink" href="#register-suse-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-suse-register_suse_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-suse-register_suse_overview.xml</li><li><span class="ds-label">ID: </span>register-suse-overview</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#register-suse-installation"><span class="number">1.1 </span><span class="name">Registering SLES during the Installation</span></a></span></dt><dt><span class="section"><a href="#register-suse-already-installed"><span class="number">1.2 </span><span class="name">Registering SLES from the Installed System</span></a></span></dt><dt><span class="section"><a href="#register-suse-automated"><span class="number">1.3 </span><span class="name">Registering SLES during Automated Deployment</span></a></span></dt></dl></div></div><p>
  To get technical support and product updates, you need to register and
  activate your SUSE product with the SUSE Customer Center. It is recommended to register
  during the installation, since this will enable you to install the system
  with the latest updates and patches available. However, if you are offline or
  want to skip the registration step, you can register at any time later from
  the installed system.
 </p><div id="id-1.3.3.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   In case your organization does not provide a local registration server,
   registering SLES requires a SUSE account. In case you do not have a
   SUSE account yet, go to the SUSE Customer Center home page
   (<a class="link" href="https://scc.suse.com/" target="_blank">https://scc.suse.com/</a>) to
   create one.
  </p></div><div class="sect1" id="register-suse-installation"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Registering SLES during the Installation</span> <a title="Permalink" class="permalink" href="#register-suse-installation">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-suse-register_suse_install.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-suse-register_suse_install.xml</li><li><span class="ds-label">ID: </span>register-suse-installation</li></ul></div></div></div></div><p>
  To register your system, provide the E-mail address associated with the
  SUSE account you or your organization uses to manage subscriptions. In case
  you do not have a SUSE account yet, go to the SUSE Customer Center home page
  (<a class="link" href="https://scc.suse.com/" target="_blank">https://scc.suse.com/</a>) to
  create one.
 </p><p>
  Enter the Registration Code you received with your copy of SUSE Linux Enterprise Server. Proceed
  with <span class="guimenu ">Next</span> to start the registration process.
 </p><p>
  By default the system is registered with the SUSE Customer Center. However, if your
  organization provides local registration servers you can either choose one
  from the list of auto-detected servers or provide the URL at
  <code class="literal">Register System via local SMT Server</code>. Proceed with
  <span class="guimenu ">Next</span>.
 </p><p>
  During the registration, the online update repositories will be added to your
  installation setup. When finished, you can choose whether to install the
  latest available package versions from the update repositories. This ensures
  that SUSE Linux Enterprise Server is installed with the latest security updates available. If you
  choose No, all packages will be installed from the installation media.
  Proceed with Next.
 </p><p>
  If the system was successfully registered during installation, YaST will
  disable repositories from local installation media such as CD/DVD or flash
  disks when the installation has been completed. This prevents problems if the
  installation source is no longer available and ensures that you always get
  the latest updates from the online repositories.
 </p></div><div class="sect1" id="register-suse-already-installed"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Registering SLES from the Installed System</span> <a title="Permalink" class="permalink" href="#register-suse-already-installed">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-suse-register_suse_already_installed.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-suse-register_suse_already_installed.xml</li><li><span class="ds-label">ID: </span>register-suse-already-installed</li></ul></div></div></div></div><div class="sect2" id="id-1.3.3.2.5.2"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Registering from the Installed System</span> <a title="Permalink" class="permalink" href="#id-1.3.3.2.5.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-suse-register_suse_already_installed.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-suse-register_suse_already_installed.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If you have skipped the registration during the installation or want to
   re-register your system, you can register the system at any time using the
   YaST module <span class="guimenu ">Product Registration</span> or the command line
   tool <code class="command">SUSEConnect</code>.
  </p><p>
   <span class="bold"><strong>Registering with YaST</strong></span>
  </p><p>
   To register the system start
   <span class="guimenu ">YaST</span> › <span class="guimenu ">Software</span> › <span class="guimenu ">Product
   Registration</span>. Provide the E-mail address associated
   with the SUSE account you or your organization uses to manage
   subscriptions. In case you do not have a SUSE account yet, go to the SUSE Customer Center
   homepage (<a class="link" href="https://scc.suse.com/" target="_blank">https://scc.suse.com/</a>) to create one.
  </p><p>
   Enter the Registration Code you received with your copy of SUSE Linux Enterprise Server. Proceed
   with <span class="guimenu ">Next</span> to start the registration process.
  </p><p>
   By default the system is registered with the SUSE Customer Center. However, if your
   organization provides local registration servers you can either choose one
   form the list of auto-detected servers or provide the URl at
   <span class="guimenu ">Register System via local SMT Server</span>. Proceed with
   <span class="guimenu ">Next</span>.
  </p><p>
   <span class="bold"><strong>Registering with SUSEConnect</strong></span>
  </p><p>
   To register from the command line, use the command
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo SUSEConnect -r <em class="replaceable ">REGISTRATION_CODE</em> -e <em class="replaceable ">EMAIL_ADDRESS</em></pre></div><p>
   Replace <em class="replaceable ">REGISTRATION_CODE</em> with the Registration
   Code you received with your copy of SUSE Linux Enterprise Server. Replace
   <em class="replaceable ">EMAIL_ADDRESS</em> with the E-mail address associated
   with the SUSE account you or your organization uses to manage
   subscriptions. To register with a local registration server, also provide
   the URL to the server:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo SUSEConnect -r <em class="replaceable ">REGISTRATION_CODE</em> -e <em class="replaceable ">EMAIL_ADDRESS</em> \
--url "https://suse_register.example.com/"</pre></div></div></div><div class="sect1" id="register-suse-automated"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Registering SLES during Automated Deployment</span> <a title="Permalink" class="permalink" href="#register-suse-automated">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-suse-register_suse_automated.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-suse-register_suse_automated.xml</li><li><span class="ds-label">ID: </span>register-suse-automated</li></ul></div></div></div></div><p>
  If you deploy your instances automatically using AutoYaST, you can register
  the system during the installation by providing the respective information in
  the AutoYaST control file. Refer to
  <a class="link" href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-autoyast/#CreateProfile-Register" target="_blank">https://documentation.suse.com/sles/15-SP1/single-html/SLES-autoyast/#CreateProfile-Register</a>
  for details.
 </p></div></div><div class="chapter " id="min-hardware"><div class="titlepage"><div><div><h2 class="title"><span class="number">2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Hardware and Software Support Matrix</span> <a title="Permalink" class="permalink" href="#min-hardware">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-hw_support_matrix.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-hw_support_matrix.xml</li><li><span class="ds-label">ID: </span>min-hardware</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#hw-support-openstackvers"><span class="number">2.1 </span><span class="name">OpenStack Version Information</span></a></span></dt><dt><span class="section"><a href="#hw-support-hardwareconfig"><span class="number">2.2 </span><span class="name">Supported Hardware Configurations</span></a></span></dt><dt><span class="section"><a href="#core-noncore-openstack"><span class="number">2.3 </span><span class="name">Support for Core and Non-Core OpenStack Features</span></a></span></dt><dt><span class="section"><a href="#hw-support-scaling"><span class="number">2.4 </span><span class="name">Cloud Scaling</span></a></span></dt><dt><span class="section"><a href="#hw-support-software"><span class="number">2.5 </span><span class="name">Supported Software</span></a></span></dt><dt><span class="section"><a href="#hw-support-perfnotes"><span class="number">2.6 </span><span class="name">Notes About Performance</span></a></span></dt><dt><span class="section"><a href="#hw-support-kvmguestos"><span class="number">2.7 </span><span class="name">KVM Guest OS Support</span></a></span></dt><dt><span class="section"><a href="#hw-support-esxguestos"><span class="number">2.8 </span><span class="name">ESX Guest OS Support</span></a></span></dt><dt><span class="section"><a href="#hw-support-ironicguestos"><span class="number">2.9 </span><span class="name">Ironic Guest OS Support</span></a></span></dt></dl></div></div><p>
  This document lists the details about the supported hardware and software for
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9
 </p><div class="sect1" id="hw-support-openstackvers"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">OpenStack Version Information</span> <a title="Permalink" class="permalink" href="#hw-support-openstackvers">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-hw_support_openstackvers.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-hw_support_openstackvers.xml</li><li><span class="ds-label">ID: </span>hw-support-openstackvers</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 services have been updated to the
  <a class="link" href="https://www.openstack.org/software/rocky" target="_blank">OpenStack
  Rocky</a> release.
 </p></div><div class="sect1" id="hw-support-hardwareconfig"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Supported Hardware Configurations</span> <a title="Permalink" class="permalink" href="#hw-support-hardwareconfig">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-hw_support_hardwareconfig.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-hw_support_hardwareconfig.xml</li><li><span class="ds-label">ID: </span>hw-support-hardwareconfig</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 supports hardware that is certified for SLES through the YES
  certification program. You will find a database of certified hardware at <a class="link" href="https://www.suse.com/yessearch/" target="_blank">https://www.suse.com/yessearch/</a>.
 </p></div><div class="sect1" id="core-noncore-openstack"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Support for Core and Non-Core OpenStack Features</span> <a title="Permalink" class="permalink" href="#core-noncore-openstack">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-core_non-core_openstack_support.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-core_non-core_openstack_support.xml</li><li><span class="ds-label">ID: </span>core-noncore-openstack</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /><col class="col6" /><col class="col7" /></colgroup><tbody><tr><td><span class="bold"><strong><span class="productname">OpenStack</span> Service</strong></span></td><td><span class="bold"><strong>Packages</strong></span></td><td><span class="bold"><strong>Supported</strong></span></td><td>  </td><td><span class="bold"><strong><span class="productname">OpenStack</span> Service</strong></span></td><td><span class="bold"><strong>Packages</strong></span></td><td><span class="bold"><strong>Supported</strong></span></td></tr><tr><td>aodh</td><td>No</td><td>No</td><td>  </td><td>barbican</td><td>Yes</td><td>Yes</td></tr><tr><td>ceilometer</td><td>Yes</td><td>Yes</td><td>  </td><td>cinder</td><td>Yes</td><td>Yes</td></tr><tr><td>designate</td><td>Yes</td><td>Yes</td><td>  </td><td>glance</td><td>Yes</td><td>Yes</td></tr><tr><td>heat</td><td>Yes</td><td>Yes</td><td>  </td><td>horizon</td><td>Yes</td><td>Yes</td></tr><tr><td>ironic</td><td>Yes</td><td>Yes</td><td>  </td><td>keystone</td><td>Yes</td><td>Yes</td></tr><tr><td>Magnum</td><td>Yes</td><td>Yes</td><td>  </td><td>manila</td><td>Yes</td><td>Yes</td></tr><tr><td>monasca</td><td>Yes</td><td>Yes</td><td>  </td><td>monasca-ceilometer</td><td>Yes</td><td>Yes</td></tr><tr><td>neutron</td><td>Yes</td><td>Yes</td><td>  </td><td>neutron(LBaaSv2)</td><td>No</td><td>No</td></tr><tr><td>neutron(VPNaaS)</td><td>Yes</td><td>Yes</td><td>  </td><td>neutron(FWaaS)</td><td>Yes</td><td>Yes</td></tr><tr><td>nova</td><td>Yes</td><td>Yes</td><td>  </td><td>Octavia</td><td>Yes</td><td>Yes</td></tr><tr><td>swift</td><td>Yes</td><td>Yes</td><td>  </td><td> </td><td> </td><td> </td></tr></tbody></table></div><p><span class="bold"><strong>nova</strong></span></p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th><p>Supported</p></th><th><p>Not Supported</p></th></tr></thead><tbody><tr><td><p>SLES KVM Hypervisor</p></td><td><p>Xen hypervisor</p></td></tr><tr><td><p>VMware ESX Hypervisor</p></td><td><p>Hyper-V</p></td></tr><tr><td> </td><td><p>Non-x86 Architectures</p></td></tr></tbody></table></div><p><span class="bold"><strong>neutron</strong></span></p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th><p>Supported</p></th><th><p>Not Supported</p></th></tr></thead><tbody><tr><td>
        <p>Tenant networks</p>
        <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>IPv6</p></li><li class="listitem "><p>SR-IOV</p></li><li class="listitem "><p>PCI-PT</p></li><li class="listitem "><p>DPDK</p></li></ul></div>
       </td><td>
        <p>
         Distributed Virtual Router (DVR) with any of the following:
        </p>
        <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>IPv6</p></li><li class="listitem "><p>BGP/Fast Path Exit</p></li><li class="listitem "><p>L2 gateway</p></li><li class="listitem "><p>SNAT HA</p></li></ul></div>
       </td></tr><tr><td><p>VMware ESX Hypervisor</p></td><td><p>QoS</p></td></tr></tbody></table></div><p><span class="bold"><strong>glance Supported Features</strong></span></p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>swift and Ceph backends</p></li></ul></div><p><span class="bold"><strong>cinder</strong></span></p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th><p>Supported</p></th><th><p>Not Supported</p></th></tr></thead><tbody><tr><td><p>Encrypted &amp; private volumes</p></td><td><p>VSA</p></td></tr><tr><td><p>Incremental backup, backup attached volume, encrypted volume backup, backup
      snapshots</p></td><td></td></tr></tbody></table></div><p><span class="bold"><strong>swift</strong></span></p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th><p>Supported</p></th><th><p>Not Supported</p></th></tr></thead><tbody><tr><td><p>Erasure coding</p></td><td><p>Geographically distributed clusters</p></td></tr><tr><td><p>Dispersion report</p></td><td></td></tr><tr><td><p>swift zones</p></td><td></td></tr></tbody></table></div><p><span class="bold"><strong>keystone</strong></span></p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th><p>Supported</p></th><th><p>Not Supported</p></th></tr></thead><tbody><tr><td><p>Domains</p></td><td><p>Web SSO</p></td></tr><tr><td><p>Fernet tokens</p></td><td><p>Multi-Factor authentication</p></td></tr><tr><td>LDAP integration</td><td><p>Federation keystone to keystone</p></td></tr><tr><td> </td><td><p>Hierarchical multi-tenancy</p></td></tr></tbody></table></div><p><span class="bold"><strong>barbican Supported Features</strong></span></p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Encryption for the following:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>cinder</p></li><li class="listitem "><p>Hardware security model</p></li><li class="listitem "><p>Encrypted data volumes</p></li><li class="listitem "><p>Symmetric keys</p></li><li class="listitem "><p>Storage keys</p></li></ul></div></li><li class="listitem "><p>CADF format auditing events</p></li></ul></div><p><span class="bold"><strong>ceilometer</strong></span></p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th><p>Supported</p></th><th><p>Not Supported</p></th></tr></thead><tbody><tr><td><p>keystone v3 support</p></td><td><p>Gnocchi</p></td></tr><tr><td><p>glance v2 API</p></td><td><p>IPMI and SNMP</p></td></tr><tr><td> </td><td><p>ceilometer Compute Agent</p></td></tr></tbody></table></div><p><span class="bold"><strong>heat Features Not Supported</strong></span></p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Multi-region stack</p></li></ul></div><p><span class="bold"><strong>ironic</strong></span></p><p>
    The table below shows supported node configurations. <code class="literal">UEFI
    secure</code> is not supported.
   </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="1" /><col class="2" /><col class="3" /><col class="4" /><col class="5" /><col class="6" /></colgroup><thead><tr><th rowspan="2">Hardware Type</th><th colspan="5" align="center">Interface</th></tr><tr><th>Boot</th><th>Deploy</th><th>Inspect</th><th>Management</th><th>Power</th></tr></thead><tbody><tr><td>ilo</td><td>ilo-virtual-media</td><td>direct</td><td>ilo</td><td>ilo</td><td>ilo</td></tr><tr><td>ilo</td><td>ilo-pxe</td><td>iscsi</td><td>ilo</td><td>ilo</td><td>ilo</td></tr><tr><td>ipmi</td><td>pxe</td><td>direct</td><td>no-inspect</td><td>ipmitool</td><td>ipmitool</td></tr><tr><td>ipmi</td><td>pxe</td><td>iscsi</td><td>no-inspect</td><td>ipmitool</td><td>ipmitool</td></tr><tr><td>redfish</td><td>pxe</td><td>iscsi</td><td>no-inspect</td><td>redfish</td><td>redfish</td></tr></tbody></table></div></div><div class="sect1" id="hw-support-scaling"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cloud Scaling</span> <a title="Permalink" class="permalink" href="#hw-support-scaling">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-hw_support_scaling.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-hw_support_scaling.xml</li><li><span class="ds-label">ID: </span>hw-support-scaling</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 has been tested and qualified with a total of 200 total compute
  nodes in a single region (Region0).
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 has been tested and qualified with a total of 12,000 virtual
  machines across a total of 200 compute nodes.
 </p><p>
  Larger configurations are possible, but SUSE has tested and qualified this
  configuration size. Typically larger configurations are enabled with
  services and engineering engagements.
 </p></div><div class="sect1" id="hw-support-software"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Supported Software</span> <a title="Permalink" class="permalink" href="#hw-support-software">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-hw_support_software.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-hw_support_software.xml</li><li><span class="ds-label">ID: </span>hw-support-software</li></ul></div></div></div></div><p>
  <span class="bold"><strong>Supported ESXi versions</strong></span>
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 currently supports the following ESXi versions:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    ESXi version 6.0
   </p></li><li class="listitem "><p>
    ESXi version 6.0 (Update 1b)
   </p></li><li class="listitem "><p>
    ESXi version 6.5
   </p></li></ul></div><p>
  The following are the requirements for your vCenter server:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Software: vCenter (It is recommended to run the same server version as the
    ESXi hosts.)
   </p></li><li class="listitem "><p>
    License Requirements: vSphere Enterprise Plus license
   </p></li></ul></div></div><div class="sect1" id="hw-support-perfnotes"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Notes About Performance</span> <a title="Permalink" class="permalink" href="#hw-support-perfnotes">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-hw_support_perfnotes.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-hw_support_perfnotes.xml</li><li><span class="ds-label">ID: </span>hw-support-perfnotes</li></ul></div></div></div></div><p>
  We have the following recommendations to ensure good performance of your
  cloud environment:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    On the control plane nodes, you will want good I/O performance. Your array
    controllers must have cache controllers and we advise against the use of
    RAID-5.
   </p></li><li class="listitem "><p>
    On compute nodes, the I/O performance will influence the virtual machine
    start-up performance. We also recommend the use of cache controllers in
    your storage arrays.
   </p></li><li class="listitem "><p>
    If you are using dedicated object storage (swift) nodes, in particular the
    account, container, and object servers, we recommend that your storage
    arrays have cache controllers.
   </p></li><li class="listitem "><p>
    For best performance on, set the servers power management
    setting in the iLO to OS Control Mode. This power mode setting is only
    available on servers that include the HP Power Regulator.
   </p></li></ul></div></div><div class="sect1" id="hw-support-kvmguestos"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">KVM Guest OS Support</span> <a title="Permalink" class="permalink" href="#hw-support-kvmguestos">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-hw_support_kvmguestos.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-hw_support_kvmguestos.xml</li><li><span class="ds-label">ID: </span>hw-support-kvmguestos</li></ul></div></div></div></div><p>
  For a list of the supported VM guests, see
  <a class="link" href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-virtualization/#virt-support-guests" target="_blank">https://documentation.suse.com/sles/15-SP1/single-html/SLES-virtualization/#virt-support-guests</a>
 </p></div><div class="sect1" id="hw-support-esxguestos"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">ESX Guest OS Support</span> <a title="Permalink" class="permalink" href="#hw-support-esxguestos">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-hw_support_esxguestos.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-hw_support_esxguestos.xml</li><li><span class="ds-label">ID: </span>hw-support-esxguestos</li></ul></div></div></div></div><p>
  For ESX, refer to the <a class="link" href="https://www.vmware.com/resources/compatibility/search.php?deviceCategory=software&amp;details=1&amp;releases=273,274,338&amp;productNames=15&amp;page=1&amp;display_interval=500&amp;sortColumn=Partner&amp;sortOrder=Asc&amp;testConfig=16" target="_blank">VMware
  Compatibility Guide</a>. The information for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is below the
  search form.
 </p></div><div class="sect1" id="hw-support-ironicguestos"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ironic Guest OS Support</span> <a title="Permalink" class="permalink" href="#hw-support-ironicguestos">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-hw_support_ironicguestos.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-hw_support_ironicguestos.xml</li><li><span class="ds-label">ID: </span>hw-support-ironicguestos</li></ul></div></div></div></div><p>
  A <span class="bold"><strong>Verified</strong></span> Guest OS has been tested by
  SUSE and appears to function properly as a bare metal instance on
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9.
 </p><p>
  A <span class="bold"><strong>Certified</strong></span> Guest OS has been officially
  tested by the operating system vendor, or by SUSE under the vendor's
  authorized program, and will be supported by the operating system vendor as a
  bare metal instance on <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>ironic Guest Operating System</th><th>Verified</th><th>Certified</th></tr></thead><tbody><tr><td>SUSE Linux Enterprise Server 12 SP4</td><td>Yes</td><td>Yes</td></tr></tbody></table></div></div></div><div class="chapter " id="idg-planning-planning-recommended-hardware-minimums-xml-1"><div class="titlepage"><div><div><h2 class="title"><span class="number">3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recommended Hardware Minimums for the Example Configurations</span> <a title="Permalink" class="permalink" href="#idg-planning-planning-recommended-hardware-minimums-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-recommended_hardware_minimums.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-recommended_hardware_minimums.xml</li><li><span class="ds-label">ID: </span>idg-planning-planning-recommended-hardware-minimums-xml-1</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#rec-min-entryscale-kvm"><span class="number">3.1 </span><span class="name">Recommended Hardware Minimums for an Entry-scale KVM</span></a></span></dt><dt><span class="section"><a href="#rec-min-entryscale-esx-kvm"><span class="number">3.2 </span><span class="name">Recommended Hardware Minimums for an Entry-scale ESX KVM Model</span></a></span></dt><dt><span class="section"><a href="#rec-min-entryscale-esx-kvm-mml"><span class="number">3.3 </span><span class="name">Recommended Hardware Minimums for an Entry-scale ESX, KVM with Dedicated Cluster for Metering, Monitoring, and Logging</span></a></span></dt><dt><span class="section"><a href="#rec-min-ironic"><span class="number">3.4 </span><span class="name">Recommended Hardware Minimums for an Ironic Flat Network Model</span></a></span></dt><dt><span class="section"><a href="#rec-min-swift"><span class="number">3.5 </span><span class="name">Recommended Hardware Minimums for an Entry-scale Swift Model</span></a></span></dt></dl></div></div><div class="sect1" id="rec-min-entryscale-kvm"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recommended Hardware Minimums for an Entry-scale KVM</span> <a title="Permalink" class="permalink" href="#rec-min-entryscale-kvm">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-rec_min_entryscale_kvm.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-rec_min_entryscale_kvm.xml</li><li><span class="ds-label">ID: </span>rec-min-entryscale-kvm</li></ul></div></div></div></div><p>
  These recommended minimums are based on example configurations included with
  the installation models (see <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a>). They
  are suitable only for demo environments. For production systems you will
  want to consider your capacity and performance requirements when making
  decisions about your hardware.
 </p><div id="id-1.3.3.4.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   The disk requirements detailed below can be met with logical drives, logical
   volumes, or external storage such as a 3PAR array.
  </p></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /><col class="c6" /><col class="c7" /></colgroup><thead><tr><th rowspan="2">Node Type</th><th rowspan="2">Role Name</th><th rowspan="2">Required Number</th><th colspan="4" align="center">
      Server Hardware - Minimum Requirements and Recommendations
     </th></tr><tr><th>Disk</th><th>Memory</th><th>Network</th><th>CPU</th></tr></thead><tbody><tr><td>Dedicated Cloud Lifecycle Manager (optional)</td><td>Lifecycle-manager</td><td>1</td><td>300 GB</td><td>8 GB</td><td>1 x 10 Gbit/s with PXE Support</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Control Plane</td><td>Controller</td><td>3</td><td>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         1 x 600 GB (minimum) - operating system drive
        </p></li><li class="listitem "><p>
         2 x 600 GB (minimum) - Data drive
        </p></li><li class="listitem "><p>
         Fast disks or SSDs are recommended.
        </p></li></ul></div>
     </td><td>128 GB</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Compute</td><td>Compute</td><td>1-3</td><td>2 x 600 GB (minimum)</td><td>32 GB (memory must be sized based on the virtual machine instances hosted on the
            Compute node)</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>8 CPU (64-bit) cores total (Intel x86_64) with hardware virtualization support. The
            CPU cores must be sized based on the VM instances hosted by the Compute node.</td></tr></tbody></table></div><p>
  For more details about the supported network requirements, see
  <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a>.
 </p></div><div class="sect1" id="rec-min-entryscale-esx-kvm"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recommended Hardware Minimums for an Entry-scale ESX KVM Model</span> <a title="Permalink" class="permalink" href="#rec-min-entryscale-esx-kvm">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-rec_min_entryscale_esx_kvm.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-rec_min_entryscale_esx_kvm.xml</li><li><span class="ds-label">ID: </span>rec-min-entryscale-esx-kvm</li></ul></div></div></div></div><p>
  These recommended minimums are based on example configurations included with
  the installation models (see <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a>). They
  are suitable only for demo environments. For production systems you will want
  to consider your capacity and performance requirements when making decisions
  about your hardware.
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> currently supports the following ESXi versions:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    ESXi version 6.0
   </p></li><li class="listitem "><p>
    ESXi version 6.0 (Update 1b)
   </p></li><li class="listitem "><p>
    ESXi version 6.5
   </p></li></ul></div><p>
  The following are the requirements for your vCenter server:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Software: vCenter (It is recommended to run the same
    server version as the ESXi hosts.)
   </p></li><li class="listitem "><p>
    License Requirements: vSphere Enterprise Plus license
   </p></li></ul></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /><col class="c6" /><col class="c7" /></colgroup><thead><tr><th rowspan="2">Node Type</th><th rowspan="2">Role Name</th><th rowspan="2">Required Number</th><th colspan="4" align="center">Server Hardware - Minimum Requirements and
            Recommendations</th></tr><tr><th>Disk</th><th>Memory</th><th>Network</th><th>CPU </th></tr></thead><tbody><tr><td>Dedicated Cloud Lifecycle Manager (optional)</td><td>Lifecycle-manager</td><td>1</td><td>300 GB</td><td>8 GB</td><td>1 x 10 Gbit/s with PXE Support</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Control Plane</td><td>Controller</td><td>3</td><td>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         1 x 600 GB (minimum) - operating system drive
        </p></li><li class="listitem "><p>
         2 x 600 GB (minimum) - Data drive
        </p></li><li class="listitem "><p>
         Fast disks or SSDs are recommended.
        </p></li></ul></div>
     </td><td>128 GB</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Compute (ESXi hypervisor)</td><td> </td><td>2</td><td>2 x 1 TB (minimum, shared across all nodes)</td><td>128 GB (minimum)</td><td>2 x 10 Gbit/s +1 NIC (for DC access)</td><td>16 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Compute (KVM hypervisor)</td><td>kvm-compute</td><td>1-3</td><td>2 x 600 GB (minimum)</td><td>
      32 GB (memory must be sized based on the virtual machine instances
      hosted on the Compute node)
     </td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>
      8 CPU (64-bit) cores total (Intel x86_64) with hardware virtualization
      support. The CPU cores must be sized based on the VM instances hosted
      by the Compute node.
     </td></tr><tr><td>OVSvApp VM</td><td>on VMWare cluster</td><td>1</td><td>80 GB</td><td>4 GB</td><td>3 VMXNET Virtual Network Adapters</td><td>2 vCPU</td></tr><tr><td>nova proxy VM</td><td>on VMWare cluster</td><td>1 per cluster</td><td>80 GB</td><td>4 GB</td><td>3 VMXNET Virtual Network Adapters</td><td>2 vCPU</td></tr></tbody></table></div></div><div class="sect1" id="rec-min-entryscale-esx-kvm-mml"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recommended Hardware Minimums for an Entry-scale ESX, KVM with Dedicated Cluster for Metering, Monitoring, and Logging</span> <a title="Permalink" class="permalink" href="#rec-min-entryscale-esx-kvm-mml">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-rec_min_entryscale_esx_kvm_mml.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-rec_min_entryscale_esx_kvm_mml.xml</li><li><span class="ds-label">ID: </span>rec-min-entryscale-esx-kvm-mml</li></ul></div></div></div></div><p>
  These recommended minimums are based on example configurations included with
  the installation models (see <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a>). They
  are suitable only for demo environments. For production systems you will want
  to consider your capacity and performance requirements when making decisions
  about your hardware.
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> currently supports the following ESXi versions:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    ESXi version 6.0
   </p></li><li class="listitem "><p>
    ESXi version 6.0 (Update 1b)
   </p></li><li class="listitem "><p>
    ESXi version 6.5
   </p></li></ul></div><p>
  The following are the requirements for your vCenter server:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Software: vCenter (It is recommended to run the same
    server version as the ESXi hosts.)
   </p></li><li class="listitem "><p>
    License Requirements: vSphere Enterprise Plus license
   </p></li></ul></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /><col class="c6" /><col class="c7" /></colgroup><thead><tr><th rowspan="2">Node Type</th><th rowspan="2">Role Name</th><th rowspan="2">Required Number</th><th colspan="4" align="center">
      Server Hardware - Minimum Requirements and Recommendations
     </th></tr><tr><th>Disk</th><th>Memory</th><th>Network</th><th>CPU</th></tr></thead><tbody><tr><td>Dedicated Cloud Lifecycle Manager (optional)</td><td>Lifecycle-manager</td><td>1</td><td>300 GB</td><td>8 GB</td><td>1 x 10 Gbit/s with PXE Support</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td rowspan="3">Control Plane</td><td>Core-API Controller</td><td>2</td><td>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         1 x 600 GB (minimum) - operating system drive
        </p></li><li class="listitem "><p>
         2 x 300 GB (minimum) - swift drive
        </p></li></ul></div>
     </td><td>128 GB</td><td>2 x 10 Gbit/s with PXE Support</td><td>24 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>DBMQ Cluster</td><td>3</td><td>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         1 x 600 GB (minimum) - operating system drive
        </p></li><li class="listitem "><p>
         1 x 300 GB (minimum) - MariaDB drive
        </p></li></ul></div>
     </td><td>96 GB</td><td>2 x 10 Gbit/s with PXE Support</td><td>24 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Metering Mon/Log Cluster</td><td>3</td><td>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         1 x 600 GB (minimum) - operating system drive
        </p></li></ul></div>
     </td><td>128 GB</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>24 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Compute (ESXi hypervisor)</td><td> </td><td>2 (minimum)</td><td>2 X 1 TB (minimum, shared across all nodes)</td><td>64 GB (memory must be sized based on the virtual machine instances hosted on the
            Compute node)</td><td>2 x 10 Gbit/s +1 NIC (for Data Center access)</td><td>16 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Compute (KVM hypervisor)</td><td>kvm-compute</td><td>1-3</td><td>2 X 600 GB (minimum)</td><td>32 GB (memory must be sized based on the virtual machine instances hosted on the
            Compute node)</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>8 CPU (64-bit) cores total (Intel x86_64) with hardware virtualization support. The
            CPU cores must be sized based on the VM instances hosted by the Compute node.</td></tr><tr><td>OVSvApp VM</td><td>on VMWare cluster</td><td>1</td><td>80 GB</td><td>4 GB</td><td>3 VMXNET Virtual Network Adapters</td><td>2 vCPU</td></tr><tr><td>nova proxy VM</td><td>on VMWare cluster</td><td>1 per cluster</td><td>80 GB</td><td>4 GB</td><td>3 VMXNET Virtual Network Adapters</td><td>2 vCPU</td></tr></tbody></table></div></div><div class="sect1" id="rec-min-ironic"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recommended Hardware Minimums for an Ironic Flat Network Model</span> <a title="Permalink" class="permalink" href="#rec-min-ironic">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-rec_min_ironic.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-rec_min_ironic.xml</li><li><span class="ds-label">ID: </span>rec-min-ironic</li></ul></div></div></div></div><p>
  When using the <code class="literal">agent_ilo</code> driver, you should ensure that
  the most recent iLO controller firmware is installed. A recommended minimum
  for the iLO4 controller is version 2.30.
 </p><p>
  The recommended minimum hardware requirements are based on the
  <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a> included with the base installation
  and are suitable only for demo environments. For production systems you will
  want to consider your capacity and performance requirements when making
  decisions about your hardware.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /><col class="c6" /><col class="c7" /></colgroup><thead><tr><th rowspan="2">Node Type</th><th rowspan="2">Role Name</th><th rowspan="2">Required Number</th><th colspan="4" align="center">
      Server Hardware - Minimum Requirements and Recommendations</th></tr><tr><th>Disk </th><th>Memory</th><th>Network</th><th>CPU </th></tr></thead><tbody><tr><td>Dedicated Cloud Lifecycle Manager (optional)</td><td>Lifecycle-manager</td><td>1</td><td>300 GB</td><td>8 GB</td><td>1 x 10 Gbit/s with PXE Support</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Control Plane</td><td>Controller</td><td>3</td><td>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         1 x 600 GB (minimum) - operating system drive
        </p></li><li class="listitem "><p>
         2 x 600 GB (minimum) - Data drive
        </p></li><li class="listitem "><p>
         Fast disks or SSDs are recommended.
        </p></li></ul></div>
     </td><td>128 GB</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Compute</td><td>Compute</td><td>1</td><td>1 x 600 GB (minimum)</td><td>16 GB</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>16 CPU (64-bit) cores total (Intel x86_64)</td></tr></tbody></table></div><p>
  For more details about the supported network requirements, see
  <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a>.
 </p></div><div class="sect1" id="rec-min-swift"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recommended Hardware Minimums for an Entry-scale Swift Model</span> <a title="Permalink" class="permalink" href="#rec-min-swift">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-rec_min_swift.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-rec_min_swift.xml</li><li><span class="ds-label">ID: </span>rec-min-swift</li></ul></div></div></div></div><p>
  These recommended minimums are based on the included
  <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a> included with the base installation
  and are suitable only for demo environments. For production systems you will
  want to consider your capacity and performance requirements when making
  decisions about your hardware.
 </p><p>
  The <code class="literal">entry-scale-swift</code> example runs the swift proxy,
  account and container services on the three controller servers. However, it
  is possible to extend the model to include the swift proxy, account and
  container services on dedicated servers (typically referred to as the swift
  proxy servers). If you are using this model, we have included the recommended
  swift proxy servers specs in the table below.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /><col class="c6" /><col class="c7" /></colgroup><thead><tr><th rowspan="2">Node Type</th><th rowspan="2">Role Name</th><th rowspan="2">Required Number</th><th colspan="4" align="center">Server Hardware - Minimum Requirements and
            Recommendations</th></tr><tr><th>Disk </th><th>Memory</th><th>Network</th><th>CPU </th></tr></thead><tbody><tr><td>Dedicated Cloud Lifecycle Manager (optional)</td><td>Lifecycle-manager</td><td>1</td><td>300 GB</td><td>8 GB</td><td>1 x 10 Gbit/s with PXE Support</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Control Plane</td><td>Controller</td><td>3</td><td>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         1 x 600 GB (minimum) - operating system drive
        </p></li><li class="listitem "><p>
         2 x 600 GB (minimum) - swift account/container data drive
        </p></li><li class="listitem "><p>
         Fast disks or SSDs are recommended.
        </p></li></ul></div>
     </td><td>128 GB</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>swift Object</td><td>swobj</td><td>3</td><td>
      <p>
       If using x3 replication only:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         1 x 600 GB (minimum, see considerations at bottom of page for more
         details)
        </p></li></ul></div>
      <p>
       If using Erasure Codes only or a mix of x3 replication and Erasure
       Codes:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         6 x 600 GB (minimum, see considerations at bottom of page for more
         details)
        </p></li></ul></div>
     </td><td>32 GB (see considerations at bottom of page for more details)</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>swift Proxy, Account, and Container</td><td>swpac</td><td>3</td><td>2 x 600 GB (minimum, see considerations at bottom of page for more details)</td><td>64 GB (see considerations at bottom of page for more details)</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr></tbody></table></div><div id="id-1.3.3.4.6.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   The disk speeds (RPM) chosen should be consistent within the same ring or
   storage policy. It is best to not use disks with mixed disk speeds within the
   same swift ring.
  </p></div><p>
  <span class="bold"><strong>Considerations for your swift object and proxy,
  account, container servers RAM and disk capacity needs</strong></span>
 </p><p>
  swift can have a diverse number of hardware configurations. For example, a
  swift object server may have just a few disks (minimum of 6 for erasure
  codes) or up to 70 and beyond. The memory requirement needs to be increased
  as more disks are added. The general rule of thumb for memory needed is 0.5
  GB per TB of storage. For example, a system with 24 hard drives at 8TB each,
  giving a total capacity of 192TB, should use 96GB of RAM. However, this does
  not work well for a system with a small number of small hard drives or a very
  large number of very large drives. So, if after calculating the memory given
  this guideline, if the answer is less than 32GB then go with 32GB of memory
  minimum and if the answer is over 256GB then use 256GB maximum, no need to
  use more memory than that.
 </p><p>
  When considering the capacity needs for the swift proxy, account, and
  container (PAC) servers, you should calculate 2% of the total raw storage
  size of your object servers to specify the storage required for the PAC
  servers. So, for example, if you were using the example we provided earlier
  and you had an object server setup of 24 hard drives with 8TB each for a
  total of 192TB and you had a total of 6 object servers, that would give a raw
  total of 1152TB. So you would take 2% of that, which is 23TB, and ensure that
  much storage capacity was available on your swift proxy, account, and
  container (PAC) server cluster. If you had a cluster of three swift PAC
  servers, that would be ~8TB each.
 </p><p>
  Another general rule of thumb is that if you are expecting to have more than
  a million objects in a container then you should consider using SSDs on the
  swift PAC servers rather than HDDs.
 </p></div></div><div class="chapter " id="HP3-0HA"><div class="titlepage"><div><div><h2 class="title"><span class="number">4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">High Availability</span> <a title="Permalink" class="permalink" href="#HP3-0HA">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>HP3-0HA</li></ul></div></div><div><div class="abstract"><p>
    This chapter covers High Availability concepts overview and cloud
    infrastructure.
   </p></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#concepts-overview"><span class="number">4.1 </span><span class="name">High Availability Concepts Overview</span></a></span></dt><dt><span class="section"><a href="#highly-available-cloud-infrastructure"><span class="number">4.2 </span><span class="name">Highly Available Cloud Infrastructure</span></a></span></dt><dt><span class="section"><a href="#high-availablity-controllers"><span class="number">4.3 </span><span class="name">High Availability of Controllers</span></a></span></dt><dt><span class="section"><a href="#CVR"><span class="number">4.4 </span><span class="name">High Availability Routing - Centralized</span></a></span></dt><dt><span class="section"><a href="#availability-zones"><span class="number">4.5 </span><span class="name">Availability Zones</span></a></span></dt><dt><span class="section"><a href="#compute-kvm"><span class="number">4.6 </span><span class="name">Compute with KVM</span></a></span></dt><dt><span class="section"><a href="#nova-availability-zones"><span class="number">4.7 </span><span class="name">Nova Availability Zones</span></a></span></dt><dt><span class="section"><a href="#compute-esx"><span class="number">4.8 </span><span class="name">Compute with ESX Hypervisor</span></a></span></dt><dt><span class="section"><a href="#cinder-availability-zones"><span class="number">4.9 </span><span class="name">cinder Availability Zones</span></a></span></dt><dt><span class="section"><a href="#object-storage-swift"><span class="number">4.10 </span><span class="name">Object Storage with Swift</span></a></span></dt><dt><span class="section"><a href="#highly-available-app-workloads"><span class="number">4.11 </span><span class="name">Highly Available Cloud Applications and Workloads</span></a></span></dt><dt><span class="section"><a href="#what-not-ha"><span class="number">4.12 </span><span class="name">What is not Highly Available?</span></a></span></dt><dt><span class="section"><a href="#more-information"><span class="number">4.13 </span><span class="name">More Information</span></a></span></dt></dl></div></div><div class="sect1" id="concepts-overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">High Availability Concepts Overview</span> <a title="Permalink" class="permalink" href="#concepts-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>concepts-overview</li></ul></div></div></div></div><p>
   A highly available (HA) cloud ensures that a minimum level of cloud
   resources are always available on request, which results in uninterrupted
   operations for users.
  </p><p>
   In order to achieve this high availability of infrastructure and workloads,
   we define the scope of HA to be limited to protecting these only against
   single points of failure (SPOF). Single points of failure include:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>Hardware SPOFs</strong></span>: Hardware failures can
     take the form of server failures, memory going bad, power failures,
     hypervisors crashing, hard disks dying, NIC cards breaking, switch ports
     failing, network cables loosening, and so forth.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Software SPOFs</strong></span>: Server processes can
     crash due to software defects, out-of-memory conditions, operating system
     kernel panic, and so forth.
    </p></li></ul></div><p>
   By design, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> strives to create a system architecture resilient to
   SPOFs, and does not attempt to automatically protect the system against
   multiple cascading levels of failures; such cascading failures will result
   in an unpredictable state. The cloud operator is encouraged to recover and
   restore any failed component as soon as the first level of failure occurs.
  </p></div><div class="sect1" id="highly-available-cloud-infrastructure"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Highly Available Cloud Infrastructure</span> <a title="Permalink" class="permalink" href="#highly-available-cloud-infrastructure">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>highly-available-cloud-infrastructure</li></ul></div></div></div></div><p>
   The highly available cloud infrastructure consists of the following:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     High Availability of Controllers
    </p></li><li class="listitem "><p>
     Availability Zones
    </p></li><li class="listitem "><p>
     Compute with KVM
    </p></li><li class="listitem "><p>
     nova Availability Zones
    </p></li><li class="listitem "><p>
     Compute with ESX
    </p></li><li class="listitem "><p>
     Object Storage with swift
    </p></li></ul></div></div><div class="sect1" id="high-availablity-controllers"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">High Availability of Controllers</span> <a title="Permalink" class="permalink" href="#high-availablity-controllers">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>high-availablity-controllers</li></ul></div></div></div></div><p>
   The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installer deploys highly available configurations of OpenStack
   cloud services, resilient against single points of failure.
  </p><p>
   The high availability of the controller components comes in two main forms.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Many services are stateless and multiple instances are run across the
     control plane in active-active mode. The API services (nova-api,
     cinder-api, etc.) are accessed through the HA proxy load balancer whereas
     the internal services (nova-scheduler, cinder-scheduler, etc.), are
     accessed through the message broker. These services use the database
     cluster to persist any data.
    </p><div id="id-1.3.3.5.5.4.1.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The HA proxy load balancer is also run in active-active mode and
      keepalived (used for Virtual IP (VIP) Management) is run in active-active
      mode, with only one keepalived instance holding the VIP at any one point
      in time.
     </p></div></li><li class="listitem "><p>
     The high availability of the message queue service and the database
     service is achieved by running these in a clustered mode across the three
     nodes of the control plane: RabbitMQ cluster with Mirrored Queues and
     MariaDB Galera cluster.
    </p></li></ul></div><div class="figure" id="ControlPlane1"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/media-ha30-HPE_HA_Flow.png" target="_blank"><img src="images/media-ha30-HPE_HA_Flow.png" width="" alt="HA Architecture" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 4.1: </span><span class="name">HA Architecture </span><a title="Permalink" class="permalink" href="#ControlPlane1">#</a></h6></div></div><p>
   The above diagram illustrates the HA architecture with the focus on VIP
   management and load balancing. It only shows a subset of active-active API
   instances and does not show examples of other services such as
   nova-scheduler, cinder-scheduler, etc.
  </p><p>
   In the above diagram, requests from an OpenStack client to the API services
   are sent to VIP and port combination; for example, 192.0.2.26:8774 for a
   nova request. The load balancer listens for requests on that VIP and port.
   When it receives a request, it selects one of the controller nodes
   configured for handling nova requests, in this particular case, and then
   forwards the request to the IP of the selected controller node on the same
   port.
  </p><p>
   The nova-api service, which is listening for requests on the IP of its host
   machine, then receives the request and deals with it accordingly. The
   database service is also accessed through the load balancer. RabbitMQ, on
   the other hand, is not currently accessed through VIP/HA proxy as the
   clients are configured with the set of nodes in the RabbitMQ cluster and
   failover between cluster nodes is automatically handled by the clients.
  </p></div><div class="sect1" id="CVR"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">High Availability Routing - Centralized</span> <a title="Permalink" class="permalink" href="#CVR">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>CVR</li></ul></div></div></div></div><p>
   Incorporating High Availability into a system involves implementing
   redundancies in the component that is being made highly available. In
   Centralized Virtual Router (CVR), that element is the Layer 3 agent (L3
   agent). By making L3 agent highly available, upon failure all HA routers are
   migrated from the primary L3 agent to a secondary L3 agent. The
   implementation efficiency of an HA subsystem is measured by the number of
   packets that are lost when the secondary L3 agent is made the master.
  </p><p>
   In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the primary and secondary L3 agents run continuously, and
   failover involves a rapid switchover of mastership to the secondary agent
   (IEFT RFC 5798). The failover essentially involves a switchover from an
   already running master to an already running slave. This substantially
   reduces the latency of the HA. The mechanism used by the master and the
   slave to implement a failover is implemented using Linux’s pacemaker HA
   resource manager. This CRM (Cluster resource manager) uses VRRP (Virtual
   Router Redundancy Protocol) to implement the HA mechanism. VRRP is a
   industry standard protocol and defined in RFC 5798.
  </p><div class="figure" id="Layer3HA"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/media-ha30-HPE_HA_Layer-3HA.png" target="_blank"><img src="images/media-ha30-HPE_HA_Layer-3HA.png" width="" alt="Layer-3 HA" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 4.2: </span><span class="name">Layer-3 HA </span><a title="Permalink" class="permalink" href="#Layer3HA">#</a></h6></div></div><p>
   L3 HA uses of VRRP comes with several benefits.
  </p><p>
   The primary benefit is that the failover mechanism does not involve
   interprocess communication overhead. Such overhead would be in the order of
   10s of seconds. By not using an RPC mechanism to invoke the secondary agent
   to assume the primary agents role enables VRRP to achieve failover within
   1-2 seconds.
  </p><p>
   In VRRP, the primary and secondary routers are all active. As the routers
   are running, it is a matter of making the router aware of its primary/master
   status. This switchover takes less than 2 seconds instead of 60+ seconds it
   would have taken to start a backup router and failover.
  </p><p>
   The failover depends upon a heartbeat link between the primary and
   secondary. That link in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses keepalived package of the
   pacemaker resource manager. The heartbeats are sent at a 2 second intervals
   between the primary and secondary. As per the VRRP protocol, if the
   secondary does not hear from the master after 3 intervals, it assumes the
   function of the primary.
  </p><p>
   Further, all the routable IP addresses, that is the VIPs (virtual IPs) are
   assigned to the primary agent.
  </p></div><div class="sect1" id="availability-zones"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Availability Zones</span> <a title="Permalink" class="permalink" href="#availability-zones">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>availability-zones</li></ul></div></div></div></div><div class="figure" id="DeploymentZones"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/media-ha30-HA_AvailabilityZones_3.png" target="_blank"><img src="images/media-ha30-HA_AvailabilityZones_3.png" width="" alt="Availability Zones" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 4.3: </span><span class="name">Availability Zones </span><a title="Permalink" class="permalink" href="#DeploymentZones">#</a></h6></div></div><p>
   While planning your OpenStack deployment, you should decide on how to zone
   various types of nodes - such as compute, block storage, and object storage.
   For example, you may decide to place all servers in the same rack in the
   same zone. For larger deployments, you may plan more elaborate redundancy
   schemes for redundant power, network ISP connection, and even physical
   firewalling between zones (<span class="emphasis"><em>this aspect is outside the scope of
   this document</em></span>).
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> offers APIs, CLIs and horizon UIs for the administrator to define
   and user to consume, availability zones for nova, cinder and swift services.
   This section outlines the process to deploy specific types of nodes to
   specific physical servers, and makes a statement of available support for
   these types of availability zones in the current release.
  </p><div id="id-1.3.3.5.7.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    By default, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is deployed in a single availability zone upon
    installation. Multiple availability zones can be configured by an
    administrator post-install, if required. Refer to <a class="link" href="https://docs.openstack.org/openstack-ansible/rocky/admin/maintenance-tasks.html" target="_blank">OpenStack
    Documentation</a>
   </p></div></div><div class="sect1" id="compute-kvm"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute with KVM</span> <a title="Permalink" class="permalink" href="#compute-kvm">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>compute-kvm</li></ul></div></div></div></div><p>
   You can deploy your KVM nova-compute nodes either during initial
   installation or by adding compute nodes post initial installation.
  </p><p>
   While adding compute nodes post initial installation, you can specify the
   target physical servers for deploying the compute nodes.
  </p><p>
   Learn more about adding compute nodes in
   <span class="intraxref">Book “Operations Guide CLM”, Chapter 15 “System Maintenance”, Section 15.1 “Planned System Maintenance”, Section 15.1.3 “Planned Compute Maintenance”, Section 15.1.3.4 “Adding Compute Node”</span>.
  </p></div><div class="sect1" id="nova-availability-zones"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Nova Availability Zones</span> <a title="Permalink" class="permalink" href="#nova-availability-zones">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>nova-availability-zones</li></ul></div></div></div></div><p>
   nova host aggregates and nova availability zones can be used to segregate
   nova compute nodes across different failure zones.
  </p></div><div class="sect1" id="compute-esx"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute with ESX Hypervisor</span> <a title="Permalink" class="permalink" href="#compute-esx">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>compute-esx</li></ul></div></div></div></div><p>
   Compute nodes deployed on ESX Hypervisor can be made highly available using
   the HA feature of VMware ESX Clusters. For more information on VMware
   HA, please refer to your VMware ESX documentation.
  </p></div><div class="sect1" id="cinder-availability-zones"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">cinder Availability Zones</span> <a title="Permalink" class="permalink" href="#cinder-availability-zones">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>cinder-availability-zones</li></ul></div></div></div></div><p>
   cinder availability zones are not supported for general consumption in the
   current release.
  </p></div><div class="sect1" id="object-storage-swift"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Object Storage with Swift</span> <a title="Permalink" class="permalink" href="#object-storage-swift">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>object-storage-swift</li></ul></div></div></div></div><p>
   High availability in swift is achieved at two levels.
  </p><p>
   <span class="bold"><strong>Control Plane</strong></span>
  </p><p>
   The swift API is served by multiple swift proxy nodes. Client requests are
   directed to all swift proxy nodes by the HA Proxy load balancer in
   round-robin fashion. The HA Proxy load balancer regularly checks the node is
   responding, so that if it fails, traffic is directed to the remaining nodes.
   The swift service will continue to operate and respond to client requests as
   long as at least one swift proxy server is running.
  </p><p>
   If a swift proxy node fails in the middle of a transaction, the transaction
   fails. However it is standard practice for swift clients to retry
   operations. This is transparent to applications that use the
   python-swiftclient library.
  </p><p>
   The entry-scale example cloud models contain three swift proxy nodes.
   However, it is possible to add additional clusters with additional swift
   proxy nodes to handle a larger workload or to provide additional resiliency.
  </p><p>
   <span class="bold"><strong>Data</strong></span>
  </p><p>
   Multiple replicas of all data is stored. This happens for account, container
   and object data. The example cloud models recommend a replica count of
   three. However, you may change this to a higher value if needed.
  </p><p>
   When swift stores different replicas of the same item on disk, it ensures
   that as far as possible, each replica is stored in a different zone, server
   or drive. This means that if a single server of disk drives fails, there
   should be two copies of the item on other servers or disk drives.
  </p><p>
   If a disk drive is failed, swift will continue to store three replicas. The
   replicas that would normally be stored on the failed drive are “handed
   off” to another drive on the system. When the failed drive is replaced,
   the data on that drive is reconstructed by the replication process. The
   replication process re-creates the <span class="quote">“<span class="quote ">missing</span>”</span> replicas by
   copying them to the drive using one of the other remaining replicas. While
   this is happening, swift can continue to store and retrieve data.
  </p></div><div class="sect1" id="highly-available-app-workloads"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Highly Available Cloud Applications and Workloads</span> <a title="Permalink" class="permalink" href="#highly-available-app-workloads">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>highly-available-app-workloads</li></ul></div></div></div></div><p>
   Projects writing applications to be deployed in the cloud must be aware of
   the cloud architecture and potential points of failure and architect their
   applications accordingly for high availability.
  </p><p>
   Some guidelines for consideration:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Assume intermittent failures and plan for retries
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>OpenStack Service APIs</strong></span>: invocations can
       fail - you should carefully evaluate the response of each invocation,
       and retry in case of failures.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Compute</strong></span>: VMs can die - monitor and
       restart them
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Network</strong></span>: Network calls can fail - retry
       should be successful
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Storage</strong></span>: Storage connection can hiccup
       - retry should be successful
      </p></li></ul></div></li><li class="listitem "><p>
     Build redundancy into your application tiers
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Replicate VMs containing stateless services such as Web application tier
       or Web service API tier and put them behind load balancers. You must
       implement your own HA Proxy type load balancer in your application VMs.
      </p></li><li class="listitem "><p>
       Boot the replicated VMs into different nova availability zones.
      </p></li><li class="listitem "><p>
       If your VM stores state information on its local disk (Ephemeral
       Storage), and you cannot afford to lose it, then boot the VM off a
       cinder volume.
      </p></li><li class="listitem "><p>
       Take periodic snapshots of the VM which will back it up to swift through
       glance.
      </p></li><li class="listitem "><p>
       Your data on ephemeral may get corrupted (but not your backup data in
       swift and not your data on cinder volumes).
      </p></li><li class="listitem "><p>
       Take regular snapshots of cinder volumes and also back up cinder volumes
       or your data exports into swift.
      </p></li></ul></div></li><li class="listitem "><p>
     Instead of rolling your own highly available stateful services, use
     readily available <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> platform services such as designate, the DNS
     service.
    </p></li></ol></div></div><div class="sect1" id="what-not-ha"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">What is not Highly Available?</span> <a title="Permalink" class="permalink" href="#what-not-ha">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>what-not-ha</li></ul></div></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.3.5.14.2.1"><span class="term ">Cloud Lifecycle Manager</span></dt><dd><p>
      The Cloud Lifecycle Manager in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is not highly available.
     </p></dd><dt id="id-1.3.3.5.14.2.2"><span class="term ">Control Plane</span></dt><dd><p>
      High availability (HA) is supported for the Network Service FWaaS. HA is
      <span class="bold"><strong>not</strong></span> supported for VPNaaS.
     </p></dd><dt id="id-1.3.3.5.14.2.3"><span class="term ">cinder Volume and Backup Services</span></dt><dd><p>
      cinder Volume and Backup Services are not high availability and started
      on one controller node at a time.
      More information on cinder Volume and Backup Services can
      be found in <span class="intraxref">Book “Operations Guide CLM”, Chapter 8 “Managing Block Storage”, Section 8.1 “Managing Block Storage using Cinder”, Section 8.1.3 “Managing cinder Volume and Backup Services”</span>.
     </p></dd><dt id="id-1.3.3.5.14.2.4"><span class="term ">keystone Cron Jobs</span></dt><dd><p>
      The keystone cron job is a singleton service, which can only run on a
      single node at a time. A manual setup process for this job will be
      required in case of a node failure.
      More information on enabling the cron job for keystone on
      the other nodes can be found in <span class="intraxref">Book “Operations Guide CLM”, Chapter 5 “Managing Identity”, Section 5.12 “Identity Service Notes and Limitations”, Section 5.12.4 “System cron jobs need setup”</span>.
     </p></dd></dl></div></div><div class="sect1" id="more-information"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">More Information</span> <a title="Permalink" class="permalink" href="#more-information">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-high_availability.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-high_availability.xml</li><li><span class="ds-label">ID: </span>more-information</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <a class="link" href="https://docs.openstack.org/ha-guide/" target="_blank">OpenStack
     High-availability Guide</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="http://12factor.net/" target="_blank">12-Factor Apps</a>
    </p></li></ul></div></div></div></div><div class="part" id="architecture"><div class="titlepage"><div><div><h1 class="title"><span class="number">Part II </span><span class="name">Cloud Lifecycle Manager Overview </span><a title="Permalink" class="permalink" href="#architecture">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-architecture_index.xml" title="Edit the source file for this section">Edit source</a></h1></div><div><div class="abstract"><p>
    This section contains information on the Input Model and the Example
    Configurations.
   </p></div></div></div></div><div class="toc"><dl><dt><span class="chapter"><a href="#cha-input-model-intro-concept"><span class="number">5 </span><span class="name">Input Model</span></a></span></dt><dd class="toc-abstract"><p>
  This document describes how <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> input models can be used to define and
  configure the cloud.
 </p></dd><dt><span class="chapter"><a href="#configurationobjects"><span class="number">6 </span><span class="name">Configuration Objects</span></a></span></dt><dd class="toc-abstract"><p>
  The top-level cloud configuration file, <code class="filename">cloudConfig.yml</code>,
  defines some global values for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, as described in the table below.
 </p></dd><dt><span class="chapter"><a href="#othertopics"><span class="number">7 </span><span class="name">Other Topics</span></a></span></dt><dd class="toc-abstract"><p>Names are generated by the configuration processor for all allocated IP addresses. A server connected to multiple networks will have multiple names associated with it. One of these may be assigned as the hostname for a server via the network-group configuration (see Section 6.12, “NIC Mappings”). Na…</p></dd><dt><span class="chapter"><a href="#cpinfofiles"><span class="number">8 </span><span class="name">Configuration Processor Information Files</span></a></span></dt><dd class="toc-abstract"><p>
  In addition to producing all of the data needed to deploy and configure the
  cloud, the configuration processor also creates a number of information files
  that provide details of the resulting configuration.
 </p></dd><dt><span class="chapter"><a href="#example-configurations"><span class="number">9 </span><span class="name">Example Configurations</span></a></span></dt><dd class="toc-abstract"><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 system ships with a collection of pre-qualified example
  configurations. These are designed to help you to get up and running quickly
  with a minimum number of configuration changes.
 </p></dd><dt><span class="chapter"><a href="#modify-compute-input-model"><span class="number">10 </span><span class="name">Modifying Example Configurations for Compute Nodes</span></a></span></dt><dd class="toc-abstract"><p>
  This section contains detailed information about the Compute Node parts of
  the input model. For example input models, see <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a>. For general descriptions of the input
  model, see <a class="xref" href="#configobj-networks" title="6.14. Networks">Section 6.14, “Networks”</a>.
 </p></dd><dt><span class="chapter"><a href="#modify-input-model"><span class="number">11 </span><span class="name">Modifying Example Configurations for Object Storage using Swift</span></a></span></dt><dd class="toc-abstract"><p>This section contains detailed descriptions about the swift-specific parts of the input model. For example input models, see Chapter 9, Example Configurations. For general descriptions of the input model, see Section 6.14, “Networks”. In addition, the swift ring specifications are available in the ~…</p></dd><dt><span class="chapter"><a href="#alternative-configurations"><span class="number">12 </span><span class="name">Alternative Configurations</span></a></span></dt><dd class="toc-abstract"><p>
    In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 there are alternative configurations that we recommend
    for specific purposes.
   </p></dd></dl></div><div class="chapter " id="cha-input-model-intro-concept"><div class="titlepage"><div><div><h2 class="title"><span class="number">5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Input Model</span> <a title="Permalink" class="permalink" href="#cha-input-model-intro-concept">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-input_model.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-input_model.xml</li><li><span class="ds-label">ID: </span>cha-input-model-intro-concept</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#input-model-introduction"><span class="number">5.1 </span><span class="name">Introduction to the Input Model</span></a></span></dt><dt><span class="section"><a href="#concepts"><span class="number">5.2 </span><span class="name">Concepts</span></a></span></dt></dl></div></div><div class="sect1" id="input-model-introduction"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Introduction to the Input Model</span> <a title="Permalink" class="permalink" href="#input-model-introduction">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-input_model_introduction.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-input_model_introduction.xml</li><li><span class="ds-label">ID: </span>input-model-introduction</li></ul></div></div></div></div><p>
  This document describes how <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> input models can be used to define and
  configure the cloud.
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> ships with a set of example input models that can be used as
  starting points for defining a custom cloud. An input model allows you, the
  cloud administrator, to describe the cloud configuration in terms of:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Which OpenStack services run on which server nodes
   </p></li><li class="listitem "><p>
    How individual servers are configured in terms of disk and network adapters
   </p></li><li class="listitem "><p>
    The overall network configuration of the cloud
   </p></li><li class="listitem "><p>
    Network traffic separation
   </p></li><li class="listitem "><p>
    CIDR and VLAN assignments
   </p></li></ul></div><p>
  The input model is consumed by the configuration processor which parses and
  validates the input model and outputs the effective configuration that will
  be deployed to each server that makes up your cloud.
 </p><p>
  The document is structured as follows:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <span class="guimenu ">Concepts</span> - This explains the ideas behind the
    declarative model approach used in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 and the core concepts
    used in describing that model
   </p></li><li class="listitem "><p>
    <span class="guimenu ">Input Model</span> - This section provides a description of
    each of the configuration entities in the input model
   </p></li><li class="listitem "><p>
    <span class="guimenu ">Core Examples</span> - In this section we provide samples and
    definitions of some of the more important configuration entities
   </p></li></ul></div></div><div class="sect1" id="concepts"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Concepts</span> <a title="Permalink" class="permalink" href="#concepts">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-concepts-concepts.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-concepts.xml</li><li><span class="ds-label">ID: </span>concepts</li></ul></div></div></div></div><p>
  An <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 cloud is defined by a declarative model that is described
  in a series of configuration objects. These configuration objects are
  represented in YAML files which together constitute the various example
  configurations provided as templates with this release. These examples can be
  used nearly unchanged, with the exception of necessary changes to IP
  addresses and other site and hardware-specific identifiers. Alternatively,
  the examples may be customized to meet site requirements.
 </p><p>
  The following diagram shows the set of configuration objects and their
  relationships. All objects have a name that you may set to be something
  meaningful for your context. In the examples these names are provided in
  capital letters as a convention. These names have no significance to
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, rather it is the relationships between them that define the
  configuration.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-inputmodel-HPE_InputModel_Flow_40.png" target="_blank"><img src="images/media-inputmodel-HPE_InputModel_Flow_40.png" width="" /></a></div></div><p>
  The configuration processor reads and validates the input model described in
  the YAML files discussed above, combines it with the service definitions
  provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> and any persisted state information about the current
  deployment to produce a set of Ansible variables that can be used to deploy
  the cloud. It also produces a set of information files that provide details
  about the configuration.
 </p><p>
  The relationship between the file systems on the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> deployment server
  and the configuration processor is shown in the following diagram. Below the
  line are the directories that you, the cloud administrator, edit to declare
  the cloud configuration. Above the line are the directories that are
  internal to the Cloud Lifecycle Manager such as Ansible playbooks and variables.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-inputmodel-hphelionopenstack_directories.png" target="_blank"><img src="images/media-inputmodel-hphelionopenstack_directories.png" width="" /></a></div></div><p>
  The input model is read from the
  <code class="filename">~/openstack/my_cloud/definition</code> directory. Although the
  supplied examples use separate files for each type of object in the model,
  the names and layout of the files have no significance to the configuration
  processor, it simply reads all of the .yml files in this directory. Cloud
  administrators are therefore free to use whatever structure is best for their
  context. For example, you may decide to maintain separate files or
  sub-directories for each physical rack of servers.
 </p><p>
  As mentioned, the examples use the conventional upper casing for object
  names, but these strings are used only to define the relationship between
  objects. They have no specific significance to the configuration processor.
 </p><div class="sect2" id="concept-cloud"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cloud</span> <a title="Permalink" class="permalink" href="#concept-cloud">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-concepts-cloud.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-cloud.xml</li><li><span class="ds-label">ID: </span>concept-cloud</li></ul></div></div></div></div><p>
  The Cloud definition includes a few top-level configuration values such as
  the name of the cloud, the host prefix, details of external services (NTP,
  DNS, SMTP) and the firewall settings.
 </p><p>
  The location of the cloud configuration file also tells the configuration
  processor where to look for the files that define all of the other objects in
  the input model.
 </p></div><div class="sect2" id="concept-controlplanes"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Control Planes</span> <a title="Permalink" class="permalink" href="#concept-controlplanes">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-concepts-controlplanes.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-controlplanes.xml</li><li><span class="ds-label">ID: </span>concept-controlplanes</li></ul></div></div></div></div><p>
  <span class="emphasis"><em>A control-plane runs one or more <span class="guimenu ">services</span>
  distributed across <span class="guimenu ">clusters</span> and <span class="guimenu ">resource
  groups</span></em></span>.
 </p><p>
  <span class="emphasis"><em>A control-plane uses servers with a particular
  <span class="guimenu ">server-role</span></em></span>.
 </p><p>
  A <span class="guimenu ">control-plane</span> provides the operating environment for a
  set of <span class="guimenu ">services</span>; normally consisting of a set of shared
  services (MariaDB, RabbitMQ, HA Proxy, Apache, etc.), OpenStack control
  services (API, schedulers, etc.) and the <span class="guimenu ">resources</span> they
  are managing (compute, storage, etc.).
 </p><p>
  A simple cloud may have a single <span class="guimenu ">control-plane</span> which runs
  all of the <span class="guimenu ">services</span>. A more complex cloud may have
  multiple <span class="guimenu ">control-planes</span> to allow for more than one
  instance of some services.
  
   Services that need to consume (use) another service
  (such as neutron consuming MariaDB, nova consuming neutron) always use the
  service within the same <span class="guimenu ">control-plane</span>.
  In addition a control-plane can describe which services can be consumed
  from other control-planes. It is one of the functions of the configuration
  processor to resolve these relationships and make sure that each
  consumer/service is provided with the configuration details to connect to the
  appropriate provider/service.
 </p><p>
  Each <span class="guimenu ">control-plane</span> is structured as
  <span class="guimenu ">clusters</span> and <span class="guimenu ">resources</span>. The
  <span class="guimenu ">clusters</span> are typically used to host the OpenStack services
  that manage the cloud such as API servers, database servers, neutron agents,
  and swift proxies, while the <span class="guimenu ">resources</span> are used to host
  the scale-out OpenStack services such as nova-Compute or swift-Object
  services. This is a representation convenience rather than a strict rule, for
  example it is possible to run the swift-Object service in the management
  cluster in a smaller-scale cloud that is not designed for scale-out object
  serving.
 </p><p>
  A cluster can contain one or more <span class="guimenu ">servers</span> and you can have
  one or more <span class="guimenu ">clusters</span> depending on the capacity and
  scalability needs of the cloud that you are building. Spreading services
  across multiple <span class="guimenu ">clusters</span> provides greater scalability, but
  it requires a greater number of physical servers. A common pattern for a
  large cloud is to run high data volume services such as monitoring and
  logging in a separate cluster. A cloud with a high object storage requirement
  will typically also run the swift service in its own cluster.
 </p><p>
  Clusters in this context are a mechanism for grouping service components in
  physical servers, but all instances of a component in a
  <span class="guimenu ">control-plane</span> work collectively. For example, if HA Proxy
  is configured to run on multiple clusters within the same
  <span class="guimenu ">control-plane</span> then all of those instances will work as a
  single instance of the ha-proxy service.
 </p><p>
  Both <span class="guimenu ">clusters</span> and <span class="guimenu ">resources</span> define the
  type (via a list of <span class="guimenu ">server-roles</span>) and number of servers
  (min and max or count) they require.
 </p><p>
  The <span class="guimenu ">control-plane</span> can also define a list of failure-zones
  (<span class="guimenu ">server-groups</span>) from which to allocate servers.
 </p><div class="sect3" id="concept-controlplanes-regions"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Control Planes and Regions</span> <a title="Permalink" class="permalink" href="#concept-controlplanes-regions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-concepts-controlplanes_regions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-controlplanes_regions.xml</li><li><span class="ds-label">ID: </span>concept-controlplanes-regions</li></ul></div></div></div></div><p>
  A region in OpenStack terms is a collection of URLs that together provide a
  consistent set of services (nova, neutron, swift, etc). Regions are
  represented in the keystone identity service catalog. In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>,
  multiple regions are not supported. Only <code class="literal">Region0</code> is valid.
 </p><p>
  In a simple single control-plane cloud, there is no need for a separate
  region definition and the control-plane itself can define the region name.
 </p></div></div><div class="sect2" id="concept-services"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Services</span> <a title="Permalink" class="permalink" href="#concept-services">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-concepts-services.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-services.xml</li><li><span class="ds-label">ID: </span>concept-services</li></ul></div></div></div></div><p>
  <span class="emphasis"><em>A <span class="guimenu ">control-plane</span> runs one or more
  <span class="guimenu ">services</span>.</em></span>
 </p><p>
  A service is the collection of <span class="guimenu ">service-components</span> that
  provide a particular feature; for example, nova provides the compute service
  and consists of the following service-components: nova-api, nova-scheduler,
  nova-conductor, nova-novncproxy, and nova-compute. Some services, like the
  authentication/identity service keystone, only consist of a single
  service-component.
 </p><p>
  To define your cloud, all you need to know about a service are the names of
  the <span class="guimenu ">service-components</span>. The details of the services
  themselves and how they interact with each other is captured in service
  definition files provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p><p>
  When specifying your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud you have to decide where components will
  run and how they connect to the networks. For example, should they all run in
  one <span class="guimenu ">control-plane</span> sharing common services or be
  distributed across multiple <span class="guimenu ">control-planes</span> to provide
  separate instances of some services? The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> supplied examples provide
  solutions for some typical configurations.
 </p><p>
  Where services run is defined in the <span class="guimenu ">control-plane</span>. How
  they connect to networks is defined in the <span class="guimenu ">network-groups</span>.
 </p></div><div class="sect2" id="concept-serverroles"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Server Roles</span> <a title="Permalink" class="permalink" href="#concept-serverroles">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-concepts-serverroles.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-serverroles.xml</li><li><span class="ds-label">ID: </span>concept-serverroles</li></ul></div></div></div></div><p>
  <span class="emphasis"><em><span class="guimenu ">Clusters</span> and <span class="guimenu ">resources</span> use
  <span class="guimenu ">servers</span> with a particular set of
  <span class="guimenu ">server-role</span>s.</em></span>
 </p><p>
  You are going to be running the services on physical
  <span class="guimenu ">servers</span>, and you are going to need a way to specify
  which type of servers you want to use where. This is defined via the
  <span class="guimenu ">server-role</span>. Each <span class="guimenu ">server-role</span> describes
  how to configure the physical aspects of a server to fulfill the needs of a
  particular role. You will generally use a different role whenever the servers
  are physically different (have different disks or network interfaces) or if
  you want to use some specific servers in a particular role (for example to
  choose which of a set of identical servers are to be used in the control
  plane).
 </p><p>
  Each <span class="guimenu ">server-role</span> has a relationship to four other
  entities:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    The <span class="guimenu ">disk-model</span> specifies how to configure and use a
    server's local storage and it specifies disk sizing information for
    virtual machine servers. The disk model is described in the next section.
   </p></li><li class="listitem "><p>
    The <span class="guimenu ">interface-model</span> describes how a server's network
    interfaces are to be configured and used. This is covered in more details
    in the networking section.
   </p></li><li class="listitem "><p>
    An optional <span class="guimenu ">memory-model</span> specifies how to configure
    and use huge pages. The memory-model specifies memory sizing information
    for virtual machine servers.
   </p></li><li class="listitem "><p>
    An optional <span class="bold"><strong> <span class="guimenu ">cpu-model</span> </strong></span>
    specifies how the CPUs will be used by nova and by DPDK.
    The cpu-model specifies CPU sizing information for virtual machine
    servers.
   </p></li></ul></div></div><div class="sect2" id="concept-diskmodel"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Disk Model</span> <a title="Permalink" class="permalink" href="#concept-diskmodel">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-concepts-diskmodel.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-diskmodel.xml</li><li><span class="ds-label">ID: </span>concept-diskmodel</li></ul></div></div></div></div><p>
  <span class="emphasis"><em>Each physical disk device is associated with a
  <span class="guimenu ">device-group</span> or a
  <span class="guimenu ">volume-group</span>.</em></span>
 </p><p>
  <span class="emphasis"><em><span class="guimenu ">Device-groups</span> are consumed by
  <span class="guimenu ">services</span>.</em></span>
 </p><p>
  <span class="emphasis"><em><span class="guimenu ">Volume-groups</span> are divided into
  <span class="guimenu ">logical-volumes</span>.</em></span>
 </p><p>
  <span class="emphasis"><em><span class="guimenu ">Logical-volumes</span> are mounted as file systems or
  consumed by services.</em></span>
 </p><p>
  Disk-models define how local storage is to be configured and presented to
  <span class="guimenu ">services</span>. Disk-models are identified by a name, which you
  will specify. The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> examples provide some typical configurations. As
  this is an area that varies with respect to the services that are hosted on a
  server and the number of disks available, it is impossible to cover all
  possible permutations you may need to express via modifications to the
  examples.
 </p><p>
  Within a <span class="guimenu ">disk-model</span>, disk devices are assigned to either a
  <span class="guimenu ">device-group</span> or a <span class="guimenu ">volume-group</span>.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-inputmodel-hphelionopenstack_diskmodels.png" target="_blank"><img src="images/media-inputmodel-hphelionopenstack_diskmodels.png" width="" /></a></div></div><p>
  A <span class="guimenu ">device-group</span> is a set of one or more disks that are to
  be consumed directly by a service. For example, a set of disks to be used by
  swift. The device-group identifies the list of disk devices, the service, and
  a few service-specific attributes that tell the service about the intended
  use (for example, in the case of swift this is the ring names). When a device
  is assigned to a device-group, the associated service is responsible for the
  management of the disks. This management includes the creation and mounting
  of file systems. (swift can provide additional data integrity when it has
  full control over the file systems and mount points.)
 </p><p>
  A <span class="guimenu ">volume-group</span> is used to present disk devices in a LVM
  volume group. It also contains details of the logical volumes to be created
  including the file system type and mount point. Logical volume sizes are
  expressed as a percentage of the total capacity of the volume group. A
  <span class="guimenu ">logical-volume</span> can also be consumed by a service in the
  same way as a <span class="guimenu ">device-group</span>. This allows services to manage
  their own devices on configurations that have limited numbers of disk drives.
 </p><p>
  Disk models also provide disk sizing information for virtual machine servers.
 </p></div><div class="sect2" id="concept-memorymodel"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Memory Model</span> <a title="Permalink" class="permalink" href="#concept-memorymodel">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-concepts-memorymodel.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-memorymodel.xml</li><li><span class="ds-label">ID: </span>concept-memorymodel</li></ul></div></div></div></div><p>
  Memory models define how the memory of a server should be configured to meet
  the needs of a particular role. It allows a number of HugePages to be defined
  at both the server and numa-node level.
 </p><p>
  Memory models also provide memory sizing information for virtual machine
  servers.
 </p><p>
  Memory models are optional - it is valid to have a server role without a
  memory model.
 </p></div><div class="sect2" id="concept-cpumodel"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">CPU Model</span> <a title="Permalink" class="permalink" href="#concept-cpumodel">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-concepts-cpumodel.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-cpumodel.xml</li><li><span class="ds-label">ID: </span>concept-cpumodel</li></ul></div></div></div></div><p>
  CPU models define how CPUs of a server will be used. The model allows CPUs to
  be assigned for use by components such as nova (for VMs) and Open vSwitch
  (for DPDK). It also allows those CPUs to be isolated from the general kernel
  SMP balancing and scheduling algorithms.
 </p><p>
  CPU models also provide CPU sizing information for virtual machine servers.
 </p><p>
  CPU models are optional - it is valid to have a server role without a cpu
  model.
 </p></div><div class="sect2" id="concept-servers"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Servers</span> <a title="Permalink" class="permalink" href="#concept-servers">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-concepts-servers.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-servers.xml</li><li><span class="ds-label">ID: </span>concept-servers</li></ul></div></div></div></div><p>
  <span class="emphasis"><em><span class="guimenu ">Servers</span> have a <span class="guimenu ">server-role</span>
  which determines how they will be used in the cloud.</em></span>
 </p><p>
  <span class="guimenu ">Servers</span> (in the input model) enumerate the resources
  available for your cloud. In addition, in this definition file you can either
  provide <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> with all of the details it needs to PXE boot and install an
  operating system onto the server, or, if you prefer to use your own operating
  system installation tooling you can simply provide the details needed to be
  able to SSH into the servers and start the deployment.
 </p><p>
  The address specified for the server will be the one used by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> for
  lifecycle management and must be part of a network which is in the input
  model. If you are using <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to install the operating system this network
  must be an untagged VLAN. The first server must be installed manually from
  the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> ISO and this server must be included in the input model as well.
 </p><p>
  In addition to the network details used to install or connect to the server,
  each server defines what its <span class="guimenu ">server-role</span> is and to which
  <span class="guimenu ">server-group</span> it belongs.
 </p></div><div class="sect2" id="concept-servergroups"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Server Groups</span> <a title="Permalink" class="permalink" href="#concept-servergroups">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-concepts-servergroups.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-servergroups.xml</li><li><span class="ds-label">ID: </span>concept-servergroups</li></ul></div></div></div></div><p>
  <span class="emphasis"><em>A <span class="guimenu ">server</span> is associated with a
  <span class="guimenu ">server-group</span>.</em></span>
 </p><p>
  <span class="emphasis"><em>A <span class="guimenu ">control-plane</span> can use
  <span class="guimenu ">server-groups</span> as failure zones for server
  allocation.</em></span>
 </p><p>
  <span class="emphasis"><em>A <span class="guimenu ">server-group</span> may be associated with a list of
  <span class="guimenu ">networks</span>.</em></span>
 </p><p>
  <span class="emphasis"><em>A <span class="guimenu ">server-group</span> can contain other
  <span class="guimenu ">server-groups</span>.</em></span>
 </p><p>
  The practice of locating physical servers in a number of racks or enclosures
  in a data center is common. Such racks generally provide a degree of physical
  isolation that allows for separate power and/or network connectivity.
 </p><p>
  In the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> model we support this configuration by allowing you to define
  a hierarchy of <span class="guimenu ">server-groups</span>. Each
  <span class="guimenu ">server</span> is associated with one
  <span class="guimenu ">server-group</span>, normally at the bottom of the hierarchy.
 </p><p>
  <span class="guimenu ">Server-groups</span> are an optional part of the input model - if
  you do not define any, then all <span class="guimenu ">servers</span> and
  <span class="guimenu ">networks</span> will be allocated as if they are part of the same
  <span class="guimenu ">server-group</span>.
 </p><div class="sect3" id="concept-servergroups-failurezones"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.2.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Server Groups and Failure Zones</span> <a title="Permalink" class="permalink" href="#concept-servergroups-failurezones">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-concepts-servergroups_failurezones.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-servergroups_failurezones.xml</li><li><span class="ds-label">ID: </span>concept-servergroups-failurezones</li></ul></div></div></div></div><p>
  A <span class="guimenu ">control-plane</span> defines a list of
  <span class="guimenu ">server-groups</span> as the failure zones from which it wants to
  use servers. All servers in a <span class="guimenu ">server-group</span> listed as a
  failure zone in the <span class="guimenu ">control-plane</span> and any
  <span class="guimenu ">server-groups</span> they contain are considered part of that
  failure zone for allocation purposes. The following example shows how three
  levels of <span class="guimenu ">server-groups</span> can be used to model a failure
  zone consisting of multiple racks, each of which in turn contains a number of
  <span class="guimenu ">servers</span>.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-inputmodel-hphelionopenstack_servergroups.png" target="_blank"><img src="images/media-inputmodel-hphelionopenstack_servergroups.png" width="" /></a></div></div><p>
  When allocating <span class="guimenu ">servers</span>, the configuration processor will
  traverse down the hierarchy of <span class="guimenu ">server-groups</span> listed as
  failure zones until it can find an available server with the required
  <span class="guimenu ">server-role</span>. If the allocation policy is defined to be
  strict, it will allocate <span class="guimenu ">servers</span> equally across each of
  the failure zones. A <span class="guimenu ">cluster</span> or
  <span class="guimenu ">resource-group</span> can also independently specify the failure
  zones it wants to use if needed.
 </p></div><div class="sect3" id="concept-servergroups-networks"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.2.9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Server Groups and Networks</span> <a title="Permalink" class="permalink" href="#concept-servergroups-networks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-concepts-servergroups_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-servergroups_networks.xml</li><li><span class="ds-label">ID: </span>concept-servergroups-networks</li></ul></div></div></div></div><p>
  Each L3 <span class="guimenu ">network</span> in a cloud must be associated with all or
  some of the <span class="guimenu ">servers</span>, typically following a physical
  pattern (such as having separate networks for each rack or set of racks).
  This is also represented in the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> model via
  <span class="guimenu ">server-groups</span>, each group lists zero or more networks to
  which <span class="guimenu ">servers</span> associated with
  <span class="guimenu ">server-groups</span> at or below this point in the hierarchy are
  connected.
 </p><p>
  When the configuration processor needs to resolve the specific
  <span class="guimenu ">network</span> a <span class="guimenu ">server</span> should be configured
  to use, it traverses up the hierarchy of <span class="guimenu ">server-groups</span>,
  starting with the group the server is directly associated with, until it
  finds a server-group that lists a network in the required network group.
 </p><p>
  The level in the <span class="guimenu ">server-group</span> hierarchy at which a
  <span class="guimenu ">network</span> is associated will depend on the span of
  connectivity it must provide. In the above example there might be networks in
  some <span class="guimenu ">network-groups</span> which are per rack (that is Rack 1 and
  Rack 2 list different networks from the same
  <span class="guimenu ">network-group</span>) and <span class="guimenu ">networks</span> in a
  different <span class="guimenu ">network-group</span> that span failure zones (the
  network used to provide floating IP addresses to virtual machines for
  example).
 </p></div></div><div class="sect2" id="concept-networking"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networking</span> <a title="Permalink" class="permalink" href="#concept-networking">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-concepts-networking.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-networking.xml</li><li><span class="ds-label">ID: </span>concept-networking</li></ul></div></div></div></div><p>
  In addition to the mapping of <span class="guimenu ">services</span> to specific
  <span class="guimenu ">clusters</span> and <span class="guimenu ">resources</span> we must also be
  able to define how the <span class="guimenu ">services</span> connect to one or more
  <span class="guimenu ">networks</span>.
 </p><p>
  In a simple cloud there may be a single L3 network but more typically there
  are functional and physical layers of network separation that need to be
  expressed.
 </p><p>
  Functional network separation provides different networks for different types
  of traffic; for example, it is common practice in even small clouds to
  separate the External APIs that users will use to access the cloud and the
  external IP addresses that users will use to access their virtual machines.
  In more complex clouds it is common to also separate out virtual networking
  between virtual machines, block storage traffic, and volume traffic onto
  their own sets of networks. In the input model, this level of separation is
  represented by <span class="guimenu ">network-groups</span>.
 </p><p>
  Physical separation is required when there are separate L3 network segments
  providing the same type of traffic; for example, where each rack uses a
  different subnet. This level of separation is represented in the input model
  by the <span class="guimenu ">networks</span> within each
  <span class="guimenu ">network-group</span>.
 </p><div class="sect3" id="concept-networkgroups"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.2.10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Groups</span> <a title="Permalink" class="permalink" href="#concept-networkgroups">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-concepts-networkgroups.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-networkgroups.xml</li><li><span class="ds-label">ID: </span>concept-networkgroups</li></ul></div></div></div></div><p>
  <span class="emphasis"><em>Service endpoints attach to <span class="guimenu ">networks</span> in a
  specific <span class="guimenu ">network-group</span>.</em></span>
 </p><p>
  <span class="emphasis"><em><span class="guimenu ">Network-groups</span> can define routes to other
  <span class="guimenu ">networks</span>.</em></span>
 </p><p>
  <span class="emphasis"><em><span class="guimenu ">Network-groups</span> encapsulate the configuration for
  <span class="guimenu ">services</span> via <span class="guimenu ">network-tags</span></em></span>
 </p><p>
  A <span class="guimenu ">network-group</span> defines the traffic separation model and
  all of the properties that are common to the set of L3 networks that carry
  each type of traffic. They define where services are attached to the network
  model and the routing within that model.
 </p><p>
  In terms of <span class="guimenu ">service</span> connectivity, all that has to be
  captured in the <span class="guimenu ">network-groups</span> definition are the same
  service-component names that are used when defining
  <span class="guimenu ">control-planes</span>. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> also allows a default attachment
  to be used to specify "all service-components" that are not explicitly
  connected to another <span class="guimenu ">network-group</span>. So, for example, to
  isolate swift traffic, the swift-account, swift-container, and swift-object
  service components are attached to an "Object"
  <span class="guimenu ">network-group</span> and all other services are connected to
  "MANAGEMENT" <span class="guimenu ">network-group</span> via the default relationship.
 </p><div id="id-1.3.4.3.3.19.6.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   The name of the "MANAGEMENT" <span class="guimenu ">network-group</span> cannot be
   changed. It must be upper case. Every SUSE <span class="productname">OpenStack</span> Cloud requires this network group in
   order to be valid.
  </p></div><p>
  The details of how each service connects, such as what port it uses, if it
  should be behind a load balancer, if and how it should be registered in
  keystone, and so forth, are defined in the service definition files provided
  by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p><p>
  In any configuration with multiple networks, controlling the routing is a
  major consideration. In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, routing is controlled at the
  <span class="guimenu ">network-group</span> level. First, all
  <span class="guimenu ">networks</span> are configured to provide the route to any other
  <span class="guimenu ">networks</span> in the same <span class="guimenu ">network-group</span>. In
  addition, a <span class="guimenu ">network-group</span> may be configured to provide the
  route any other <span class="guimenu ">networks</span> in the same
  <span class="guimenu ">network-group</span>; for example, if the internal APIs are in a
  dedicated <span class="guimenu ">network-group</span> (a common configuration in a
  complex network because a network group with load balancers cannot be
  segmented) then other <span class="guimenu ">network-groups</span> may need to include a
  route to the internal API <span class="guimenu ">network-group</span> so that services
  can access the internal API endpoints. Routes may also be required to define
  how to access an external storage network or to define a general default
  route.
 </p><p>
  As part of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> deployment, networks are configured to act as the
  default route for all traffic that was received via that network (so that
  response packets always return via the network the request came from).
 </p><p>
  Note that <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> will configure the routing rules on the servers it deploys
  and will validate that the routes between services exist in the model, but
  ensuring that gateways can provide the required routes is the responsibility
  of your network configuration. The configuration processor provides
  information about the routes it is expecting to be configured.
 </p><p>
  For a detailed description of how the configuration processor validates
  routes, refer to <a class="xref" href="#networkroutevalidation" title="7.6. Network Route Validation">Section 7.6, “Network Route Validation”</a>.
 </p><div class="sect4" id="concept-loadbalancers"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.2.10.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Load Balancers</span> <a title="Permalink" class="permalink" href="#concept-loadbalancers">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-concepts-loadbalancers.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-loadbalancers.xml</li><li><span class="ds-label">ID: </span>concept-loadbalancers</li></ul></div></div></div></div><p>
  <span class="guimenu ">Load-balancers</span> provide a specific type of routing and are
  defined

  as a relationship between the virtual IP address (VIP) on a network in one
  <span class="guimenu ">network group</span> and a set of service endpoints (which may be
  on <span class="guimenu ">networks</span> in the same or a different
  <span class="guimenu ">network-group</span>).
 </p><p>
  As each <span class="guimenu ">load-balancer</span> is defined providing a virtual IP on
  a <span class="guimenu ">network-group</span>, it follows that those
  <span class="guimenu ">network-group</span>s
  
  can each only have one <span class="guimenu ">network</span> associated to them.
 </p><p>
  The <span class="guimenu ">load-balancer</span> definition includes a list of
  <span class="guimenu ">service-components</span> and endpoint roles it will provide a
  virtual IP for. This model allows service-specific
  <span class="guimenu ">load-balancers</span> to be defined on different
  <span class="guimenu ">network-groups</span>. A "default" value is used to express "all
  service-components" which require a virtual IP address and are not explicitly
  configured in another <span class="guimenu ">load-balancer</span> configuration. The
  details of how the <span class="guimenu ">load-balancer</span> should be configured for
  each service, such as which ports to use, how to check for service liveness,
  etc., are provided in the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> supplied service definition files.
 </p><p>
  Where there are multiple instances of a service (for example, in a cloud with multiple
  control-planes), each control-plane needs its own set of virtual IP address
  and different values for some properties such as the external name and
  security certificate. To accommodate this in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9, load-balancers
  are defined as part of the control-plane, with the network groups defining
  just which load-balancers are attached to them.
 </p><p>
  Load balancers are always implemented by an ha-proxy service in the same
  control-plane as the services.
 </p></div><div class="sect4" id="concept-endpoints"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.2.10.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Separation of Public, Admin, and Internal Endpoints</span> <a title="Permalink" class="permalink" href="#concept-endpoints">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-concepts-endpoints.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-endpoints.xml</li><li><span class="ds-label">ID: </span>concept-endpoints</li></ul></div></div></div></div><p>
  The list of endpoint roles for a <span class="guimenu ">load-balancer</span> make it
  possible to configure separate <span class="guimenu ">load-balancers</span> for public
  and internal access to services, and the configuration processor uses this
  information to both ensure the correct registrations in keystone and to make
  sure the internal traffic is routed to the correct endpoint. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  services are configured to only connect to other services via internal
  virtual IP addresses and endpoints, allowing the name and security
  certificate of public endpoints to be controlled by the customer and set to
  values that may not be resolvable/accessible from the servers making up the
  cloud.
 </p><p>
  Note that each <span class="guimenu ">load-balancer</span> defined in the input model
  will be allocated a separate virtual IP address even when the load-balancers
  are part of the same <span class="guimenu ">network-group</span>. Because of the need to
  be able to separate both public and internal access, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> will not allow
  a single <span class="guimenu ">load-balancer</span> to provide both public and internal
  access. <span class="guimenu ">Load-balancers</span> in this context are logical
  entities (sets of rules to transfer traffic from a virtual IP address to one
  or more endpoints).
  
 </p><p>
  The following diagram shows a possible configuration in which the hostname
  associated with the public URL has been configured to resolve to a firewall
  controlling external access to the cloud. Within the cloud, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services
  are configured to use the internal URL to access a separate virtual IP
  address.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-inputmodel-hphelionopenstack_loadbalancers.png" target="_blank"><img src="images/media-inputmodel-hphelionopenstack_loadbalancers.png" width="" /></a></div></div></div><div class="sect4" id="concept-networktags"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.2.10.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Tags</span> <a title="Permalink" class="permalink" href="#concept-networktags">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-concepts-networktags.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-networktags.xml</li><li><span class="ds-label">ID: </span>concept-networktags</li></ul></div></div></div></div><p>
  Network tags are defined by some <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  <span class="guimenu ">service-components</span> and are used to convey information
  between the network model and the service, allowing the dependent aspects of
  the service to be automatically configured.
  
 </p><p>
  Network tags also convey requirements a service may have for aspects of the
  server network configuration, for example, that a bridge is required on the
  corresponding network device on a server where that service-component is
  installed.
 </p><p>
  See <a class="xref" href="#configobj-networktags" title="6.13.2. Network Tags">Section 6.13.2, “Network Tags”</a> for more information on specific
  tags and their usage.
 </p></div></div><div class="sect3" id="concept-networks"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.2.10.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networks</span> <a title="Permalink" class="permalink" href="#concept-networks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-concepts-networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-networks.xml</li><li><span class="ds-label">ID: </span>concept-networks</li></ul></div></div></div></div><p>
  <span class="emphasis"><em>A <span class="guimenu ">network</span> is part of a
  <span class="guimenu ">network-group</span>.</em></span>
 </p><p>
  <span class="guimenu ">Networks</span> are fairly simple definitions. Each
  <span class="guimenu ">network</span> defines the details of its VLAN, optional address
  details (CIDR, start and end address, gateway address), and which
  <span class="guimenu ">network-group</span> it is a member of.
 </p></div><div class="sect3" id="concept-interfacemodel"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.2.10.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Interface Model</span> <a title="Permalink" class="permalink" href="#concept-interfacemodel">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-concepts-interfacemodel.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-interfacemodel.xml</li><li><span class="ds-label">ID: </span>concept-interfacemodel</li></ul></div></div></div></div><p>
  <span class="emphasis"><em>A <span class="guimenu ">server-role</span> identifies an
  <span class="guimenu ">interface-model</span> that describes how its network interfaces
  are to be configured and used.</em></span>
 </p><p>
  Network groups are mapped onto specific network interfaces via an
  <span class="guimenu ">interface-model</span>, which describes the network devices that
  need to be created (bonds, ovs-bridges, etc.) and their properties.
 </p><p>
  An <span class="guimenu ">interface-model</span> acts like a template; it can define how
  some or all of the <span class="guimenu ">network-groups</span> are to be mapped for a
  particular combination of physical NICs. However, it is the
  <span class="guimenu ">service-components</span> on each server that determine which
  <span class="guimenu ">network-groups</span> are required and hence which interfaces and
  <span class="guimenu ">networks</span> will be configured. This means that
  <span class="guimenu ">interface-models</span> can be shared between different
  <span class="guimenu ">server-roles</span>. For example, an API role and a database role
  may share an interface model even though they may have different disk models
  and they will require a different subset of the
  <span class="guimenu ">network-groups</span>.
 </p><p>
  Within an <span class="guimenu ">interface-model</span>, physical ports are identified
  by a device name, which in turn is resolved to a physical port on a server
  basis via a <span class="guimenu ">nic-mapping</span>. To allow different physical
  servers to share an <span class="guimenu ">interface-model</span>, the
  <span class="guimenu ">nic-mapping</span> is defined as a property of each
  <span class="guimenu ">server</span>.
 </p><p>
  The <code class="literal">interface-model</code> can also used to describe how network
  devices are to be configured for use with DPDK, SR-IOV, and PCI Passthrough.
 </p></div><div class="sect3" id="concept-nicmapping"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.2.10.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">NIC Mapping</span> <a title="Permalink" class="permalink" href="#concept-nicmapping">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-concepts-nicmapping.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-nicmapping.xml</li><li><span class="ds-label">ID: </span>concept-nicmapping</li></ul></div></div></div></div><p>
  When a <span class="guimenu ">server</span> has more than a single physical network
  port, a <span class="guimenu ">nic-mapping</span> is required to unambiguously identify
  each port. Standard Linux mapping of ports to interface names at the time of
  initial discovery (for example, <code class="literal">eth0</code>,
  <code class="literal">eth1</code>, <code class="literal">eth2</code>, ...) is not uniformly
  consistent
  from server to server, so a mapping of PCI bus address to interface name is
  instead.
 </p><p>
  NIC mappings are also used to specify the device type for interfaces that are
  to be used for SR-IOV or PCI Passthrough. Each <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> release includes the
  data for the supported device types.
 </p></div><div class="sect3" id="concept-firewallconfiguration"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.2.10.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Firewall Configuration</span> <a title="Permalink" class="permalink" href="#concept-firewallconfiguration">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-concepts-firewallconfiguration.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-firewallconfiguration.xml</li><li><span class="ds-label">ID: </span>concept-firewallconfiguration</li></ul></div></div></div></div><p>
  The configuration processor uses the details it has about which networks and
  ports <span class="guimenu ">service-components</span> use to create a set of firewall
  rules for each server. The model allows additional user-defined rules on a
  per <span class="guimenu ">network-group</span> basis.
 </p></div></div><div class="sect2" id="concept-configuration-data"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuration Data</span> <a title="Permalink" class="permalink" href="#concept-configuration-data">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-concepts-configuration_data.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-concepts-configuration_data.xml</li><li><span class="ds-label">ID: </span>concept-configuration-data</li></ul></div></div></div></div><p>
  Configuration Data is used to provide settings which have to be applied in a
  specific context, or where the data needs to be verified against or merged
  with other values in the input model.
 </p><p>
  For example, when defining a neutron provider network to be used by Octavia,
  the network needs to be included in the routing configuration generated by
  the Configuration Processor.
 </p></div></div></div><div class="chapter " id="configurationobjects"><div class="titlepage"><div><div><h2 class="title"><span class="number">6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuration Objects</span> <a title="Permalink" class="permalink" href="#configurationobjects">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-configurationobjects.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-configurationobjects.xml</li><li><span class="ds-label">ID: </span>configurationobjects</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#configobj-cloud"><span class="number">6.1 </span><span class="name">Cloud Configuration</span></a></span></dt><dt><span class="section"><a href="#configobj-controlplane"><span class="number">6.2 </span><span class="name">Control Plane</span></a></span></dt><dt><span class="section"><a href="#configobj-load-balancers"><span class="number">6.3 </span><span class="name">Load Balancers</span></a></span></dt><dt><span class="section"><a href="#configobj-regions"><span class="number">6.4 </span><span class="name">Regions</span></a></span></dt><dt><span class="section"><a href="#configobj-servers"><span class="number">6.5 </span><span class="name">Servers</span></a></span></dt><dt><span class="section"><a href="#configobj-servergroups"><span class="number">6.6 </span><span class="name">Server Groups</span></a></span></dt><dt><span class="section"><a href="#configobj-serverroles"><span class="number">6.7 </span><span class="name">Server Roles</span></a></span></dt><dt><span class="section"><a href="#configobj-diskmodels"><span class="number">6.8 </span><span class="name">
  Disk Models</span></a></span></dt><dt><span class="section"><a href="#configobj-memorymodels"><span class="number">6.9 </span><span class="name">Memory Models</span></a></span></dt><dt><span class="section"><a href="#configobj-cpumodels"><span class="number">6.10 </span><span class="name">
  CPU Models</span></a></span></dt><dt><span class="section"><a href="#configobj-interfacemodels"><span class="number">6.11 </span><span class="name">Interface Models</span></a></span></dt><dt><span class="section"><a href="#configobj-nicmappings"><span class="number">6.12 </span><span class="name">NIC Mappings</span></a></span></dt><dt><span class="section"><a href="#configobj-networkgroups"><span class="number">6.13 </span><span class="name">Network Groups</span></a></span></dt><dt><span class="section"><a href="#configobj-networks"><span class="number">6.14 </span><span class="name">Networks</span></a></span></dt><dt><span class="section"><a href="#configobj-firewallrules"><span class="number">6.15 </span><span class="name">Firewall Rules</span></a></span></dt><dt><span class="section"><a href="#configobj-configurationdata"><span class="number">6.16 </span><span class="name">Configuration Data</span></a></span></dt><dt><span class="section"><a href="#passthrough"><span class="number">6.17 </span><span class="name">Pass Through</span></a></span></dt></dl></div></div><div class="sect1" id="configobj-cloud"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cloud Configuration</span> <a title="Permalink" class="permalink" href="#configobj-cloud">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-cloud.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-cloud.xml</li><li><span class="ds-label">ID: </span>configobj-cloud</li></ul></div></div></div></div><p>
  The top-level cloud configuration file, <code class="filename">cloudConfig.yml</code>,
  defines some global values for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, as described in the table below.
 </p><p>
  The snippet below shows the start of the control plane definition file.
 </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  cloud:
    name: entry-scale-kvm

    hostname-data:
        host-prefix: ardana
        member-prefix: -m

    ntp-servers:
        - "ntp-server1"

    # dns resolving configuration for your site
    dns-settings:
      nameservers:
        - name-server1

    firewall-settings:
        enable: true
        # log dropped packets
        logging: true

    audit-settings:
       audit-dir: /var/audit
       default: disabled
       enabled-services:
         - keystone</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>An administrator-defined name for the cloud</td></tr><tr><td>hostname-data (optional)</td><td>
      <p>
       Provides control over some parts of the generated names (see )
      </p>
      <p>
       Consists of two values:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         host-prefix - default is to use the cloud name (above)
        </p></li><li class="listitem "><p>
         member-prefix - default is "-m"
        </p></li></ul></div>
     </td></tr><tr><td>ntp-servers (optional)</td><td>
      <p>
       A list of external NTP servers your cloud has access to. If specified
       by name then the names need to be resolvable via the external DNS
       nameservers you specify in the next section. All servers running the
       "ntp-server" component will be configured to use these external NTP
       servers.
      </p>
     </td></tr><tr><td>dns-settings (optional)</td><td>
      <p>
       DNS configuration data that will be applied to all servers. See
       example configuration for a full list of values.
      </p>
     </td></tr><tr><td>smtp-settings (optional)</td><td>
      <p>
       SMTP client configuration data that will be applied to all servers.
       See example configurations for a full list of values.
      </p>
     </td></tr><tr><td>firewall-settings (optional)</td><td>
      <p>
       Used to enable/disable the firewall feature and to enable/disable
       logging of dropped packets.
      </p>
      <p>
       The default is to have the firewall enabled.
      </p>
     </td></tr><tr><td>audit-settings (optional)</td><td>
      <p>
       Used to enable/disable the production of audit data from services.
      </p>
      <p>
       The default is to have audit disabled for all services.
      </p>
     </td></tr></tbody></table></div></div><div class="sect1" id="configobj-controlplane"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Control Plane</span> <a title="Permalink" class="permalink" href="#configobj-controlplane">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-controlplane.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-controlplane.xml</li><li><span class="ds-label">ID: </span>configobj-controlplane</li></ul></div></div></div></div><p>
  The snippet below shows the start of the control plane definition file.
 </p><div class="verbatim-wrap"><pre class="screen">---
  product:
     version: 2

  control-planes:
     - name: control-plane-1
       control-plane-prefix: cp1
       region-name: region0
       failure-zones:
         - AZ1
         - AZ2
         - AZ3
       configuration-data:
         - NEUTRON-CONFIG-CP1
         - OCTAVIA-CONFIG-CP1
       common-service-components:
         - logging-producer
         - monasca-agent
         - stunnel
         - lifecycle-manager-target
       clusters:
         - name: cluster1
           cluster-prefix: c1
           server-role: CONTROLLER-ROLE
           member-count: 3
           allocation-policy: strict
           service-components:
             - lifecycle-manager
             - ntp-server
             - swift-ring-builder
             - mysql
             - ip-cluster
             ...

       resources:
         - name: compute
           resource-prefix: comp
           server-role: COMPUTE-ROLE
           allocation-policy: any
           min-count: 0
           service-components:
              - ntp-client
              - nova-compute
              - nova-compute-kvm
              - neutron-l3-agent
              ...</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>
      <p>
       This name identifies the control plane. This value is used to persist
       server allocations <a class="xref" href="#persisteddata" title="7.3. Persisted Data">Section 7.3, “Persisted Data”</a> and cannot be
       changed once servers have been allocated.
      </p>
     </td></tr><tr><td>control-plane-prefix (optional)</td><td>
      <p>
       The control-plane-prefix is used as part of the hostname (see
       <a class="xref" href="#namegeneration" title="7.2. Name Generation">Section 7.2, “Name Generation”</a>). If not specified, the control plane
       name is used.
      </p>
     </td></tr><tr><td>region-name</td><td>
      <p>
       This name identifies the keystone region within which services in the
       control plane will be registered. In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, multiple regions are
       not supported. Only <code class="literal">Region0</code> is valid.
      </p>
      <p>
       For clouds consisting of multiple control planes, this attribute should
       be omitted and the regions object should be used to set the region
       name (<code class="literal">Region0</code>).
      </p>
     </td></tr><tr><td>uses (optional)</td><td>
      <p>
       Identifies the services this control will consume from other control
       planes (see <a class="xref" href="#configobj-multiple-control-planes" title="6.2.3. Multiple Control Planes">Section 6.2.3, “Multiple Control Planes”</a>).
      </p>
     </td></tr><tr><td>load-balancers (optional)</td><td>
      <p>
       A list of load balancer definitions for this control plane (see
       <a class="xref" href="#configobj-load-balancer-definitions" title="6.2.4. Load Balancer Definitions in Control Planes">Section 6.2.4, “Load Balancer Definitions in Control Planes”</a>).
      </p>
      <p>
       For a multi control-plane cloud load balancers must be defined in each
       control-plane. For a single control-plane cloud they may be defined
       either in the control plane or as part of a network group.
      </p>
     </td></tr><tr><td>common-service-components (optional)</td><td>
      <p>
       This lists a set of service components that run on all servers in the
       control plane (clusters and resource pools).
      </p>
     </td></tr><tr><td>failure-zones (optional)</td><td>
      <p>
       A list of <span class="guimenu ">server-group</span> names that servers for this
       control plane will be allocated from. If no failure-zones are
       specified, only servers not associated with a
       <span class="guimenu ">server-group</span> will be used. (See
       <a class="xref" href="#concept-servergroups-failurezones" title="5.2.9.1. Server Groups and Failure Zones">Section 5.2.9.1, “Server Groups and Failure Zones”</a> for a description
       of server-groups as failure zones.)
      </p>
     </td></tr><tr><td>configuration-data (optional)</td><td>
      <p>
       A list of configuration data settings to be used for services in this
       control plane (see <a class="xref" href="#concept-configuration-data" title="5.2.11. Configuration Data">Section 5.2.11, “Configuration Data”</a>).
      </p>
     </td></tr><tr><td>clusters</td><td>
      <p>
       A list of clusters for this control plane (see
       <a class="xref" href="#configobj-clusters" title="6.2.1.  Clusters">Section 6.2.1, “
  Clusters”</a>).
      </p>
     </td></tr><tr><td>resources</td><td>
      <p>
       A list of resource groups for this control plane (see
       <a class="xref" href="#configobj-resources" title="6.2.2. Resources">Section 6.2.2, “Resources”</a>).
      </p>
     </td></tr></tbody></table></div><div class="sect2" id="configobj-clusters"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">
  Clusters</span> <a title="Permalink" class="permalink" href="#configobj-clusters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-clusters.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-clusters.xml</li><li><span class="ds-label">ID: </span>configobj-clusters</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>
      <p>
       Cluster and resource names must be unique within a control plane. This
       value is used to persist server allocations (see
       <a class="xref" href="#persisteddata" title="7.3. Persisted Data">Section 7.3, “Persisted Data”</a>) and cannot be changed once servers
       have been allocated.
      </p>
     </td></tr><tr><td>cluster-prefix (optional)</td><td>
      <p>
       The cluster prefix is used in the hostname (see
       <a class="xref" href="#namegeneration" title="7.2. Name Generation">Section 7.2, “Name Generation”</a>). If not supplied then the cluster
       name is used.
      </p>
     </td></tr><tr><td>server-role</td><td>
      <p>
       This can either be a string (for a single role) or a list of roles.
       Only servers matching one of the specified
       <span class="guimenu ">server-roles</span> will be allocated to this cluster.
       (see <a class="xref" href="#concept-serverroles" title="5.2.4. Server Roles">Section 5.2.4, “Server Roles”</a> for a description of server
       roles)
      </p>
     </td></tr><tr><td>service-components</td><td>
      <p>
       The list of <span class="guimenu ">service-components</span> to be deployed on
       the servers allocated for the cluster. (The common-service-components
       for the control plane are also deployed.)
      </p>
     </td></tr><tr><td>
      <p>
       member-count
      </p>
      <p>
       min-count
      </p>
      <p>
       max-count
      </p>
      <p>
       (all optional)
      </p>
     </td><td>
      <p>
       Defines the number of servers to add to the cluster.
      </p>
      <p>
       The number of servers that can be supported in a cluster depends on the
       services it is running. For example MariaDB and RabbitMQ can only be
       deployed on clusters on 1 (non-HA) or 3 (HA) servers. Other services may
       support different sizes of cluster.
      </p>
      <p>
       If min-count is specified, then at least that number of servers will be
       allocated to the cluster. If min-count is not specified it defaults to a
       value of 1.
      </p>
      <p>
       If max-count is specified, then the cluster will be limited to that
       number of servers. If max-count is not specified then all servers
       matching the required role and failure-zones will be allocated to the
       cluster.
      </p>
      <p>
       Specifying member-count is equivalent to specifying min-count and
       max-count with the same value.
      </p>
     </td></tr><tr><td>failure-zones (optional)</td><td>
      <p>
       A list of <span class="guimenu ">server-groups</span> that servers will be
       allocated from. If specified, it overrides the list of values
       specified for the control-plane. If not specified, the control-plane
       value is used. (see
       <a class="xref" href="#concept-servergroups-failurezones" title="5.2.9.1. Server Groups and Failure Zones">Section 5.2.9.1, “Server Groups and Failure Zones”</a> for a description
       of server groups as failure zones).
      </p>
     </td></tr><tr><td>allocation-policy (optional)</td><td>
      <p>
       Defines how failure zones will be used when allocating servers.
      </p>
      <p>
       <span class="bold"><strong>strict</strong></span>: Server allocations will be
       distributed across all specified failure zones. (if max-count is not a
       whole number, an exact multiple of the number of zones, then some zones
       may provide one more server than other zones)
      </p>
      <p>
       <span class="bold"><strong>any</strong></span>: Server allocations will be made
       from any combination of failure zones.
      </p>
      <p>
       The default allocation-policy for a cluster is
       <span class="emphasis"><em>strict</em></span>.
      </p>
     </td></tr><tr><td>configuration-data (optional)</td><td>
      <p>
       A list of configuration-data settings that will be applied to the
       services in this cluster. The values for each service will be combined
       with any values defined as part of the configuration-data list for the
       control-plane. If a value is specified by settings in both lists, the
       value defined here takes precedence.
      </p>
     </td></tr></tbody></table></div></div><div class="sect2" id="configobj-resources"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Resources</span> <a title="Permalink" class="permalink" href="#configobj-resources">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-resources.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-resources.xml</li><li><span class="ds-label">ID: </span>configobj-resources</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>
      <p>
       The name of this group of resources. Cluster names and resource-node
       names must be unique within a control plane. Additionally, clusters and
       resources cannot share names within a control-plane.
      </p>
      <p>
       This value is used to persist server allocations (see
       <a class="xref" href="#persisteddata" title="7.3. Persisted Data">Section 7.3, “Persisted Data”</a>) and cannot be changed once servers
       have been allocated.
      </p>
     </td></tr><tr><td>resource-prefix</td><td>
      The resource-prefix is used in the name generation. (see
      <a class="xref" href="#namegeneration" title="7.2. Name Generation">Section 7.2, “Name Generation”</a>)
     </td></tr><tr><td>server-role</td><td>
      This can either be a string (for a single role) or a list of roles.
      Only servers matching one of the specified
      <span class="guimenu ">server-roles</span> will be allocated to this resource
      group. (see <a class="xref" href="#concept-serverroles" title="5.2.4. Server Roles">Section 5.2.4, “Server Roles”</a> for a description of
      server roles).
     </td></tr><tr><td>service-components</td><td>
      The list of <span class="guimenu ">service-components</span> to be deployed on the
      servers in this resource group. (The common-service-components for the
      control plane are also deployed.)
     </td></tr><tr><td>
      <p>
       member-count
      </p>
      <p>
       min-count
      </p>
      <p>
       max-count
      </p>
      <p>
       (all optional)
      </p>
     </td><td>
      <p>
       Defines the number of servers to add to the cluster.
      </p>
      <p>
       The number of servers that can be supported in a cluster depends on the
       services it is running. For example MariaDB and RabbitMQ can only be
       deployed on clusters on 1 (non-HA) or 3 (HA) servers. Other services may
       support different sizes of cluster.
      </p>
      <p>
       If min-count is specified, then at least that number of servers will be
       allocated to the cluster. If min-count is not specified it defaults to a
       value of 1.
      </p>
      <p>
       If max-count is specified, then the cluster will be limited to that
       number of servers. If max-count is not specified then all servers
       matching the required role and failure-zones will be allocated to the
       cluster.
      </p>
      <p>
       Specifying member-count is equivalent to specifying min-count and
       max-count with the same value.
      </p>
     </td></tr><tr><td>failure-zones (optional)</td><td>
      A list of <span class="guimenu ">server-groups</span> that servers will be
      allocated from. If specified, it overrides the list of values specified
      for the control-plane. If not specified, the control-plane value is
      used. (see <a class="xref" href="#concept-servergroups-failurezones" title="5.2.9.1. Server Groups and Failure Zones">Section 5.2.9.1, “Server Groups and Failure Zones”</a> for a
      description of server groups as failure zones).
     </td></tr><tr><td>allocation-policy (optional)</td><td>
      <p>
       Defines how failure zones will be used when allocating servers.
      </p>
      <p>
       <span class="bold"><strong>strict</strong></span>: Server allocations will be
       distributed across all specified failure zones. (if max-count is not a
       whole number, an exact multiple of the number of zones, then some zones
       may provide one more server than other zones)
      </p>
      <p>
       <span class="bold"><strong>any</strong></span>: Server allocations will be made
       from any combination of failure zones.
      </p>
      <p>
       The default allocation-policy for resources is <span class="emphasis"><em>any</em></span>.
      </p>
     </td></tr><tr><td>configuration-data (optional)</td><td>
      A list of configuration-data settings that will be applied to the
      services in this cluster. The values for each service will be combined
      with any values defined as part of the configuration-data list for the
      control-plane. If a value is specified by settings in both lists, the
      value defined here takes precedence.
     </td></tr></tbody></table></div></div><div class="sect2" id="configobj-multiple-control-planes"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Multiple Control Planes</span> <a title="Permalink" class="permalink" href="#configobj-multiple-control-planes">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-multiple_control_planes.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-multiple_control_planes.xml</li><li><span class="ds-label">ID: </span>configobj-multiple-control-planes</li></ul></div></div></div></div><p>
  The dependencies between service components (for example, nova needs
  MariaDB and keystone API) is defined as part of the service definitions
  provide by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the control-planes define how those dependencies will
  be met. For clouds consisting of multiple control-planes, the relationship
  between services in different control planes is defined by a
  <code class="literal">uses</code> attribute in its control-plane object. Services
  will always use other services in the same control-plane before looking to
  see if the required service can be provided from another control-plane.
  For example, a service component in control-plane
  <code class="literal">cp-2</code> (for example, nova-api) might use service
  components from control-plane <code class="literal">cp-shared</code> (for example,
  keystone-api).
 </p><div class="verbatim-wrap"><pre class="screen">control-planes:
    - name: cp-2
      uses:
        - from: cp-shared
          service-components:
            - any</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>from</td><td>
      The name of the control-plane providing services which may be consumed
      by this control-plane.
     </td></tr><tr><td>service-components</td><td>
      A list of service components from the specified control-plane which may
      be consumed by services in this control-plane. The reserved keyword
      <code class="literal">any</code> indicates that any service component from the
      specified control-plane may be consumed by services in this
      control-plane.
     </td></tr></tbody></table></div></div><div class="sect2" id="configobj-load-balancer-definitions"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Load Balancer Definitions in Control Planes</span> <a title="Permalink" class="permalink" href="#configobj-load-balancer-definitions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-load_balancer_definitions.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-load_balancer_definitions.xml</li><li><span class="ds-label">ID: </span>configobj-load-balancer-definitions</li></ul></div></div></div></div><p>
  Starting in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9, a load-balancer may be defined within a
  control-plane object, and referenced by name from a network-groups object.
  The following example shows load balancer <code class="literal">extlb</code> defined in
  control-plane <code class="literal">cp1</code> and referenced from the EXTERNAL-API
  network group. See section Load balancers for a complete description of load
  balance attributes.
 </p><div class="verbatim-wrap"><pre class="screen">network-groups:
    - name: EXTERNAL-API
      load-balancers:
        - extlb

  control-planes:
    - name: cp1
      load-balancers:
        - provider: ip-cluster
          name: extlb
          external-name:
          tls-components:
            - default
          roles:
            - public
          cert-file: cp1-extlb-cert</pre></div></div></div><div class="sect1" id="configobj-load-balancers"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Load Balancers</span> <a title="Permalink" class="permalink" href="#configobj-load-balancers">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-load_balancers.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-load_balancers.xml</li><li><span class="ds-label">ID: </span>configobj-load-balancers</li></ul></div></div></div></div><p>
  Load balancers may be defined as part of a network-group object, or as part
  of a control-plane object. When a load-balancer is defined in a
  control-plane, it must be referenced by name only from the associated
  network-group object.
 </p><p>
  For clouds consisting of multiple control planes, load balancers must be
  defined as part of a control-plane object. This allows different load
  balancer configurations for each control plane.
 </p><p>
  In either case, a load-balancer definition has the following attributes:
 </p><div class="verbatim-wrap"><pre class="screen">load-balancers:
        - provider: ip-cluster
          name: extlb
          external-name:

          tls-components:
            - default
          roles:
            - public
          cert-file: cp1-extlb-cert</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>
      An administrator defined name for the load balancer. This name is used
      to make the association from a network-group.
     </td></tr><tr><td>provider</td><td>
      The service component that implements the load balancer. Currently only
      <code class="literal">ip-cluster</code> (ha-proxy) is supported. Future releases
      will provide support for external load balancers.
     </td></tr><tr><td>roles</td><td>
      The list of endpoint roles that this load balancer provides (see
      below). 
      Valid roles are
      <code class="literal">public</code>, <code class="literal">internal</code>, and
      <code class="literal">admin</code>. To ensure separation of concerns, the role
      <code class="literal">public</code> cannot be combined with any other role. See
      Load Balancers for an example of how the role provides endpoint
      separation.
     </td></tr><tr><td>components (optional)</td><td>
      The list of service-components for which the load balancer provides a
      non-encrypted virtual IP address for.
     </td></tr><tr><td>tls-components (optional)</td><td>
      The list of service-components for which the load balancer provides
      TLS-terminated virtual IP addresses for.
     </td></tr><tr><td>external-name (optional)</td><td>
      The name to be registered in keystone for the publicURL. If not
      specified, the virtual IP address will be registered. Note that this
      value cannot be changed after the initial deployment.
     </td></tr><tr><td>cert-file (optional)</td><td>
      The name of the certificate file to be used for tls endpoints. If not
      specified, a file name will be constructed using the format
      <code class="literal"><em class="replaceable ">CP-NAME</em>-<em class="replaceable ">LB-NAME</em>-cert</code>,
      where <code class="literal"><em class="replaceable ">CP-NAME</em></code> is the
      control-plane name and
      <code class="literal"><em class="replaceable ">LB-NAME</em></code> is the
      load-balancer name.
     </td></tr></tbody></table></div></div><div class="sect1" id="configobj-regions"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Regions</span> <a title="Permalink" class="permalink" href="#configobj-regions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-regions.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-regions.xml</li><li><span class="ds-label">ID: </span>configobj-regions</li></ul></div></div></div></div><p>
  The regions configuration object is used to define how a set of services from
  one or more control-planes are mapped into Openstack regions (entries within
  the keystone catalog). In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, multiple regions are not
  supported. Only <code class="literal">Region0</code> is valid.
 </p><p>
  Within each region a given service is provided by one control plane, but the
  set of services in the region may be provided by multiple control planes.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>The name of the region in the keystone service catalog. </td></tr><tr><td>includes</td><td>
      A list of services to include in this region, broken down by the
      control planes providing the services.
     </td></tr></tbody></table></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>control-plane</td><td>A control-plane name.</td></tr><tr><td>services</td><td>
      A list of service names. This list specifies the services from this
      control-plane to be included in this region. The reserved keyword
      <code class="literal">all</code> may be used when all services from the
      control-plane are to be included.
     </td></tr></tbody></table></div></div><div class="sect1" id="configobj-servers"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Servers</span> <a title="Permalink" class="permalink" href="#configobj-servers">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-servers.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-servers.xml</li><li><span class="ds-label">ID: </span>configobj-servers</li></ul></div></div></div></div><p>
  The <span class="guimenu ">servers</span> configuration object is used to list the
  available servers for deploying the cloud.
 </p><p>
  Optionally, it can be used as an input file to the operating system
  installation process, in which case some additional fields (identified below)
  will be necessary.
 </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  baremetal:
    subnet: 192.168.10.0
    netmask: 255.255.255.0

  servers:
    - id: controller1
      ip-addr: 192.168.10.3
      role: CONTROLLER-ROLE
      server-group: RACK1
      nic-mapping: HP-DL360-4PORT
      mac-addr: b2:72:8d:ac:7c:6f
      ilo-ip: 192.168.9.3
      ilo-password: password
      ilo-user: admin

    - id: controller2
      ip-addr: 192.168.10.4
      role: CONTROLLER-ROLE
      server-group: RACK2
      nic-mapping: HP-DL360-4PORT
      mac-addr: 8a:8e:64:55:43:76
      ilo-ip: 192.168.9.4
      ilo-password: password
      ilo-user: admin</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>id</td><td>
      An administrator-defined identifier for the server. IDs must be unique
      and are used to track server allocations. (see
      <a class="xref" href="#persisteddata" title="7.3. Persisted Data">Section 7.3, “Persisted Data”</a>).
     </td></tr><tr><td>ip-addr</td><td>
      <p>
       The IP address is used by the configuration processor to install and
       configure the service components on this server.
      </p>
      <p>
       This IP address must be within the range of a <span class="guimenu ">network</span>
       defined in this model.
      </p>
      <p>
       When the servers file is being used for operating system installation,
       this IP address will be assigned to the node by the installation
       process, and the associated <span class="guimenu ">network</span> must be an
       untagged VLAN.
      </p>
     </td></tr><tr><td>hostname (optional)</td><td>
      The value to use for the hostname of the server. If specified this will
      be used to set the hostname value of the server which will in turn be
      reflected in systems such as nova, monasca, etc. If not specified the
      hostname will be derived based on where the server is used and the
      network defined to provide hostnames.
     </td></tr><tr><td>role</td><td>
      Identifies the <span class="guimenu ">server-role</span> of the server.
      
     </td></tr><tr><td>nic-mapping</td><td>
      Name of the <span class="guimenu ">nic-mappings</span> entry to apply to this
      server. (See <a class="xref" href="#configobj-nicmappings" title="6.12. NIC Mappings">Section 6.12, “NIC Mappings”</a>.)
     </td></tr><tr><td>server-group (optional)</td><td>
      Identifies the <span class="guimenu ">server-groups</span> entry that this server
      belongs to. (see <a class="xref" href="#concept-servergroups" title="5.2.9. Server Groups">Section 5.2.9, “Server Groups”</a>)
     </td></tr><tr><td>boot-from-san (optional)</td><td>
      Must be set to true is the server needs to be configured to boot from
      SAN storage. Default is False
     </td></tr><tr><td>fcoe-interfaces (optional)</td><td>
      A list of network devices that will be used for accessing FCoE storage.
      This is only needed for devices that present as native FCoE, not
      devices such as Emulex which present as a FC device.
     </td></tr><tr><td>ansible-options (optional)</td><td>
      A string of additional variables to be set when defining the server as
      a host in Ansible. For example, <code class="literal">ansible_ssh_port=5986</code>
     </td></tr><tr><td>mac-addr (optional)</td><td>
      Needed when the servers file is being used for operating system
      installation. This identifies the MAC address on the server that will
      be used to network install the operating system.
     </td></tr><tr><td>kopt-extras (optional)</td><td>
      Provides additional command line arguments to be passed to the booting
      network kernel. For example, <code class="literal">vga=769</code> sets the video
      mode for the install to low resolution which can be useful for remote
      console users.
     </td></tr><tr><td>ilo-ip (optional)</td><td>
      Needed when the servers file is being used for operating system
      installation. This provides the IP address of the power management
      (for example, IPMI, iLO) subsystem.
     </td></tr><tr><td>ilo-user (optional)</td><td>
      Needed when the servers file is being used for operating system
      installation. This provides the user name of the power management (for
      example, IPMI, iLO) subsystem.
     </td></tr><tr><td>ilo-password (optional)</td><td>
      Needed when the servers file is being used for operating system
      installation. This provides the user password of the power management
      (for example, IPMI, iLO) subsystem.
     </td></tr><tr><td>ilo-extras (optional)</td><td>
      Needed when the servers file is being used for operating system
      installation. Additional options to pass to ipmitool. For example, this
      may be required if the servers require additional IPMI addressing
      parameters.
     </td></tr><tr><td>moonshot (optional)</td><td>
      Provides the node identifier for HPE Moonshot servers, for example,
      <code class="literal">c4n1</code> where c4 is the cartridge and n1 is node.
     </td></tr><tr><td>hypervisor-id (optional)</td><td>
      This attribute serves two purposes: it indicates that this server is a
      virtual machine (VM), and it specifies the server id of the Cloud Lifecycle Manager
      hypervisor that will host the VM.
     </td></tr><tr><td>ardana-hypervisor (optional)</td><td>
      When set to True, this attribute identifies a server as a Cloud Lifecycle Manager
      hypervisor. A Cloud Lifecycle Manager hypervisor is a server that may be used to host
      other servers that are themselves virtual machines. Default value is
      <code class="literal">False</code>.
     </td></tr></tbody></table></div></div><div class="sect1" id="configobj-servergroups"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Server Groups</span> <a title="Permalink" class="permalink" href="#configobj-servergroups">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-servergroups.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-servergroups.xml</li><li><span class="ds-label">ID: </span>configobj-servergroups</li></ul></div></div></div></div><p>
  The server-groups configuration object provides a mechanism for organizing
  servers and networks into a hierarchy that can be used for allocation and
  network resolution.
 </p><div class="verbatim-wrap"><pre class="screen">---
  product:
     version: 2

     - name: CLOUD
        server-groups:
         - AZ1
         - AZ2
         - AZ3
        networks:
         - EXTERNAL-API-NET
         - EXTERNAL-VM-NET
         - GUEST-NET
         - MANAGEMENT-NET

     #
     # Create a group for each failure zone
     #
     - name: AZ1
       server-groups:
         - RACK1

     - name: AZ2
       server-groups:
         - RACK2

     - name: AZ3
       server-groups:
         - RACK3

     #
     # Create a group for each rack
     #
     - name: RACK1
     - name: RACK2
     - name: RACK3</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>
      An administrator-defined name for the server group. The name is used to
      link server-groups together and to identify server-groups to be used as
      failure zones in a <span class="guimenu ">control-plane</span>. (see
      <a class="xref" href="#configobj-controlplane" title="6.2. Control Plane">Section 6.2, “Control Plane”</a>)
     </td></tr><tr><td>server-groups (optional)</td><td>
      A list of server-group names that are nested below this group in the
      hierarchy. Each server group can only be listed in one other server
      group (that is in a strict tree topology).
     </td></tr><tr><td>networks (optional)</td><td>
      A list of network names (see <a class="xref" href="#concept-networks" title="5.2.10.2. Networks">Section 5.2.10.2, “Networks”</a>). See
      <a class="xref" href="#concept-servergroups-networks" title="5.2.9.2. Server Groups and Networks">Section 5.2.9.2, “Server Groups and Networks”</a> for a description of
      how networks are matched to servers via server groups.
     </td></tr></tbody></table></div></div><div class="sect1" id="configobj-serverroles"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Server Roles</span> <a title="Permalink" class="permalink" href="#configobj-serverroles">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-serverroles.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-serverroles.xml</li><li><span class="ds-label">ID: </span>configobj-serverroles</li></ul></div></div></div></div><p>
  The server-roles configuration object is a list of the various server roles
  that you can use in your cloud. Each server role is linked to other
  configuration objects:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Disk model (<a class="xref" href="#configobj-diskmodels" title="6.8.  Disk Models">Section 6.8, “
  Disk Models”</a>)
   </p></li><li class="listitem "><p>
    Interface model (<a class="xref" href="#configobj-interfacemodels" title="6.11. Interface Models">Section 6.11, “Interface Models”</a>)
   </p></li><li class="listitem "><p>
    Memory model (<a class="xref" href="#configobj-memorymodels" title="6.9. Memory Models">Section 6.9, “Memory Models”</a>)
   </p></li><li class="listitem "><p>
    CPU model (<a class="xref" href="#configobj-cpumodels" title="6.10.  CPU Models">Section 6.10, “
  CPU Models”</a>)
   </p></li></ul></div><p>
  Server roles are referenced in the servers (see
  <a class="xref" href="#configobj-serverroles" title="6.7. Server Roles">Section 6.7, “Server Roles”</a>) configuration object above.
 </p><div class="verbatim-wrap"><pre class="screen">---
  product:
     version: 2

  server-roles:

     - name: CONTROLLER-ROLE
       interface-model: CONTROLLER-INTERFACES
       disk-model: CONTROLLER-DISKS

     - name: COMPUTE-ROLE
       interface-model: COMPUTE-INTERFACES
       disk-model: COMPUTE-DISKS
       memory-model: COMPUTE-MEMORY
       cpu-model: COMPUTE-CPU</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>An administrator-defined name for the role.</td></tr><tr><td>interface-model</td><td>
      <p>
       The name of the <span class="guimenu ">interface-model</span> to be used for this
       server-role.
      </p>
      <p>
       Different server-roles can use the same interface-model.
      </p>
     </td></tr><tr><td>disk-model</td><td>
      <p>
       The name of the <span class="guimenu ">disk-model</span> to use for this
       server-role.
      </p>
      <p>
       Different server-roles can use the same disk-model.
      </p>
     </td></tr><tr><td>memory-model (optional)</td><td>
      <p>
       The name of the <span class="guimenu ">memory-model</span> to use for this
       server-role.
      </p>
      <p>
       Different server-roles can use the same memory-model.
      </p>
     </td></tr><tr><td>cpu-model (optional)</td><td>
      <p>
       The name of the <span class="guimenu ">cpu-model</span> to use for this
       server-role.
      </p>
      <p>
       Different server-roles can use the same cpu-model.
      </p>
     </td></tr></tbody></table></div></div><div class="sect1" id="configobj-diskmodels"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">
  Disk Models</span> <a title="Permalink" class="permalink" href="#configobj-diskmodels">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-diskmodels.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-diskmodels.xml</li><li><span class="ds-label">ID: </span>configobj-diskmodels</li></ul></div></div></div></div><p>
  The disk-models configuration object is used to specify how the directly
  attached disks on the server should be configured. It can also identify which
  service or service component consumes the disk, for example, swift object
  server, and provide service-specific information associated with the disk.
  It is also used to specify disk sizing information for virtual machine
  servers.
 </p><p>
  Disks can be used as raw devices or as logical volumes and the disk model
  provides a configuration item for each.
 </p><p>
  If the operating system has been installed by the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installation
  process then the root disk will already have been set up as a volume-group
  with a single logical-volume. This logical-volume will have been created on a
  partition identified, symbolically, in the configuration files as
  <code class="filename">/dev/sda_root</code>. This is due to the fact that different
  BIOS systems (UEFI, Legacy) will result in different partition numbers on the
  root disk.
 </p><div class="verbatim-wrap"><pre class="screen">---
  product:
     version: 2

  disk-models:
  - name: SES-DISKS

    volume-groups:
       - ...
    device-groups:
       - ...
    vm-size:
       ...</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>
      The name of the disk-model that is referenced from one or more
      server-roles.
     </td></tr><tr><td>volume-groups</td><td>
      A list of volume-groups to be configured (see below). There must be at
      least one volume-group describing the root file system.
      
     </td></tr><tr><td>device-groups (optional)</td><td>A list of device-groups (see below)</td></tr></tbody></table></div><div class="sect2" id="configobj-volumegroups"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Volume Groups</span> <a title="Permalink" class="permalink" href="#configobj-volumegroups">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-volumegroups.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-volumegroups.xml</li><li><span class="ds-label">ID: </span>configobj-volumegroups</li></ul></div></div></div></div><p>
  The <span class="guimenu ">volume-groups</span> configuration object is used to define
  volume groups and their constituent logical volumes.
 </p><p>
  Note that volume-groups are not exact analogs of device-groups. A
  volume-group specifies a set of physical volumes used to make up a
  volume-group that is then subdivided into multiple logical volumes.
 </p><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> operating system installation automatically creates a
  volume-group name "ardana-vg" on the first drive in the system. It creates a
  "root" logical volume there. The volume-group can be expanded by adding more
  physical-volumes (see examples). In addition, it is possible to create more
  logical-volumes on this volume-group to provide dedicated capacity for
  different services or file system mounts.
 </p><div class="verbatim-wrap"><pre class="screen">   volume-groups:
     - name: ardana-vg
       physical-volumes:
         - /dev/sda_root

       logical-volumes:
         - name: root
           size: 35%
           fstype: ext4
           mount: /

         - name: log
           size: 50%
           mount: /var/log
           fstype: ext4
           mkfs-opts: -O large_file

         - ...

     - name: vg-comp
       physical-volumes:
         - /dev/sdb
       logical-volumes:
         - name: compute
           size: 95%
           mount: /var/lib/nova
           fstype: ext4
           mkfs-opts: -O large_file</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Descriptions</th></tr></thead><tbody><tr><td>name</td><td>The name that will be assigned to the volume-group</td></tr><tr><td>physical-volumes</td><td>
      <p>
       A list of physical disks that make up the volume group.
      </p>
      <p>
       As installed by the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> operating system install process, the
       volume group "ardana-vg" will use a large partition (sda_root) on the
       first disk. This can be expanded by adding additional disk(s).
      </p>
     </td></tr><tr><td>logical-volumes</td><td>
      A list of logical volume devices to create from the above named volume
      group.
     </td></tr><tr><td>name</td><td>The name to assign to the logical volume.</td></tr><tr><td>size</td><td>
      The size, expressed as a percentage of the entire volume group
      capacity, to assign to the logical volume.
     </td></tr><tr><td>fstype (optional)</td><td>
      The file system type to create on the logical volume. If none
      specified, the volume is not formatted.
     </td></tr><tr><td>mkfs-opts (optional)</td><td>
      Options, for example, <code class="literal">-O large_file</code> to pass to the
      mkfs command.
     </td></tr><tr><td>mode (optional)</td><td>
      The <code class="literal">mode</code> changes the root file system mode bits,
      which can be either a symbolic representation or an octal number
      representing the bit pattern for the new mode bits.
     </td></tr><tr><td>mount (optional)</td><td>Mount point for the file system.</td></tr><tr><td>consumer attributes (optional, consumer dependent)</td><td>
      <p>
       These will vary according to the service consuming the device group. The
       examples section provides sample content for the different services.
      </p>
     </td></tr></tbody></table></div><div id="id-1.3.4.4.9.7.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   Multipath storage should be listed as the corresponding
   <code class="filename">/dev/mapper/mpath<em class="replaceable ">X</em></code>
  </p></div></div><div class="sect2" id="configobj-devicegroups"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Device Groups</span> <a title="Permalink" class="permalink" href="#configobj-devicegroups">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-devicegroups.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-devicegroups.xml</li><li><span class="ds-label">ID: </span>configobj-devicegroups</li></ul></div></div></div></div><p>
  The device-groups configuration object provides the mechanism to make the
  whole of a physical disk available to a service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Descriptions</th></tr></thead><tbody><tr><td>name</td><td>An administrator-defined name for the device group.</td></tr><tr><td>devices</td><td>
      <p>
       A list of named devices to be assigned to this group. There must be at
       least one device in the group.
      </p>
      <p>
       Multipath storage should be listed as the corresponding
       <code class="filename">/dev/mapper/mpath<em class="replaceable ">X</em>f</code>
      </p>
     </td></tr><tr><td>consumer</td><td>
      <p>
       Identifies the name of one of the storage services (for example, one
       of the following: swift, cinder, etc.) that will consume the disks in
       this device group.
      </p>
     </td></tr><tr><td>consumer attributes</td><td>
      <p>
       These will vary according to the service consuming the device group.
       The examples section provides sample content for the different
       services.
      </p>
     </td></tr></tbody></table></div></div></div><div class="sect1" id="configobj-memorymodels"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Memory Models</span> <a title="Permalink" class="permalink" href="#configobj-memorymodels">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-memorymodels.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-memorymodels.xml</li><li><span class="ds-label">ID: </span>configobj-memorymodels</li></ul></div></div></div></div><p>
  The memory-models configuration object describes details of the optional
  configuration of Huge Pages. It also describes the amount of memory to be
  allocated for virtual machine servers.
 </p><p>
  The memory-model allows the number of pages of a particular size to be
  configured at the server level or at the numa-node level.
 </p><p>
  The following example would configure:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    five 2 MB pages in each of numa nodes 0 and 1
   </p></li><li class="listitem "><p>
    three 1 GB pages (distributed across all numa nodes)
   </p></li><li class="listitem "><p>
    six 2 MB pages (distributed across all numa nodes)
   </p></li></ul></div><div class="verbatim-wrap"><pre class="screen">memory-models:
    - name: COMPUTE-MEMORY-NUMA
      default-huge-page-size: 2M
      huge-pages:
        - size: 2M
          count: 5
          numa-node: 0
        - size: 2M
          count: 5
          numa-node: 1
        - size: 1G
          count: 3
        - size: 2M
          count: 6
    - name: VIRTUAL-CONTROLLER-MEMORY
      vm-size:
        ram: 6G</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>The name of the memory-model that is referenced from one or more
              server-roles.</td></tr><tr><td>default-huge-page-size
              (optional)
            </td><td>
      <p>
       The default page size that will be used is specified when allocating
       huge pages.
      </p>
      <p>
       If not specified, the default is set by the operating system.
      </p>
     </td></tr><tr><td>huge-pages</td><td>A list of huge page definitions (see below).</td></tr></tbody></table></div><div class="sect2" id="configobj-huge-pages"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Huge Pages</span> <a title="Permalink" class="permalink" href="#configobj-huge-pages">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-huge_pages.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-huge_pages.xml</li><li><span class="ds-label">ID: </span>configobj-huge-pages</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>size</td><td>
      <p>
       The page size in kilobytes, megabytes, or gigabytes specified as
       <span class="emphasis"><em>n</em></span>X where:
      </p>
      <div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.4.10.8.2.1.4.1.2.2.1"><span class="term "><span class="emphasis"><em>n</em></span>
        </span></dt><dd><p>
          is an integer greater than zero
         </p></dd><dt id="id-1.3.4.4.10.8.2.1.4.1.2.2.2"><span class="term ">X</span></dt><dd><p>
          is one of "K", "M" or "G"
         </p></dd></dl></div>
     </td></tr><tr><td>count</td><td>The number of pages of this size to create (must be greater than zero).</td></tr><tr><td>numa-node (optional) </td><td>
      <p>
       If specified the pages will be created in the memory associated with
       this numa node.
      </p>
      <p>
       If not specified the pages are distributed across numa nodes by the
       operating system.
      </p>
     </td></tr></tbody></table></div></div></div><div class="sect1" id="configobj-cpumodels"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">
  CPU Models</span> <a title="Permalink" class="permalink" href="#configobj-cpumodels">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-cpumodels.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-cpumodels.xml</li><li><span class="ds-label">ID: </span>configobj-cpumodels</li></ul></div></div></div></div><p>
  The <code class="literal">cpu-models</code> configuration object describes how CPUs are
  assigned for use by service components such as nova (for VMs) and Open
  vSwitch (for DPDK), and whether or not those CPUs are isolated from the
  general kernel SMP balancing and scheduling algorithms.
  It also describes the number of vCPUs for virtual machine servers.
 </p><div class="verbatim-wrap"><pre class="screen">---
  product:
     version: 2

  cpu-models:
    - name: COMPUTE-CPU
      assignments:
        - components:
            - nova-compute-kvm
          cpu:
            - processor-ids: 0-1,3,5-7
              role: vm
        - components:
            - openvswitch
          cpu:
            - processor-ids: 4,12
              isolate: False
              role: eal
            - processor-ids: 2,10
              role: pmd
    - name: VIRTUAL-CONTROLLER-CPU
      vm-size:
         vcpus: 4</pre></div><p>
  <span class="bold"><strong>cpu-models</strong></span>
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>An administrator-defined name for the cpu model.</td></tr><tr><td>assignments</td><td>A list of CPU assignments .</td></tr></tbody></table></div><div class="sect2" id="configobj-cpu-assignments"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">CPU Assignments</span> <a title="Permalink" class="permalink" href="#configobj-cpu-assignments">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-cpu_assignments.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-cpu_assignments.xml</li><li><span class="ds-label">ID: </span>configobj-cpu-assignments</li></ul></div></div></div></div><p>
  <span class="bold"><strong>assignments</strong></span>
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>components</td><td>A list of components to which the CPUs will be assigned.</td></tr><tr><td>cpu</td><td>
      A list of CPU usage objects (see <a class="xref" href="#configobj-cpu-usage" title="6.10.2. CPU Usage">Section 6.10.2, “CPU Usage”</a>
      below).
     </td></tr></tbody></table></div></div><div class="sect2" id="configobj-cpu-usage"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.10.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">CPU Usage</span> <a title="Permalink" class="permalink" href="#configobj-cpu-usage">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-cpu_usage.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-cpu_usage.xml</li><li><span class="ds-label">ID: </span>configobj-cpu-usage</li></ul></div></div></div></div><p>
  <span class="bold"><strong>cpu</strong></span>
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>processor-ids</td><td>A list of CPU IDs as seen by the operating system.</td></tr><tr><td>isolate (optional) </td><td>
      <p>
       A Boolean value which indicates if the CPUs are to be isolated from the
       general kernel SMP balancing and scheduling algorithms. The specified
       processor IDs will be configured in the Linux kernel isolcpus parameter.
      </p>
      <p>
       The default value is True.
      </p>
     </td></tr><tr><td>role</td><td>A role within the component for which the CPUs will be used.</td></tr></tbody></table></div></div><div class="sect2" id="configobj-cpu-components-roles"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.10.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Components and Roles in the CPU Model</span> <a title="Permalink" class="permalink" href="#configobj-cpu-components-roles">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-cpu_components_roles.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-cpu_components_roles.xml</li><li><span class="ds-label">ID: </span>configobj-cpu-components-roles</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Component</th><th>Role</th><th>Description</th></tr></thead><tbody><tr><td>nova-compute-kvm</td><td>vm</td><td>
      <p>
       The specified processor IDs will be configured in the nova
       vcpu_pin_set option.
      </p>
     </td></tr><tr><td rowspan="2">openvswitch</td><td>eal</td><td>
      <p>
       The specified processor IDs will be configured in the Open vSwitch
       DPDK EAL -c (coremask) option. Refer to the DPDK documentation for
       details.
      </p>
     </td></tr><tr><td>pmd</td><td>
      <p>
       The specified processor IDs will be configured in the <span class="productname">Open vSwitch</span>
       pmd-cpu-mask option. Refer to the <span class="productname">Open vSwitch</span> documentation and the
       ovs-vswitchd.conf.db man page for details.
      </p>
     </td></tr></tbody></table></div></div></div><div class="sect1" id="configobj-interfacemodels"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Interface Models</span> <a title="Permalink" class="permalink" href="#configobj-interfacemodels">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-interfacemodels.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-interfacemodels.xml</li><li><span class="ds-label">ID: </span>configobj-interfacemodels</li></ul></div></div></div></div><p>
  The interface-models configuration object describes how network interfaces
  are bonded and the mapping of network groups onto interfaces. Interface
  devices are identified by name and mapped to a particular physical port by
  the <span class="guimenu ">nic-mapping</span> (see <a class="xref" href="#concept-nicmapping" title="5.2.10.4. NIC Mapping">Section 5.2.10.4, “NIC Mapping”</a>).
 </p><div class="verbatim-wrap"><pre class="screen">---
  product:
     version: 2

  interface-models:
     - name: INTERFACE_SET_CONTROLLER
       network-interfaces:
          - name: BONDED_INTERFACE
            device:
              name: bond0
            bond-data:
              provider: linux
              devices:
                - name: hed3
                - name: hed4
              options:
                mode: active-backup
                miimon: 200
                primary: hed3
            network-groups:
               - EXTERNAL_API
               - EXTERNAL_VM
               - GUEST

          - name: UNBONDED_INTERFACE
            device:
               name: hed0
            network-groups:
               - MGMT


       fcoe-interfaces:
          - name: FCOE_DEVICES
            devices:
              - eth7
              - eth8


     - name: INTERFACE_SET_DPDK
       network-interfaces:
          - name: BONDED_DPDK_INTERFACE
            device:
              name: bond0
            bond-data:
              provider: openvswitch
              devices:
                - name: dpdk0
                - name: dpdk1
              options:
                mode: active-backup
            network-groups:
               - GUEST
          - name: UNBONDED_DPDK_INTERFACE
            device:
               name: dpdk2
            network-groups:
               - PHYSNET2
       dpdk-devices:
         - devices:
             - name: dpdk0
             - name: dpdk1
             - name: dpdk2
               driver: igb_uio
           components:
             - openvswitch
           eal-options:
             - name: socket-mem
               value: 1024,0
             - name: n
               value: 2
           component-options:
             - name: n-dpdk-rxqs
               value: 64</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>An administrator-defined name for the interface model.</td></tr><tr><td>network-interfaces</td><td>A list of network interface definitions.</td></tr><tr><td>
      fcoe-interfaces (optional): <a class="xref" href="#configobj-fcoeinterfaces" title="6.11.2. fcoe-interfaces">Section 6.11.2, “fcoe-interfaces”</a>
      </td><td>
      <p>
       A list of network interfaces that will be used for Fibre Channel over
       Ethernet (FCoE). This is only needed for devices that present as a
       native FCoE device, not cards such as Emulex which present FCoE as a FC
       device.
      </p>
     </td></tr><tr><td>dpdk-devices (optional)</td><td>A list of DPDK device definitions.</td></tr></tbody></table></div><div id="id-1.3.4.4.12.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   The devices must be <span class="quote">“<span class="quote ">raw</span>”</span> device names, not names controlled
   via a nic-mapping.
  </p></div><div class="sect2" id="configobj-network-interfaces"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.11.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">
  network-interfaces</span> <a title="Permalink" class="permalink" href="#configobj-network-interfaces">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-network_interfaces.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-network_interfaces.xml</li><li><span class="ds-label">ID: </span>configobj-network-interfaces</li></ul></div></div></div></div><p>
  The network-interfaces configuration object has the following attributes:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>An administrator-defined name for the interface</td></tr><tr><td>device</td><td>
      <p>
       A dictionary containing the network device name (as seen on the
       associated server) and associated properties (see
       <a class="xref" href="#configobj-network-interfaces-device" title="6.11.1.1. network-interfaces device">Section 6.11.1.1, “network-interfaces device”</a> for details).
      </p>
      
     </td></tr><tr><td>network-groups (optional if forced-network-groups is defined)</td><td>
      A list of one or more <span class="guimenu ">network-groups</span> (see
      <a class="xref" href="#configobj-networkgroups" title="6.13. Network Groups">Section 6.13, “Network Groups”</a>) containing
      <span class="guimenu ">networks</span> (see <a class="xref" href="#configobj-networks" title="6.14. Networks">Section 6.14, “Networks”</a>)
      that can be accessed via this interface. Networks in these groups will
      only be configured if there is at least one
      <span class="guimenu ">service-component</span> on the server which matches the
      list of component-endpoints defined in the
      <span class="guimenu ">network-group</span>.
     </td></tr><tr><td>forced-network-groups (optional if network-groups is defined)</td><td>
      A list of one or more <span class="guimenu ">network-groups</span> (see
      <a class="xref" href="#configobj-networkgroups" title="6.13. Network Groups">Section 6.13, “Network Groups”</a>) containing
      <span class="guimenu ">networks</span> (see <a class="xref" href="#configobj-networks" title="6.14. Networks">Section 6.14, “Networks”</a>)
      that can be accessed via this interface. Networks in these groups are
      always configured on the server.
     </td></tr><tr><td>passthrough-network-groups (optional)</td><td>
      A list of one or more network-groups (see
      <a class="xref" href="#configobj-networkgroups" title="6.13. Network Groups">Section 6.13, “Network Groups”</a>) containing networks (see
      <a class="xref" href="#configobj-networks" title="6.14. Networks">Section 6.14, “Networks”</a>) that can be accessed by servers
      running as virtual machines on an Cloud Lifecycle Manager hypervisor server. Networks in
      these groups are not configured on the Cloud Lifecycle Manager hypervisor server unless
      they also are specified in the <code class="literal">network-groups</code> or
      <code class="literal">forced-network-groups</code> attributes.
     </td></tr></tbody></table></div><div class="sect3" id="configobj-network-interfaces-device"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.11.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">network-interfaces device</span> <a title="Permalink" class="permalink" href="#configobj-network-interfaces-device">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-network_interfaces_device.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-network_interfaces_device.xml</li><li><span class="ds-label">ID: </span>configobj-network-interfaces-device</li></ul></div></div></div></div><p>
  <span class="bold"><strong>network-interfaces device</strong></span>
 </p><p>
  The network-interfaces device configuration object has the following
  attributes:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>
      <p>
       When configuring a bond, this is used as the bond device name - the
       names of the devices to be bonded are specified in the bond-data
       section.
      </p>
      <p>
       If the interface is not bonded, this must be the name of the device
       specified by the nic-mapping (see NIC Mapping).
       
      </p>
     </td></tr><tr><td>vf-count (optional)</td><td>
      <p>
       Indicates that the interface is to be used for SR-IOV. The value is the
       number of virtual functions to be created. The associated device
       specified by the nic-mapping must have a valid nice-device-type.
      </p>
      <p>
       vf-count cannot be specified on bonded interfaces
      </p>
      <p>
       Interfaces used for SR-IOV must be associated with a network with
       <code class="literal">tagged-vlan: false</code>.
      </p>
     </td></tr><tr><td>sriov-only (optional) </td><td>
      <p>
       Only valid when vf-count is specified. If set to true then the interface
       is to be used for virtual functions only and the physical function will
       not be used.
      </p>
      <p>
       The default value is False.
      </p>
     </td></tr><tr><td>pci-pt (optional) </td><td>
      <p>
       If set to true then the interface is used for PCI passthrough.
      </p>
      <p>
       The default value is False.
      </p>
     </td></tr></tbody></table></div></div></div><div class="sect2" id="configobj-fcoeinterfaces"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.11.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">fcoe-interfaces</span> <a title="Permalink" class="permalink" href="#configobj-fcoeinterfaces">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-fcoeinterfaces.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-fcoeinterfaces.xml</li><li><span class="ds-label">ID: </span>configobj-fcoeinterfaces</li></ul></div></div></div></div><p>
  The fcoe-interfaces configuration object has the following attributes:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>An administrator-defined name for the group of FCOE interfaces </td></tr><tr><td>devices</td><td>
      <p>
       A list of network devices that will be configured for FCOE
      </p>
      <p>
       Entries in this must be the name of a device specified by the
       nic-mapping (see <a class="xref" href="#configobj-nicmappings" title="6.12. NIC Mappings">Section 6.12, “NIC Mappings”</a>).
      </p>
     </td></tr></tbody></table></div></div><div class="sect2" id="configobj-dpdkdevices"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.11.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">dpdk-devices</span> <a title="Permalink" class="permalink" href="#configobj-dpdkdevices">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-dpdkdevices.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-dpdkdevices.xml</li><li><span class="ds-label">ID: </span>configobj-dpdkdevices</li></ul></div></div></div></div><p>
  The dpdk-devices configuration object has the following attributes:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Descriptions</th></tr></thead><tbody><tr><td>devices</td><td>
      <p>
       A list of network devices to be configured for DPDK. See
       <a class="xref" href="#configobj-dpdkdevices-devices" title="6.11.3.1.  dpdk-devices devices">Section 6.11.3.1, “
  dpdk-devices devices”</a>.
      </p>
     </td></tr><tr><td>eal-options</td><td>
      <p>
       A list of key-value pairs that may be used to set DPDK Environmental
       Abstraction Layer (EAL) options. Refer to the DPDK documentation for
       details.
      </p>
      <p>
       Note that the cpu-model should be used to specify the processor IDs to
       be used by EAL for this component. The EAL coremask
       (<code class="literal">-c</code>) option will be set automatically based on the
       information in the cpu-model, and so should not be specified here. See
       <a class="xref" href="#configobj-cpumodels" title="6.10.  CPU Models">Section 6.10, “
  CPU Models”</a>.
      </p>
     </td></tr><tr><td>component-options</td><td>
      <p>
       A list of key-value pairs that may be used to set component-specific
       configuration options.
      </p>
     </td></tr></tbody></table></div><div class="sect3" id="configobj-dpdkdevices-devices"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.11.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">
  dpdk-devices devices</span> <a title="Permalink" class="permalink" href="#configobj-dpdkdevices-devices">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-dpdkdevices_devices.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-dpdkdevices_devices.xml</li><li><span class="ds-label">ID: </span>configobj-dpdkdevices-devices</li></ul></div></div></div></div><p>
  The devices configuration object within dpdk-devices has the following
  attributes:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Descriptions</th></tr></thead><tbody><tr><td>name</td><td>The name of a network device to be used with DPDK. The device names must be the
            logical-name specified by the nic-mapping
            (see <a class="xref" href="#configobj-nicmappings" title="6.12. NIC Mappings">Section 6.12, “NIC Mappings”</a>).</td></tr><tr><td>driver (optional) </td><td>
      <p>
       Defines the userspace I/O driver to be used for network devices where
       the native device driver does not provide userspace I/O capabilities.
      </p>
      <p>
       The default value is <code class="literal">igb_uio</code>.
      </p>
     </td></tr></tbody></table></div></div><div class="sect3" id="configobj-dpdk-componentoptions"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.11.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DPDK component-options for the openvswitch component</span> <a title="Permalink" class="permalink" href="#configobj-dpdk-componentoptions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-dpdk_componentoptions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-dpdk_componentoptions.xml</li><li><span class="ds-label">ID: </span>configobj-dpdk-componentoptions</li></ul></div></div></div></div><p>
  The following options are supported for use with the openvswitch component:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Name</th><th>Value Descriptions</th></tr></thead><tbody><tr><td>n-dpdk-rxqs</td><td>
      <p>
       Number of rx queues for each DPDK interface. Refer to the Open vSwitch
       documentation and the <code class="literal">ovs-vswitchd.conf.db</code> man page
       for details.
      </p>
     </td></tr></tbody></table></div><p>
  Note that the cpu-model should be used to define the CPU affinity of the <span class="productname">Open vSwitch</span>
  PMD (Poll Mode Driver) threads. The <span class="productname">Open vSwitch</span>
  <code class="literal">pmd-cpu-mask</code> option will be set automatically based on the
  information in the cpu-model. See <a class="xref" href="#configobj-cpumodels" title="6.10.  CPU Models">Section 6.10, “
  CPU Models”</a>.
 </p></div></div></div><div class="sect1" id="configobj-nicmappings"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">NIC Mappings</span> <a title="Permalink" class="permalink" href="#configobj-nicmappings">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-nicmappings.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-nicmappings.xml</li><li><span class="ds-label">ID: </span>configobj-nicmappings</li></ul></div></div></div></div><p>
  The <span class="guimenu ">nic-mappings</span> configuration object is used to ensure
  that the network device name used by the operating system always maps to the
  same physical device. A <span class="guimenu ">nic-mapping</span> is associated to a
  <span class="guimenu ">server</span> in the server definition file.  Devices should be named <code class="literal">hedN</code> to
  avoid name clashes with any other devices configured during the operating
  system install as well as any interfaces that are not being managed by
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, ensuring that all devices on a baremetal machine are specified in
  the file. An excerpt from <code class="filename">nic_mappings.yml</code> illustrates:
 </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  nic-mappings:

    - name: HP-DL360-4PORT
      physical-ports:
        - logical-name: hed1
          type: simple-port
          bus-address: "0000:07:00.0"

        - logical-name: hed2
          type: simple-port
          bus-address: "0000:08:00.0"
          nic-device-type: '8086:10fb'

        - logical-name: hed3
          type: multi-port
          bus-address: "0000:09:00.0"
          port-attributes:
              port-num: 0

        - logical-name: hed4
          type: multi-port
          bus-address: "0000:09:00.0"
          port-attributes:
              port-num: 1</pre></div><p>
  Each entry in the <span class="guimenu ">nic-mappings</span> list has the following
  attributes:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>
      An administrator-defined name for the mapping. This name may be used in
      a server definition (see <a class="xref" href="#configobj-servers" title="6.5. Servers">Section 6.5, “Servers”</a>) to apply
      the mapping to that server.
     </td></tr><tr><td>physical-ports</td><td>A list containing device name to address mapping information.</td></tr></tbody></table></div><p>
  Each entry in the <span class="guimenu ">physical-ports</span> list has the following
  attributes:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>logical-name</td><td>
      The network device name that will be associated with the device at the
      specified <span class="emphasis"><em>bus-address</em></span>. The logical-name specified
      here can be used as a device name in network interface model
      definitions. (See <a class="xref" href="#configobj-interfacemodels" title="6.11. Interface Models">Section 6.11, “Interface Models”</a>.)
     </td></tr><tr><td>type</td><td>
      <p>
       The type of port. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 supports "simple-port" and
       "multi-port". Use "simple-port" if your device has a unique bus-address.
       Use "multi-port" if your hardware requires a "port-num" attribute to
       identify a single port on a multi-port device. An examples of such a
       device is:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Mellanox Technologies MT26438 [ConnectX VPI PCIe 2.0 5GT/s - IB QDR /
         10GigE Virtualization+]
        </p></li></ul></div>
     </td></tr><tr><td>bus-address</td><td>
      PCI bus address of the port. Enclose the bus address in quotation marks
      so yaml does not misinterpret the embedded colon
      (<code class="literal">:</code>) characters. See
      <a class="xref" href="#preinstall-checklist" title="Chapter 14. Pre-Installation Checklist">Chapter 14, <em>Pre-Installation Checklist</em></a> for details on how to determine
      this value. </td></tr><tr><td>
      port-attributes (required if type is
      <code class="literal">multi-port</code>)
     </td><td>
      Provides a list of attributes for the physical port. The current
      implementation supports only one attribute, "port-num". Multi-port
      devices share a bus-address. Use the "port-num" attribute to identify
      which physical port on the multi-port device to map. See
      <a class="xref" href="#preinstall-checklist" title="Chapter 14. Pre-Installation Checklist">Chapter 14, <em>Pre-Installation Checklist</em></a> for details on how to determine
      this value.</td></tr><tr><td>nic-device-type (optional) </td><td>
      Specifies the PCI vendor ID and device ID of the port in the format of
      <code class="literal"><em class="replaceable ">VENDOR_ID</em>:<em class="replaceable ">DEVICE_ID</em></code>, for example,
      <code class="literal">8086:10fb</code>.
     </td></tr></tbody></table></div></div><div class="sect1" id="configobj-networkgroups"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Groups</span> <a title="Permalink" class="permalink" href="#configobj-networkgroups">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-networkgroups.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-networkgroups.xml</li><li><span class="ds-label">ID: </span>configobj-networkgroups</li></ul></div></div></div></div><p>
  Network-groups define the overall network topology, including where
  service-components connect, what load balancers are to be deployed, which
  connections use TLS, and network routing. They also provide the data needed
  to map neutron's network configuration to the physical networking.
 </p><div id="id-1.3.4.4.14.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   The name of the "MANAGEMENT" <span class="guimenu ">network-group</span> cannot be
   changed. It must be upper case. Every SUSE <span class="productname">OpenStack</span> Cloud requires this network group in
   order to be valid.
  </p></div><div class="verbatim-wrap"><pre class="screen">---
  product:
     version: 2

  network-groups:

     - name: EXTERNAL-API
       hostname-suffix: extapi

       load-balancers:
         - provider: ip-cluster
           name: extlb
           external-name:

           tls-components:
             - default
           roles:
            - public
           cert-file: my-public-entry-scale-kvm-cert

      - name: EXTERNAL-VM
        tags:
          - neutron.l3_agent.external_network_bridge

      - name: GUEST
        hostname-suffix: guest
        tags:
          - neutron.networks.vxlan

      - name: MANAGEMENT
        hostname-suffix: mgmt
        hostname: true

        component-endpoints:
          - default

        routes:
          - default

        load-balancers:
          - provider: ip-cluster
            name: lb
            components:
              - default
            roles:
              - internal
              - admin

        tags:
          - neutron.networks.vlan:
              provider-physical-network: physnet1</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>
      An administrator-defined name for the network group. The name is used
      to make references from other parts of the input model.
     </td></tr><tr><td>component-endpoints (optional)</td><td>
      The list of <span class="guimenu ">service-components</span> that will bind to or
      need direct access to networks in this network-group.</td></tr><tr><td>hostname (optional)</td><td>
      <p>
       If set to true, the name of the address associated with a network in
       this group will be used to set the hostname of the server.
      </p>
     </td></tr><tr><td>hostname-suffix (optional)</td><td>
      If supplied, this string will be used in the name generation (see
      <a class="xref" href="#namegeneration" title="7.2. Name Generation">Section 7.2, “Name Generation”</a>). If not specified, the name of the
      network-group will be used.
     </td></tr><tr><td>load-balancers (optional)</td><td>
      <p>
       A list of load balancers to be configured on networks in this
       network-group. Because load balances need a virtual IP address, any
       network group that contains a load balancer can only have one network
       associated with it.
      </p>
      <p>
       For clouds consisting of a single control plane, a load balancer may be
       fully defined within a <code class="literal">network-group</code> object. See Load
       balancer definitions in network groups.
      </p>
      <p>
       Starting in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9, a load balancer may be defined within a
       <code class="literal">control-plane</code> object and referenced by name from a
       <code class="literal">network-group</code> object. See
       <a class="xref" href="#configobj-lb-defs-networkgroups" title="6.13.1. Load Balancer Definitions in Network Groups">Section 6.13.1, “Load Balancer Definitions in Network Groups”</a>
       in control planes.
      </p>
     </td></tr><tr><td>routes (optional)</td><td>
      <p>
       A list of <span class="guimenu ">network-groups</span> that networks in this group
       provide access to via their gateway. This can include the value
       <code class="literal">default</code> to define the default route.
      </p>
      <p>
       A network group with no services attached to it can be used to define
       routes to external networks.
      </p>
      <p>
       The name of a neutron provide network defined via configuration-data
       (see <a class="xref" href="#configobj-neutron-provider-networks" title="6.16.2.1. neutron-provider-networks">Section 6.16.2.1, “neutron-provider-networks”</a>) can also be
       included in this list.
      </p>
     </td></tr><tr><td>tags (optional)</td><td>
      <p>
       A list of network tags. Tags provide the linkage between the physical
       network configuration and the neutron network configuration.
      </p>
      <p>
       Starting in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9, network tags may be defined as part of a
       neutron <code class="literal">configuration-data</code> object rather than as part
       of a <code class="literal">network-group</code> object (see
       <a class="xref" href="#configobj-configurationdata-neutron" title="6.16.2. Neutron Configuration Data">Section 6.16.2, “Neutron Configuration Data”</a>).
      </p>
     </td></tr><tr><td>mtu (optional)</td><td>
      <p>
       Specifies the MTU value required for networks in this network group If
       not specified a default value of 1500 is used.
      </p>
      <p>
       See <a class="xref" href="#configobj-mtu" title="6.13.3. MTU (Maximum Transmission Unit)">Section 6.13.3, “MTU (Maximum Transmission Unit)”</a> on how MTU settings are applied to
       interfaces when there are multiple tagged networks on the same
       interface.
      </p>
     </td></tr></tbody></table></div><div id="id-1.3.4.4.14.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   <code class="literal">hostname</code><span class="bold"><strong>must</strong></span> be set to
   <code class="literal">true</code> for one, and only one, of your network groups.
  </p></div><p>
  A load balancer definition has the following attributes:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>An administrator-defined name for the load balancer.</td></tr><tr><td>provider</td><td>
      The service component that implements the load balancer. Currently only
      <code class="literal">ip-cluster</code> (ha-proxy) is supported. Future releases
      will provide support for external load balancers.
     </td></tr><tr><td>roles</td><td>
      The list of endpoint roles that this load balancer provides (see
      below). Valid roles are "public", "internal", and "admin'. To ensure
      separation of concerns, the role "public" cannot be combined with any
      other role. See <a class="xref" href="#concept-loadbalancers" title="5.2.10.1.1. Load Balancers">Section 5.2.10.1.1, “Load Balancers”</a> for an example
      of how the role provides endpoint separation.
     </td></tr><tr><td>components (optional)</td><td>The list of <span class="guimenu ">service-components</span> for which the load
                        balancer provides a non-encrypted virtual IP address for.</td></tr><tr><td>tls-components (optional)</td><td>
      The list of <span class="guimenu ">service-components</span> for which the load
      balancer provides TLS-terminated virtual IP addresses for. In
      <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, TLS is supported both for internal and public endpoints.
     </td></tr><tr><td>external-name (optional)</td><td>
      The name to be registered in keystone for the publicURL. If not
      specified, the virtual IP address will be registered. Note that this
      value cannot be changed after the initial deployment.
     </td></tr><tr><td>cert-file (optional)</td><td>
      The name of the certificate file to be used for TLS endpoints.
     </td></tr></tbody></table></div><div class="sect2" id="configobj-lb-defs-networkgroups"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.13.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Load Balancer Definitions in Network Groups</span> <a title="Permalink" class="permalink" href="#configobj-lb-defs-networkgroups">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-lb_defs_networkgroups.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-lb_defs_networkgroups.xml</li><li><span class="ds-label">ID: </span>configobj-lb-defs-networkgroups</li></ul></div></div></div></div><p>
  In a cloud consisting of a single control-plane, a
  <code class="literal">load-balancer</code> may be fully defined within a
  <code class="literal">network-groups</code> object as shown in the examples above. See
  section <a class="xref" href="#configobj-load-balancers" title="6.3. Load Balancers">Section 6.3, “Load Balancers”</a> for a complete description
  of load balancer attributes.
 </p><p>
  Starting in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9, a <code class="literal">load-balancer</code> may be
  defined within a <code class="literal">control-plane</code> object in which case the
  network-group provides just a list of load balancer names as shown below. See
  section <a class="xref" href="#configobj-load-balancers" title="6.3. Load Balancers">Section 6.3, “Load Balancers”</a> definitions in control
  planes.
 </p><div class="verbatim-wrap"><pre class="screen">network-groups:

     - name: EXTERNAL-API
       hostname-suffix: extapi

       load-balancers:
         - lb-cp1
         - lb-cp2</pre></div><p>
  The same load balancer name can be used in multiple control-planes to make
  the above list simpler.
 </p></div><div class="sect2" id="configobj-networktags"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.13.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Tags</span> <a title="Permalink" class="permalink" href="#configobj-networktags">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-networktags.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-networktags.xml</li><li><span class="ds-label">ID: </span>configobj-networktags</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> supports a small number of network tags which may be used to convey
  information between the input model and the service components (currently
  only neutron uses network tags). A network tag consists minimally of a tag
  name; but some network tags have additional attributes.
 </p><div class="table" id="neutron-networks-vxlan"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 6.1: </span><span class="name">neutron.networks.vxlan </span><a title="Permalink" class="permalink" href="#neutron-networks-vxlan">#</a></h6></div><div class="table-contents"><table class="table" summary="neutron.networks.vxlan" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Tag</th><th>Value Description</th></tr></thead><tbody><tr><td>neutron.networks.vxlan</td><td>This tag causes neutron to be configured to use VxLAN as the underlay for
                        tenant networks. The associated network group will carry the VxLAN
                        traffic.</td></tr><tr><td>tenant-vxlan-id-range (optional)</td><td>Used to specify the VxLAN identifier range in the format
     <span class="quote">“<span class="quote "><em class="replaceable ">MIN-ID</em>:<em class="replaceable ">MAX-ID</em></span>”</span>. The
     default range is <span class="quote">“<span class="quote ">1001:65535</span>”</span>. Enclose the range in quotation
     marks. Multiple ranges can be specified as a comma-separated
     list. </td></tr></tbody></table></div></div><p>
  Example using the default ID range:
 </p><div class="verbatim-wrap"><pre class="screen">  tags:
    - neutron.networks.vxlan</pre></div><p>
  Example using a user-defined ID range:
 </p><div class="verbatim-wrap"><pre class="screen">  tags:
    - neutron.networks.vxlan:
        tenant-vxlan-id-range: “1:20000”</pre></div><p>
  Example using multiple user-defined ID range:
 </p><div class="verbatim-wrap"><pre class="screen">  tags:
    - neutron.networks.vxlan:
        tenant-vxlan-id-range: “1:2000,3000:4000,5000:6000”</pre></div><div class="table" id="neutron-networks-vlan"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 6.2: </span><span class="name">neutron.networks.vlan </span><a title="Permalink" class="permalink" href="#neutron-networks-vlan">#</a></h6></div><div class="table-contents"><table class="table" summary="neutron.networks.vlan" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Tag</th><th>Value Description</th></tr></thead><tbody><tr><td>neutron.networks.vlan</td><td>
      <p>
       This tag causes neutron to be configured for provider VLAN networks, and
       optionally to use VLAN as the underlay for tenant networks. The
       associated network group will carry the VLAN traffic. This tag can be
       specified on multiple network groups. However, this tag does not cause
       any neutron networks to be created, that must be done in neutron after
       the cloud is deployed.
      </p>
     </td></tr><tr><td>provider-physical-network</td><td>The provider network name. This is the name to be used in the neutron API
                        for the <span class="emphasis"><em>provider:physical_network</em></span> parameter of network
                        objects.</td></tr><tr><td>tenant-vlan-id-range (optional)</td><td>This attribute causes neutron to use VLAN for tenant networks; omit
     this attribute if you are using provider VLANs only. It specifies the VLAN
     ID range for tenant networks, in the format
     <span class="quote">“<span class="quote "><em class="replaceable ">MIN-ID</em>:<em class="replaceable ">MAX-ID</em></span>”</span>. Enclose
     the range in quotation marks. Multiple ranges can be specified as a
     comma-separated list.</td></tr></tbody></table></div></div><p>
  Example using a provider vlan only (may be used with tenant VxLAN):
 </p><div class="verbatim-wrap"><pre class="screen">  tags:
    - neutron.networks.vlan:
        provider-physical-network: physnet1</pre></div><p>
  Example using a tenant and provider VLAN:
 </p><div class="verbatim-wrap"><pre class="screen">  tags:
    - neutron.networks.vlan:
        provider-physical-network: physnet1
        tenant-vlan-id-range: “30:50,100:200”</pre></div><div class="table" id="neutron-networks-flat"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 6.3: </span><span class="name">neutron.networks.flat </span><a title="Permalink" class="permalink" href="#neutron-networks-flat">#</a></h6></div><div class="table-contents"><table class="table" summary="neutron.networks.flat" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Tag</th><th>Value Description</th></tr></thead><tbody><tr><td>neutron.networks.flat</td><td>
      <p>
       This tag causes neutron to be configured for provider flat networks. The
       associated network group will carry the traffic. This tag can be
       specified on multiple network groups. However, this tag does not cause
       any neutron networks to be created, that must be done in neutron after
       the cloud is deployed.
      </p>
     </td></tr><tr><td>provider-physical-network</td><td>The provider network name. This is the name to be used in the neutron API
                        for the <span class="emphasis"><em>provider:physical_network</em></span> parameter of network
                        objects. When specified on multiple network groups, the name must be unique
                        for each network group.</td></tr></tbody></table></div></div><p>
  Example using a provider flat network:
 </p><div class="verbatim-wrap"><pre class="screen">  tags:
    - neutron.networks.flat:
        provider-physical-network: flatnet1</pre></div><div class="table" id="neutron-l3-agent"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 6.4: </span><span class="name">neutron.l3_agent.external_network_bridge </span><a title="Permalink" class="permalink" href="#neutron-l3-agent">#</a></h6></div><div class="table-contents"><table class="table" summary="neutron.l3_agent.external_network_bridge" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Tag</th><th>Value Description</th></tr></thead><tbody><tr><td>neutron.l3_agent.external_network_bridge</td><td>
      <p>
       This tag causes the neutron L3 Agent to be configured to use the
       associated network group as the neutron external network for floating IP
       addresses. A CIDR <span class="bold"><strong>should not</strong></span> be defined
       for the associated physical network, as that will cause addresses from
       that network to be configured in the hypervisor. When this tag is used,
       provider networks cannot be used as external networks. However, this tag
       does not cause a neutron external networks to be created, that must be
       done in neutron after the cloud is deployed.
      </p>
     </td></tr></tbody></table></div></div><p>
  Example using neutron.l3_agent.external_network_bridge:
 </p><div class="verbatim-wrap"><pre class="screen">  tags:
    - neutron.l3_agent.external_network_bridge</pre></div></div><div class="sect2" id="configobj-mtu"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.13.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">MTU (Maximum Transmission Unit)</span> <a title="Permalink" class="permalink" href="#configobj-mtu">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-mtu.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-mtu.xml</li><li><span class="ds-label">ID: </span>configobj-mtu</li></ul></div></div></div></div><p>
  A network group may optionally specify an MTU for its networks to use.
  Because a network-interface in the interface-model may have a mix of one
  untagged-vlan network group and one or more tagged-vlan network groups, there
  are some special requirements when specifying an MTU on a network group.
 </p><p>
  If the network group consists of untagged-vlan network(s) then its specified
  MTU must be greater than or equal to the MTU of any tagged-vlan network
  groups which are co-located on the same network-interface.
 </p><p>
  For example consider a network group with untagged VLANs, NET-GROUP-1, which
  is going to share (via a Network Interface definition) a device (eth0) with
  two network groups with tagged VLANs: NET-GROUP-2 (ID=201, MTU=1550) and
  NET-GROUP-3 (ID=301, MTU=9000).
 </p><p>
  The device (eth0) must have an MTU which is large enough to accommodate the
  VLAN in NET-GROUP-3. Since NET-GROUP-1 has untagged VLANS it will also be
  using this device and so it must also have an MTU of 9000, which results in
  the following configuration.
 </p><div class="verbatim-wrap"><pre class="screen">    +eth0 (9000)   &lt;------ this MTU comes from NET-GROUP-1
    | |
    | |----+ vlan201@eth0 (1550)
    \------+ vlan301@eth0 (9000)</pre></div><p>
  Where an interface is used only by network groups with tagged VLANs the MTU
  of the device or bond will be set to the highest MTU value in those groups.
 </p><p>
  For example if bond0 is configured to be used by three network groups:
  NET-GROUP-1 (ID=101, MTU=3000), NET-GROUP-2 (ID=201, MTU=1550) and
  NET-GROUP-3 (ID=301, MTU=9000).
 </p><p>
  Then the resulting configuration would be:
 </p><div class="verbatim-wrap"><pre class="screen">    +bond0 (9000)   &lt;------ because of NET-GROUP-3
    | | |
    | | |--+vlan101@bond0 (3000)
    | |----+vlan201@bond0 (1550)
    |------+vlan301@bond0 (9000)</pre></div></div></div><div class="sect1" id="configobj-networks"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networks</span> <a title="Permalink" class="permalink" href="#configobj-networks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-networks.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-networks.xml</li><li><span class="ds-label">ID: </span>configobj-networks</li></ul></div></div></div></div><p>
  A network definition represents a physical L3 network used by the cloud
  infrastructure. Note that these are different from the network definitions
  that are created/configured in neutron, although some of the networks may be
  used by neutron.
 </p><div class="verbatim-wrap"><pre class="screen">---
   product:
     version: 2

   networks:
     - name: NET_EXTERNAL_VM
       vlanid: 102
       tagged-vlan: true
       network-group: EXTERNAL_VM

     - name: NET_GUEST
       vlanid: 103
       tagged-vlan: true
       cidr: 10.1.1.0/24
       gateway-ip: 10.1.1.1
       network-group: GUEST

     - name: NET_MGMT
       vlanid: 100
       tagged-vlan: false
       cidr: 10.2.1.0/24
       addresses:
       - 10.2.1.10-10.2.1.20
       - 10.2.1.24
       - 10.2.1.30-10.2.1.36
       gateway-ip: 10.2.1.1
       network-group: MGMT</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>
      The name of this network. The network <span class="emphasis"><em>name</em></span> may be
      used in a server-group definition (see
      <a class="xref" href="#configobj-servergroups" title="6.6. Server Groups">Section 6.6, “Server Groups”</a>) to specify a particular
      network from within a network-group to be associated with a set of
      servers.
     </td></tr><tr><td>network-group</td><td>The name of the associated network group.</td></tr><tr><td>vlanid (optional)</td><td>
      The IEEE 802.1Q VLAN Identifier, a value in the range 1 through 4094. A
      <span class="emphasis"><em>vlanid</em></span> must be specified when
      <span class="emphasis"><em>tagged-vlan</em></span> is true.
     </td></tr><tr><td>tagged-vlan (optional)</td><td>
      May be set to <code class="literal">true</code> or <code class="literal">false</code>. If
      true, packets for this network carry the
      <span class="emphasis"><em>vlanid </em></span>in the packet header; such packets are
      referred to as VLAN-tagged frames in IEEE 1Q.
     </td></tr><tr><td>cidr (optional)</td><td>The IP subnet associated with this network.</td></tr><tr><td>addresses (optional)</td><td>
      <p>
       A list of IP addresses or IP address ranges (specified as
       <code class="literal"><em class="replaceable ">START_ADDRESS_RANGE</em>-<em class="replaceable ">END_ADDRESS_RANGE</em></code>
       from which server addresses may be allocated. The default value is the
       first host address within the CIDR (for example, the
       <code class="literal">.1</code> address).
      </p>
      <p>
       The <code class="literal">addresses</code> parameter provides more flexibility
       than the <code class="literal">start-address</code> and
       <code class="literal">end-address</code> parameters and so is the preferred means
       of specifying this data.
      </p>
     </td></tr><tr><td>start-address (optional) (deprecated)</td><td>
      <p>
       An IP address within the <span class="emphasis"><em>CIDR</em></span> which will be used as
       the start of the range of IP addresses from which server addresses may
       be allocated. The default value is the first host address within the
       <span class="emphasis"><em>CIDR</em></span> (for example, the .1 address).
      </p>
     </td></tr><tr><td>end-address (optional) (deprecated)</td><td>
      <p>
       An IP address within the <span class="emphasis"><em>CIDR</em></span> which will be used as
       the end of the range of IP addresses from which server addresses may be
       allocated. The default value is the last host address within the
       <span class="emphasis"><em>CIDR</em></span> (for example, the .254 address of a /24). This
       parameter is deprecated in favor of the new <code class="literal">addresses</code>
       parameter. This parameter may be removed in a future release.
      </p>
     </td></tr><tr><td>gateway-ip (optional)</td><td>
      The IP address of the gateway for this network. Gateway addresses must
      be specified if the associated
      <span class="guimenu ">network-group</span> provides routes.
     </td></tr></tbody></table></div></div><div class="sect1" id="configobj-firewallrules"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.15 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Firewall Rules</span> <a title="Permalink" class="permalink" href="#configobj-firewallrules">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-firewallrules.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-firewallrules.xml</li><li><span class="ds-label">ID: </span>configobj-firewallrules</li></ul></div></div></div></div><p>
  The configuration processor will automatically generate "allow" firewall
  rules for each server based on the services deployed and block all other
  ports. The firewall rules in the input model allow the customer to define
  additional rules for each network group.
 </p><p>
  Administrator-defined rules are applied after all rules generated by the
  Configuration Processor.
 </p><div class="verbatim-wrap"><pre class="screen">---
  product:
     version: 2

  firewall-rules:

     - name: PING
       network-groups:
       - MANAGEMENT
       - GUEST
       - EXTERNAL-API
       rules:
       # open ICMP echo request (ping)
       - type: allow
         remote-ip-prefix:  0.0.0.0/0
         # icmp type
         port-range-min: 8
         # icmp code
         port-range-max: 0
         protocol: icmp</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>An administrator-defined name for the group of rules.</td></tr><tr><td>network-groups</td><td>
      <p>
       A list of <span class="guimenu ">network-group</span> names that the rules apply
       to. A value of "all" matches all network-groups.
      </p>
     </td></tr><tr><td>rules</td><td>
      <p>
       A list of rules. Rules are applied in the order in which they appear
       in the list, apart from the control provided by the "final" option
       (see above). The order between sets of rules is indeterminate.
       
      </p>
     </td></tr></tbody></table></div><div class="sect2" id="configobj-rule"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.15.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Rule</span> <a title="Permalink" class="permalink" href="#configobj-rule">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-rule.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-rule.xml</li><li><span class="ds-label">ID: </span>configobj-rule</li></ul></div></div></div></div><p>
  Each rule in the list takes the following parameters (which match the
  parameters of a neutron security group rule):
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>type</td><td>Must <code class="literal">allow</code>
     </td></tr><tr><td>remote-ip-prefix</td><td>
      Range of remote addresses in CIDR format that this rule applies
      to.
     </td></tr><tr><td>
      <p>
       port-range-min
      </p>
      <p>
       port-range-max
      </p>
     </td><td>
      Defines the range of ports covered by the rule. Note that if the
      protocol is <code class="literal">icmp</code> then port-range-min is the ICMP
      type and port-range-max is the ICMP code.
     </td></tr><tr><td>protocol</td><td>
      Must be one of <code class="literal">tcp</code>, <code class="literal">udp</code>, or
      <code class="literal">icmp</code>. </td></tr></tbody></table></div></div></div><div class="sect1" id="configobj-configurationdata"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.16 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuration Data</span> <a title="Permalink" class="permalink" href="#configobj-configurationdata">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-configurationdata.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-configurationdata.xml</li><li><span class="ds-label">ID: </span>configobj-configurationdata</li></ul></div></div></div></div><p>
  Configuration data allows values to be passed into the model to be used in
  the context of a specific control plane or cluster. The content and format of
  the data is service specific.
 </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  configuration-data:
    - name:  NEUTRON-CONFIG-CP1
      services:
        - neutron
      data:
        neutron_provider_networks:
        - name: OCTAVIA-MGMT-NET
          provider:
            - network_type: vlan
              physical_network: physnet1
              segmentation_id: 106
          cidr: 172.30.1.0/24
          no_gateway:  True
          enable_dhcp: True
          allocation_pools:
            - start: 172.30.1.10
              end: 172.30.1.250
          host_routes:
            # route to MANAGEMENT-NET-1
            - destination: 192.168.245.0/24
              nexthop:  172.30.1.1

        neutron_external_networks:
        - name: ext-net
          cidr: 172.31.0.0/24
          gateway: 172.31.0.1
          provider:
            - network_type: vlan
              physical_network: physnet1
              segmentation_id: 107
          allocation_pools:
            - start: 172.31.0.2
              end: 172.31.0.254

      network-tags:
        - network-group: MANAGEMENT
          tags:
            - neutron.networks.vxlan
            - neutron.networks.vlan:
                provider-physical-network: physnet1
        - network-group: EXTERNAL-VM
          tags:
            - neutron.l3_agent.external_network_bridge</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>An administrator-defined name for the set of configuration data.</td></tr><tr><td>services</td><td>
      <p>
       A list of services that the data applies to. Note that these are
       service names (for example,
       <code class="literal">neutron</code>, <code class="literal">octavia</code>, etc.) not
       service-component names
       (<code class="literal">neutron-server</code>, <code class="literal">octavia-api</code>,
       etc.).
      </p>
     </td></tr><tr><td>data</td><td>A service specific data structure (see below). </td></tr><tr><td>network-tags (optional, neutron-only)</td><td>
      <p>
       A list of network tags. Tags provide the linkage between the physical
       network configuration and the neutron network configuration.
      </p>
      <p>
       Starting in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9, network tags may be defined as part of a
       neutron <code class="literal">configuration-data</code> object rather than as part
       of a <code class="literal">network-group</code> object.
      </p>
     </td></tr></tbody></table></div><div class="sect2" id="configobj-neutron-network-tags"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.16.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">neutron network-tags</span> <a title="Permalink" class="permalink" href="#configobj-neutron-network-tags">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-neutron_network_tags.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-neutron_network_tags.xml</li><li><span class="ds-label">ID: </span>configobj-neutron-network-tags</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>network-group</td><td>The name of the network-group with which the tags are associated.</td></tr><tr><td>tags</td><td>A list of network tags. Tags provide the linkage between the physical
                        network configuration and the neutron network configuration. See section
                        Network Tags. </td></tr></tbody></table></div></div><div class="sect2" id="configobj-configurationdata-neutron"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.16.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Neutron Configuration Data</span> <a title="Permalink" class="permalink" href="#configobj-configurationdata-neutron">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-configurationdata_neutron.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-configurationdata_neutron.xml</li><li><span class="ds-label">ID: </span>configobj-configurationdata-neutron</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>neutron-provider-networks</td><td>A list of provider networks that will be created in neutron.</td></tr><tr><td>neutron-external-networks</td><td>
      A list of external networks that will be created in neutron. These
      networks will have the “router:external” attribute set to True.
     </td></tr></tbody></table></div><div class="sect3" id="configobj-neutron-provider-networks"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.16.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">neutron-provider-networks</span> <a title="Permalink" class="permalink" href="#configobj-neutron-provider-networks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span>configobj-neutron-provider-networks</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>
      <p>
       The name for this network in neutron.
      </p>
      <p>
       This name must be distinct from the names of any Network Groups in the
       model to enable it to be included in the “routes” value of a network
       group.
      </p>
     </td></tr><tr><td>provider</td><td>
      <p>
       Details of network to be created
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         network_type
        </p></li><li class="listitem "><p>
         physical_network
        </p></li><li class="listitem "><p>
         segmentation_id
        </p></li></ul></div>
      <p>
       These values are passed as <code class="literal">--provider:</code> options to the
       <code class="command">openstack network create</code> command
      </p>
     </td></tr><tr><td>cidr</td><td>
      <p>
       The CIDR to use for the network. This is passed to the
       <code class="command">openstack subnet create</code> command.
      </p>
     </td></tr><tr><td>shared (optional)</td><td>
      <p>
       A Boolean value that specifies if the network can be shared.
      </p>
      <p>
       This value is passed to the <code class="command">openstack network create</code>
       command.
      </p>
     </td></tr><tr><td>allocation_pools (optional)</td><td>
      <p>
       A list of start and end address pairs that limit the set of IP addresses
       that can be allocated for this network.
      </p>
      <p>
       These values are passed to the <code class="command">openstack subnet
       create</code> command.
      </p>
     </td></tr><tr><td>host_routes (optional)</td><td>
      <p>
       A list of routes to be defined for the network. Each route consists of a
       <code class="literal">destination</code> in cidr format and a
       <code class="literal">nexthop</code> address.
      </p>
      <p>
       These values are passed to the <code class="command">openstack subnet create</code> command.
      </p>
     </td></tr><tr><td>gateway_ip (optional)</td><td>
      <p>
       A gateway address for the network.
      </p>
      <p>
       This value is passed to the <code class="command">openstack subnet create</code>
       command.
      </p>
     </td></tr><tr><td>no_gateway (optional)</td><td>
      <p>
       A Boolean value indicating that the gateway should not be distributed on
       this network.
      </p>
      <p>
       This is translated into the <code class="literal">no-gateway</code> option to the
       <code class="command">openstack subnet create</code> command.
      </p>
     </td></tr><tr><td>enable_dhcp (optional)</td><td>
      <p>
       A Boolean value indicating that DHCP should be enabled. The default if
       not specified is to not enable DHCP.
      </p>
      <p>
       This value is passed to the <code class="command">openstack subnet create</code> command.
      </p>
     </td></tr></tbody></table></div></div><div class="sect3" id="configobj-neutron-external-networks"><div class="titlepage"><div><div><h4 class="title"><span class="number">6.16.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">neutron-external-networks</span> <a title="Permalink" class="permalink" href="#configobj-neutron-external-networks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-neutron_external_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-neutron_external_networks.xml</li><li><span class="ds-label">ID: </span>configobj-neutron-external-networks</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>name</td><td>
      <p>
       The name for this network in neutron.
      </p>
      <p>
       This name must be distinct from the names of any Network Groups in the
       model to enable it to be included in the “routes” value of a network
       group.
      </p>
     </td></tr><tr><td>provider (optional)</td><td>
      <p>
       The provider attributes are specified when using neutron provider
       networks as external networks. Provider attributes should not be
       specified when the external network is configured with the
       <code class="literal">neutron.l3_agent.external_network_bridge</code>.
      </p>
      <p>
       Standard provider network attributes may be specified:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         network_type
        </p></li><li class="listitem "><p>
         physical_network
        </p></li><li class="listitem "><p>
         segmentation_id
        </p></li></ul></div>
      <p>
       These values are passed as <code class="literal">--provider:</code> options to the
       <code class="command">openstack network create</code> command
      </p>
     </td></tr><tr><td>cidr</td><td>
      <p>
       The CIDR to use for the network. This is passed to the
       <code class="command">openstack subnet create</code> command.
      </p>
     </td></tr><tr><td>allocation_pools (optional)</td><td>
      <p>
       A list of start and end address pairs that limit the set of IP addresses
       that can be allocated for this network.
      </p>
      <p>
       These values are passed to the <code class="command">openstack subnet create</code> command.
      </p>
     </td></tr><tr><td>gateway (optional)</td><td>
      <p>
       A gateway address for the network.
      </p>
      <p>
       This value is passed to the <code class="command">openstack subnet create</code> command.
      </p>
     </td></tr></tbody></table></div></div></div><div class="sect2" id="configobj-configurationdata-octavia"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.16.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Octavia Configuration Data</span> <a title="Permalink" class="permalink" href="#configobj-configurationdata-octavia">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-configurationdata_octavia.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-configurationdata_octavia.xml</li><li><span class="ds-label">ID: </span>configobj-configurationdata-octavia</li></ul></div></div></div></div><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  configuration-data:
    - name: OCTAVIA-CONFIG-CP1
      services:
        - octavia
      data:
        amp_network_name: OCTAVIA-MGMT-NET</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>amp_network_name</td><td>
      The name of the neutron provider network that Octavia will use for
      management access to load balancers.
     </td></tr></tbody></table></div></div><div class="sect2" id="configobj-configurationdata-ironic"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.16.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ironic Configuration Data</span> <a title="Permalink" class="permalink" href="#configobj-configurationdata-ironic">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-configurationdata_ironic.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-configurationdata_ironic.xml</li><li><span class="ds-label">ID: </span>configobj-configurationdata-ironic</li></ul></div></div></div></div><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  configuration-data:
    - name:  IRONIC-CONFIG-CP1
      services:
        - ironic
      data:
        cleaning_network: guest-network
        enable_node_cleaning: true
        enable_oneview: false

        oneview_manager_url:
        oneview_username:
        oneview_encrypted_password:
        oneview_allow_insecure_connections:
        tls_cacert_file:
        enable_agent_drivers: true</pre></div><p>
  Refer to the documentation on configuring ironic for details of the above
  attributes.
 </p></div><div class="sect2" id="configobj-configurationdata-swift"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.16.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Swift Configuration Data</span> <a title="Permalink" class="permalink" href="#configobj-configurationdata-swift">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-configurationdata_swift.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-configurationdata_swift.xml</li><li><span class="ds-label">ID: </span>configobj-configurationdata-swift</li></ul></div></div></div></div><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  configuration-data:
  - name: SWIFT-CONFIG-CP1
    services:
      - swift
    data:
      control_plane_rings:
        swift-zones:
          - id: 1
            server-groups:
              - AZ1
          - id: 2
            server-groups:
              - AZ2
          - id: 3
            server-groups:
              - AZ3
        rings:
          - name: account
            display-name: Account Ring
            min-part-hours: 16
            partition-power: 12
            replication-policy:
              replica-count: 3

          - name: container
            display-name: Container Ring
            min-part-hours: 16
            partition-power: 12
            replication-policy:
              replica-count: 3

          - name: object-0
            display-name: General
            default: yes
            min-part-hours: 16
            partition-power: 12
            replication-policy:
              replica-count: 3</pre></div><p>
  Refer to the documentation on <a class="xref" href="#ring-specification" title="11.10. Understanding Swift Ring Specifications">Section 11.10, “Understanding Swift Ring Specifications”</a> for
  details of the above attributes.
 </p></div></div><div class="sect1" id="passthrough"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.17 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Pass Through</span> <a title="Permalink" class="permalink" href="#passthrough">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-configobj-passthrough.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-configobj-passthrough.xml</li><li><span class="ds-label">ID: </span>passthrough</li></ul></div></div></div></div><p>
  Through pass_through definitions, certain configuration values can be
  assigned and used.
 </p><div class="verbatim-wrap"><pre class="screen">product:
  version: 2

pass-through:
  global:
    esx_cloud: true
  servers:
      data:
        vmware:
          cert_check: false
          vcenter_cluster: Cluster1
          vcenter_id: BC9DED4E-1639-481D-B190-2B54A2BF5674
          vcenter_ip: 10.1.200.41
          vcenter_port: 443
          vcenter_username: administrator@vsphere.local
          id: 7d8c415b541ca9ecf9608b35b32261e6c0bf275a</pre></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value Description</th></tr></thead><tbody><tr><td>global</td><td>These values will be used at the cloud level.</td></tr><tr><td>servers </td><td>
      These values will be assigned to a specific server(s) using the
      server-id.
     </td></tr></tbody></table></div></div></div><div class="chapter " id="othertopics"><div class="titlepage"><div><div><h2 class="title"><span class="number">7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Other Topics</span> <a title="Permalink" class="permalink" href="#othertopics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-other_topics-othertopics.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-other_topics-othertopics.xml</li><li><span class="ds-label">ID: </span>othertopics</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#services-components"><span class="number">7.1 </span><span class="name">Services and Service Components</span></a></span></dt><dt><span class="section"><a href="#namegeneration"><span class="number">7.2 </span><span class="name">Name Generation</span></a></span></dt><dt><span class="section"><a href="#persisteddata"><span class="number">7.3 </span><span class="name">Persisted Data</span></a></span></dt><dt><span class="section"><a href="#serverallocation"><span class="number">7.4 </span><span class="name">Server Allocation</span></a></span></dt><dt><span class="section"><a href="#servernetworkselection"><span class="number">7.5 </span><span class="name">Server Network Selection</span></a></span></dt><dt><span class="section"><a href="#networkroutevalidation"><span class="number">7.6 </span><span class="name">Network Route Validation</span></a></span></dt><dt><span class="section"><a href="#configneutronprovidervlans"><span class="number">7.7 </span><span class="name">Configuring neutron Provider VLANs</span></a></span></dt><dt><span class="section"><a href="#standalonedeployer"><span class="number">7.8 </span><span class="name">Standalone Cloud Lifecycle Manager</span></a></span></dt></dl></div></div><div class="sect1" id="services-components"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Services and Service Components</span> <a title="Permalink" class="permalink" href="#services-components">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-other_topics-services_components.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-other_topics-services_components.xml</li><li><span class="ds-label">ID: </span>services-components</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Type</th><th>Service</th><th>Service Components</th></tr></thead><tbody><tr><td colspan="3"><span class="bold"><strong>Compute</strong></span></td></tr><tr><td>Virtual Machine Provisioning</td><td>nova</td><td>
<div class="verbatim-wrap"><pre class="screen">nova-api
nova-compute
nova-compute-hyperv
nova-compute-ironic
nova-compute-kvm
nova-conductor
nova-console-auth
nova-esx-compute-proxy
nova-metadata
nova-novncproxy
nova-scheduler
nova-scheduler-ironic
nova-placement-api</pre></div>
     </td></tr><tr><td>Bare Metal Provisioning</td><td>ironic</td><td>
<div class="verbatim-wrap"><pre class="screen">ironic-api
ironic-conductor</pre></div>
     </td></tr><tr><td colspan="3"><span class="bold"><strong>Networking</strong></span></td></tr><tr><td>Networking</td><td>neutron</td><td>
<div class="verbatim-wrap"><pre class="screen">infoblox-ipam-agent
neutron-dhcp-agent
neutron-l2gateway-agent
neutron-l3-agent
neutron-metadata-agent
neutron-ml2-plugin
neutron-openvswitch-agent
neutron-ovsvapp-agent
neutron-server
neutron-sriov-nic-agent
neutron-vpn-agent</pre></div>
     </td></tr><tr><td>Network Load Balancer</td><td>octavia</td><td>
<div class="verbatim-wrap"><pre class="screen">octavia-api
octavia-health-manager</pre></div>
     </td></tr><tr><td>Domain Name Service (DNS)</td><td>designate</td><td>
<div class="verbatim-wrap"><pre class="screen">designate-api
designate-central
designate-mdns
designate-mdns-external
designate-pool-manager
designate-zone-manager</pre></div>
     </td></tr><tr><td colspan="3"><span class="bold"><strong>Storage</strong></span></td></tr><tr><td>Block Storage</td><td>cinder</td><td>
<div class="verbatim-wrap"><pre class="screen">cinder-api
cinder-backup
cinder-scheduler
cinder-volume</pre></div>
     </td></tr><tr><td>Object Storage</td><td>swift</td><td>
<div class="verbatim-wrap"><pre class="screen">swift-account
swift-common
swift-container
swift-object
swift-proxy
swift-ring-builder
swift-rsync</pre></div>
     </td></tr><tr><td colspan="3"><span class="bold"><strong>Image</strong></span></td></tr><tr><td>Image Management</td><td>glance</td><td>
<div class="verbatim-wrap"><pre class="screen">glance-api
glance-registry</pre></div>
     </td></tr><tr><td colspan="3"><span class="bold"><strong>Security</strong></span></td></tr><tr><td>Key Management</td><td>barbican</td><td>
<div class="verbatim-wrap"><pre class="screen">barbican-api
barbican-worker</pre></div>
     </td></tr><tr><td>Identity and Authentication</td><td>keystone</td><td>
<div class="verbatim-wrap"><pre class="screen">keystone-api</pre></div>
     </td></tr><tr><td colspan="3"><span class="bold"><strong>Orchestration</strong></span></td></tr><tr><td>Orchestration</td><td>heat</td><td>
<div class="verbatim-wrap"><pre class="screen">heat-api
heat-api-cfn
heat-api-cloudwatch
heat-engine</pre></div>
     </td></tr><tr><td colspan="3"><span class="bold"><strong>Operations</strong></span></td></tr><tr><td>Telemetry</td><td>ceilometer</td><td>
<div class="verbatim-wrap"><pre class="screen">ceilometer-agent-notification
ceilometer-common
ceilometer-polling</pre></div>
     </td></tr><tr><td>Cloud Lifecycle Manager</td><td>ardana</td><td>
<div class="verbatim-wrap"><pre class="screen">ardana-ux-services
lifecycle-manager
lifecycle-manager-target</pre></div>
     </td></tr><tr><td>Dashboard</td><td>horizon</td><td>
<div class="verbatim-wrap"><pre class="screen">horizon</pre></div>
     </td></tr><tr><td>Centralized Logging</td><td>logging</td><td>
<div class="verbatim-wrap"><pre class="screen">logging-api
logging-producer
logging-rotate
logging-server</pre></div>
     </td></tr><tr><td>Monitoring</td><td>monasca</td><td>
<div class="verbatim-wrap"><pre class="screen">monasca-agent
monasca-api
monasca-dashboard
monasca-liveness-check
monasca-notifier
monasca-persister
monasca-threshold
monasca-transform</pre></div>
     </td></tr><tr><td>Operations Console</td><td>operations</td><td>
<div class="verbatim-wrap"><pre class="screen">ops-console-web</pre></div>
     </td></tr><tr><td>Openstack Functional Test Suite</td><td>tempest</td><td>
<div class="verbatim-wrap"><pre class="screen">tempest</pre></div>
     </td></tr><tr><td colspan="3"><span class="bold"><strong>Foundation</strong></span></td></tr><tr><td>OpenStack Clients</td><td>clients</td><td>
<div class="verbatim-wrap"><pre class="screen">barbican-client
cinder-client
designate-client
glance-client
heat-client
ironic-client
keystone-client
monasca-client
neutron-client
nova-client
openstack-client
swift-client</pre></div>
     </td></tr><tr><td>Supporting Services</td><td>foundation</td><td>
<div class="verbatim-wrap"><pre class="screen">apache2
bind
bind-ext
influxdb
ip-cluster
kafka
memcached
mysql
ntp-client
ntp-server
openvswitch
rabbitmq
spark
storm
cassandra
zookeeper</pre></div>
     </td></tr></tbody></table></div></div><div class="sect1" id="namegeneration"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Name Generation</span> <a title="Permalink" class="permalink" href="#namegeneration">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-other_topics-namegeneration.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-other_topics-namegeneration.xml</li><li><span class="ds-label">ID: </span>namegeneration</li></ul></div></div></div></div><p>
  Names are generated by the configuration processor for all allocated IP
  addresses. A server connected to multiple networks will have multiple names
  associated with it. One of these may be assigned as the hostname for a server
  via the network-group configuration (see
  <a class="xref" href="#configobj-nicmappings" title="6.12. NIC Mappings">Section 6.12, “NIC Mappings”</a>). Names are generated from data taken
  from various parts of the input model as described in the following sections.
 </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.4.5.3.3"><span class="name">Clusters</span><a title="Permalink" class="permalink" href="#id-1.3.4.5.3.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-other_topics-namegeneration.xml" title="Edit the source file for this section">Edit source</a></h5></div><p>
  Names generated for servers in a cluster have the following form:
 </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable ">CLOUD</em>-<em class="replaceable ">CONTROL-PLANE</em>-<em class="replaceable ">CLUSTER</em><em class="replaceable ">MEMBER-PREFIX</em><em class="replaceable ">MEMBER_ID</em>-<em class="replaceable ">NETWORK</em></pre></div><p>
  Example: <code class="literal">ardana-cp1-core-m1-mgmt</code>
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Name</th><th>Description</th></tr></thead><tbody><tr><td><em class="replaceable ">CLOUD</em></td><td>
      Comes from the hostname-data section of the
      <span class="guimenu ">cloud</span> object (see
      <a class="xref" href="#configobj-cloud" title="6.1. Cloud Configuration">Section 6.1, “Cloud Configuration”</a>)
     </td></tr><tr><td><em class="replaceable ">CONTROL-PLANE</em></td><td>
      is the <span class="guimenu ">control-plane</span> prefix or name (see
      <a class="xref" href="#configobj-controlplane" title="6.2. Control Plane">Section 6.2, “Control Plane”</a>)
     </td></tr><tr><td><em class="replaceable ">CLUSTER</em></td><td>
      is the <span class="guimenu ">cluster-prefix</span> name (see
      <a class="xref" href="#configobj-clusters" title="6.2.1.  Clusters">Section 6.2.1, “
  Clusters”</a>)
     </td></tr><tr><td><em class="replaceable ">member-prefix</em></td><td>
      comes from the hostname-data section of the
      <span class="guimenu ">cloud</span> object (see
      <a class="xref" href="#configobj-cloud" title="6.1. Cloud Configuration">Section 6.1, “Cloud Configuration”</a>)
     </td></tr><tr><td><em class="replaceable ">member_id</em></td><td>
      is the ordinal within the cluster, generated by the configuration
      processor as servers are allocated to the cluster</td></tr><tr><td><em class="replaceable ">network</em></td><td>
      comes from the <span class="guimenu ">hostname-suffix</span> of the network group
      to which the network belongs (see
      <a class="xref" href="#configobj-nicmappings" title="6.12. NIC Mappings">Section 6.12, “NIC Mappings”</a>).
     </td></tr></tbody></table></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.4.5.3.8"><span class="name">Resource Nodes</span><a title="Permalink" class="permalink" href="#id-1.3.4.5.3.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-other_topics-namegeneration.xml" title="Edit the source file for this section">Edit source</a></h5></div><p>
  Names generated for servers in a resource group have the following form:
 </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable ">CLOUD</em>-<em class="replaceable ">CONTROL-PLANE</em>-<em class="replaceable ">RESOURCE-PREFIX</em><em class="replaceable ">MEMBER_ID</em>-<em class="replaceable ">NETWORK</em></pre></div><p>
  Example: <code class="literal">ardana-cp1-comp0001-mgmt</code>
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Name</th><th>Description</th></tr></thead><tbody><tr><td><em class="replaceable ">CLOUD</em></td><td>
      comes from the hostname-data section of the
      <span class="guimenu ">cloud</span> object (see
      <a class="xref" href="#configobj-cloud" title="6.1. Cloud Configuration">Section 6.1, “Cloud Configuration”</a>).
     </td></tr><tr><td><em class="replaceable ">CONTROL-PLANE</em></td><td>
      is the <span class="guimenu ">control-plane</span> prefix or name (see
      <a class="xref" href="#configobj-controlplane" title="6.2. Control Plane">Section 6.2, “Control Plane”</a>).
     </td></tr><tr><td><em class="replaceable ">RESOURCE-PREFIX</em></td><td>
      is the <span class="guimenu ">resource-prefix</span> value name (see
      <a class="xref" href="#configobj-resources" title="6.2.2. Resources">Section 6.2.2, “Resources”</a>).
     </td></tr><tr><td><em class="replaceable ">MEMBER_ID</em></td><td>
      is the ordinal within the cluster, generated by the configuration
      processor as servers are allocated to the cluster, padded with leading
      zeroes to four digits.
     </td></tr><tr><td><em class="replaceable ">NETWORK</em></td><td>
      comes from the <span class="guimenu ">hostname-suffix</span> of the network group
      to which the network belongs to (see
      <a class="xref" href="#configobj-nicmappings" title="6.12. NIC Mappings">Section 6.12, “NIC Mappings”</a>)
     </td></tr></tbody></table></div></div><div class="sect1" id="persisteddata"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Persisted Data</span> <a title="Permalink" class="permalink" href="#persisteddata">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-other_topics-persisteddata.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-other_topics-persisteddata.xml</li><li><span class="ds-label">ID: </span>persisteddata</li></ul></div></div></div></div><p>
  The configuration processor makes allocation decisions on servers and IP
  addresses which it needs to remember between successive runs so that if new
  servers are added to the input model they do not disrupt the previously
  deployed allocations.
 </p><p>
  To allow users to make multiple iterations of the input model before
  deployment <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> will only persist data when the administrator confirms
  that they are about to deploy the results via the "ready-deployment"
  operation. To understand this better, consider the following example:
 </p><p>
  Imagine you have completed your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> deployment with servers A, B, and C
  and you want to add two new compute nodes by adding servers D and E to the
  input model.
 </p><p>
  When you add these to the input model and re-run the configuration processor
  it will read the persisted data for A, B, and C and allocate D and E as new
  servers. The configuration processor now has allocation data for A, B, C, D,
  and E -- which it keeps in a staging area (actually a special branch in Git)
  until we get confirmation that the configuration processor has done what you
  intended and you are ready to deploy the revised configuration.
 </p><p>
  If you notice that the role of E is wrong and it became a swift node instead
  of a nova node you need to be able to change the input model and re-run the
  configuration processor. This is fine because the allocations of D and E have
  not been confirmed, and so the configuration processor will re-read the data
  about A, B, C and re-allocate D and E now to the correct clusters, updating
  the persisted data in the staging area.
 </p><p>
  You can loop though this as many times as needed. Each time, the
  configuration processor is processing the deltas to what is deployed, not the
  results of the previous run. When you are ready to use the results of the
  configuration processor, you run <code class="literal">ready-deployment.yml</code>
  which commits the data in the staging area into the persisted data. The next
  run of the configuration processor will then start from the persisted data
  for A, B, C, D, and E.
 </p><div class="sect2" id="persistedserverallocations"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Persisted Server Allocations</span> <a title="Permalink" class="permalink" href="#persistedserverallocations">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-other_topics-persisteddata.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-other_topics-persisteddata.xml</li><li><span class="ds-label">ID: </span>persistedserverallocations</li></ul></div></div></div></div><p>
   Server allocations are persisted by the administrator-defined server ID (see
   <a class="xref" href="#configobj-servers" title="6.5. Servers">Section 6.5, “Servers”</a>), and include the
   control plane, cluster/resource name, and ordinal within the cluster or
   resource group.
  </p><p>
   To guard against data loss, the configuration processor persists server
   allocations even when the server ID no longer exists in the input model --
   for example, if a server was removed accidentally and the configuration
   processor allocated a new server to the same ordinal, then it would be very
   difficult to recover from that situation.
  </p><p>
   The following example illustrates the behavior:
  </p><p>
   A cloud is deployed with four servers with IDs of A, B, C, and D that can
   all be used in a resource group with <code class="literal">min-size=0</code> and
   <code class="literal">max-size=3</code>. At the end of this deployment they persisted
   state is as follows:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /><col class="c6" /></colgroup><thead><tr><th>ID</th><th>Control Plane</th><th>Resource Group</th><th>Ordinal</th><th>State</th><th>Deployed As</th></tr></thead><tbody><tr><td>A</td><td>ccp</td><td>compute</td><td>1</td><td>Allocated</td><td>mycloud-ccp-comp0001</td></tr><tr><td>B</td><td>ccp</td><td>compute</td><td>2</td><td>Allocated</td><td>mycloud-ccp-comp0002</td></tr><tr><td>C</td><td>ccp</td><td>compute</td><td>3</td><td>Allocated</td><td>mycloud-ccp-comp0003</td></tr><tr><td>D</td><td> </td><td> </td><td> </td><td>Available</td><td> </td></tr></tbody></table></div><p>
   (In this example server D has not been allocated because the group is at its
   max size, and there are no other groups that required this server)
  </p><p>
   If server B is removed from the input model and the configuration processor
   is re-run, the state is changed to:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /><col class="c6" /></colgroup><thead><tr><th>ID</th><th>Control Plane</th><th>Resource Group</th><th>Ordinal</th><th>State</th><th>Deployed As</th></tr></thead><tbody><tr><td>A</td><td>ccp</td><td>compute</td><td>1</td><td>Allocated</td><td>mycloud-ccp-comp0001</td></tr><tr><td>B</td><td>ccp</td><td>compute</td><td>2</td><td>Deleted</td><td> </td></tr><tr><td>C</td><td>ccp</td><td>compute</td><td>3</td><td>Allocated</td><td>mycloud-ccp-comp0003</td></tr><tr><td>D</td><td>ccp</td><td>compute</td><td>4</td><td>Allocated</td><td>mycloud-ccp-comp0004</td></tr></tbody></table></div><p>
   The details associated with server B are still retained, but the
   configuration processor will not generate any deployment data for this
   server. Server D has been added to the group to meet the minimum size
   requirement but has been given a different ordinal and hence will get
   different names and IP addresses than were given to server B.
  </p><p>
   If server B is added back into the input model the resulting state will be:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /><col class="c6" /></colgroup><thead><tr><th>ID</th><th>Control Plane</th><th>Resource Group</th><th>Ordinal</th><th>State</th><th>Deployed As</th></tr></thead><tbody><tr><td>A</td><td>ccp</td><td>compute</td><td>1</td><td>Allocated</td><td>mycloud-ccp-comp0001</td></tr><tr><td>B</td><td>ccp</td><td>compute</td><td>2</td><td>Deleted</td><td> </td></tr><tr><td>C</td><td>ccp</td><td>compute</td><td>3</td><td>Allocated</td><td>mycloud-ccp-comp0003</td></tr><tr><td>D</td><td>ccp</td><td>compute</td><td>4</td><td>Allocated</td><td>mycloud-ccp-comp0004</td></tr></tbody></table></div><p>
   The configuration processor will issue a warning that server B cannot be
   returned to the compute group because it would exceed the max-size
   constraint. However, because the configuration processor knows that server B
   is associated with this group it will not allocate it to any other group that
   could use it, since that might lead to data loss on that server.
  </p><p>
   If the max-size value of the group was increased, then server B would be
   allocated back to the group, with its previous name and addresses
   (<code class="literal">mycloud-cp1-compute0002</code>).
  </p><p>
   Note that the configuration processor relies on the server ID to identify a
   physical server. If the ID value of a server is changed the configuration
   processor will treat it as a new server. Conversely, if a different physical
   server is added with the same ID as a deleted server the configuration
   processor will assume that it is the original server being returned to the
   model.
  </p><p>
   You can force the removal of persisted data for servers that are no longer
   in the input model by running the configuration processor with the
   <code class="literal">remove_deleted_servers</code> option, like below:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml \
-e remove_deleted_servers="y"</pre></div></div><div class="sect2" id="persistedaddressallocations"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Persisted Address Allocations</span> <a title="Permalink" class="permalink" href="#persistedaddressallocations">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-other_topics-persisteddata.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-other_topics-persisteddata.xml</li><li><span class="ds-label">ID: </span>persistedaddressallocations</li></ul></div></div></div></div><p>
   The configuration processor persists IP address allocations by the generated
   name (see <a class="xref" href="#namegeneration" title="7.2. Name Generation">Section 7.2, “Name Generation”</a> for how names are generated). As
   with servers. once an address has been allocated that address will remain
   allocated until the configuration processor is explicitly told that it is no
   longer required. The configuration processor will generate warnings for
   addresses that are persisted but no longer used.
  </p><p>
   You can remove persisted address allocations that are no longer used in the
   input model by running the configuration processor with the
   <code class="literal">free_unused_addresses</code> option, like below:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml \
-e free_unused_addresses="y"</pre></div></div></div><div class="sect1" id="serverallocation"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Server Allocation</span> <a title="Permalink" class="permalink" href="#serverallocation">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-other_topics-serverallocation.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-other_topics-serverallocation.xml</li><li><span class="ds-label">ID: </span>serverallocation</li></ul></div></div></div></div><p>
  The configuration processor allocates servers to a cluster or resource group
  in the following sequence:
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Any <span class="guimenu ">servers</span> that are persisted with a state of
    "allocated" are first returned to the <span class="guimenu ">cluster</span> or
    <span class="guimenu ">resource group</span>. Such servers are always allocated even
    if this contradicts the cluster size, failure-zones, or list of server
    roles since it is assumed that these servers are actively deployed.
   </p></li><li class="listitem "><p>
    If the <span class="guimenu ">cluster</span> or <span class="guimenu ">resource group</span> is
    still below its minimum size, then any <span class="guimenu ">servers</span> that are
    persisted with a state of "deleted", but where the server is now listed in
    the input model (that is, the server was removed but is now back), are added
    to the group providing they meet the <span class="guimenu ">failure-zone</span> and
    <span class="guimenu ">server-role</span> criteria. If they do not meet the criteria
    then a warning is given and the <span class="guimenu ">server</span> remains in a
    deleted state (that is, it is still not allocated to any other cluster or
    group). These <span class="guimenu ">servers</span> are not part of the current
    deployment, and so you must resolve any conflicts before they can be
    redeployed.
   </p></li><li class="listitem "><p>
    If the <span class="guimenu ">cluster</span> or <span class="guimenu ">resource group</span> is
    still below its minimum size, the configuration processor will allocate
    additional <span class="guimenu ">servers</span> that meet the
    <span class="guimenu ">failure-zone</span> and <span class="guimenu ">server-role</span>
    criteria. If the allocation policy is set to "strict" then the failure
    zones of servers already in the cluster or resource group are not
    considered until an equal number of servers has been allocated from each
    zone.
   </p></li></ol></div></div><div class="sect1" id="servernetworkselection"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Server Network Selection</span> <a title="Permalink" class="permalink" href="#servernetworkselection">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-other_topics-servernetworkselection.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-other_topics-servernetworkselection.xml</li><li><span class="ds-label">ID: </span>servernetworkselection</li></ul></div></div></div></div><p>
  Once the configuration processor has allocated a <span class="guimenu ">server</span> to
  a <span class="guimenu ">cluster</span> or <span class="guimenu ">resource group</span> it uses the
  information in the associated <span class="guimenu ">interface-model</span> to determine
  which <span class="guimenu ">networks</span> need to be configured. It does this by:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Looking at the <span class="guimenu ">service-components</span> that are to run on the
    server (from the <span class="guimenu ">control-plane</span> definition)
   </p></li><li class="step "><p>
    Looking to see which <span class="guimenu ">network-group</span> each of those
    components is attached to (from the <span class="guimenu ">network-groups</span>
    definition)
   </p></li><li class="step "><p>
    Looking to see if there are any <span class="guimenu ">network-tags</span> related to
    a <span class="guimenu ">service-component</span> running on this server, and if so,
    adding those <span class="guimenu ">network-groups</span> to the list (also from the
    <span class="guimenu ">network-groups</span> definition)
   </p></li><li class="step "><p>
    Looking to see if there are any <span class="guimenu ">network-groups</span> that the
    <span class="guimenu ">interface-model</span> says should be forced onto the server
   </p></li><li class="step "><p>
    It then searches the <span class="guimenu ">server-group</span> hierarchy (as
    described in <a class="xref" href="#concept-servergroups-networks" title="5.2.9.2. Server Groups and Networks">Section 5.2.9.2, “Server Groups and Networks”</a>) to find a
    <span class="guimenu ">network</span> in each of the <span class="guimenu ">network-groups</span>
    it needs to attach to
   </p></li></ol></div></div><p>
  If there is no <span class="guimenu ">network</span> available to a server, either
  because the <span class="guimenu ">interface-model</span> does not include the required
  <span class="guimenu ">network-group</span>, or there is no <span class="guimenu ">network</span>
  from that group in the appropriate part of the
  <span class="guimenu ">server-groups</span> hierarchy, then the configuration processor
  will generate an error.
 </p><p>
  The configuration processor will also generate an error if the
  <span class="guimenu ">server</span> address does not match any of the networks it will
  be connected to.
 </p></div><div class="sect1" id="networkroutevalidation"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Route Validation</span> <a title="Permalink" class="permalink" href="#networkroutevalidation">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-other_topics-networkroutevalidation.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-other_topics-networkroutevalidation.xml</li><li><span class="ds-label">ID: </span>networkroutevalidation</li></ul></div></div></div></div><p>
  Once the configuration processor has allocated all of the required
  <span class="guimenu ">servers</span> and matched them to the appropriate
  <span class="guimenu ">networks</span>, it validates that all
  <span class="guimenu ">service-components</span> have the required network routes to
  other <span class="guimenu ">service-components</span>.
 </p><p>
  It does this by using the data in the services section of the input model
  which provides details of which <span class="guimenu ">service-components</span> need to
  connect to each other. This data is not configurable by the administrator;
  however, it is provided as part of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> release.
 </p><p>
  For each <span class="guimenu ">server</span>, the configuration processor looks at the
  list of <span class="guimenu ">service-components</span> it runs and determines the
  network addresses of every other <span class="guimenu ">service-component</span> it
  needs to connect to (depending on the service, this might be a virtual IP
  address on a load balancer or a set of addresses for the service).
 </p><p>
  If the target address is on a <span class="guimenu ">network</span> that this
  <span class="guimenu ">server</span> is connected to, then there is no routing required.
  If the target address is on a different <span class="guimenu ">network</span>, then the
  Configuration Processor looks at each <span class="guimenu ">network</span> the server
  is connected to and looks at the routes defined in the corresponding
  <span class="guimenu ">network-group</span>. If the <span class="guimenu ">network-group</span>
  provides a route to the <span class="guimenu ">network-group</span> of the target
  address, then that route is considered valid.
 </p><p>
  <span class="guimenu ">Networks</span> within the same <span class="guimenu ">network-group</span>
  are always considered as routed to each other; <span class="guimenu ">networks</span>
  from different <span class="guimenu ">network-groups</span> must have an explicit entry
  in the <code class="literal">routes</code> stanza of the
  <span class="guimenu ">network-group</span> definition. Routes to a named
  <span class="guimenu ">network-group</span> are always considered before a "default"
  route.
 </p><p>
  A warning is given for any routes which are using the "default" route since
  it is possible that the user did not intend to route this traffic. Such
  warning can be removed by adding the appropriate
  <span class="guimenu ">network-group</span> to the list of routes.
 </p><p>
  The configuration processor provides details of all routes between networks
  that it is expecting to be configured in the
  <code class="literal">info/route_info.yml</code> file.
 </p><p>
  To illustrate how network routing is defined in the input model, consider the
  following example:
 </p><p>
  A compute server is configured to run <code class="literal">nova-compute</code> which requires access to
  the neutron API servers and a block storage service. The neutron API
  servers have a virtual IP address provided by a load balancer in the
  INTERNAL-API network-group and the storage service is connected to the ISCSI
  network-group. <code class="literal">nova-compute</code> itself is part of the set of components attached
  by default to the MANAGEMENT network-group. The intention is to have virtual
  machines on the compute server connect to the block storage via the ISCSI
  network.
 </p><p>
  The physical network is shown below:
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-inputmodel-hphelionopenstack_networkroutevalidation.png" target="_blank"><img src="images/media-inputmodel-hphelionopenstack_networkroutevalidation.png" width="" /></a></div></div><p>
  The corresponding entries in the <span class="guimenu ">network-groups</span> are:
 </p><div class="verbatim-wrap"><pre class="screen">  - name: INTERNAL-API
    hostname-suffix: intapi

    load-balancers:
       - provider: ip-cluster
         name: lb
         components:
           - default
         roles:
           - internal
           - admin

       - name: MANAGEMENT
         hostname-suffix: mgmt
         hostname: true

         component-endpoints:
           - default

         routes:
           - INTERNAL-API
           - default

       - name: ISCSI
         hostname-suffix: iscsi

         component-endpoints:
            - storage service</pre></div><p>
  And the <span class="guimenu ">interface-model</span> for the compute server looks like
  this:
 </p><div class="verbatim-wrap"><pre class="screen">  - name: INTERFACE_SET_COMPUTE
    network-interfaces:
      - name: BOND0
        device:
           name: bond0
        bond-data:
           options:
              mode: active-backup
              miimon: 200
              primary: hed5
           provider: linux
           devices:
              - name: hed4
              - name: hed5
        network-groups:
          - MANAGEMENT
          - ISCSI</pre></div><p>
  When validating the route from <code class="literal">nova-compute</code> to the neutron API, the
  configuration processor will detect that the target address is on a network
  in the INTERNAL-API network group, and that the MANAGEMENT network (which is
  connected to the compute server) provides a route to this network, and thus
  considers this route valid.
 </p><p>
  When validating the route from <code class="literal">nova-compute</code> to a storage service, the
  configuration processor will detect that the target address is on a network
  in the ISCSInetwork group. However, because there is no service component on
  the compute server connected to the ISCSI network (according to the
  network-group definition) the ISCSI network will not have been configured on
  the compute server (see <a class="xref" href="#servernetworkselection" title="7.5. Server Network Selection">Section 7.5, “Server Network Selection”</a>. The
  configuration processor will detect that the MANAGEMENT network-group provides
  a "default" route and thus considers the route as valid (it is, of course,
  valid to route ISCSI traffic). However, because this is using the default
  route, a warning will be issued:
 </p><div class="verbatim-wrap"><pre class="screen">#   route-generator-2.0       WRN: Default routing used between networks
The following networks are using a 'default' route rule. To remove this warning
either add an explicit route in the source network group or force the network to
attach in the interface model used by the servers.
  MANAGEMENT-NET-RACK1 to ISCSI-NET
    ardana-ccp-comp0001
  MANAGEMENT-NET-RACK 2 to ISCSI-NET
    ardana-ccp-comp0002
  MANAGEMENT-NET-RACK 3 to SCSI-NET
    ardana-ccp-comp0003</pre></div><p>
  To remove this warning, you can either add ISCSI to the list of routes in the
  MANAGEMENT network group (routed ISCSI traffic is still a valid
  configuration) or force the compute server to attach to the ISCSI
  network-group by adding it as a forced-network-group in the interface-model,
  like this:
 </p><div class="verbatim-wrap"><pre class="screen">  - name: INTERFACE_SET_COMPUTE
      network-interfaces:
        - name: BOND0
          device:
            name: bond0
          bond-data:
            options:
               mode: active-backup
               miimon: 200
               primary: hed5
            provider: linux
            devices:
               - name: hed4
               - name: hed5
          network-groups:
             - MANAGEMENT
          forced-network-groups:
             - ISCSI</pre></div><p>
  With the attachment to the ISCSI network group forced, the configuration
  processor will attach the compute server to a network in that group and
  validate the route as either being direct or between networks in the same
  network-group.
 </p><p>
  The generated <code class="literal">route_info.yml</code> file will include entries
  such as the following, showing the routes that are still expected to be
  configured between networks in the MANAGEMENT network group and the
  INTERNAL-API network group.
 </p><div class="verbatim-wrap"><pre class="screen">  MANAGEMENT-NET-RACK1:
     INTERNAL-API-NET:
        default: false
        used_by:
           nova-compute:
              neutron-server:
              - ardana-ccp-comp0001
   MANAGEMENT-NET-RACK2:
     INTERNAL-API-NET:
        default: false
        used_by:
          nova-compute:
            neutron-server:
            - ardana-ccp-comp0003</pre></div></div><div class="sect1" id="configneutronprovidervlans"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring neutron Provider VLANs</span> <a title="Permalink" class="permalink" href="#configneutronprovidervlans">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-other_topics-configneutronprovidervlans.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-other_topics-configneutronprovidervlans.xml</li><li><span class="ds-label">ID: </span>configneutronprovidervlans</li></ul></div></div></div></div><p>
  neutron provider VLANs are networks that map directly to an 802.1Q VLAN in
  the cloud provider’s physical network infrastructure. There are four
  aspects to a provider VLAN configuration:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Network infrastructure configuration (for example, the top-of-rack switch)
   </p></li><li class="listitem "><p>
    Server networking configuration (for compute nodes and neutron network
    nodes)
   </p></li><li class="listitem "><p>
    neutron configuration file settings
   </p></li><li class="listitem "><p>
    Creation of the corresponding network objects in neutron
   </p></li></ul></div><p>
  The physical network infrastructure must be configured to convey the provider
  VLAN traffic as tagged VLANs to the cloud compute nodes and neutron network
  nodes. Configuration of the physical network infrastructure is outside the
  scope of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 software.
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 automates the server networking configuration and the neutron
  configuration based on information in the cloud definition. To configure the
  system for provider VLANs, specify the
  <code class="literal">neutron.networks.vlan</code> tag with a
  <code class="literal">provider-physical-network</code> attribute on one or more
  <span class="guimenu ">network-groups</span> as described in
  <a class="xref" href="#configobj-networktags" title="6.13.2. Network Tags">Section 6.13.2, “Network Tags”</a>. For example (some
  attributes omitted for brevity):
 </p><div class="verbatim-wrap"><pre class="screen">  network-groups:

    - name: NET_GROUP_A
      tags:
        - neutron.networks.vlan:
              provider-physical-network: physnet1

    - name: NET_GROUP_B
      tags:
        - neutron.networks.vlan:
              provider-physical-network: physnet2</pre></div><p>
  A <span class="guimenu ">network-group</span> is associated with a server network
  interface via an <span class="guimenu ">interface-model</span> as described in
  <a class="xref" href="#configobj-interfacemodels" title="6.11. Interface Models">Section 6.11, “Interface Models”</a>. For example (some
  attributes omitted for brevity):
 </p><div class="verbatim-wrap"><pre class="screen">  interface-models:
     - name: INTERFACE_SET_X
       network-interfaces:
        - device:
              name: bond0
          network-groups:
            - NET_GROUP_A
        - device:
              name: hed3
          network-groups:
            - NET_GROUP_B</pre></div><p>
  A <span class="guimenu ">network-group</span> used for provider VLANs may contain only a
  single <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="guimenu ">network</span>, because that VLAN must span all
  compute nodes and any neutron network nodes/controllers (that is, it is a single
  L2 segment). The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="guimenu ">network</span> must be defined with
  <code class="literal">tagged-vlan: false</code>, otherwise a Linux VLAN network
  interface will be created. For example:
 </p><div class="verbatim-wrap"><pre class="screen">  networks:
     - name: NET_A
       tagged-vlan: false
       network-group: NET_GROUP_A
     - name: NET_B
       tagged-vlan: false
       network-group: NET_GROUP_B</pre></div><p>
  When the cloud is deployed, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 will create the appropriate
  bridges on the servers, and set the appropriate attributes in the neutron
  configuration files (for example, bridge_mappings).
 </p><p>
  After the cloud has been deployed, create neutron network objects for each
  provider VLAN using the OpenStackClient CLI:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo openstack network create --provider:network_type vlan \
--provider:physical_network <em class="replaceable ">PHYSNET1</em> --provider:segmentation_id <em class="replaceable ">101</em> <em class="replaceable ">MYNET101</em></pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo openstack network create --provider:network_type vlan \
--provider:physical_network <em class="replaceable ">PHYSNET2</em> --provider:segmentation_id <em class="replaceable ">234</em> <em class="replaceable ">MYNET234</em></pre></div></div><div class="sect1" id="standalonedeployer"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Standalone Cloud Lifecycle Manager</span> <a title="Permalink" class="permalink" href="#standalonedeployer">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-other_topics-standalonedeployer.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-other_topics-standalonedeployer.xml</li><li><span class="ds-label">ID: </span>standalonedeployer</li></ul></div></div></div></div><p>
  All the example configurations use a <span class="quote">“<span class="quote ">deployer-in-the-cloud</span>”</span>
  scenario where the first controller is also the deployer/Cloud Lifecycle Manager. If you want
  to use a standalone Cloud Lifecycle Manager, you need to add the relevant details in
  <code class="literal">control_plane.yml</code>, <code class="literal">servers.yml</code> and
  related configuration files. Detailed instructions are available at <a class="xref" href="#standalone-deployer" title="12.1. Using a Dedicated Cloud Lifecycle Manager Node">Section 12.1, “Using a Dedicated Cloud Lifecycle Manager Node”</a>.
 </p></div></div><div class="chapter " id="cpinfofiles"><div class="titlepage"><div><div><h2 class="title"><span class="number">8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuration Processor Information Files</span> <a title="Permalink" class="permalink" href="#cpinfofiles">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-cpinfofiles-cpinfofiles.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-cpinfofiles.xml</li><li><span class="ds-label">ID: </span>cpinfofiles</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#address-info-yml"><span class="number">8.1 </span><span class="name">address_info.yml</span></a></span></dt><dt><span class="section"><a href="#firewall-info-yml"><span class="number">8.2 </span><span class="name">firewall_info.yml</span></a></span></dt><dt><span class="section"><a href="#route-info-yml"><span class="number">8.3 </span><span class="name">route_info.yml</span></a></span></dt><dt><span class="section"><a href="#server-info-yml"><span class="number">8.4 </span><span class="name">server_info.yml</span></a></span></dt><dt><span class="section"><a href="#service-info-yml"><span class="number">8.5 </span><span class="name">service_info.yml</span></a></span></dt><dt><span class="section"><a href="#control-plane-topology-yml"><span class="number">8.6 </span><span class="name">control_plane_topology.yml</span></a></span></dt><dt><span class="section"><a href="#network-topology-yml"><span class="number">8.7 </span><span class="name">network_topology.yml</span></a></span></dt><dt><span class="section"><a href="#region-topology-yml"><span class="number">8.8 </span><span class="name">region_topology.yml</span></a></span></dt><dt><span class="section"><a href="#service-topology-yml"><span class="number">8.9 </span><span class="name">service_topology.yml</span></a></span></dt><dt><span class="section"><a href="#private-data-metadata-ccp-yml"><span class="number">8.10 </span><span class="name">private_data_metadata_ccp.yml</span></a></span></dt><dt><span class="section"><a href="#password-change-yml"><span class="number">8.11 </span><span class="name">password_change.yml</span></a></span></dt><dt><span class="section"><a href="#explain-txt"><span class="number">8.12 </span><span class="name">explain.txt</span></a></span></dt><dt><span class="section"><a href="#clouddiagram-txt"><span class="number">8.13 </span><span class="name">CloudDiagram.txt</span></a></span></dt><dt><span class="section"><a href="#html-representation"><span class="number">8.14 </span><span class="name">HTML Representation</span></a></span></dt></dl></div></div><p>
  In addition to producing all of the data needed to deploy and configure the
  cloud, the configuration processor also creates a number of information files
  that provide details of the resulting configuration.
 </p><p>
  These files can be found in <code class="filename">~/openstack/my_cloud/info</code>
  after the first configuration processor run. This directory is also rebuilt
  each time the Configuration Processor is run.
 </p><p>
  Most of the files are in YAML format, allowing them to be used in further
  automation tasks if required.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /></colgroup><thead><tr><th>File</th><th>Provides details of</th></tr></thead><tbody><tr><td><code class="filename">address_info.yml</code>
     </td><td>
      IP address assignments on each network. See
      <a class="xref" href="#address-info-yml" title="8.1. address_info.yml">Section 8.1, “address_info.yml”</a>
     </td></tr><tr><td><code class="filename">firewall_info.yml</code>
     </td><td>
      All ports that are open on each network by the firewall configuration.
      Can be used if you want to configure an additional firewall in front of
      the API network, for example. See <a class="xref" href="#firewall-info-yml" title="8.2. firewall_info.yml">Section 8.2, “firewall_info.yml”</a>
     </td></tr><tr><td><code class="filename">route_info.yml</code>
     </td><td>
      Routes that need to be configured between networks. See
      <a class="xref" href="#route-info-yml" title="8.3. route_info.yml">Section 8.3, “route_info.yml”</a>
     </td></tr><tr><td><code class="filename">server_info.yml</code>
     </td><td>
      How servers have been allocated, including their network configuration.
      Allows details of a server to be found from its ID. See
      <a class="xref" href="#server-info-yml" title="8.4. server_info.yml">Section 8.4, “server_info.yml”</a>
     </td></tr><tr><td><code class="filename">service_info.yml</code>
     </td><td>
      Details of where components of each service are deployed. See
      <a class="xref" href="#service-info-yml" title="8.5. service_info.yml">Section 8.5, “service_info.yml”</a>
     </td></tr><tr><td><code class="filename">control_plane_topology.yml</code>
     </td><td>
      Details the structure of the cloud from the perspective of each
      control-plane. See <a class="xref" href="#control-plane-topology-yml" title="8.6. control_plane_topology.yml">Section 8.6, “control_plane_topology.yml”</a>
     </td></tr><tr><td><code class="filename">network_topology.yml</code>
     </td><td>
      Details the structure of the cloud from the perspective of each
      control-plane. See <a class="xref" href="#network-topology-yml" title="8.7. network_topology.yml">Section 8.7, “network_topology.yml”</a>
     </td></tr><tr><td><code class="filename">region_topology.yml</code>
     </td><td>
      Details the structure of the cloud from the perspective of each region.
      See <a class="xref" href="#region-topology-yml" title="8.8. region_topology.yml">Section 8.8, “region_topology.yml”</a>
     </td></tr><tr><td><code class="filename">service_topology.yml</code>
     </td><td>
      Details the structure of the cloud from the perspective of each
      service. See <a class="xref" href="#service-topology-yml" title="8.9. service_topology.yml">Section 8.9, “service_topology.yml”</a>
     </td></tr><tr><td><code class="filename">private_data_metadata_ccp.yml</code>
     </td><td>
      Details the secrets that are generated by the configuration processor –
      the names of the secrets, along with the service(s) that use each
      secret and a list of the clusters on which the service that consumes
      the secret is deployed. See <a class="xref" href="#private-data-metadata-ccp-yml" title="8.10. private_data_metadata_ccp.yml">Section 8.10, “private_data_metadata_ccp.yml”</a>
     </td></tr><tr><td><code class="filename">password_change.yml</code>
     </td><td>
      Details the secrets that have been changed by the configuration
      processor – information for each secret is the same as for
      <code class="literal">private_data_metadata_ccp.yml</code>. See
      <a class="xref" href="#password-change-yml" title="8.11. password_change.yml">Section 8.11, “password_change.yml”</a>
     </td></tr><tr><td><code class="filename">explain.txt</code>
     </td><td>
      An explanation of the decisions the configuration processor has made
      when allocating servers and networks. See <a class="xref" href="#explain-txt" title="8.12. explain.txt">Section 8.12, “explain.txt”</a>
     </td></tr><tr><td><code class="filename">CloudDiagram.txt</code>
     </td><td>A pictorial representation of the cloud. See <a class="xref" href="#clouddiagram-txt" title="8.13. CloudDiagram.txt">Section 8.13, “CloudDiagram.txt”</a>
     </td></tr></tbody></table></div><p>
  The examples are taken from the <code class="literal">entry-scale-kvm</code>
  example configuration.
 </p><div class="sect1" id="address-info-yml"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">address_info.yml</span> <a title="Permalink" class="permalink" href="#address-info-yml">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-cpinfofiles-address_info_yml.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-address_info_yml.xml</li><li><span class="ds-label">ID: </span>address-info-yml</li></ul></div></div></div></div><p>
  This file provides details of all the IP addresses allocated by the
  Configuration Processor:
 </p><div class="verbatim-wrap"><pre class="screen">  <em class="replaceable ">NETWORK GROUPS</em>
     <em class="replaceable ">LIST OF NETWORKS</em>
        <em class="replaceable ">IP ADDRESS</em>
           <em class="replaceable ">LIST OF ALIASES</em></pre></div><p>
  Example:
 </p><div class="verbatim-wrap"><pre class="screen">  EXTERNAL-API:
     EXTERNAL-API-NET:
        10.0.1.2:
           - ardana-cp1-c1-m1-extapi
        10.0.1.3:
           - ardana-cp1-c1-m2-extapi
        10.0.1.4:
           - ardana-cp1-c1-m3-extapi
        10.0.1.5:
           - ardana-cp1-vip-public-SWF-PRX-extapi
           - ardana-cp1-vip-public-FRE-API-extapi
           - ardana-cp1-vip-public-GLA-API-extapi
           - ardana-cp1-vip-public-HEA-ACW-extapi
           - ardana-cp1-vip-public-HEA-ACF-extapi
           - ardana-cp1-vip-public-NEU-SVR-extapi
           - ardana-cp1-vip-public-KEY-API-extapi
           - ardana-cp1-vip-public-MON-API-extapi
           - ardana-cp1-vip-public-HEA-API-extapi
           - ardana-cp1-vip-public-NOV-API-extapi
           - ardana-cp1-vip-public-CND-API-extapi
           - ardana-cp1-vip-public-CEI-API-extapi
           - ardana-cp1-vip-public-SHP-API-extapi
           - ardana-cp1-vip-public-OPS-WEB-extapi
           - ardana-cp1-vip-public-HZN-WEB-extapi
           - ardana-cp1-vip-public-NOV-VNC-extapi
  EXTERNAL-VM:
     EXTERNAL-VM-NET: {}
  GUEST:
     GUEST-NET:
        10.1.1.2:
           - ardana-cp1-c1-m1-guest
        10.1.1.3:
           - ardana-cp1-c1-m2-guest
        10.1.1.4:
           - ardana-cp1-c1-m3-guest
        10.1.1.5:
           - ardana-cp1-comp0001-guest
  MANAGEMENT:
  ...</pre></div></div><div class="sect1" id="firewall-info-yml"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">firewall_info.yml</span> <a title="Permalink" class="permalink" href="#firewall-info-yml">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-cpinfofiles-firewall_info_yml.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-firewall_info_yml.xml</li><li><span class="ds-label">ID: </span>firewall-info-yml</li></ul></div></div></div></div><p>
  This file provides details of all the network ports that will be opened on
  the deployed cloud. Data is ordered by network. If you want to configure an
  external firewall in front of the External API network, then you would need
  to open the ports listed in that section.
 </p><div class="verbatim-wrap"><pre class="screen">  <em class="replaceable ">NETWORK NAME</em>
     List of:
        <em class="replaceable ">PORT</em>
        <em class="replaceable ">PROTOCOL</em>
        <em class="replaceable ">LIST OF IP ADDRESSES</em>
        <em class="replaceable ">LIST OF COMPONENTS</em></pre></div><p>
  Example:
 </p><div class="verbatim-wrap"><pre class="screen">  EXTERNAL-API:
  -   addresses:
      - 10.0.1.5
      components:
      - horizon
      port: '443'
      protocol: tcp
  -   addresses:
      - 10.0.1.5
      components:
      - keystone-api
      port: '5000'
      protocol: tcp</pre></div><p>
  <span class="emphasis"><em>Port 443 (tcp) is open on network EXTERNAL-API for address 10.0.1.5
  because it is used by horizon</em></span>
 </p><p>
  <span class="emphasis"><em>Port 5000 (tcp) is open on network EXTERNAL-API for address
  10.0.1.5 because it is used by keystone API</em></span>
 </p></div><div class="sect1" id="route-info-yml"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">route_info.yml</span> <a title="Permalink" class="permalink" href="#route-info-yml">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-cpinfofiles-route_info_yml.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-route_info_yml.xml</li><li><span class="ds-label">ID: </span>route-info-yml</li></ul></div></div></div></div><p>
  This file provides details of routes between networks that need to be
  configured. Available routes are defined in the input model as part of the
  <span class="guimenu ">network-groups</span> data; this file shows which routes will
  actually be used. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> will reconfigure routing rules on the servers, you
  must configure the corresponding routes within your physical network. Routes
  must be configured to be symmetrical -- only the direction in which a
  connection is initiated is captured in this file.
 </p><p>
  Note that simple models may not require any routes, with all servers being
  attached to common L3 networks. The following example is taken from the
  <code class="literal">tech-preview/mid-scale-kvm</code> example.
 </p><div class="verbatim-wrap"><pre class="screen">  <em class="replaceable ">SOURCE-NETWORK-NAME</em>
      <em class="replaceable ">TARGET-NETWORK-NAME</em>
           default:   <em class="replaceable ">TRUE IF THIS IS THIS THE RESULT OF A "DEFAULT" ROUTE RULE</em>
           used_by:
                <em class="replaceable ">SOURCE-SERVICE</em>
                     <em class="replaceable ">TARGET-SERVICE</em>
                     <em class="replaceable ">LIST OF HOSTS USING THIS ROUTE</em></pre></div><p>
  Example:
 </p><div class="verbatim-wrap"><pre class="screen">MANAGEMENT-NET-RACK1:
    INTERNAL-API-NET:
         default: false
         used_by:
            - ardana-cp1-mtrmon-m1
            keystone-api:
            - ardana-cp1-mtrmon-m1
     MANAGEMENT-NET-RACK2:
         default: false
         used_by:
            cinder-backup:
            rabbitmq:
            - ardana-cp1-core-m1</pre></div><p>
  A route is required from network
  <span class="bold"><strong>MANAGEMENT-NET-RACK1</strong></span> to network
  <span class="bold"><strong>MANAGEMENT-NET-RACK2</strong></span> so that
  <span class="bold"><strong>cinder-backup</strong></span> can connect to
  <span class="bold"><strong>rabbitmq</strong></span> from server
  <span class="bold"><strong>ardana-cp1-core-m1</strong></span>
 </p></div><div class="sect1" id="server-info-yml"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">server_info.yml</span> <a title="Permalink" class="permalink" href="#server-info-yml">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-cpinfofiles-server_info_yml.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-server_info_yml.xml</li><li><span class="ds-label">ID: </span>server-info-yml</li></ul></div></div></div></div><p>
  This file provides details of how servers have been allocated by the
  Configuration Processor. This provides the easiest way to find where a
  specific physical server (identified by <code class="literal">server-id</code>) is
  being used.
 </p><div class="verbatim-wrap"><pre class="screen">   <em class="replaceable ">SERVER-ID</em>
         failure-zone: <em class="replaceable ">FAILURE ZONE THAT THE SERVER WAS ALLOCATED FROM</em>
         hostname: <em class="replaceable ">HOSTNAME OF THE SERVER</em>
         net_data: <em class="replaceable ">NETWORK CONFIGURATION</em>
         state: <em class="replaceable "> "allocated" | "available" </em></pre></div><p>
  Example:
 </p><div class="verbatim-wrap"><pre class="screen">   controller1:
         failure-zone: AZ1
         hostname: ardana-cp1-c1-m1-mgmt
         net_data:
              BOND0:
                   EXTERNAL-API-NET:
                       addr: 10.0.1.2
                       tagged-vlan: true
                       vlan-id: 101
                   EXTERNAL-VM-NET:
                       addr: null
                       tagged-vlan: true
                       vlan-id: 102
                   GUEST-NET:
                       addr: 10.1.1.2
                       tagged-vlan: true
                       vlan-id: 103
                   MANAGEMENT-NET:
                       addr: 192.168.10.3
                       tagged-vlan: false
                       vlan-id: 100
         state: allocated</pre></div></div><div class="sect1" id="service-info-yml"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">service_info.yml</span> <a title="Permalink" class="permalink" href="#service-info-yml">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-cpinfofiles-service_info_yml.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-service_info_yml.xml</li><li><span class="ds-label">ID: </span>service-info-yml</li></ul></div></div></div></div><p>
  This file provides details of how services are distributed across the cloud.
 </p><div class="verbatim-wrap"><pre class="screen">  <em class="replaceable ">CONTROL-PLANE</em>
      <em class="replaceable ">SERVICE</em>
          <em class="replaceable ">SERVICE COMPONENT</em>
               <em class="replaceable ">LIST OF HOSTS</em></pre></div><p>
  Example:
 </p><div class="verbatim-wrap"><pre class="screen">  control-plane-1:
        neutron:
             neutron-client:
                - ardana-cp1-c1-m1-mgmt
                - ardana-cp1-c1-m2-mgmt
                - ardana-cp1-c1-m3-mgmt
             neutron-dhcp-agent:
                - ardana-cp1-c1-m1-mgmt
                - ardana-cp1-c1-m2-mgmt
                - ardana-cp1-c1-m3-mgmt
             neutron-l3-agent:
                 - ardana-cp1-comp0001-mgmt
        ...</pre></div></div><div class="sect1" id="control-plane-topology-yml"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">control_plane_topology.yml</span> <a title="Permalink" class="permalink" href="#control-plane-topology-yml">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-cpinfofiles-control_plane_topology_yml.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-control_plane_topology_yml.xml</li><li><span class="ds-label">ID: </span>control-plane-topology-yml</li></ul></div></div></div></div><p>
  This file provides details of the topology of the cloud from the perspective
  of each control plane:
 </p><div class="verbatim-wrap"><pre class="screen">control_planes:
  <em class="replaceable ">CONTROL-PLANE-NAME</em>
      load-balancers:
         <em class="replaceable ">LOAD-BALANCER-NAME</em>:
             address:  <em class="replaceable ">IP ADDRESS OF VIP</em>
             cert-file:  <em class="replaceable ">NAME OF CERT FILE</em>
             external-name: <em class="replaceable ">NAME TO USED FOR ENDPOINTS</em>
             network: <em class="replaceable ">NAME OF THE NETWORK THIS LB IS CONNECTED TO</em>
             network_group: <em class="replaceable ">NAME OF THE NETWORK GROUP THIS LB IS CONNECT TO</em>
             provider: <em class="replaceable ">SERVICE COMPONENT PROVIDING THE LB</em>
             roles:  <em class="replaceable ">LIST OF ROLES OF THIS LB</em>
             services:
                <em class="replaceable ">SERVICE-NAME</em>:
                    <em class="replaceable ">COMPONENT-NAME</em>:
                        aliases:
                           <em class="replaceable ">ROLE</em>:  <em class="replaceable ">NAME IN /etc/hosts</em>
                        host-tls:  <em class="replaceable ">BOOLEAN, TRUE IF CONNECTION FROM LB USES TLS</em>
                        hosts:  <em class="replaceable ">LIST OF HOSTS FOR THIS SERVICE</em>
                        port:  <em class="replaceable ">PORT USED FOR THIS COMPONENT</em>
                        vip-tls: <em class="replaceable ">BOOLEAN, TRUE IF THE VIP TERMINATES TLS</em>
      clusters:
          <em class="replaceable ">CLUSTER-NAME</em>
              failure-zones:
                 <em class="replaceable ">FAILURE-ZONE-NAME</em>:
                    <em class="replaceable ">LIST OF HOSTS</em>
              services:
                 <em class="replaceable ">SERVICE NAME</em>:
                     components:
                        <em class="replaceable ">LIST OF SERVICE COMPONENTS</em>
                     regions:
                        <em class="replaceable ">LIST OF REGION NAMES</em>
      resources:
         <em class="replaceable ">RESOURCE-NAME</em>:
             <em class="replaceable ">AS FOR CLUSTERS ABOVE</em></pre></div><p>
  <span class="bold"><strong>Example:</strong></span>
 </p><div class="verbatim-wrap"><pre class="screen">    control_planes:
    control-plane-1:
        clusters:
            cluster1:
                failure_zones:
                    AZ1:
                    - ardana-cp1-c1-m1-mgmt
                    AZ2:
                    - ardana-cp1-c1-m2-mgmt
                    AZ3:
                    - ardana-cp1-c1-m3-mgmt
                services:
                    barbican:
                        components:
                        - barbican-api
                        - barbican-worker
                        regions:
                        - region1
                                               …
        load-balancers:
            extlb:
                address: 10.0.1.5
                cert-file: my-public-entry-scale-kvm-cert
                external-name: ''
                network: EXTERNAL-API-NET
                network-group: EXTERNAL-API
                provider: ip-cluster
                roles:
                - public
                services:
                    barbican:
                        barbican-api:
                            aliases:
                                public: ardana-cp1-vip-public-KEYMGR-API-extapi
                            host-tls: true
                            hosts:
                            - ardana-cp1-c1-m1-mgmt
                            - ardana-cp1-c1-m2-mgmt
                            - ardana-cp1-c1-m3-mgmt
                            port: '9311'
                            vip-tls: true</pre></div></div><div class="sect1" id="network-topology-yml"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">network_topology.yml</span> <a title="Permalink" class="permalink" href="#network-topology-yml">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-cpinfofiles-network_topology_yml.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-network_topology_yml.xml</li><li><span class="ds-label">ID: </span>network-topology-yml</li></ul></div></div></div></div><p>
  This file provides details of the topology of the cloud from the perspective
  of each network_group:
 </p><div class="verbatim-wrap"><pre class="screen">network-groups:
  <em class="replaceable ">NETWORK-GROUP-NAME</em>:
      <em class="replaceable ">NETWORK-NAME</em>:
          control-planes:
              <em class="replaceable ">CONTROL-PLANE-NAME</em>:
                  clusters:
                     <em class="replaceable ">CLUSTER-NAME</em>:
                         servers:
                            <em class="replaceable ">ARDANA-SERVER-NAME</em>: <em class="replaceable ">ip address</em>
                         vips:
                            <em class="replaceable ">IP ADDRESS</em>: <em class="replaceable ">load balancer name</em>
                  resources:
                     <em class="replaceable ">RESOURCE-GROUP-NAME</em>:
                         servers:
                            <em class="replaceable ">ARDANA-SERVER-NAME</em>: <em class="replaceable ">ip address</em></pre></div><p>
  <span class="bold"><strong>Example:</strong></span>
 </p><div class="verbatim-wrap"><pre class="screen">   network_groups:
    EXTERNAL-API:
        EXTERNAL-API-NET:
            control_planes:
                control-plane-1:
                    clusters:
                        cluster1:
                            servers:
                                ardana-cp1-c1-m1: 10.0.1.2
                                ardana-cp1-c1-m2: 10.0.1.3
                                ardana-cp1-c1-m3: 10.0.1.4
                            vips:
                                10.0.1.5: extlb
    EXTERNAL-VM:
        EXTERNAL-VM-NET:
            control_planes:
                control-plane-1:
                    clusters:
                        cluster1:
                            servers:
                                ardana-cp1-c1-m1: null
                                ardana-cp1-c1-m2: null
                                ardana-cp1-c1-m3: null
                    resources:
                        compute:
                            servers:
                                ardana-cp1-comp0001: null</pre></div></div><div class="sect1" id="region-topology-yml"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">region_topology.yml</span> <a title="Permalink" class="permalink" href="#region-topology-yml">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-cpinfofiles-region_topology_yml.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-region_topology_yml.xml</li><li><span class="ds-label">ID: </span>region-topology-yml</li></ul></div></div></div></div><p>
  This file provides details of the topology of the cloud from the perspective
  of each region.  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, multiple regions are not supported. Only
  <code class="literal">Region0</code> is valid.
 </p><div class="verbatim-wrap"><pre class="screen">regions:
  <em class="replaceable ">REGION-NAME</em>:
      control-planes:
          <em class="replaceable ">CONTROL-PLANE-NAME</em>:
              services:
                 <em class="replaceable ">SERVICE-NAME</em>:
                     <em class="replaceable ">LIST OF SERVICE COMPONENTS</em></pre></div><p>
  <span class="bold"><strong>Example:</strong></span>
 </p><div class="verbatim-wrap"><pre class="screen">regions:
    region0:
        control-planes:
            control-plane-1:
                services:
                    barbican:
                    - barbican-api
                    - barbican-worker
                    ceilometer:
                    - ceilometer-common
                    - ceilometer-agent-notification
                    - ceilometer-polling
                    cinder:
                    - cinder-api
                    - cinder-volume
                    - cinder-scheduler
                    - cinder-backup</pre></div></div><div class="sect1" id="service-topology-yml"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">service_topology.yml</span> <a title="Permalink" class="permalink" href="#service-topology-yml">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-cpinfofiles-service_topology_yml.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-service_topology_yml.xml</li><li><span class="ds-label">ID: </span>service-topology-yml</li></ul></div></div></div></div><p>
  This file provides details of the topology of the cloud from the perspective
  of each service:
 </p><div class="verbatim-wrap"><pre class="screen">services:
    <em class="replaceable ">SERVICE-NAME</em>:
        components:
            <em class="replaceable ">COMPONENT-NAME</em>:
                control-planes:
                    <em class="replaceable ">CONTROL-PLANE-NAME</em>:
                        clusters:
                            <em class="replaceable ">CLUSTER-NAME</em>:
                                <em class="replaceable ">LIST OF SERVERS</em>
                        resources:
                            <em class="replaceable ">RESOURCE-GROUP-NAME</em>:
                                <em class="replaceable ">LIST OF SERVERS</em>
                        regions:
                            <em class="replaceable ">LIST OF REGIONS</em></pre></div></div><div class="sect1" id="private-data-metadata-ccp-yml"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">private_data_metadata_ccp.yml</span> <a title="Permalink" class="permalink" href="#private-data-metadata-ccp-yml">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-cpinfofiles-private_data_metadata_yml.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-private_data_metadata_yml.xml</li><li><span class="ds-label">ID: </span>private-data-metadata-ccp-yml</li></ul></div></div></div></div><p>
  This file provide details of the secrets that are generated by the
  configuration processor. The details include:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    The names of each secret
   </p></li><li class="listitem "><p>
    Metadata about each secret. This is a list where each element contains
    details about each <code class="literal">component</code> service that uses the
    secret.
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      The <code class="literal">component</code> service that uses the secret, and if
      applicable the service that this component "consumes" when using the
      secret
     </p></li><li class="listitem "><p>
      The list of clusters on which the <code class="literal">component</code> service is
      deployed
     </p></li><li class="listitem "><p>
      The control plane <code class="literal">cp</code> on which the services are
      deployed
     </p></li></ul></div></li><li class="listitem "><p>
    A version number (the model version number)
   </p></li></ul></div><div class="verbatim-wrap"><pre class="screen">  <em class="replaceable ">SECRET</em>
      <em class="replaceable ">METADATA</em>
           <em class="replaceable ">LIST OF METADATA</em>
               <em class="replaceable ">CLUSTERS</em>
                   <em class="replaceable ">LIST OF CLUSTERS</em>
               <em class="replaceable ">COMPONENT</em>
               <em class="replaceable ">CONSUMES</em>
               <em class="replaceable ">CONTROL-PLANE</em>
       <em class="replaceable ">VERSION</em></pre></div><p>
  For example:
 </p><div class="verbatim-wrap"><pre class="screen">barbican_admin_password:
    metadata:
    -   clusters:
        - cluster1
        component: barbican-api
        cp: ccp
    version: '2.0'
keystone_swift_password:
    metadata:
    -   clusters:
        - cluster1
        component: swift-proxy
        consumes: keystone-api
        cp: ccp
    version: '2.0'
metadata_proxy_shared_secret:
    metadata:
    -   clusters:
        - cluster1
        component: nova-metadata
        cp: ccp
    -   clusters:
        - cluster1
        - compute
        component: neutron-metadata-agent
        cp: ccp
    version: '2.0'
    …</pre></div></div><div class="sect1" id="password-change-yml"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">password_change.yml</span> <a title="Permalink" class="permalink" href="#password-change-yml">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-cpinfofiles-password_change_yml.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-password_change_yml.xml</li><li><span class="ds-label">ID: </span>password-change-yml</li></ul></div></div></div></div><p>
  This file provides details equivalent to those in private_data_metadata_ccp.yml
  for passwords which have been changed from their original values, using the
  procedure outlined in the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> documentation
 </p></div><div class="sect1" id="explain-txt"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">explain.txt</span> <a title="Permalink" class="permalink" href="#explain-txt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-cpinfofiles-explain_txt.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-explain_txt.xml</li><li><span class="ds-label">ID: </span>explain-txt</li></ul></div></div></div></div><p>
  This file provides details of the server allocation and network configuration
  decisions the configuration processor has made. The sequence of information
  recorded is:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Any service components that are automatically added
   </p></li><li class="listitem "><p>
    Allocation of servers to clusters and resource groups
   </p></li><li class="listitem "><p>
    Resolution of the network configuration for each server
   </p></li><li class="listitem "><p>
    Resolution of the network configuration of each load balancer
   </p></li></ul></div><p>
  Example:
 </p><div class="verbatim-wrap"><pre class="screen">        Add required services to control plane control-plane-1
        ======================================================
        control-plane-1: Added nova-metadata required by nova-api
        control-plane-1: Added swift-common required by swift-proxy
        control-plane-1: Added swift-rsync required by swift-account

        Allocate Servers for control plane control-plane-1
        ==================================================

        cluster: cluster1
        -----------------
          Persisted allocation for server 'controller1' (AZ1)
          Persisted allocation for server 'controller2' (AZ2)
          Searching for server with role ['CONTROLLER-ROLE'] in zones: set(['AZ3'])
          Allocated server 'controller3' (AZ3)

        resource: compute
        -----------------
          Persisted allocation for server 'compute1' (AZ1)
          Searching for server with role ['COMPUTE-ROLE'] in zones: set(['AZ1', 'AZ2', 'AZ3'])

        Resolve Networks for Servers
        ============================
        server: ardana-cp1-c1-m1
        ------------------------
          add EXTERNAL-API for component ip-cluster
          add MANAGEMENT for component ip-cluster
          add MANAGEMENT for lifecycle-manager (default)
          add MANAGEMENT for ntp-server (default)
          ...
          add MANAGEMENT for swift-rsync (default)
          add GUEST for tag neutron.networks.vxlan (neutron-openvswitch-agent)
          add EXTERNAL-VM for tag neutron.l3_agent.external_network_bridge (neutron-vpn-agent)
          Using persisted address 10.0.1.2 for server ardana-cp1-c1-m1 on network EXTERNAL-API-NET
          Using address 192.168.10.3 for server ardana-cp1-c1-m1 on network MANAGEMENT-NET
          Using persisted address 10.1.1.2 for server ardana-cp1-c1-m1 on network GUEST-NET

        …
        Define load balancers
        =====================

        Load balancer: extlb
        --------------------
          Using persisted address 10.0.1.5 for vip extlb ardana-cp1-vip-extlb-extapi on network EXTERNAL-API-NET
          Add nova-api for roles ['public'] due to 'default'
          Add glance-api for roles ['public'] due to 'default'
          ...

        Map load balancers to providers
        ===============================

        Network EXTERNAL-API-NET
        ------------------------
          10.0.1.5: ip-cluster nova-api roles: ['public'] vip-port: 8774 host-port: 8774
          10.0.1.5: ip-cluster glance-api roles: ['public'] vip-port: 9292 host-port: 9292
          10.0.1.5: ip-cluster keystone-api roles: ['public'] vip-port: 5000 host-port: 5000
          10.0.1.5: ip-cluster swift-proxy roles: ['public'] vip-port: 8080 host-port: 8080
          10.0.1.5: ip-cluster monasca-api roles: ['public'] vip-port: 8070 host-port: 8070
          10.0.1.5: ip-cluster heat-api-cfn roles: ['public'] vip-port: 8000 host-port: 8000
          10.0.1.5: ip-cluster ops-console-web roles: ['public'] vip-port: 9095 host-port: 9095
          10.0.1.5: ip-cluster heat-api roles: ['public'] vip-port: 8004 host-port: 8004
          10.0.1.5: ip-cluster nova-novncproxy roles: ['public'] vip-port: 6080 host-port: 6080
          10.0.1.5: ip-cluster neutron-server roles: ['public'] vip-port: 9696 host-port: 9696
          10.0.1.5: ip-cluster heat-api-cloudwatch roles: ['public'] vip-port: 8003 host-port: 8003
          10.0.1.5: ip-cluster horizon roles: ['public'] vip-port: 443 host-port: 80
          10.0.1.5: ip-cluster cinder-api roles: ['public'] vip-port: 8776 host-port: 8776</pre></div></div><div class="sect1" id="clouddiagram-txt"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">CloudDiagram.txt</span> <a title="Permalink" class="permalink" href="#clouddiagram-txt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-cpinfofiles-clouddiagram_txt.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-clouddiagram_txt.xml</li><li><span class="ds-label">ID: </span>clouddiagram-txt</li></ul></div></div></div></div><p>
  This file provides a pictorial representation of the cloud. Although this
  file is still produced, it is superseded by the HTML output described in the
  following section.
 </p></div><div class="sect1" id="html-representation"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HTML Representation</span> <a title="Permalink" class="permalink" href="#html-representation">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-input_model-cpinfofiles-html_representation.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-input_model-cpinfofiles-html_representation.xml</li><li><span class="ds-label">ID: </span>html-representation</li></ul></div></div></div></div><p>
  An HTML representation of the cloud can be found in
  <code class="filename">~/openstack/my_cloud/html</code> after the first Configuration
  Processor run. This directory is also rebuilt each time the Configuration
  Processor is run. These files combine the data in the input model with
  allocation decisions made by the Configuration processor to allow the
  configured cloud to be viewed from a number of different perspectives.
 </p><p>
  Most of the entries on the HTML pages provide either links to other parts of
  the HTML output or additional details via hover text.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-inputmodel-html_representation_1.png" target="_blank"><img src="images/media-inputmodel-html_representation_1.png" width="" /></a></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-inputmodel-html_representation_2.png" target="_blank"><img src="images/media-inputmodel-html_representation_2.png" width="" /></a></div></div></div></div><div class="chapter " id="example-configurations"><div class="titlepage"><div><div><h2 class="title"><span class="number">9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Example Configurations</span> <a title="Permalink" class="permalink" href="#example-configurations">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-example_configurations.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-example_configurations.xml</li><li><span class="ds-label">ID: </span>example-configurations</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#example-configs"><span class="number">9.1 </span><span class="name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Example Configurations</span></a></span></dt><dt><span class="section"><a href="#alternative"><span class="number">9.2 </span><span class="name">Alternative Configurations</span></a></span></dt><dt><span class="section"><a href="#kvm-examples"><span class="number">9.3 </span><span class="name">KVM Examples</span></a></span></dt><dt><span class="section"><a href="#esx-examples"><span class="number">9.4 </span><span class="name">ESX Examples</span></a></span></dt><dt><span class="section"><a href="#swift-examples"><span class="number">9.5 </span><span class="name">Swift Examples</span></a></span></dt><dt><span class="section"><a href="#ironic-examples"><span class="number">9.6 </span><span class="name">Ironic Examples</span></a></span></dt></dl></div></div><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 system ships with a collection of pre-qualified example
  configurations. These are designed to help you to get up and running quickly
  with a minimum number of configuration changes.
 </p><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> input model allows a wide variety of configuration parameters
  that can, at first glance, appear daunting. The example configurations are
  designed to simplify this process by providing pre-built and pre-qualified
  examples that need only a minimum number of modifications to get started.
 </p><div class="sect1" id="example-configs"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Example Configurations</span> <a title="Permalink" class="permalink" href="#example-configs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-example_configurations.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-example_configurations.xml</li><li><span class="ds-label">ID: </span>example-configs</li></ul></div></div></div></div><p>
   This section briefly describes the various example configurations and their
   capabilities. It also describes in detail, for the entry-scale-kvm
   example, how you can adapt the input model to work in your environment.
  </p><p>
   The following pre-qualified examples are shipped with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Name</th><th>Location</th></tr></thead><tbody><tr><td>
      <a class="xref" href="#entry-scale-kvm" title="9.3.1. Entry-Scale Cloud">Section 9.3.1, “Entry-Scale Cloud”</a>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-kvm</code>
     </td></tr><tr><td>
      <a class="xref" href="#entry-scale-kvm-mml" title="9.3.2. Entry Scale Cloud with Metering and Monitoring Services">Section 9.3.2, “Entry Scale Cloud with Metering and Monitoring Services”</a>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-kvm-mml</code>
     </td></tr><tr><td><a class="xref" href="#entry-scale-kvm-esx" title="9.4.1. Single-Region Entry-Scale Cloud with a Mix of KVM and ESX Hypervisors">Section 9.4.1, “Single-Region Entry-Scale Cloud with a Mix of KVM and ESX Hypervisors”</a>
     </td><td><code class="filename">~/openstack/examples/entry-scale-kvm-esx</code>
     </td></tr><tr><td>
      <a class="xref" href="#entry-scale-kvm-esx-mml" title="9.4.2. Single-Region Entry-Scale Cloud with Metering and Monitoring Services, and a Mix of KVM and ESX Hypervisors">Section 9.4.2, “Single-Region Entry-Scale Cloud with Metering and Monitoring Services, and a Mix of KVM and ESX Hypervisors”</a>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-kvm-esx-mml</code>
     </td></tr><tr><td>
      <a class="xref" href="#entryscale-swift" title="9.5.1. Entry-scale swift Model">Section 9.5.1, “Entry-scale swift Model”</a>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-swift</code>
     </td></tr><tr><td>
      <a class="xref" href="#entryscale-ironic" title="9.6.1. Entry-Scale Cloud with Ironic Flat Network">Section 9.6.1, “Entry-Scale Cloud with Ironic Flat Network”</a>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-ironic-flat-network</code>
     </td></tr><tr><td>
      <a class="xref" href="#entryscale-ironic-multi-tenancy" title="9.6.2. Entry-Scale Cloud with Ironic Multi-Tenancy">Section 9.6.2, “Entry-Scale Cloud with Ironic Multi-Tenancy”</a>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-ironic-multi-tenancy</code>
     </td></tr><tr><td><a class="xref" href="#mid-scale-kvm" title="9.3.3. Single-Region Mid-Size Model">Section 9.3.3, “Single-Region Mid-Size Model”</a>
     </td><td>
      <code class="filename">~/openstack/examples/mid-scale-kvm</code>
     </td></tr></tbody></table></div><p>
   The entry-scale systems are designed to provide an entry-level solution that
   can be scaled from a small number of nodes to a moderately high node count
   (approximately 100 compute nodes, for example).
  </p><p>
   In the mid-scale model, the cloud control plane is subdivided into a number
   of dedicated service clusters to provide more processing power for
   individual control plane elements. This enables a greater number of
   resources to be supported (compute nodes, swift object servers). This model
   also shows how a segmented network can be expressed in the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> model.
  </p></div><div class="sect1" id="alternative"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Alternative Configurations</span> <a title="Permalink" class="permalink" href="#alternative">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-example_configurations.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-example_configurations.xml</li><li><span class="ds-label">ID: </span>alternative</li></ul></div></div></div></div><p>
   In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 there are alternative configurations that we recommend
   for specific purposes and this section we will outline them.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <a class="xref" href="#standalone-deployer" title="12.1. Using a Dedicated Cloud Lifecycle Manager Node">Section 12.1, “Using a Dedicated Cloud Lifecycle Manager Node”</a>
    </p></li><li class="listitem "><p>
     <a class="xref" href="#without-dvr" title="12.2. Configuring SUSE OpenStack Cloud without DVR">Section 12.2, “Configuring <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> without DVR”</a>
    </p></li><li class="listitem "><p>
     <a class="xref" href="#without-l3agent" title="12.3. Configuring SUSE OpenStack Cloud with Provider VLANs and Physical Routers Only">Section 12.3, “Configuring <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> with Provider VLANs and Physical Routers Only”</a>
    </p></li><li class="listitem "><p>
     <a class="xref" href="#twosystems" title="12.4. Considerations When Installing Two Systems on One Subnet">Section 12.4, “Considerations When Installing Two Systems on One Subnet”</a>
    </p></li></ul></div><p>
   The ironic multi-tenancy feature uses neutron to manage the tenant
   networks. The interaction between neutron and the physical switch is
   facilitated by neutron's Modular Layer 2 (ML2) plugin. The neutron ML2
   plugin supports drivers to interact with various networks, as each vendor
   may have their own extensions. Those drivers are referred to as <span class="emphasis"><em>neutron ML2
   mechanism drivers</em></span>, or simply <span class="emphasis"><em>mechanism drivers</em></span>.
  </p><p>
   The ironic multi-tenancy feature has been validated using <span class="productname">OpenStack</span>
   genericswitch mechanism driver. However, if the given physical switch
   requires a different mechanism driver, you must update the input model
   accordingly. To update the input model with a custom ML2 mechanism driver,
   specify the relevant information in the
   <code class="literal">multi_tenancy_switch_config:</code> section of the
   <code class="filename">data/ironic/ironic_config.yml</code> file.
  </p></div><div class="sect1" id="kvm-examples"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">KVM Examples</span> <a title="Permalink" class="permalink" href="#kvm-examples">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-examples-kvm_examples.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-kvm_examples.xml</li><li><span class="ds-label">ID: </span>kvm-examples</li></ul></div></div></div></div><div class="sect2" id="entry-scale-kvm"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Entry-Scale Cloud</span> <a title="Permalink" class="permalink" href="#entry-scale-kvm">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-examples-entry-scale-kvm.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-entry-scale-kvm.xml</li><li><span class="ds-label">ID: </span>entry-scale-kvm</li></ul></div></div></div></div><p>
  This example deploys an entry-scale cloud.
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.7.6.2.3.1"><span class="term ">Control Plane</span></dt><dd><p>
     <span class="bold"><strong>Cluster1</strong></span> 3 nodes of type
     <code class="literal">CONTROLLER-ROLE</code> run the core <span class="productname">OpenStack</span> services, such as
     keystone, nova API, glance API, neutron API, horizon, and heat
     API.
    </p></dd><dt id="id-1.3.4.7.6.2.3.2"><span class="term ">Cloud Lifecycle Manager</span></dt><dd><p>
     The Cloud Lifecycle Manager runs on one of the control-plane nodes of type
     <code class="literal">CONTROLLER-ROLE</code>. The IP address of the node that will
     run the Cloud Lifecycle Manager needs to be included in the
     <code class="filename">data/servers.yml</code> file.
    </p></dd><dt id="id-1.3.4.7.6.2.3.3"><span class="term ">Resource Nodes</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Compute</strong></span> One node of type
       <code class="literal">COMPUTE-ROLE</code> runs nova Compute and associated
       services.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Object Storage</strong></span> Minimal swift
       resources are provided by the control plane.
      </p></li></ul></div><p>
     Additional resource nodes can be added to the configuration.
    </p></dd><dt id="id-1.3.4.7.6.2.3.4"><span class="term ">Networking</span></dt><dd><p>
     This example requires the following networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>IPMI</strong></span> network connected to the
       lifecycle-manager and the IPMI ports of all servers.
      </p></li></ul></div><p>
     Nodes require a pair of bonded NICs which are used by the following
     networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>External API</strong></span> The network for making
       requests to the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>External VM</strong></span> This network provides
       access to VMs via floating IP addresses.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cloud Management</strong></span> This network is used
       for all internal traffic between the cloud services. It is also used to
       install and configure the nodes. The network needs to be on an untagged
       VLAN.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Guest</strong></span> The network that carries traffic
       between VMs on private networks within the cloud.
      </p></li></ul></div><p>
     The <code class="literal">EXTERNAL API</code> network must be reachable from the
     <code class="literal">EXTERNAL VM</code> network for VMs to be able to make API
     calls to the cloud.
    </p><p>
     An example set of networks is defined in
     <code class="filename">data/networks.yml</code>. The file needs to be modified to
     reflect your environment.
    </p><p>
     The example uses the devices <code class="filename">hed3</code> and
     <code class="filename">hed4</code> as a bonded network interface for all services.
     The name given to a network interface by the system is configured in the
     file <code class="filename">data/net_interfaces.yml</code>. That file needs to be
     edited to match your system.
    </p></dd><dt id="id-1.3.4.7.6.2.3.5"><span class="term ">Local Storage</span></dt><dd><p>
     All servers should present a single OS disk, protected by a RAID
     controller. This disk needs to be at least 512 GB in capacity. In
     addition the example configures one additional disk depending on the role
     of the server:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Controllers</strong></span>
       <code class="filename">/dev/sdb</code> and <code class="filename">/dev/sdc</code> are
       configured to be used by swift.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Compute Servers</strong></span>
       <code class="filename">/dev/sdb</code> is configured as an additional Volume
       Group to be used for VM storage
      </p></li></ul></div><p>
     Additional disks can be configured for any of these roles by editing the
     corresponding <code class="filename">data/disks_*.yml</code> file
    </p></dd></dl></div></div><div class="sect2" id="entry-scale-kvm-mml"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Entry Scale Cloud with Metering and Monitoring Services</span> <a title="Permalink" class="permalink" href="#entry-scale-kvm-mml">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-examples-entry-scale-kvm-mml.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-entry-scale-kvm-mml.xml</li><li><span class="ds-label">ID: </span>entry-scale-kvm-mml</li></ul></div></div></div></div><p>
  This example deploys an entry-scale cloud that provides metering and
  monitoring services and runs the database and messaging services in their own
  cluster.
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.7.6.3.3.1"><span class="term ">Control Plane</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Cluster1</strong></span> 2 nodes of type
       <code class="literal">CONTROLLER-ROLE</code> run the core OpenStack services, such
       as keystone, nova API, glance API, neutron API, horizon, and
       heat API.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cluster2</strong></span> 3 nodes of type
       <code class="literal">MTRMON-ROLE</code>, run the OpenStack services for metering
       and monitoring (for example, ceilometer, monasca and Logging).
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cluster3</strong></span> 3 nodes of type
       <code class="literal">DBMQ-ROLE</code> that run clustered database and RabbitMQ
       services to support the cloud infrastructure. 3 nodes are required for
       high availability.
      </p></li></ul></div></dd><dt id="id-1.3.4.7.6.3.3.2"><span class="term ">Cloud Lifecycle Manager</span></dt><dd><p>
     The Cloud Lifecycle Manager runs on one of the control-plane nodes of type
     <code class="literal">CONTROLLER-ROLE</code>. The IP address of the node that will
     run the Cloud Lifecycle Manager needs to be included in the
     <code class="filename">data/servers.yml</code>file.
    </p></dd><dt id="id-1.3.4.7.6.3.3.3"><span class="term ">Resource Nodes</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Compute</strong></span> 1 node of type
       <code class="literal">COMPUTE-ROLE</code> runs nova Compute and associated
       services.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Object Storage</strong></span> Minimal swift
       resources are provided by the control plane.
      </p></li></ul></div><p>
     Additional resource nodes can be added to the configuration.
    </p></dd><dt id="id-1.3.4.7.6.3.3.4"><span class="term ">Networking</span></dt><dd><p>
     This example requires the following networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>IPMI</strong></span> network connected to the
       lifecycle-manager and the IPMI ports of all servers.
      </p></li></ul></div><p>
     Nodes require a pair of bonded NICs which are used by the following
     networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>External API</strong></span> The network for making
       requests to the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>External VM</strong></span> The network that provides
       access to VMs via floating IP addresses.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cloud Management</strong></span> This is the network
       that is used for all internal traffic between the cloud services. It is
       also used to install and configure the nodes. The network needs to be on
       an untagged VLAN.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Guest</strong></span> The network that carries traffic
       between VMs on private networks within the cloud.
      </p></li></ul></div><p>
     The <code class="literal">EXTERNAL API</code> network must be reachable from the
     <code class="literal">EXTERNAL VM</code> network for VMs to be able to make API
     calls to the cloud.
    </p><p>
     An example set of networks is defined in
     <code class="filename">data/networks.yml</code>. The file needs to be modified to
     reflect your environment.
    </p><p>
     The example uses the devices <code class="filename">hed3</code> and
     <code class="filename">hed4</code> as a bonded network interface for all services.
     The name given to a network interface by the system is configured in the
     file <code class="filename">data/net_interfaces.yml</code>. That file needs to be
     edited to match your system.
    </p></dd><dt id="id-1.3.4.7.6.3.3.5"><span class="term ">Local Storage</span></dt><dd><p>
     All servers should present a single OS disk, protected by a RAID
     controller. This disk needs to be at least 512 GB of capacity. In addition,
     the example configures one additional disk depending on the role of
     the server:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Core Controllers</strong></span>
       <code class="filename">/dev/sdb</code> and <code class="filename">/dev/sdc</code> is
       configured to be used by swift.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>DBMQ Controllers</strong></span>
       <code class="filename">/dev/sdb</code> is configured as an additional Volume
       Group to be used by the database and RabbitMQ.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Compute Servers</strong></span>
       <code class="filename">/dev/sdb</code> is configured as an additional Volume
       Group to be used for VM storage.
      </p></li></ul></div><p>
     Additional disks can be configured for any of these roles by editing the
     corresponding <code class="filename">data/disks_*.yml</code> file.
    </p></dd></dl></div></div><div class="sect2" id="mid-scale-kvm"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Single-Region Mid-Size Model</span> <a title="Permalink" class="permalink" href="#mid-scale-kvm">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-examples-mid-scale-kvm.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-mid-scale-kvm.xml</li><li><span class="ds-label">ID: </span>mid-scale-kvm</li></ul></div></div></div></div><p>
  The mid-size model is intended as a template for a moderate sized cloud. The
  Control plane is made up of multiple server clusters to provide sufficient
  computational, network and IOPS capacity for a mid-size production style
  cloud.
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.7.6.4.3.1"><span class="term ">Control Plane</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Core Cluster</strong></span> runs core OpenStack
       Services, such as keystone, nova API, glance API, neutron API,
       horizon, and heat API. Default configuration is two nodes of role
       type <code class="literal">CORE-ROLE</code>.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Metering and Monitoring Cluster</strong></span> runs
       the OpenStack Services for metering and monitoring (for example,
       ceilometer, monasca and logging). Default configuration is three
       nodes of role type <code class="literal">MTRMON-ROLE</code>.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Database and Message Queue Cluster</strong></span> runs
       clustered MariaDB and RabbitMQ services to support the Ardana cloud
       infrastructure. Default configuration is three nodes of role type
       <code class="literal">DBMQ-ROLE</code>. Three nodes are required for high
       availability.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>swift PAC Cluster</strong></span> runs the swift
       Proxy, Account and Container services. Default configuration is three
       nodes of role type <code class="literal">SWPAC-ROLE</code>.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>neutron Agent Cluster</strong></span> Runs neutron
       VPN (L3), DHCP, Metadata and OpenVswitch agents. Default configuration
       is two nodes of role type <code class="literal">NEUTRON-ROLE</code>.
      </p></li></ul></div></dd><dt id="id-1.3.4.7.6.4.3.2"><span class="term ">Cloud Lifecycle Manager</span></dt><dd><p>
     The Cloud Lifecycle Manager runs on one of the control-plane nodes of type
     <code class="literal">CONTROLLER-ROLE</code>. The IP address of the node that will
     run the Cloud Lifecycle Manager needs to be included in the
     <code class="filename">data/servers.yml</code> file.
    </p></dd><dt id="id-1.3.4.7.6.4.3.3"><span class="term ">Resource Nodes</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Compute</strong></span> runs nova Compute and
       associated services. Runs on nodes of role type
       <code class="literal">COMPUTE-ROLE</code>. This model lists 3 nodes. 1 node is the
       minimum requirement.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Object Storage</strong></span> 3 nodes of type
       <code class="literal">SOWBJ-ROLE</code> run the swift Object service. The
       minimum node count should match your swift replica count.
      </p></li></ul></div><p>
     The minimum node count required to run this model unmodified is 19 nodes.
     This can be reduced by consolidating services on the control plane
     clusters.
    </p></dd><dt id="id-1.3.4.7.6.4.3.4"><span class="term ">Networking</span></dt><dd><p>
     This example requires the following networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>IPMI</strong></span> network connected to the
       lifecycle-manager and the IPMI ports of all servers.
      </p></li></ul></div><p>
     Nodes require a pair of bonded NICs which are used by the following
     networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>External API</strong></span> The network for making
       requests to the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Internal API</strong></span> This network is used
       within the cloud for API access between services.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>External VM</strong></span> This network provides
       access to VMs via floating IP addresses.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cloud Management</strong></span> This network is used
       for all internal traffic between the cloud services. It is also used to
       install and configure the nodes. The network needs to be on an untagged
       VLAN.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Guest</strong></span> The network that carries traffic
       between VMs on private networks within the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>SWIFT</strong></span> This network is used for internal
       swift communications between the swift nodes.
      </p></li></ul></div><p>
     The <code class="literal">EXTERNAL API</code> network must be reachable from the
     <code class="literal">EXTERNAL VM</code> network for VMs to be able to make API
     calls to the cloud.
    </p><p>
     An example set of networks is defined in
     <code class="filename">data/networks.yml</code>. The file needs to be modified to
     reflect your environment.
    </p><p>
     The example uses the devices <code class="filename">hed3</code> and
     <code class="filename">hed4</code> as a bonded network interface for all services.
     The name given to a network interface by the system is configured in the
     file <code class="filename">data/net_interfaces.yml</code>. That file needs to be
     edited to match your system.
    </p></dd></dl></div><div class="sect3" id="id-1.3.4.7.6.4.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.3.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adapting the Mid-Size Model to Fit Your Environment</span> <a title="Permalink" class="permalink" href="#id-1.3.4.7.6.4.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-examples-mid-scale-kvm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-mid-scale-kvm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The minimum set of changes you need to make to adapt the model for your
   environment are:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Update <code class="filename">servers.yml</code> to list the details of your
     baremetal servers.
    </p></li><li class="listitem "><p>
     Update the <code class="filename">networks.yml</code> file to replace network CIDRs
     and VLANs with site specific values.
    </p></li><li class="listitem "><p>
     Update the <code class="filename">nic_mappings.yml</code> file to ensure that
     network devices are mapped to the correct physical port(s).
    </p></li><li class="listitem "><p>
     Review the disk models (<code class="filename">disks_*.yml</code>) and confirm that
     the associated servers have the number of disks required by the disk
     model. The device names in the disk models might need to be adjusted to
     match the probe order of your servers. The default number of disks for the
     swift nodes (3 disks) is set low on purpose to facilitate deployment on
     generic hardware. For production scale swift the servers should have
     more disks. For example, 6 on SWPAC nodes and 12 on SWOBJ nodes. If you
     allocate more swift disks then you should review the ring power in the
     swift ring configuration. This is documented in the swift section.
     Disk models are provided as follows:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       DISK SET CONTROLLER: Minimum 1 disk
      </p></li><li class="listitem "><p>
       DISK SET DBMQ: Minimum 3 disks
      </p></li><li class="listitem "><p>
       DISK SET COMPUTE: Minimum 2 disks
      </p></li><li class="listitem "><p>
       DISK SET SWPAC: Minimum 3 disks
      </p></li><li class="listitem "><p>
       DISK SET SWOBJ: Minimum 3 disks
      </p></li></ul></div></li><li class="listitem "><p>
     Update the <code class="filename">netinterfaces.yml</code> file to match the server
     NICs used in your configuration. This file has a separate interface model
     definition for each of the following:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       INTERFACE SET CONTROLLER
      </p></li><li class="listitem "><p>
       INTERFACE SET DBMQ
      </p></li><li class="listitem "><p>
       INTERFACE SET SWPAC
      </p></li><li class="listitem "><p>
       INTERFACE SET SWOBJ
      </p></li><li class="listitem "><p>
       INTERFACE SET COMPUTE
      </p></li></ul></div></li></ul></div></div></div></div><div class="sect1" id="esx-examples"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">ESX Examples</span> <a title="Permalink" class="permalink" href="#esx-examples">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-examples-esx_examples.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-esx_examples.xml</li><li><span class="ds-label">ID: </span>esx-examples</li></ul></div></div></div></div><div class="sect2" id="entry-scale-kvm-esx"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Single-Region Entry-Scale Cloud with a Mix of KVM and ESX Hypervisors</span> <a title="Permalink" class="permalink" href="#entry-scale-kvm-esx">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-examples-entry-scale-kvm-esx.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-entry-scale-kvm-esx.xml</li><li><span class="ds-label">ID: </span>entry-scale-kvm-esx</li></ul></div></div></div></div><p>
  This example deploys a cloud which mixes KVM and ESX hypervisors.
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.7.7.2.3.1"><span class="term ">Control Plane</span></dt><dd><p>
     <span class="bold"><strong>Cluster1</strong></span> 3 nodes of type
     <code class="literal">CONTROLLER-ROLE</code> run the core <span class="productname">OpenStack</span> services, such as
     keystone, nova API, glance API, neutron API, horizon, and heat
     API.
    </p></dd><dt id="id-1.3.4.7.7.2.3.2"><span class="term ">Cloud Lifecycle Manager</span></dt><dd><p>
     The Cloud Lifecycle Manager runs on one of the control-plane nodes of type
     <code class="literal">CONTROLLER-ROLE</code>. The IP address of the node that will
     run the Cloud Lifecycle Manager needs to be included in the
     <code class="filename">data/servers.yml</code> file.
    </p></dd><dt id="id-1.3.4.7.7.2.3.3"><span class="term ">Resource Nodes</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Compute:
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         <span class="bold"><strong>KVM</strong></span> runs nova Computes and
         associated services. It runs on nodes of role type
         <code class="literal">COMPUTE-ROLE</code>.
        </p></li><li class="listitem "><p>
         <span class="bold"><strong>ESX</strong></span> provides ESX Compute services. OS
         and software on this node is installed by user.
        </p></li></ul></div></li></ul></div></dd><dt id="id-1.3.4.7.7.2.3.4"><span class="term ">ESX Resource Requirements</span></dt><dd><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
       User needs to supply vSphere server
      </p></li><li class="listitem "><p>
       User needs to deploy the ovsvapp network resources using the
       vSphere GUI (<a class="xref" href="#create-esxi-mgmt-dvs" title="27.8.2. Creating ESXi MGMT DVS and Required Portgroup">Section 27.8.2, “Creating ESXi MGMT DVS and Required Portgroup”</a>) by running the
       <code class="literal">neutron-create-ovsvapp-resources.yml</code> playbook
       (<a class="xref" href="#config-ansible-playbook" title="27.8.3. Configuring OVSvApp Network Resources Using Ansible-Playbook">Section 27.8.3, “Configuring OVSvApp Network Resources Using Ansible-Playbook”</a>) or via Python-Networking-vSphere
       (<a class="xref" href="#config-ovsvapp-python-vsphere" title="27.8.4. Configuring OVSVAPP Using Python-Networking-vSphere">Section 27.8.4, “Configuring OVSVAPP Using Python-Networking-vSphere”</a>)
      </p><p>
       The following DVS and DVPGs need to be created and configured for each
       cluster in each ESX hypervisor that will host an OvsVapp appliance. The
       settings for each DVS and DVPG are specific to your system and network
       policies. A JSON file example is provided in the documentation, but it
       needs to be edited to match your requirements.
      </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /></colgroup><tbody><tr><td><span class="bold"><strong>DVS</strong></span></td><td><span class="bold"><strong>Port Groups assigned to DVS</strong></span></td></tr><tr><td>MGMT</td><td>MGMT-PG, ESX-CONF-PG, GUEST-PG</td></tr><tr><td>TRUNK</td><td>TRUNK-PG</td></tr></tbody></table></div></li><li class="listitem "><p>
       User needs to deploy ovsvapp appliance (<code class="literal">OVSVAPP-ROLE</code>)
       and nova-proxy appliance (<code class="literal">ESX-COMPUTE-ROLE</code>)
      </p></li><li class="listitem "><p>
       User needs to add required information related to compute proxy and
       OVSvApp Nodes
      </p></li></ol></div></dd><dt id="id-1.3.4.7.7.2.3.5"><span class="term ">Networking</span></dt><dd><p>
     This example requires the following networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>IPMI</strong></span>network connected to the
       lifecycle-manager and the IPMI ports of all nodes, except the ESX
       hypervisors.
      </p></li></ul></div><p>
     Nodes require a pair of bonded NICs which are used by the following
     networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>External API</strong></span> The network for
       making requests to the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>External VM</strong></span> The network that
       provides access to VMs via floating IP addresses.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cloud Management</strong></span> The network
       used for all internal traffic between the cloud services. It is also
       used to install and configure the nodes. The network needs to be on an
       untagged VLAN.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Guest</strong></span> This network carries
       traffic between VMs on private networks within the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>SES</strong></span> This is the network that
       control-plane and compute-node clients use to talk to the external SUSE Enterprise Storage.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>TRUNK</strong></span> is the network that is used
       to apply security group rules on tenant traffic. It is managed by the
       cloud admin and is restricted to the vCenter environment.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>ESX-CONF-NET</strong></span> network is used
       only to configure the ESX compute nodes in the cloud. This network
       should be different from the network used with PXE to stand up the cloud
       control-plane.
      </p></li></ul></div><p>
     This example's set of networks is defined in
     <code class="filename">data/networks.yml</code>. The file needs to be modified to
     reflect your environment.
    </p><p>
     The example uses the devices <code class="filename">hed3</code> and
     <code class="filename">hed4</code> as a bonded network interface for all services.
     The name given to a network interface by the system is configured in the
     file <code class="filename">data/net_interfaces.yml</code>. That file needs to be
     edited to match your system.
    </p></dd><dt id="id-1.3.4.7.7.2.3.6"><span class="term ">Local Storage</span></dt><dd><p>
     All servers should present a single OS disk, protected by a RAID
     controller. This disk needs to be at least 512 GB in capacity. In addition,
     the example configures additional disk depending on the node's role:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Controllers</strong></span>
       <code class="filename">/dev/sdb</code> and <code class="filename">/dev/sdc</code> are
       configured to be used by swift
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Compute Servers</strong></span>
       <code class="filename">/dev/sdb</code> is configured as an additional Volume
       Group to be used for VM storage
      </p></li></ul></div><p>
     Additional disks can be configured for any of these roles by editing the
     corresponding <code class="filename">data/disks_*.yml</code> file.
    </p></dd></dl></div></div><div class="sect2" id="entry-scale-kvm-esx-mml"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Single-Region Entry-Scale Cloud with Metering and Monitoring Services, and a Mix of KVM and ESX Hypervisors</span> <a title="Permalink" class="permalink" href="#entry-scale-kvm-esx-mml">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-examples-entry-scale-kvm-esx-mml.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-entry-scale-kvm-esx-mml.xml</li><li><span class="ds-label">ID: </span>entry-scale-kvm-esx-mml</li></ul></div></div></div></div><p>
  This example deploys a cloud which mixes KVM and ESX hypervisors, provides
  metering and monitoring services, and runs the database and messaging
  services in their own cluster.
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.7.7.3.3.1"><span class="term ">Control Plane</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Cluster1</strong></span> 2 nodes of type
       <code class="literal">CONTROLLER-ROLE</code> run the core <span class="productname">OpenStack</span> services, such
       as keystone, nova API, glance API, neutron API, horizon, and
       heat API.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cluster2</strong></span> 3 nodes of type
       <code class="literal">MTRMON-ROLE</code>, run the <span class="productname">OpenStack</span> services for metering
       and monitoring (for example, ceilometer, monasca and Logging).
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cluster3</strong></span> 3 nodes of type
       <code class="literal">DBMQ-ROLE</code>, run clustered database and RabbitMQ
       services to support the cloud infrastructure. 3 nodes are required for
       high availability.
      </p></li></ul></div></dd><dt id="id-1.3.4.7.7.3.3.2"><span class="term ">Cloud Lifecycle Manager</span></dt><dd><p>
     The Cloud Lifecycle Manager runs on one of the control-plane nodes of type
     <code class="literal">CONTROLLER-ROLE</code>. The IP address of the node that will
     run the Cloud Lifecycle Manager needs to be included in the
     <code class="filename">data/servers.yml</code> file.
    </p></dd><dt id="id-1.3.4.7.7.3.3.3"><span class="term ">Resource Nodes</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Compute:
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         <span class="bold"><strong>KVM</strong></span> runs nova Computes and
         associated services. It runs on nodes of role type
         <code class="literal">COMPUTE-ROLE</code>.
        </p></li><li class="listitem "><p>
         <span class="bold"><strong>ESX</strong></span> provides ESX Compute services. OS
         and software on this node is installed by user.
        </p></li></ul></div></li></ul></div></dd><dt id="id-1.3.4.7.7.3.3.4"><span class="term "> ESX Resource Requirements </span></dt><dd><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
       User needs to supply vSphere server
      </p></li><li class="listitem "><p>
       User needs to deploy the ovsvapp network resources using the
       vSphere GUI or by running the
       <code class="literal">neutron-create-ovsvapp-resources.yml</code> playbook
      </p><p>
       The following DVS and DVPGs need to be created and configured for each
       cluster in each ESX hypervisor that will host an OvsVapp appliance. The
       settings for each DVS and DVPG are specific to your system and network
       policies. A JSON file example is provided in the documentation, but it
       needs to be edited to match your requirements.
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         ESX-CONF (DVS and DVPG) connected to ovsvapp eth0 and compute-proxy
         eth0
        </p></li><li class="listitem "><p>
         MANAGEMENT (DVS and DVPG) connected to ovsvapp eth1, eth2, eth3 and compute-proxy
         eth1
        </p></li></ul></div></li><li class="listitem "><p>
       User needs to deploy ovsvapp appliance (<code class="literal">OVSVAPP-ROLE</code>)
       and nova-proxy appliance (<code class="literal">ESX-COMPUTE-ROLE</code>)
      </p></li><li class="listitem "><p>
       User needs to add required information related to compute proxy and
       OVSvApp Nodes
      </p></li></ol></div></dd><dt id="id-1.3.4.7.7.3.3.5"><span class="term ">Networking</span></dt><dd><p>
     This example requires the following networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>IPMI</strong></span>network connected to the
       lifecycle-manager and the IPMI ports of all nodes, except the ESX
       hypervisors.
      </p></li></ul></div><p>
     Nodes require a pair of bonded NICs which are used by the following
     networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>External API</strong></span> The network for
       making requests to the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>External VM</strong></span> The network that
       provides access to VMs (via floating IP addresses).
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cloud Management</strong></span> This
       network is used for all internal traffic between the cloud services. It
       is also used to install and configure the nodes. The network needs to be
       on an untagged VLAN.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Guest</strong></span> This is the network that will
       carry traffic between VMs on private networks within the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>TRUNK</strong></span> is the network that will be used
       to apply security group rules on tenant traffic. It is managed by the
       cloud admin and is restricted to the vCenter environment.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>ESX-CONF-NET</strong></span> network is used
       only to configure the ESX compute nodes in the cloud. This network
       should be different from the network used with PXE to stand up the cloud
       control-plane.
      </p></li></ul></div><p>
     This example's set of networks is defined in
     <code class="filename">data/networks.yml</code>. The file needs to be modified to
     reflect your environment.
    </p><p>
     The example uses the devices <code class="filename">hed3</code> and
     <code class="filename">hed4</code> as a bonded network interface for all services.
     The name given to a network interface by the system is configured in the
     file <code class="filename">data/net_interfaces.yml</code>. That file needs to be
     edited to match your system.
    </p></dd><dt id="id-1.3.4.7.7.3.3.6"><span class="term ">Local Storage</span></dt><dd><p>
     All servers should present a single OS disk, protected by a RAID
     controller. This disk needs to be at least 512 GB in capacity. In
     addition, the example configures additional disk depending on the node's
     role:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Controllers</strong></span>
       <code class="filename">/dev/sdb</code> and <code class="filename">/dev/sdc</code> are
       configured to be used by swift.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Compute Servers</strong></span>
       <code class="filename">/dev/sdb</code> is configured as an additional Volume
       Group to be used for VM storage
      </p></li></ul></div><p>
     Additional disks can be configured for any of these roles by editing the
     corresponding <code class="filename">data/disks_*.yml</code> file
    </p></dd></dl></div></div></div><div class="sect1" id="swift-examples"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Swift Examples</span> <a title="Permalink" class="permalink" href="#swift-examples">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-examples-swift_examples.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-swift_examples.xml</li><li><span class="ds-label">ID: </span>swift-examples</li></ul></div></div></div></div><div class="sect2" id="entryscale-swift"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Entry-scale swift Model</span> <a title="Permalink" class="permalink" href="#entryscale-swift">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-examples-entryscale_swift.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-entryscale_swift.xml</li><li><span class="ds-label">ID: </span>entryscale-swift</li></ul></div></div></div></div><p>
  This example shows how <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> can be configured to provide a swift-only
  configuration, consisting of three controllers and one or more swift object
  servers.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-examples-entry_scale_swift.png" target="_blank"><img src="images/media-examples-entry_scale_swift.png" width="" /></a></div></div><p>
  The example requires the following networks:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <span class="bold"><strong>External API</strong></span> - The network for making
    requests to the cloud.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>swift</strong></span> - The network for all data traffic
    between the swift services.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Management</strong></span> - This network that is used for
    all internal traffic between the cloud services, including node
    provisioning. This network must be on an untagged VLAN.
   </p></li></ul></div><p>
  All of these networks are configured to be presented via a pair of bonded
  NICs. The example also enables provider VLANs to be configured in neutron on
  this interface.
 </p><p>
  In the diagram "External Routing" refers to whatever routing you want to
  provide to allow users to access the External API. "Internal Routing" refers
  to whatever routing you want to provide to allow administrators to access the
  Management network.
 </p><p>
  If you are using <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to install the operating system, then an IPMI
  network connected to the IPMI ports of all servers and routable from the
  Cloud Lifecycle Manager is also required for BIOS and power management of the node during the
  operating system installation process.
 </p><p>
  In the example the controllers use one disk for the operating system and two
  disks for swift proxy and account storage. The swift object servers use one
  disk for the operating system and four disks for swift storage. These values
  can be modified to suit your environment.
 </p><p>
  These recommended minimums are based on the included with the base
  installation and are suitable only for demo environments. For production
  systems you will want to consider your capacity and performance requirements
  when making decisions about your hardware.
 </p><p>
  The <code class="literal">entry-scale-swift</code> example runs the swift proxy,
  account and container services on the three controller servers. However, it
  is possible to extend the model to include the swift proxy, account and
  container services on dedicated servers (typically referred to as the swift
  proxy servers). If you are using this model, we have included the recommended
  swift proxy servers specs in the table below.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /><col class="c6" /><col class="c7" /></colgroup><thead><tr><th rowspan="2">Node Type</th><th rowspan="2">Role Name</th><th rowspan="2">Required Number</th><th colspan="4" align="center">Server Hardware - Minimum Requirements and
            Recommendations</th></tr><tr><th>Disk </th><th>Memory</th><th>Network</th><th>CPU </th></tr></thead><tbody><tr><td>Dedicated Cloud Lifecycle Manager (optional)</td><td>Lifecycle-manager</td><td>1</td><td>300 GB</td><td>8 GB</td><td>1 x 10 Gbit/s with PXE Support</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>Control Plane</td><td>Controller</td><td>3</td><td>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         1 x 600 GB (minimum) - operating system drive
        </p></li><li class="listitem "><p>
         2 x 600 GB (minimum) - swift account/container data drive
        </p></li></ul></div>
     </td><td>64 GB</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>swift Object</td><td>swobj</td><td>3</td><td>
      <p>
       If using x3 replication only:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         1 x 600 GB (minimum, see considerations at bottom of page for more
         details)
        </p></li></ul></div>
      <p>
       If using Erasure Codes only or a mix of x3 replication and Erasure
       Codes:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         6 x 600 GB (minimum, see considerations at bottom of page for more
         details)
        </p></li></ul></div>
     </td><td>32 GB (see considerations at bottom of page for more details)</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr><tr><td>swift Proxy, Account, and Container</td><td>swpac</td><td>3</td><td>2 x 600 GB (minimum, see considerations at bottom of page for more details)</td><td>64 GB (see considerations at bottom of page for more details)</td><td>2 x 10 Gbit/s with one PXE enabled port</td><td>8 CPU (64-bit) cores total (Intel x86_64)</td></tr></tbody></table></div><div id="id-1.3.4.7.8.2.13" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   The disk speeds (RPM) chosen should be consistent within the same ring or
   storage policy. It is best to not use disks with mixed disk speeds within the
   same swift ring.
  </p></div><p>
  <span class="bold"><strong>Considerations for your swift object and proxy,
  account, container servers RAM and disk capacity needs</strong></span>
 </p><p>
  swift can have a diverse number of hardware configurations. For example, a
  swift object server may have just a few disks (minimum of 6 for erasure
  codes) or up to 70 and beyond. The memory requirement needs to be increased
  as more disks are added. The general rule of thumb for memory needed is 0.5
  GB per TB of storage. For example, a system with 24 hard drives at 8TB each,
  giving a total capacity of 192TB, should use 96GB of RAM. However, this does
  not work well for a system with a small number of small hard drives or a very
  large number of very large drives. So, if after calculating the memory given
  this guideline, if the answer is less than 32GB then go with 32GB of memory
  minimum and if the answer is over 256GB then use 256GB maximum, no need to
  use more memory than that.
 </p><p>
  When considering the capacity needs for the swift proxy, account, and
  container (PAC) servers, you should calculate 2% of the total raw storage
  size of your object servers to specify the storage required for the PAC
  servers. So, for example, if you were using the example we provided earlier
  and you had an object server setup of 24 hard drives with 8TB each for a
  total of 192TB and you had a total of 6 object servers, that would give a raw
  total of 1152TB. So you would take 2% of that, which is 23TB, and ensure that
  much storage capacity was available on your swift proxy, account, and
  container (PAC) server cluster. If you had a cluster of three swift PAC
  servers, that would be ~8TB each.
 </p><p>
  Another general rule of thumb is that if you are expecting to have more than
  a million objects in a container then you should consider using SSDs on the
  swift PAC servers rather than HDDs.
 </p></div></div><div class="sect1" id="ironic-examples"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ironic Examples</span> <a title="Permalink" class="permalink" href="#ironic-examples">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-examples-ironic_examples.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-ironic_examples.xml</li><li><span class="ds-label">ID: </span>ironic-examples</li></ul></div></div></div></div><div class="sect2" id="entryscale-ironic"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Entry-Scale Cloud with Ironic Flat Network</span> <a title="Permalink" class="permalink" href="#entryscale-ironic">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-examples-entryscale_ironic.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-entryscale_ironic.xml</li><li><span class="ds-label">ID: </span>entryscale-ironic</li></ul></div></div></div></div><p>
  This example deploys an entry scale cloud that uses the ironic service to
  provision physical machines through the Compute services API.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-exampleconfigs-entry_scale_ironic.png" target="_blank"><img src="images/media-hos.docs-exampleconfigs-entry_scale_ironic.png" width="" /></a></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.7.9.2.4.1"><span class="term ">Control Plane</span></dt><dd><p>
     <span class="bold"><strong>Cluster1</strong></span> 3 nodes of type
     <code class="literal">CONTROLLER-ROLE</code> run the core OpenStack services, such
     as keystone, nova API, glance API, neutron API, horizon, and
     heat API.
    </p></dd><dt id="id-1.3.4.7.9.2.4.2"><span class="term ">Cloud Lifecycle Manager</span></dt><dd><p>
     The Cloud Lifecycle Manager runs on one of the control-plane nodes of type
     <code class="literal">CONTROLLER-ROLE</code>. The IP address of the node that will
     run the Cloud Lifecycle Manager needs to be included in the
     <code class="filename">data/servers.yml</code> file.
    </p></dd><dt id="id-1.3.4.7.9.2.4.3"><span class="term ">Resource Nodes</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>ironic Compute</strong></span> One node of type
       <code class="literal">IRONIC-COMPUTE-ROLE</code> runs nova-compute,
       nova-compute-ironic, and other supporting services.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Object Storage</strong></span> Minimal swift
       resources are provided by the control plane.
      </p></li></ul></div></dd><dt id="id-1.3.4.7.9.2.4.4"><span class="term ">Networking</span></dt><dd><p>
     This example requires the following networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>IPMI</strong></span> network connected to the
       lifecycle-manager and the IPMI ports of all servers.
      </p></li></ul></div><p>
     Nodes require a pair of bonded NICs which are used by the following
     networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>External API</strong></span> This is the network that
       users will use to make requests to the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cloud Management</strong></span> This is the network
       that will be used for all internal traffic between the cloud services.
       This network is also used to install and configure the nodes. The
       network needs to be on an untagged VLAN.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Guest</strong></span> This is the flat network that
       will carry traffic between bare metal instances within the cloud. It is
       also used to PXE boot said bare metal instances and install the
       operating system selected by tenants.
      </p></li></ul></div><p>
     The <code class="literal">EXTERNAL API</code> network must be reachable from the
     <code class="literal">GUEST</code> network for the bare metal instances to make API
     calls to the cloud.
    </p><p>
     An example set of networks is defined in
     <code class="filename">data/networks.yml</code>. The file needs to be modified to
     reflect your environment.
    </p><p>
     The example uses the devices <code class="filename">hed3</code> and
     <code class="filename">hed4</code> as a bonded network interface for all services.
     The name given to a network interface by the system is configured in the
     file <code class="filename">data/net_interfaces.yml</code>. That file needs to be
     modified to match your system.
    </p></dd><dt id="id-1.3.4.7.9.2.4.5"><span class="term ">Local Storage</span></dt><dd><p>
     All servers should present a single OS disk, protected by a RAID
     controller. This disk needs to be at least 512 GB in capacity. In
     addition the example configures one additional disk depending on the role
     of the server:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Controllers</strong></span>
       <code class="filename">/dev/sdb</code> and <code class="filename">/dev/sdc</code>
       configured to be used by swift.
      </p></li></ul></div><p>
     Additional discs can be configured for any of these roles by editing the
     corresponding <code class="filename">data/disks_*.yml</code> file.
    </p></dd></dl></div></div><div class="sect2" id="entryscale-ironic-multi-tenancy"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Entry-Scale Cloud with Ironic Multi-Tenancy</span> <a title="Permalink" class="permalink" href="#entryscale-ironic-multi-tenancy">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-examples-entryscale_ironic_multi_tenancy.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-examples-entryscale_ironic_multi_tenancy.xml</li><li><span class="ds-label">ID: </span>entryscale-ironic-multi-tenancy</li></ul></div></div></div></div><p>
  This example deploys an entry scale cloud that uses the ironic service to
  provision physical machines through the Compute services API and supports
  multi tenancy.
 </p><div class="figure" id="multi-tenancy"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/media-ironic-Entry-ScaleIronicMultiTenancy.png" target="_blank"><img src="images/media-ironic-Entry-ScaleIronicMultiTenancy.png" width="" alt="Entry-scale Cloud with Ironic Muti-Tenancy" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 9.1: </span><span class="name">Entry-scale Cloud with Ironic Muti-Tenancy </span><a title="Permalink" class="permalink" href="#multi-tenancy">#</a></h6></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.7.9.3.4.1"><span class="term ">Control Plane</span></dt><dd><p>
     <span class="bold"><strong>Cluster1</strong></span> 3 nodes of type
     <code class="literal">CONTROLLER-ROLE</code> run the core <span class="productname">OpenStack</span> services, such as
     keystone, nova API, glance API, neutron API, horizon, and heat
     API.
    </p></dd><dt id="id-1.3.4.7.9.3.4.2"><span class="term ">Cloud Lifecycle Manager</span></dt><dd><p>
     The Cloud Lifecycle Manager runs on one of the control-plane nodes of type
     <code class="literal">CONTROLLER-ROLE</code>. The IP address of the node that will
     run the Cloud Lifecycle Manager needs to be included in the
     <code class="filename">data/servers.yml</code>file.
    </p></dd><dt id="id-1.3.4.7.9.3.4.3"><span class="term ">Resource Nodes</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>ironic Compute</strong></span> One node of type
       <code class="literal">IRONIC-COMPUTE-ROLE</code> runs nova-compute,
       nova-compute-ironic, and other supporting services.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Object Storage</strong></span> Minimal swift
       Resources are provided by the control plane.
      </p></li></ul></div></dd><dt id="id-1.3.4.7.9.3.4.4"><span class="term ">Networking</span></dt><dd><p>
     This example requires the following networks:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>IPMI</strong></span> network connected to the
       deployer and the IPMI ports of all nodes.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>External API</strong></span> network is used to make
       requests to the cloud.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Cloud Management</strong></span> This is the network
       that will be used for all internal traffic between the cloud services.
       This network is also used to install and configure the controller nodes.
       The network needs to be on an untagged VLAN.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Provisioning</strong></span> is the network used to PXE
       boot the ironic nodes and install the operating system selected by
       tenants. This network needs to be tagged on the switch for control
       plane/ironic compute nodes. For ironic bare metal nodes, VLAN
       configuration on the switch will be set by neutron driver.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Tenant VLANs</strong></span> The range of VLAN IDs
       should be reserved for use by ironic and set in the cloud configuration.
       It is configured as untagged on control plane nodes, therefore it cannot
       be combined with management network on the same network interface.
      </p></li></ul></div><p>
     The following access should be allowed by routing/firewall:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Access from Management network to IPMI. Used during cloud
       installation and during ironic bare metal node provisioning.
      </p></li><li class="listitem "><p>
       Access from Management network to switch management network. Used by
       neutron driver.
      </p></li><li class="listitem "><p>
       The <code class="literal">EXTERNAL API</code> network must be reachable from the
       tenant networks if you want bare metal nodes to be able to make API
       calls to the cloud.
      </p></li></ul></div><p>
     An example set of networks is defined in
     <code class="filename">data/networks.yml</code>. The file needs to be modified to
     reflect your environment.
    </p><p>
     The example uses <code class="filename">hed3</code> for Management and External API
     traffic, and <code class="filename">hed4</code> for provisioning and tenant network
     traffic. If you need to modify these assignments for your environment,
     they are defined in <code class="filename">data/net_interfaces.yml</code>.
    </p></dd><dt id="id-1.3.4.7.9.3.4.5"><span class="term ">Local Storage</span></dt><dd><p>
     All servers should present a single OS disk, protected by a RAID
     controller. This disk needs to be at least 512 GB in capacity. In
     addition the example configures one additional disk depending on the role
     of the server:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>Controllers</strong></span>
       <code class="filename">/dev/sdb</code> and <code class="filename">/dev/sdc</code>
       configured to be used by swift.
      </p></li></ul></div><p>
     Additional disks can be configured for any of these roles by editing the
     corresponding <code class="filename">data/disks_*.yml</code> file.
    </p></dd></dl></div></div></div></div><div class="chapter " id="modify-compute-input-model"><div class="titlepage"><div><div><h2 class="title"><span class="number">10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Modifying Example Configurations for Compute Nodes</span> <a title="Permalink" class="permalink" href="#modify-compute-input-model">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-compute-model.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-compute-model.xml</li><li><span class="ds-label">ID: </span>modify-compute-input-model</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#sles-compute-model"><span class="number">10.1 </span><span class="name">SLES Compute Nodes</span></a></span></dt></dl></div></div><p>
  This section contains detailed information about the Compute Node parts of
  the input model. For example input models, see <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a>. For general descriptions of the input
  model, see <a class="xref" href="#configobj-networks" title="6.14. Networks">Section 6.14, “Networks”</a>.
 </p><p>
  Usually, the example models provide most of the data that is required to
  create a valid input model. However, before you start to deploy, you may want
  to customize an input model using the following information about Compute
  Nodes.
 </p><div class="sect1" id="sles-compute-model"><div class="titlepage"><div><div><h2 class="title"><span class="number">10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SLES Compute Nodes</span> <a title="Permalink" class="permalink" href="#sles-compute-model">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-alternative-sles_compute_model.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-alternative-sles_compute_model.xml</li><li><span class="ds-label">ID: </span>sles-compute-model</li></ul></div></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.4.8.4.2.1"><span class="term "><code class="filename">net_interfaces.yml</code></span></dt><dd><div class="verbatim-wrap"><pre class="screen">- name: <span class="bold"><strong>SLES-COMPUTE-INTERFACES</strong></span>
 network-interfaces:
   - name: BOND0
     device:
         name: bond0
     bond-data:
         options:
             mode: active-backup
             miimon: 200
             primary: hed1
         provider: linux
         devices:
             - name: hed1
             - name: hed2
     network-groups:
       - EXTERNAL-VM
       - GUEST
       - MANAGEMENT</pre></div></dd><dt id="id-1.3.4.8.4.2.2"><span class="term "><code class="filename">servers.yml</code></span></dt><dd><div class="verbatim-wrap"><pre class="screen">    - id: compute1
      ip-addr: 10.13.111.15
      <span class="bold"><strong>role: SLES-COMPUTE-ROLE</strong></span>
      server-group: RACK1
      nic-mapping: DL360p_G8_2Port
      mac-addr: ec:b1:d7:77:d0:b0
      ilo-ip: 10.12.13.14
      ilo-password: *********
      ilo-user: Administrator
      <span class="bold"><strong>distro-id: sles12sp4-x86_64</strong></span></pre></div></dd><dt id="id-1.3.4.8.4.2.3"><span class="term "><code class="filename">server_roles.yml</code></span></dt><dd><div class="verbatim-wrap"><pre class="screen">- name: <span class="bold"><strong>SLES-COMPUTE-ROLE</strong></span>
  interface-model: <span class="bold"><strong>SLES-COMPUTE-INTERFACES</strong></span>
  disk-model: <span class="bold"><strong>SLES-COMPUTE-DISKS</strong></span></pre></div></dd><dt id="id-1.3.4.8.4.2.4"><span class="term "><code class="filename">disk_compute.yml</code></span></dt><dd><div class="verbatim-wrap"><pre class="screen">  - name: <span class="bold"><strong>SLES-COMPUTE-DISKS</strong></span>
    volume-groups:
      - name: ardana-vg
        physical-volumes:
         - /dev/sda_root

        logical-volumes:
        # The policy is not to consume 100% of the space of each volume group.
        # 5% should be left free for snapshots and to allow for some flexibility.
          - name: root
            size: 35%
            fstype: ext4
            mount: /
          - name: log
            size: 50%
            mount: /var/log
            fstype: ext4
            mkfs-opts: -O large_file
          - name: crash
            size: 10%
            mount: /var/crash
            fstype: ext4
            mkfs-opts: -O large_file

      - name: vg-comp
        # this VG is dedicated to nova Compute to keep VM IOPS off the OS disk
        physical-volumes:
          - /dev/sdb
        logical-volumes:
          - name: compute
            size: 95%
            mount: /var/lib/nova
            fstype: ext4
            mkfs-opts: -O large_file</pre></div></dd><dt id="id-1.3.4.8.4.2.5"><span class="term "><code class="filename">control_plane.yml</code></span></dt><dd><div class="verbatim-wrap"><pre class="screen">  control-planes:
    - name: control-plane-1
      control-plane-prefix: cp1
      region-name: region0
....
      resources:
        - name: <span class="bold"><strong>sles-compute</strong></span>
          resource-prefix: <span class="bold"><strong>sles-comp</strong></span>
          server-role: <span class="bold"><strong>SLES-COMPUTE-ROLE</strong></span>
          allocation-policy: any
          min-count: 1
          service-components:
            - ntp-client
            - nova-compute
            - nova-compute-kvm
            - neutron-l3-agent
            - neutron-metadata-agent
            - neutron-openvswitch-agent</pre></div></dd></dl></div></div></div><div class="chapter " id="modify-input-model"><div class="titlepage"><div><div><h2 class="title"><span class="number">11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Modifying Example Configurations for Object Storage using Swift</span> <a title="Permalink" class="permalink" href="#modify-input-model">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-swift_input_model.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-swift_input_model.xml</li><li><span class="ds-label">ID: </span>modify-input-model</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#objectstorage-overview"><span class="number">11.1 </span><span class="name">Object Storage using swift Overview</span></a></span></dt><dt><span class="section"><a href="#topic-r3k-v2c-jt"><span class="number">11.2 </span><span class="name">Allocating Proxy, Account, and Container (PAC) Servers for Object Storage</span></a></span></dt><dt><span class="section"><a href="#topic-tq1-xt5-dt"><span class="number">11.3 </span><span class="name">Allocating Object Servers</span></a></span></dt><dt><span class="section"><a href="#topic-uh2-td1-kt"><span class="number">11.4 </span><span class="name">Creating Roles for swift Nodes</span></a></span></dt><dt><span class="section"><a href="#allocating-disk-drives"><span class="number">11.5 </span><span class="name">Allocating Disk Drives for Object Storage</span></a></span></dt><dt><span class="section"><a href="#topic-d1s-hht-tt"><span class="number">11.6 </span><span class="name">Swift Requirements for Device Group Drives</span></a></span></dt><dt><span class="section"><a href="#topic-rvj-21c-jt"><span class="number">11.7 </span><span class="name">Creating a Swift Proxy, Account, and Container (PAC) Cluster</span></a></span></dt><dt><span class="section"><a href="#topic-jzk-q1c-jt"><span class="number">11.8 </span><span class="name">Creating Object Server Resource Nodes</span></a></span></dt><dt><span class="section"><a href="#topic-pcj-hzv-dt"><span class="number">11.9 </span><span class="name">Understanding Swift Network and Service Requirements</span></a></span></dt><dt><span class="section"><a href="#ring-specification"><span class="number">11.10 </span><span class="name">Understanding Swift Ring Specifications</span></a></span></dt><dt><span class="section"><a href="#swift-storage-policies"><span class="number">11.11 </span><span class="name">Designing Storage Policies</span></a></span></dt><dt><span class="section"><a href="#designing-swift-zones"><span class="number">11.12 </span><span class="name">Designing Swift Zones</span></a></span></dt><dt><span class="section"><a href="#topic-rdf-hkp-rt"><span class="number">11.13 </span><span class="name">Customizing Swift Service Configuration Files</span></a></span></dt></dl></div></div><p>
  This section contains detailed descriptions about the swift-specific parts of
  the input model. For example input models, see
  <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a>. For general descriptions of the
  input model, see <a class="xref" href="#configobj-networks" title="6.14. Networks">Section 6.14, “Networks”</a>. In addition, the swift
  ring specifications are available in the
  <code class="filename">~/openstack/my_cloud/definition/data/swift/swift_config.yml</code>
  file.
 </p><p>
  Usually, the example models provide most of the data that is required to
  create a valid input model. However, before you start to deploy, you must do
  the following:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Check the disk model used by your nodes and that all disk drives are
    correctly named and used as described in
    <a class="xref" href="#topic-d1s-hht-tt" title="11.6. Swift Requirements for Device Group Drives">Section 11.6, “Swift Requirements for Device Group Drives”</a>.
   </p></li><li class="listitem "><p>
    Select an appropriate partition power for your rings. For more information,
    see <a class="xref" href="#ring-specification" title="11.10. Understanding Swift Ring Specifications">Section 11.10, “Understanding Swift Ring Specifications”</a>.
   </p></li></ul></div><p>
  For further information, read these related pages:
 </p><div class="sect1" id="objectstorage-overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Object Storage using swift Overview</span> <a title="Permalink" class="permalink" href="#objectstorage-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-objectstorage_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-objectstorage_overview.xml</li><li><span class="ds-label">ID: </span>objectstorage-overview</li></ul></div></div></div></div><div class="sect2" id="about"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">What is the Object Storage (swift) Service?</span> <a title="Permalink" class="permalink" href="#about">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-objectstorage_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-objectstorage_overview.xml</li><li><span class="ds-label">ID: </span>about</li></ul></div></div></div></div><p>
   The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Object Storage using swift service leverages swift which uses
   software-defined storage (SDS) layered on top of industry-standard servers
   using native storage devices. swift presents an object paradigm, using an
   underlying set of disk drives. The disk drives are managed by a data
   structure called a "ring" and you can store, retrieve, and delete objects in
   containers using RESTful APIs.
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Object Storage using swift provides a highly-available, resilient,
   and scalable storage pool for unstructured data. It has a highly-durable
   architecture, with no single point of failure. In addition, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   includes the concept of cloud models, where the user can modify the cloud
   input model to provide the configuration required for their environment.
  </p></div><div class="sect2" id="services"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Object Storage (swift) Services</span> <a title="Permalink" class="permalink" href="#services">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-objectstorage_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-objectstorage_overview.xml</li><li><span class="ds-label">ID: </span>services</li></ul></div></div></div></div><p>
   A swift system consists of a number of services:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     swift-proxy provides the API for all requests to the swift system.
    </p></li><li class="listitem "><p>
     Account and container services provide storage management of the accounts
     and containers.
    </p></li><li class="listitem "><p>
     Object services provide storage management for object storage.
    </p></li></ul></div><p>
   These services can be co-located in a number of ways. The following general
   pattern exists in the example cloud models distributed in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The swift-proxy, account, container, and object services run on the same
     (PACO) node type in the control plane. This is used for smaller clouds or
     where swift is a minor element in a larger cloud. This is the model seen
     in most of the entry-scale models.
    </p></li><li class="listitem "><p>
     The swift-proxy, account, and container services run on one (PAC) node
     type in a cluster in a control plane and the object services run on
     another (OBJ) node type in a resource pool. This deployment model, known
     as the Entry-Scale swift model, is used in larger clouds or where a larger
     swift system is in use or planned. See <a class="xref" href="#entryscale-swift" title="9.5.1. Entry-scale swift Model">Section 9.5.1, “Entry-scale swift Model”</a>
     for more details.
    </p></li></ul></div><p>
   The swift storage service can be scaled both vertically (nodes with larger
   or more disks) and horizontally (more swift storage nodes) to handle an
   increased number of simultaneous user connections and provide larger storage
   space.
  </p><p>
   swift is configured through a number of YAML files in the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   implementation of the OpenStack Object Storage (swift) service. For more
   details on the configuration of the YAML files, see
   <a class="xref" href="#modify-input-model" title="Chapter 11. Modifying Example Configurations for Object Storage using Swift">Chapter 11, <em>Modifying Example Configurations for Object Storage using Swift</em></a>.
  </p></div></div><div class="sect1" id="topic-r3k-v2c-jt"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Allocating Proxy, Account, and Container (PAC) Servers for Object Storage</span> <a title="Permalink" class="permalink" href="#topic-r3k-v2c-jt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-allocate_pac.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-allocate_pac.xml</li><li><span class="ds-label">ID: </span>topic-r3k-v2c-jt</li></ul></div></div></div></div><p>
  A swift proxy, account, and container (PAC) server is a node that runs the
  swift-proxy, swift-account and swift-container services. It is used to
  respond to API requests and to store account and container data. The PAC node
  does not store object data.
 </p><p>
  This section describes the procedure to allocate PAC servers during the
  <span class="bold"><strong>initial</strong></span> deployment of the system.
 </p><div class="sect2" id="id-1.3.4.9.7.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To Allocate Swift PAC Servers</span> <a title="Permalink" class="permalink" href="#id-1.3.4.9.7.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-allocate_pac.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-allocate_pac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Perform the following steps to allocate PAC servers:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Verify if the example input model already contains a suitable server role.
     The server roles are usually described in the
     <code class="literal">data/server_roles.yml</code> file. If the server role is not
     described, you must add a suitable server role and allocate drives to
     store object data. For instructions, see
     <a class="xref" href="#topic-uh2-td1-kt" title="11.4. Creating Roles for swift Nodes">Section 11.4, “Creating Roles for swift Nodes”</a> and
     <a class="xref" href="#allocating-disk-drives" title="11.5. Allocating Disk Drives for Object Storage">Section 11.5, “Allocating Disk Drives for Object Storage”</a>.
    </p></li><li class="listitem "><p>
     Verify if the example input model has assigned a cluster to swift proxy,
     account, container servers. It is usually mentioned in the
     <code class="literal">data/control_plane.yml</code> file. If the cluster is not
     assigned, then add a suitable cluster. For instructions, see
     <a class="xref" href="#topic-rvj-21c-jt" title="11.7. Creating a Swift Proxy, Account, and Container (PAC) Cluster">Section 11.7, “Creating a Swift Proxy, Account, and Container (PAC) Cluster”</a>.
    </p></li><li class="listitem "><p>
     Identify the physical servers and their IP address and other detailed
     information.
    </p><div class="itemizedlist " id="ul-tzf-pqb-jt"><ul class="itemizedlist"><li class="listitem "><p>
       You add these details to the servers list (usually in the
       <code class="literal">data/servers.yml</code> file).
      </p></li><li class="listitem "><p>
       As with all servers, you must also verify and/or modify the
       server-groups information (usually in
       <code class="literal">data/server_groups.yml</code>)
      </p></li></ul></div></li></ul></div><p>
   The only part of this process that is unique to swift is the allocation of
   disk drives for use by the account and container rings. For instructions,
   see <a class="xref" href="#allocating-disk-drives" title="11.5. Allocating Disk Drives for Object Storage">Section 11.5, “Allocating Disk Drives for Object Storage”</a>.
  </p></div></div><div class="sect1" id="topic-tq1-xt5-dt"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Allocating Object Servers</span> <a title="Permalink" class="permalink" href="#topic-tq1-xt5-dt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-allocating_server.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-allocating_server.xml</li><li><span class="ds-label">ID: </span>topic-tq1-xt5-dt</li></ul></div></div></div></div><p>
  A swift object server is a node that runs the swift-object service
  (<span class="bold"><strong>only</strong></span>) and is used to store object
  data. It does not run the swift-proxy, swift-account, or swift-container
  services.
  
 </p><p>
  This section describes the procedure to allocate a swift object server during
  the <span class="bold"><strong>initial</strong></span> deployment of the system.
 </p><div class="sect2" id="procedure"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To Allocate a Swift Object Server</span> <a title="Permalink" class="permalink" href="#procedure">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-allocating_server.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-allocating_server.xml</li><li><span class="ds-label">ID: </span>procedure</li></ul></div></div></div></div><p>
   Perform the following steps to allocate one or more swift object servers:
  </p><div class="itemizedlist " id="ul-fbk-fj3-kt"><ul class="itemizedlist"><li class="listitem "><p>
     Verify if the example input model already contains a suitable server role.
     The server roles are usually described in the
     <code class="literal">data/server_roles.yml</code> file. If the server role is not
     described, you must add a suitable server role. For instructions, see
     <a class="xref" href="#topic-uh2-td1-kt" title="11.4. Creating Roles for swift Nodes">Section 11.4, “Creating Roles for swift Nodes”</a>. While adding a server role for the
     swift object server, you will also allocate drives to store object data.
     For instructions, see <a class="xref" href="#allocating-disk-drives" title="11.5. Allocating Disk Drives for Object Storage">Section 11.5, “Allocating Disk Drives for Object Storage”</a>.
    </p></li><li class="listitem "><p>
     Verify if the example input model has a resource node assigned to swift
     object servers. The resource nodes are usually assigned in the
     <code class="literal">data/control_plane.yml</code> file. If it is not assigned, you
     must add a suitable resource node. For instructions, see
     <a class="xref" href="#topic-jzk-q1c-jt" title="11.8. Creating Object Server Resource Nodes">Section 11.8, “Creating Object Server Resource Nodes”</a>.
    </p></li><li class="listitem "><p>
     Identify the physical servers and their IP address and other detailed
     information. Add the details for the servers in either of the following
     YAML files and verify the server-groups information:
    </p><div class="itemizedlist " id="ul-gmj-ng3-kt"><ul class="itemizedlist"><li class="listitem "><p>
       Add details in the servers list (usually in the
       <code class="literal">data/servers.yml</code> file).
      </p></li><li class="listitem "><p>
       As with all servers, you must also verify and/or modify the
       server-groups information (usually in the
       <code class="literal">data/server_groups.yml</code> file).
      </p></li></ul></div><p>
     The only part of this process that is unique to swift is the allocation of
     disk drives for use by the object ring. For instructions, see
     <a class="xref" href="#allocating-disk-drives" title="11.5. Allocating Disk Drives for Object Storage">Section 11.5, “Allocating Disk Drives for Object Storage”</a>.
    </p></li></ul></div></div></div><div class="sect1" id="topic-uh2-td1-kt"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating Roles for swift Nodes</span> <a title="Permalink" class="permalink" href="#topic-uh2-td1-kt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-creating_roles_swift_nodes.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-creating_roles_swift_nodes.xml</li><li><span class="ds-label">ID: </span>topic-uh2-td1-kt</li></ul></div></div></div></div><p>
  To create roles for swift nodes, you must edit the
  <code class="literal">data/server_roles.yml</code> file and add an entry to the
  server-roles list using the following syntax:
 </p><div class="verbatim-wrap"><pre class="screen">server-roles:
- name: <em class="replaceable ">PICK-A-NAME</em>
  interface-model: <em class="replaceable ">SPECIFY-A-NAME</em>
  disk-model: <em class="replaceable ">SPECIFY-A-NAME</em></pre></div><p>
  The fields for server roles are defined as follows:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /></colgroup><tbody><tr><td><span class="bold"><strong><code class="literal">name</code></strong></span>
     </td><td>
      Specifies a name assigned for the role. In the following example,
      <span class="bold"><strong>SWOBJ-ROLE</strong></span> is the role name.
     </td></tr><tr><td><span class="bold"><strong><code class="literal">interface-model</code></strong></span>
     </td><td>
      You can either select an existing interface model or create one
      specifically for swift object servers. In the following example
      <span class="bold"><strong>SWOBJ-INTERFACES</strong></span> is used. For more
      information, see <a class="xref" href="#topic-pcj-hzv-dt" title="11.9. Understanding Swift Network and Service Requirements">Section 11.9, “Understanding Swift Network and Service Requirements”</a>.
     </td></tr><tr><td><span class="bold"><strong><code class="literal">disk-model</code></strong></span>
     </td><td>
      You can either select an existing model or create one specifically for
      swift object servers. In the following example
      <span class="bold"><strong>SWOBJ-DISKS</strong></span> is used. For more
      information, see <a class="xref" href="#allocating-disk-drives" title="11.5. Allocating Disk Drives for Object Storage">Section 11.5, “Allocating Disk Drives for Object Storage”</a>.
     </td></tr></tbody></table></div><div class="verbatim-wrap"><pre class="screen">server-roles:
- name: <span class="bold"><strong>SWOBJ-ROLE</strong></span>
  interface-model: <span class="bold"><strong>SWOBJ-INTERFACES</strong></span>
  disk-model: <span class="bold"><strong>SWOBJ-DISKS</strong></span></pre></div></div><div class="sect1" id="allocating-disk-drives"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Allocating Disk Drives for Object Storage</span> <a title="Permalink" class="permalink" href="#allocating-disk-drives">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-allocating_disk_drives.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-allocating_disk_drives.xml</li><li><span class="ds-label">ID: </span>allocating-disk-drives</li></ul></div></div></div></div><p>
  The disk model describes the configuration of disk drives
  
  and their usage. The examples include several disk models.
  
  You must always review the disk devices before making any changes to the
  existing the disk model.
 </p><div class="sect2" id="making-changes-disk-model"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Making Changes to a Swift Disk Model</span> <a title="Permalink" class="permalink" href="#making-changes-disk-model">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-allocating_disk_drives.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-allocating_disk_drives.xml</li><li><span class="ds-label">ID: </span>making-changes-disk-model</li></ul></div></div></div></div><p>
   There are several reasons for changing the disk model:
  </p><div class="itemizedlist " id="ul-d5y-255-dt"><ul class="itemizedlist"><li class="listitem "><p>
     If you have additional drives available, you can add them to the devices
     list.
    </p></li><li class="listitem "><p>
     If the disk devices listed in the example disk model have different names
     on your servers. This may be due to different hardware drives. Edit the
     disk model and change the device names to the correct names.
    </p></li><li class="listitem "><p>
     If you prefer a different disk drive than the one listed in the model. For
     example, if <code class="literal">/dev/sdb</code> and <code class="literal">/dev/sdc</code>
     are slow hard drives and you have SDD drives available in
     <code class="literal">/dev/sdd</code> and <code class="literal">/dev/sde</code>. In this case,
     delete <code class="literal">/dev/sdb</code> and <code class="literal">/dev/sdc</code> and
     replace them with <code class="literal">/dev/sdd</code> and
     <code class="literal">/dev/sde</code>.
    </p><div id="id-1.3.4.9.10.3.3.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      Disk drives must not contain labels or file systems from a prior usage.
      For more information, see <a class="xref" href="#topic-d1s-hht-tt" title="11.6. Swift Requirements for Device Group Drives">Section 11.6, “Swift Requirements for Device Group Drives”</a>.
     </p></div><div id="id-1.3.4.9.10.3.3.3.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>
      The terms <span class="bold"><strong>add</strong></span> and
      <span class="bold"><strong>delete</strong></span> in the document
      means editing the respective YAML files to add or delete the
      configurations/values.
     </p></div></li></ul></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.4.9.10.3.4"><span class="name">Swift Consumer Syntax</span><a title="Permalink" class="permalink" href="#id-1.3.4.9.10.3.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-allocating_disk_drives.xml" title="Edit the source file for this section">Edit source</a></h5></div><p>
   The consumer field determines the usage of a disk drive or logical volume by
   swift. The syntax of the consumer field is as follows:
  </p><div class="verbatim-wrap"><pre class="screen">consumer:
    name: swift
    attrs:
        rings:
        - name: <em class="replaceable ">RING-NAME</em>
        - name: <em class="replaceable ">RING-NAME</em>
        - etc...</pre></div><p>
   The fields for consumer are defined as follows:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /></colgroup><tbody><tr><td>
       <code class="literal">name</code>
      </td><td>
       Specifies the service that uses the device group. A
       <code class="literal">name</code> field containing
       <span class="bold"><strong>swift</strong></span> indicates that the drives or
       logical volumes are used by swift.
      </td></tr><tr><td>
       <code class="literal">attrs</code>
      </td><td>
       Lists the rings that the devices are allocated to. It must contain a
       <code class="literal">rings</code> item.
      </td></tr><tr><td>
       <code class="literal">rings</code>
      </td><td>
       Contains a list of ring names. In the <code class="literal">rings</code> list,
       the <code class="literal">name</code> field is optional.
      </td></tr></tbody></table></div><p>
   The following are the different configurations (patterns) of the proxy,
   account, container, and object services:
  </p><div class="itemizedlist " id="ul-dvq-kyb-jt"><ul class="itemizedlist"><li class="listitem "><p>
     Proxy, account, container, and object (PACO) run on same node type.
    </p></li><li class="listitem "><p>
     Proxy, account, and container run on a node type (PAC) and the object
     services run on a dedicated object server (OBJ).
    </p></li></ul></div><div id="id-1.3.4.9.10.3.11" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    The proxy service does not have any rings associated with it.
   </p></div><div class="example" id="id-1.3.4.9.10.3.12"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 11.1: </span><span class="name">
    <span class="bold">PACO</span> - proxy, account, container,
    and object run on the same node type.
    </span><a title="Permalink" class="permalink" href="#id-1.3.4.9.10.3.12">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">consumer:
    name: swift
    attrs:
        rings:
        - name: account
        - name: container
        - name: object-0</pre></div></div></div><div class="example" id="id-1.3.4.9.10.3.13"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 11.2: </span><span class="name">
    <span class="bold">PAC</span> - proxy, account, and
    container run on the same node type.
    </span><a title="Permalink" class="permalink" href="#id-1.3.4.9.10.3.13">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">consumer:
    name: swift
    attrs:
        rings:
        - name: account
        - name: container</pre></div></div></div><div class="complex-example"><div class="example" id="id-1.3.4.9.10.3.14"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 11.3: </span><span class="name"><span class="bold">OBJ</span> - Dedicated object
    server </span><a title="Permalink" class="permalink" href="#id-1.3.4.9.10.3.14">#</a></h6></div><div class="example-contents"><p>
    The following example shows two Storage Policies (object-0 and object-1).
    For more information, see
    <a class="xref" href="#swift-storage-policies" title="11.11. Designing Storage Policies">Section 11.11, “Designing Storage Policies”</a>.
   </p><div class="verbatim-wrap"><pre class="screen">consumer:
    name: swift
    attrs:
        rings:
        - name: object-0
        - name: object-1</pre></div></div></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.4.9.10.3.15"><span class="name">Swift Device Groups</span><a title="Permalink" class="permalink" href="#id-1.3.4.9.10.3.15">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-allocating_disk_drives.xml" title="Edit the source file for this section">Edit source</a></h5></div><p>
   You may have several device groups if you have several different uses for
   different sets of drives.
  </p><p>
   The following example shows a configuration where one drive is used for
   account and container rings and the other drives are used by the object-0
   ring:
  </p><div class="verbatim-wrap"><pre class="screen">device-groups:

- name: swiftpac
  devices:
  - name: /dev/sdb
  consumer:
      name: swift
      attrs:
      - name: account
      - name: container
  - name: swiftobj
    devices:
    - name: /dev/sdc
    - name: /dev/sde
    - name: /dev/sdf
    consumer:
       name: swift
       attrs:
           rings:
              - name: object-0</pre></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.4.9.10.3.19"><span class="name">Swift Logical Volumes</span><a title="Permalink" class="permalink" href="#id-1.3.4.9.10.3.19">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-allocating_disk_drives.xml" title="Edit the source file for this section">Edit source</a></h5></div><div id="id-1.3.4.9.10.3.20" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
    Be careful while using logical volumes to store swift data. The data
    remains intact during an upgrade, but will be lost if the server is
    reimaged. If you use logical volumes you must ensure that you only reimage
    one server at a time. This is to allow the data from the other replicas to
    be replicated back to the logical volume once the reimage is complete.
   </p></div><p>
   swift can use a logical volume. To do this, ensure you meet the requirements
   listed in the table below:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /></colgroup><tbody><tr><td>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          <code class="literal">mount</code>
         </p></li><li class="listitem "><p>
          <code class="literal">mkfs-opts</code>
         </p></li><li class="listitem "><p>
          <code class="literal">fstype</code>
         </p></li></ul></div>
      </td><td>Do not specify these attributes.</td></tr><tr><td>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          <code class="literal">name</code>
         </p></li><li class="listitem "><p>
          <code class="literal">size</code>
         </p></li></ul></div>
      </td><td>Specify both of these attributes.</td></tr><tr><td>
       <div class="itemizedlist " id="ul-z5d-s3n-pt"><ul class="itemizedlist"><li class="listitem "><p>
          <code class="literal">consumer</code>
         </p></li></ul></div>
      </td><td>
       This attribute must have a <code class="literal">name</code> field set to
       <span class="bold"><strong>swift</strong></span>.
      </td></tr></tbody></table></div><div id="id-1.3.4.9.10.3.23" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    When setting up swift as a logical volume, the configuration processor
    will give a warning. This warning is normal and does not affect the
    configuration.
   </p></div><p>
   Following is an example of swift logical volumes:
  </p><div class="verbatim-wrap"><pre class="screen">...
   - name: swift
     size: 50%
     consumer:
         name: swift
         attrs:
             rings:
             - name: object-0
             - name: object-1</pre></div></div></div><div class="sect1" id="topic-d1s-hht-tt"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Swift Requirements for Device Group Drives</span> <a title="Permalink" class="permalink" href="#topic-d1s-hht-tt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-swift_device_groups.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-swift_device_groups.xml</li><li><span class="ds-label">ID: </span>topic-d1s-hht-tt</li></ul></div></div></div></div><p>
  To install and deploy, swift requires that the disk drives listed in the
  devices list of the device-groups item in a disk model meet the following
  criteria (if not, the deployment will fail):
 </p><div class="itemizedlist " id="ul-fsy-255-dt"><ul class="itemizedlist"><li class="listitem "><p>
    The disk device must exist on the server. For example, if you add
    <code class="filename">/dev/sd<em class="replaceable ">X</em></code> to a server with
    only three devices, then the deploy process will fail.
   </p></li><li class="listitem "><p>
    The disk device must be unpartitioned or have a single partition that uses
    the whole drive.
   </p></li><li class="listitem "><p>
    The partition must not be labeled.

   </p></li><li class="listitem "><p>
    The XFS file system must not contain a file system label.

   </p></li><li class="listitem "><p>
    If the disk drive is already labeled as described above, the
    <code class="literal">swiftlm-drive-provision</code> process will assume that the
    drive has valuable data and will not use or modify the drive.
   </p></li></ul></div></div><div class="sect1" id="topic-rvj-21c-jt"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a Swift Proxy, Account, and Container (PAC) Cluster</span> <a title="Permalink" class="permalink" href="#topic-rvj-21c-jt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-creating_pac_cluster.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-creating_pac_cluster.xml</li><li><span class="ds-label">ID: </span>topic-rvj-21c-jt</li></ul></div></div></div></div><p>
  If you already have a cluster with the server-role
  <code class="literal">SWPAC-ROLE</code> there is no need to proceed through these
  steps.
 </p><div class="sect2" id="steps"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Steps to Create a swift Proxy, Account, and Container (PAC) Cluster</span> <a title="Permalink" class="permalink" href="#steps">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-creating_pac_cluster.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-creating_pac_cluster.xml</li><li><span class="ds-label">ID: </span>steps</li></ul></div></div></div></div><p>
   To create a cluster for swift proxy, account, and container (PAC) servers,
   you must identify the control plane and node type/role:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     In the
     <code class="filename">~/openstack/my_cloud/definition/data/control_plane.yml</code>
     file, identify the control plane that the PAC servers are associated with.
    </p></li><li class="step "><p>
     Next, identify the node type/role used by the swift PAC servers. In the
     following example, <code class="literal">server-role</code> is set to
     <span class="bold"><strong>SWPAC-ROLE</strong></span>.
    </p><p>
     Add an entry to the <code class="literal">clusters</code> item in the
     <code class="literal">control-plane</code> section.
    </p><p>
     Example:
    </p><div class="verbatim-wrap"><pre class="screen">control-planes:
    - name: control-plane-1
      control-plane-prefix: cp1

  . . .
  clusters:
  . . .
     - name: swpac
       cluster-prefix: swpac
       server-role: SWPAC-ROLE
       member-count: 3
       allocation-policy: strict
       service-components:
         - ntp-client
         - swift-ring-builder
         - swift-proxy
         - swift-account
         - swift-container
         - swift-client</pre></div><div id="id-1.3.4.9.12.3.3.2.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      Do not change the name of the cluster <code class="literal">swpac</code> to ensure
      that it remains unique among clusters. Use names for its servers such as
      <code class="literal">swpac1</code>, <code class="literal">swpac2</code>, and
      <code class="literal">swpac3</code>.
     </p></div></li><li class="step "><p>
     If you have more than three servers available that have the
     <code class="literal">SWPAC-ROLE</code> assigned to them, you must change
     <code class="literal">member-count</code> to match the number of servers.
    </p><p>
     For example, if you have four servers with a role of
     <code class="literal">SWPAC-ROLE</code>, then the <code class="literal">member-count</code>
     should be 4.
    </p></li></ol></div></div></div><div class="sect2" id="components"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Service Components</span> <a title="Permalink" class="permalink" href="#components">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-creating_pac_cluster.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-creating_pac_cluster.xml</li><li><span class="ds-label">ID: </span>components</li></ul></div></div></div></div><p>
   A swift PAC server requires the following service components:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     ntp-client
    </p></li><li class="listitem "><p>
     swift-proxy
    </p></li><li class="listitem "><p>
     swift-account
    </p></li><li class="listitem "><p>
     swift-container
    </p></li><li class="listitem "><p>
     swift-ring-builder
    </p></li><li class="listitem "><p>
     swift-client
    </p></li></ul></div></div></div><div class="sect1" id="topic-jzk-q1c-jt"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating Object Server Resource Nodes</span> <a title="Permalink" class="permalink" href="#topic-jzk-q1c-jt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-creating_object_server_resource.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-creating_object_server_resource.xml</li><li><span class="ds-label">ID: </span>topic-jzk-q1c-jt</li></ul></div></div></div></div><p>
  To create a resource node for swift object servers, you must identify the
  control plane and node type/role:
 </p><div class="itemizedlist " id="ul-r4r-r1c-jt"><ul class="itemizedlist"><li class="listitem "><p>
    In the <code class="literal">data/control_plane.yml</code> file, identify the control
    plane that the object servers are associated with.
   </p></li><li class="listitem "><p>
    Next, identify the node type/role used by the swift object servers. In the
    following example, <code class="literal">server-role</code> is set to
    <span class="bold"><strong>SWOBJ-ROLE</strong></span>:
   </p><p>
    Add an entry to the <code class="literal">resources</code> item in the
    <span class="bold"><strong>control-plane</strong></span>:
   </p><div class="verbatim-wrap"><pre class="screen">control-planes:
    - name: control-plane-1
      control-plane-prefix: cp1
      region-name: region1
  . . .
  resources:
  . . .
  - name: swobj
    resource-prefix: swobj
    server-role: SWOBJ-ROLE
    allocation-policy: strict
    min-count: 0
    service-components:
    - ntp-client
    - swift-object</pre></div></li></ul></div><p>
  <span class="bold"><strong>Service Components</strong></span>
 </p><p>
  A swift object server requires the following service components:
 </p><div class="itemizedlist " id="ul-fyb-51c-jt"><ul class="itemizedlist"><li class="listitem "><p>
    <code class="literal">ntp-client</code>
   </p></li><li class="listitem "><p>
    <code class="literal">swift-object</code>
   </p></li><li class="listitem "><p>
    <code class="literal">swift-client</code> is optional; installs the
    <code class="literal">python-swiftclient</code> package on the server.
   </p></li></ul></div><p>
  Resource nodes do not have a member count attribute. So the number of servers
  allocated with the <span class="bold"><strong>SWOBJ-ROLE</strong></span> is the number
  of servers in the <code class="literal">data/servers.yml</code> file with a server role
  of <span class="bold"><strong>SWOBJ-ROLE</strong></span>.
 </p></div><div class="sect1" id="topic-pcj-hzv-dt"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Understanding Swift Network and Service Requirements</span> <a title="Permalink" class="permalink" href="#topic-pcj-hzv-dt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-allocating_network.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-allocating_network.xml</li><li><span class="ds-label">ID: </span>topic-pcj-hzv-dt</li></ul></div></div></div></div><p>
  This topic describes swift’s requirements for which service components must
  exist in the input model and how these relate to the network model. This
  information is useful if you are creating a cluster or resource node, or when
  defining the networks used by swift. The network model allows many options
  and configurations. For smooth swift operation, the following must be
  <span class="bold"><strong>true</strong></span>:
 </p><div class="itemizedlist " id="ul-ggm-rkc-jt"><ul class="itemizedlist"><li class="listitem "><p>
    The following services must have a
    <span class="bold"><strong>direct</strong></span>
    connection to the same network:
   </p><div class="itemizedlist " id="ul-x1c-5kc-jt"><ul class="itemizedlist"><li class="listitem "><p>
      <code class="literal">swift-proxy</code>
     </p></li><li class="listitem "><p>
      <code class="literal">swift-account</code>
     </p></li><li class="listitem "><p>
      <code class="literal">swift-container</code>
     </p></li><li class="listitem "><p>
      <code class="literal">swift-object</code>
     </p></li><li class="listitem "><p>
      <code class="literal">swift-ring-builder</code>
     </p></li></ul></div></li><li class="listitem "><p>
    The <code class="literal">swift-proxy</code> service must have a
    <span class="bold"><strong>direct</strong></span> connection to the same network as
    the <code class="literal">cluster-ip</code> service.
   </p></li><li class="listitem "><p>
    The memcached service must be configured on a cluster of the control plane.
    In small deployments, it is convenient to run it on the same cluster as the
    horizon service. For larger deployments, with many nodes running the
    <code class="literal">swift-proxy</code> service, it is better to
    <span class="bold"><strong>co-locate</strong></span>
    the <code class="literal">swift-proxy</code> and <code class="literal">memcached</code>
    services. The <code class="literal">swift-proxy</code> and
    <code class="literal">swift-container</code> services must have a
    <span class="bold"><strong>direct</strong></span> connection to the same network as
    the <code class="literal">memcached</code> service.
   </p></li><li class="listitem "><p>
    The <code class="literal"> swift-proxy</code> and
    <code class="literal">swift-ring-builder</code> service must be
    <span class="bold"><strong>co-located</strong></span> in the same cluster of the
    control plane.
   </p></li><li class="listitem "><p>
    The <code class="literal">ntp-client</code> service must be
    <span class="bold"><strong>present</strong></span> on all swift nodes.
   </p></li></ul></div></div><div class="sect1" id="ring-specification"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Understanding Swift Ring Specifications</span> <a title="Permalink" class="permalink" href="#ring-specification">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-ring_specifications.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-ring_specifications.xml</li><li><span class="ds-label">ID: </span>ring-specification</li></ul></div></div></div></div><p>
  In swift, the ring is responsible for mapping data on particular disks. There
  is a separate ring for account databases, container databases, and each
  object storage policy, but each ring works similarly. The
  <code class="literal">swift-ring-builder</code> utility is used to build and manage
  rings. This utility uses a builder file to contain ring information and
  additional data required to build future rings. In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9, you will
  use the cloud model to specify how the rings are configured and used. This
  model is used to automatically invoke the
  <code class="literal">swift-ring-builder</code> utility as part of the deploy process.
  (Normally, you will not run the <code class="literal">swift-ring-builder</code> utility
  directly.)
 </p><p>
  The rings are specified in the input model using the
  <span class="bold"><strong>configuration-data</strong></span> key. The
  <code class="literal">configuration-data</code> in the
  <code class="literal">control-planes</code> definition is given a name that you will
  then use in the <code class="literal">swift_config.yml</code> file. If you have several
  control planes hosting swift services, the ring specifications can use a
  shared <code class="literal">configuration-data</code> object, however it is considered
  best practice to give each swift instance its own
  <code class="literal">configuration-data</code> object.
 </p><div class="sect2" id="input-model"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ring Specifications in the Input Model</span> <a title="Permalink" class="permalink" href="#input-model">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-ring_specifications.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-ring_specifications.xml</li><li><span class="ds-label">ID: </span>input-model</li></ul></div></div></div></div><p>
   In most models, the ring-specification is mentioned in the
   <code class="filename">~/openstack/my_cloud/definition/data/swift/swift_config.yml</code>
   file. For example:
  </p><div class="verbatim-wrap"><pre class="screen">configuration-data:
  - name: SWIFT-CONFIG-CP1
    services:
      - swift
    data:
      control_plane_rings:
        swift-zones:
          - id: 1
            server-groups:
              - AZ1
          - id: 2
            server-groups:
              - AZ2
          - id: 3
            server-groups:
              - AZ3
        rings:
          - name: account
            display-name: Account Ring
            min-part-hours: 16
            partition-power: 12
            replication-policy:
              replica-count: 3

          - name: container
            display-name: Container Ring
            min-part-hours: 16
            partition-power: 12
            replication-policy:
              replica-count: 3

          - name: object-0
            display-name: General
            default: yes
            min-part-hours: 16
            partition-power: 12
            replication-policy:
              replica-count: 3</pre></div><p>
   The above sample file shows that the rings are specified using the
   <code class="literal">configuration-data</code> object
   <span class="bold"><strong>SWIFT-CONFIG-CP1</strong></span> and has three
   rings as follows:
  </p><div class="itemizedlist " id="ul-hl3-q55-dt"><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>Account ring</strong></span>: You must always specify a
     ring called <span class="bold"><strong>account</strong></span>. The account ring is
     used by swift to store metadata about the projects in your system. In
     swift, a keystone project maps to a swift account. The
     <code class="literal">display-name</code> is informational and not used.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Container ring</strong></span>:You must always specify a
     ring called <span class="bold"><strong>container</strong></span>. The
     <code class="literal">display-name</code> is informational and not used.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Object ring</strong></span>: This ring is also known as a
     storage policy. You must always specify a ring called
     <span class="bold"><strong>object-0</strong></span>. It is possible to have multiple
     object rings, which is known as <span class="emphasis"><em>storage policies</em></span>. The
     <code class="literal">display-name</code> is the name of the storage policy and can
     be used by users of the swift system when they create containers. It
     allows them to specify the storage policy that the container uses. In the
     example, the storage policy is called
     <span class="bold"><strong>General</strong></span>. There are also two aliases for
     the storage policy name: <code class="literal">GeneralPolicy</code> and
     <code class="literal">AnotherAliasForGeneral</code>. In this example, you can use
     <code class="literal">General</code>, <code class="literal">GeneralPolicy</code>, or
     <code class="literal">AnotherAliasForGeneral</code> to refer to this storage policy.
     The aliases item is optional. The <code class="literal">display-name</code> is
     required.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Min-part-hours, partition-power,
     replication-policy</strong></span> and
     <span class="bold"><strong>replica-count</strong></span> are described in the
     following section.
    </p></li></ul></div></div><div class="sect2" id="ring-parameters"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.10.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replication Ring Parameters</span> <a title="Permalink" class="permalink" href="#ring-parameters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-ring_specifications.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-ring_specifications.xml</li><li><span class="ds-label">ID: </span>ring-parameters</li></ul></div></div></div></div><p>
   The ring parameters for traditional replication rings are defined as
   follows:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Parameter</th><th>Description</th></tr></thead><tbody><tr><td><code class="literal">replica-count</code>
      </td><td>
       <p>
        Defines the number of copies of object created.
       </p>
       <p>
        Use this to control the degree of resiliency or availability. The
        <code class="literal">replica-count</code> is normally set to
        <code class="literal">3</code> (that means swift will
        keep three copies of accounts, containers, or objects). As a best
        practice, do not set the value below <code class="literal">3</code>. To achieve
        higher resiliency, increase the value.
       </p>
      </td></tr><tr><td><code class="literal">min-part-hours</code>
      </td><td>
       <p>
        Changes the value used to decide when a given partition can be moved.
        This is the number of hours that the
        <code class="command">swift-ring-builder</code> tool will enforce between ring
        rebuilds. On a small system, this can be as low as
        <code class="literal">1</code> (one hour). The
        value can be different for each ring.
       </p>
       <p>
        In the example above, the <code class="literal">swift-ring-builder</code> will
        enforce a minimum of 16 hours between ring rebuilds. However, this time
        is system-dependent so you will be unable to determine the appropriate
        value for <code class="literal">min-part-hours</code> until you have more
        experience with your system.
       </p>
       <p>
        A value of <code class="literal">0</code> (zero) is not allowed.
       </p>
       <p>
        In prior releases, this parameter was called
        <code class="literal">min-part-time</code>. The older name is still supported,
        however do not specify both <code class="literal">min-part-hours</code> and
        <code class="literal">min-part-time</code> in the same files.
       </p>
      </td></tr><tr><td><code class="literal">partition-power</code>
      </td><td>The optimal value for this parameter is related to the number of disk drives that
              you allocate to swift storage. As a best practice, you should use the same drives for
              both the account and container rings. In this case, the
                <code class="literal">partition-power</code> value should be the same. For more information,
              see <a class="xref" href="#selecting-partition-power" title="11.10.4. Selecting a Partition Power">Section 11.10.4, “Selecting a Partition Power”</a>.</td></tr><tr><td><code class="literal">replication-policy</code>
      </td><td>Specifies that a ring uses replicated storage. The duplicate copies of the object
              are created and stored on different disk drives. All replicas are identical. If one is
              lost or corrupted, the system automatically copies one of the remaining replicas to
              restore the missing replica.</td></tr><tr><td><code class="literal">default</code>
      </td><td>The default value in the above sample file of ring-specification is set to
                <span class="bold"><strong>yes</strong></span>, which means that the storage policy is enabled
              to store objects. For more information, see
              <a class="xref" href="#swift-storage-policies" title="11.11. Designing Storage Policies">Section 11.11, “Designing Storage Policies”</a>.</td></tr></tbody></table></div></div><div class="sect2" id="erasure-coded"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.10.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Erasure Coded Rings</span> <a title="Permalink" class="permalink" href="#erasure-coded">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-ring_specifications.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-ring_specifications.xml</li><li><span class="ds-label">ID: </span>erasure-coded</li></ul></div></div></div></div><p>
   In the cloud model, a <code class="literal">ring-specification</code> is mentioned in
   the <code class="filename">~/openstack/my_cloud/definition/data/swift/swift_config.yml</code>
   file. A typical erasure coded ring in this file looks like this:
  </p><div class="verbatim-wrap"><pre class="screen">- name: object-1
  display-name: EC_ring
  default: no
  min-part-hours: 16
  partition-power: 12
  erasure-coding-policy:
    ec-type: jerasure_rs_vand
    ec-num-data-fragments: 10
    ec-num-parity-fragments: 4
    ec-object-segment-size: 1048576</pre></div><p>
   The additional parameters are defined as follows:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Parameter</th><th>Description</th></tr></thead><tbody><tr><td>ec-type</td><td>
       <p>
        This is the particular erasure policy scheme that is being used. The
        supported ec_types in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 are:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          <code class="literal">jerasure_rs_vand</code> =&gt; Vandermonde Reed-Solomon
          encoding, based on Jerasure
         </p></li></ul></div>
      </td></tr><tr><td>erasure-coding-policy</td><td>This line indicates that the object ring will be of type "erasure coding"</td></tr><tr><td>ec-num-data-fragments</td><td>This indicated the number of data fragments for an object in the ring.</td></tr><tr><td>ec-num-parity-fragments</td><td>This indicated the number of parity fragments for an object in the ring.</td></tr><tr><td>ec-object-segment-size</td><td>The amount of data that will be buffered up before feeding a segment into the
              encoder/decoder. The default value is 1048576.</td></tr></tbody></table></div><p>
   When using an erasure coded ring, the number of devices in the ring must be
   greater than or equal to the total number of fragments of an object. For
   example, if you define an erasure coded ring with 10 data fragments and 4
   parity fragments, there must be at least 14 (10+4) devices added to the
   ring.
  </p><p>
   When using erasure codes, for a PUT object to be successful it must store
   <code class="literal">ec_ndata + 1</code> fragment to achieve quorum. Where the number
   of data fragments (<code class="literal">ec_ndata</code>) is 10 then at least 11
   fragments must be saved for the object PUT to be successful. The 11
   fragments must be saved to different drives. To tolerate a single object
   server going down, say in a system with 3 object servers, each object server
   must have at least 6 drives assigned to the erasure coded storage policy. So
   with a single object server down, 12 drives are available between the
   remaining object servers. This allows an object PUT to save 12 fragments,
   one more than the minimum to achieve quorum.
  </p><p>
   Unlike replication rings, none of the erasure coded parameters may be edited
   after the initial creation. Otherwise there is potential for permanent loss
   of access to the data.
  </p><p>
   On the face of it, you would expect that an erasure coded configuration that
   uses a data to parity ratio of 10:4, that the data consumed storing the
   object is 1.4 times the size of the object just like the x3 replication
   takes x3 times the size of the data when storing the object. However, for
   erasure coding, this 10:4 ratio is not correct. The efficiency (that is how
   much storage is needed to store the object) is very poor for small objects
   and improves as the object size grows. However, the improvement is not
   linear. If all of your files are less than 32K in size, erasure coding will
   take more space to store than the x3 replication.
  </p></div><div class="sect2" id="selecting-partition-power"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.10.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Selecting a Partition Power</span> <a title="Permalink" class="permalink" href="#selecting-partition-power">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-ring_specifications.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-ring_specifications.xml</li><li><span class="ds-label">ID: </span>selecting-partition-power</li></ul></div></div></div></div><p>
   When storing an object, the object storage system hashes the name. This hash
   results in a hit on a partition (so a number of different object names
   result in the same partition number). Generally, the partition is mapped to
   available disk drives. With a replica count of 3, each partition is mapped
   to three different disk drives. The hashing algorithm used hashes over a
   fixed number of partitions. The partition-power attribute determines the
   number of partitions you have.
  </p><p>
   Partition power is used to distribute the data uniformly across drives in a
   swift nodes. It also defines the storage cluster capacity. You must set the
   partition power value based on the total amount of storage you expect your
   entire ring to use.
  </p><p>
   You should select a partition power for a given ring that is appropriate to
   the number of disk drives you allocate to the ring for the following
   reasons:
  </p><div class="itemizedlist " id="ul-i4b-gv5-dt"><ul class="itemizedlist"><li class="listitem "><p>
     If you use a high partition power and have a few disk drives, each disk
     drive will have thousands of partitions. With too many partitions, audit
     and other processes in the Object Storage system cannot walk the
     partitions in a reasonable time and updates will not occur in a timely
     manner.
    </p></li><li class="listitem "><p>
     If you use a low partition power and have many disk drives, you will have
     tens (or maybe only one) partition on a drive. The Object Storage system
     does not use size when hashing to a partition - it hashes the name.
    </p><p>
     With many partitions on a drive, a large partition is cancelled out by a
     smaller partition so the overall drive usage is similar. However, with
     very small numbers of partitions, the uneven distribution of sizes can be
     reflected in uneven disk drive usage (so one drive becomes full while a
     neighboring drive is empty).
    </p></li></ul></div><p>
   An ideal number of partitions per drive is 100. If you know the number of
   drives, select a partition power that will give you approximately 100
   partitions per drive. Usually, you install a system with a specific number
   of drives and add drives as needed. However, you cannot change the value of
   the partition power. Hence you must select a value that is a compromise
   between current and planned capacity.
  </p><div id="id-1.3.4.9.15.7.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    If you are installing a small capacity system and you need to grow to a
    very large capacity but you cannot fit within any of the ranges in the
    table, please seek help from Sales Engineering to plan your system.
   </p></div><p>
   There are additional factors that can help mitigate the fixed nature of the
   partition power:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Account and container storage represents a small fraction (typically 1
     percent) of your object storage needs. Hence, you can select a smaller
     partition power (relative to object ring partition power) for the account
     and container rings.
    </p></li></ul></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     For object storage, you can add additional storage policies (that is, another
     object ring). When you have reached capacity in an existing storage
     policy, you can add a new storage policy with a higher partition power
     (because you now have more disk drives in your system). This means that
     you can install your system using a small partition power appropriate to a
     small number of initial disk drives. Later, when you have many disk
     drives, the new storage policy can have a higher value appropriate to the
     larger number of drives.
    </p></li></ul></div><p>
   However, when you continue to add storage capacity, existing containers will
   continue to use their original storage policy. Hence, the additional objects
   must be added to new containers to take advantage of the new storage policy.
  </p><p>
   Use the following table to select an appropriate partition power for each
   ring. The partition power of a ring cannot be changed, so it is important to
   select an appropriate value. This table is based on a replica count of 3. If
   your replica count is different, or you are unable to find your system in
   the table, then see <a class="xref" href="#selecting-partition-power" title="11.10.4. Selecting a Partition Power">Section 11.10.4, “Selecting a Partition Power”</a> for
   information of selecting a partition power.
  </p><p>
   The table assumes that when you first deploy swift, you have a small number
   of drives (the minimum column in the table), and later you add drives.
  </p><div id="id-1.3.4.9.15.7.14" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Use the total number of drives. For example, if you have three servers,
      each with two drives, the total number of drives is six.
     </p></li><li class="listitem "><p>
      The lookup should be done separately for each of the account, container
      and object rings. Since account and containers represent approximately 1
      to 2 percent of object storage, you will probably use fewer drives for
      the account and container rings (that is, you will have fewer proxy,
      account, and container (PAC) servers) so that your object rings may have
      a higher partition power.
     </p></li><li class="listitem "><p>
      The largest anticipated number of drives imposes a limit in the minimum
      drives you can have. (For more information, see
      <a class="xref" href="#selecting-partition-power" title="11.10.4. Selecting a Partition Power">Section 11.10.4, “Selecting a Partition Power”</a>.) This means that, if you
      anticipate significant growth, your initial system can be small, but
      under a certain limit. For example, if you determine that the maximum
      number of drives the system will grow to is 40,000, then use a partition
      power of 17 as listed in the table below. In addition, a minimum of 36
      drives is required to build the smallest system with this partition
      power.
     </p></li><li class="listitem "><p>
      The table assumes that disk drives are the same size. The actual size of
      a drive is not significant.
     </p></li></ul></div></div></div></div><div class="sect1" id="swift-storage-policies"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Designing Storage Policies</span> <a title="Permalink" class="permalink" href="#swift-storage-policies">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-storage_policies.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-storage_policies.xml</li><li><span class="ds-label">ID: </span>swift-storage-policies</li></ul></div></div></div></div><p>
  Storage policies enable you to differentiate the way objects are stored.
 </p><p>
  Reasons to use storage policies include the following:
 </p><div class="itemizedlist " id="ul-q4t-cxd-jt"><ul class="itemizedlist"><li class="listitem "><p>
    Different types or classes of disk drive
   </p><p>
    You can use different drives to store various type of data. For example,
    you can use 7.5K RPM high-capacity drives for one type of data and fast SSD
    drives for another type of data.
   </p></li><li class="listitem "><p>
    Different redundancy or availability needs
   </p><p>
    You can define the redundancy and availability based on your requirement.
    You can use a replica count of 3 for "normal" data and a replica count of 4
    for "critical" data.
   </p></li><li class="listitem "><p>
    Growing of cluster capacity
   </p><p>
    If the storage cluster capacity grows beyond the recommended partition
    power as described in <a class="xref" href="#ring-specification" title="11.10. Understanding Swift Ring Specifications">Section 11.10, “Understanding Swift Ring Specifications”</a>.
   </p></li><li class="listitem "><p>
    Erasure-coded storage and replicated storage
   </p><p>
    If you use erasure-coded storage for some objects and replicated storage
    for other objects.
   </p></li></ul></div><p>
  Storage policies are implemented on a per-container basis. If you want a
  non-default storage policy to be used for a new container, you can explicitly
  specify the storage policy to use when you create the container. You can
  change which storage policy is the default. However, this does not affect
  existing containers. Once the storage policy of a container is set, the
  policy for that container cannot be changed.
 </p><p>
  The disk drives used by storage policies can overlap or be distinct. If the
  storage policies overlap (that is, have disks in common between two storage
  policies), it is recommended to use the same set of disk drives for both
  policies. But in the case where there is a partial overlap in disk drives,
  because one storage policy receives many objects, the drives that are common
  to both policies must store more objects than drives that are only allocated
  to one storage policy. This can be appropriate for a situation where the
  overlapped disk drives are larger than the non-overlapped drives.

 </p><div class="sect2" id="id-1.3.4.9.16.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.11.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Specifying Storage Policies</span> <a title="Permalink" class="permalink" href="#id-1.3.4.9.16.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-storage_policies.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-storage_policies.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   There are two places where storage policies are specified in the input
   model:
  </p><div class="itemizedlist " id="ul-psf-lg2-jt"><ul class="itemizedlist"><li class="listitem "><p>
     The attribute of the storage policy is specified in ring-specification in
     the <code class="filename">data/swift/swift_config.yml</code> file.
    </p></li><li class="listitem "><p>
     When associating disk drives with specific rings in a disk model. This
     specifies which drives and nodes use the storage policy. In other word
     words, where data associated with a storage policy is stored.
    </p></li></ul></div><p>
   A storage policy is specified similar to other rings. However, the following
   features are unique to storage policies:
  </p><div class="itemizedlist " id="ul-k3j-ng2-jt"><ul class="itemizedlist"><li class="listitem "><p>
     Storage policies are applicable to object rings only. The account or
     container rings cannot have storage policies.
    </p></li><li class="listitem "><p>
     There is a format for the ring name:
     object-<em class="replaceable ">index</em>, where index is a number in the
     range 0 to 9 (in this release). For example: object-0.
    </p></li><li class="listitem "><p>
     The object-0 ring must always be specified.
    </p></li><li class="listitem "><p>
     Once a storage policy is deployed, it should never be deleted. You can
     remove all disk drives for the storage policy, however the ring
     specification itself cannot be deleted.
    </p></li><li class="listitem "><p>
     You can use the <code class="literal">display-name</code> attribute when creating a
     container to indicate which storage policy you want to use for that
     container.
    </p></li><li class="listitem "><p>
     One of the storage policies can be the default policy. If you do not
     specify the storage policy then the object created in new container uses
     the default storage policy.
    </p></li><li class="listitem "><p>
     If you change the default, only containers created later will have that
     changed default policy.
    </p></li></ul></div><p>
   The following example shows three storage policies in use. Note that the
   third storage policy example is an erasure coded ring.
  </p><div class="verbatim-wrap"><pre class="screen">rings:
. . .
- name: object-0
  display-name: General
  default: no
  min-part-hours: 16
  partition-power: 12
  replication-policy:
      replica-count: 3
- name: object-1
  display-name: Data
  default: yes
  min-part-hours: 16
  partition-power: 20
  replication-policy:
      replica-count: 3
- name: object-2
  display-name: Archive
  default: no
  min-part-hours: 16
  partition-power: 20
  erasure-coded-policy:
    ec-type: jerasure_rs_vand
    ec-num-data-fragments: 10
    ec-num-parity-fragments: 4
    ec-object-segment-size: 1048576</pre></div></div></div><div class="sect1" id="designing-swift-zones"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Designing Swift Zones</span> <a title="Permalink" class="permalink" href="#designing-swift-zones">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-swift_zones.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-swift_zones.xml</li><li><span class="ds-label">ID: </span>designing-swift-zones</li></ul></div></div></div></div><p>
  The concept of swift zones allows you to control the placement of replicas on
  different groups of servers. When constructing rings and allocating replicas
  to specific disk drives, swift will, where possible, allocate replicas using
  the following hierarchy so that the greatest amount of resiliency is achieved
  by avoiding single points of failure:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    swift will place each replica on a different disk drive within the same
    server.
   </p></li><li class="listitem "><p>
    swift will place each replica on a different server.
   </p></li><li class="listitem "><p>
    swift will place each replica in a different swift zone.
   </p></li></ul></div><p>
  If you have three servers and a replica count of three, it is easy for swift
  to place each replica on a different server. If you only have two servers
  though, swift will place two replicas on one server (different drives on the
  server) and one copy on the other server.
 </p><p>
  With only three servers there is no need to use the swift zone concept.
  However, if you have more servers than your replica count, the swift zone
  concept can be used to control the degree of resiliency. The following table
  shows how data is placed and explains what happens under various failure
  scenarios. In all cases, a replica count of three is assumed and that there
  are a total of six servers.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /></colgroup><thead><tr><th>Number of swift Zones</th><th>Replica Placement</th><th>Failure Scenarios</th><th>Details</th></tr></thead><tbody><tr><td rowspan="3">
      One (all servers in the same zone)
     </td><td rowspan="3">
      Replicas are placed on different servers. For any given object, you
      have no control over which servers the replicas are placed on.
     </td><td>
      One server fails
     </td><td>
      You are guaranteed that there are two other replicas.
     </td></tr><tr><td>Two servers fail</td><td>You are guaranteed that there is one remaining replica.</td></tr><tr><td>Three servers fail</td><td>
      1/3 of the objects cannot be accessed. 2/3 of the objects have three
      replicas.
     </td></tr><tr><td>Two (three servers in each swift zone)</td><td>
      Half the objects have two replicas in swift zone 1 with one replica in
      swift zone The other objects are reversed, with one replica in swift
      zone 1 and two replicas in swift zone 2.
     </td><td>One swift zone fails</td><td>
      You are guaranteed to have at least one replica. Half the objects have
      two remaining replicas and the other half have a single replica.
     </td></tr><tr><td rowspan="2">Three (two servers in each swift zone)</td><td rowspan="2">
      Each zone contains a replica. For any given object, there is a replica
      in each swift zone.
     </td><td>One swift zone fails</td><td>You are guaranteed to have two replicas of every object.</td></tr><tr><td>Two swift zones fail</td><td>You are guaranteed to have one replica of every object.</td></tr></tbody></table></div><p>
  The following sections show examples of how to specify the swift zones in
  your input model.
 </p><div class="sect2" id="server-groups"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.12.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using Server Groups to Specify swift Zones</span> <a title="Permalink" class="permalink" href="#server-groups">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-swift_zones.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-swift_zones.xml</li><li><span class="ds-label">ID: </span>server-groups</li></ul></div></div></div></div><p>
   swift zones are specified in the ring specifications using the server group
   concept. To define a swift zone, you specify:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     An id - this is the swift zone number
    </p></li><li class="listitem "><p>
     A list of associated server groups
    </p></li></ul></div><p>
   Server groups are defined in your input model. The example input models
   typically define a number of server groups. You can use these pre-defined
   server groups or create your own.
  </p><p>
   For example, the following three models use the example server groups
   <code class="literal">CLOUD</code>, <code class="literal">AZ1</code>, <code class="literal">AZ2</code> and
   <code class="literal">AZ3</code>. Each of these examples achieves the same effect –
   creating a single swift zone.
  </p><div class="verbatim-wrap"><pre class="screen">ring-specifications:
              - region: region1
              swift-zones:
              - id: 1
              server-groups:
              - CLOUD
              rings:
              …</pre></div><div class="verbatim-wrap"><pre class="screen">ring-specifications:
              - region: region1
              swift-zones:
              - id: 1
              server-groups:
              - AZ1
              - AZ2
              - AZ3
              rings:
              …</pre></div><div class="verbatim-wrap"><pre class="screen">server-groups:
              - name: ZONE_ONE
              server-groups:
              - AZ1
              - AZ2
              - AZ3
              ring-specifications:
              - region: region1
              swift-zones:
              - id: 1
              server-groups:
              - ZONE_ONE
              rings:
              …</pre></div><p>
   Alternatively, if you omit the <code class="literal">swift-zones</code> specification,
   a single swift zone is used by default for all servers.
  </p><p>
   In the following example, three swift zones are specified and mapped to the
   same availability zones that nova uses (assuming you are using one of the
   example input models):
  </p><div class="verbatim-wrap"><pre class="screen">ring-specifications:
      - region: region1
      swift-zones:
      - id: 1
      server-groups:
      - AZ1
      - id: 2
      server-groups:
      - AZ2
      - id: 3
      server-groups:
      - AZ3</pre></div><p>
   In this example, it shows a datacenter with four availability zones which
   are mapped to two swift zones. This type of setup may be used if you had two
   buildings where each building has a duplicated network infrastructure:
  </p><div class="verbatim-wrap"><pre class="screen">ring-specifications:
      - region: region1
      swift-zones:
      - id: 1
      server-groups:
      - AZ1
      - AZ2
      - id: 2
      server-groups:
      - AZ3
      - AZ4</pre></div></div><div class="sect2" id="zones-ring-level"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.12.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Specifying Swift Zones at Ring Level</span> <a title="Permalink" class="permalink" href="#zones-ring-level">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-swift_zones.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-swift_zones.xml</li><li><span class="ds-label">ID: </span>zones-ring-level</li></ul></div></div></div></div><p>
   Usually, you would use the same swift zone layout for all rings in your
   system. However, it is possible to specify a different layout for a given
   ring. The following example shows that the account, container and object-0
   rings have two zones, but the object-1 ring has a single zone.
  </p><div class="verbatim-wrap"><pre class="screen">ring-specifications:
        - region: region1
        swift-zones:
        - id: 1
        server-groups:
        - AZ1
        - id: 2
        server-groups:
        - AZ2
        rings
        - name: account
        …
        - name: container
        …
        - name: object-0
        …
        - name: object-1
        swift-zones:
        - id: 1
        server-groups:
        - CLOUD
        …</pre></div></div></div><div class="sect1" id="topic-rdf-hkp-rt"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Customizing Swift Service Configuration Files</span> <a title="Permalink" class="permalink" href="#topic-rdf-hkp-rt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-modify_swift_service_config_files.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-modify_swift_service_config_files.xml</li><li><span class="ds-label">ID: </span>topic-rdf-hkp-rt</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 enables you to modify various swift service configuration
  files. The following swift service configuration files are located on the
  Cloud Lifecycle Manager in the <code class="filename">~/openstack/my_cloud/config/swift/</code> directory:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <code class="literal">account-server.conf.j2</code>
   </p></li><li class="listitem "><p>
    <code class="literal">container-reconciler.conf.j2</code>
   </p></li><li class="listitem "><p>
    <code class="literal">container-server.conf.j2</code>
   </p></li><li class="listitem "><p>
    <code class="literal">container-sync-realms.conf.j2</code>
   </p></li><li class="listitem "><p>
    <code class="literal">object-expirer.conf.j2</code>
   </p></li><li class="listitem "><p>
    <code class="literal">object-server.conf.j2</code>
   </p></li><li class="listitem "><p>
    <code class="literal">proxy-server.conf.j2</code>
   </p></li><li class="listitem "><p>
    <code class="literal">rsyncd.conf.j2</code>
   </p></li><li class="listitem "><p>
    <code class="literal">swift.conf.j2</code>
   </p></li><li class="listitem "><p>
    <code class="literal">swift-recon.j2</code>
   </p></li></ul></div><p>
  There are many configuration options that can be set or changed, including
  <span class="bold"><strong>container rate limit</strong></span>
  and <span class="bold"><strong>logging level</strong></span>:
 </p><div class="sect2" id="configuring-swift-contianer-rate-limit"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.13.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Swift Container Rate Limit</span> <a title="Permalink" class="permalink" href="#configuring-swift-contianer-rate-limit">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-modify_swift_service_config_files.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-modify_swift_service_config_files.xml</li><li><span class="ds-label">ID: </span>configuring-swift-contianer-rate-limit</li></ul></div></div></div></div><p>
   The swift container rate limit allows you to limit the number of
   <code class="literal">PUT</code> and <code class="literal">DELETE</code> requests of an object
   based on the number of objects in a container. For example, suppose the
   <code class="literal">container_ratelimit_x = r </code>. It means that for containers
   of size <code class="literal">x</code>, limit requests per second to
   <code class="literal">r</code>.
  </p><p>
   To enable container rate limiting:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Edit the <code class="literal">DEFAULT</code> section of
     <code class="filename">~/openstack/my_cloud/config/swift/proxy-server.conf.j2</code>:
    </p><div class="verbatim-wrap"><pre class="screen">container_ratelimit_0 = 100
container_ratelimit_1000000 = 100
container_ratelimit_5000000 = 50</pre></div><p>
     This will set the <code class="literal">PUT</code> and <code class="literal">DELETE</code>
     object rate limit to 100 requests per second for containers with up to
     1,000,000 objects. Also, the <code class="literal">PUT</code> and
     <code class="literal">DELETE</code> rate for containers with between 1,000,000 and
     5,000,000 objects will vary linearly from between 100 and 50 requests per
     second as the container object count increases.
    </p></li><li class="step "><p>
     Commit your changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git commit -m "<em class="replaceable ">COMMIT_MESSAGE</em>" \
~/openstack/my_cloud/config/swift/proxy-server.conf.j2</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the <code class="literal">swift-reconfigure.yml</code> playbook to reconfigure
     the swift servers:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect2" id="modifying-swift-account-server-logging-level"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.13.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring swift Account Server Logging Level</span> <a title="Permalink" class="permalink" href="#modifying-swift-account-server-logging-level">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-modify_swift_service_config_files.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-modify_swift_service_config_files.xml</li><li><span class="ds-label">ID: </span>modifying-swift-account-server-logging-level</li></ul></div></div></div></div><p>
   By default the swift logging level is set to <code class="literal">INFO</code>. As a
   best practice, do not set the log level to DEBUG for a long period of time.
   Use it for troubleshooting issues and then change it back to INFO.
  </p><p>
   Perform the following steps to set the logging level of the
   <code class="literal">account-server</code> to <code class="literal">DEBUG</code>:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Edit the <code class="literal">DEFAULT</code> section of
     <code class="filename">~/openstack/my_cloud/config/swift/account-server.conf.j2</code>:
    </p><div class="verbatim-wrap"><pre class="screen">[DEFAULT] . . log_level = DEBUG</pre></div></li><li class="step "><p>
     Commit your changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git commit -m "<em class="replaceable ">COMMIT_MESSAGE</em>" \
~/openstack/my_cloud/config/swift/account-server.conf.j2</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the <code class="literal">swift-reconfigure.yml</code> playbook to reconfigure
     the swift servers:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect2" id="id-1.3.4.9.18.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.13.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="#id-1.3.4.9.18.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-planning-objectstorage-modify_swift_service_config_files.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-planning-objectstorage-modify_swift_service_config_files.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   For more information, see:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="intraxref">Book “Operations Guide CLM”, Chapter 13 “Managing Monitoring, Logging, and Usage Reporting”, Section 13.2 “Centralized Logging Service”, Section 13.2.5 “Configuring Centralized Logging”</span>
    </p></li><li class="listitem "><p>
     <span class="intraxref">Book “Operations Guide CLM”, Chapter 13 “Managing Monitoring, Logging, and Usage Reporting”, Section 13.2 “Centralized Logging Service”</span>
    </p></li></ul></div></div></div></div><div class="chapter " id="alternative-configurations"><div class="titlepage"><div><div><h2 class="title"><span class="number">12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Alternative Configurations</span> <a title="Permalink" class="permalink" href="#alternative-configurations">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-alternative-alternative_configurations.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-alternative-alternative_configurations.xml</li><li><span class="ds-label">ID: </span>alternative-configurations</li></ul></div></div><div><div class="abstract"><p>
    In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 there are alternative configurations that we recommend
    for specific purposes.
   </p></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#standalone-deployer"><span class="number">12.1 </span><span class="name">Using a Dedicated Cloud Lifecycle Manager Node</span></a></span></dt><dt><span class="section"><a href="#without-dvr"><span class="number">12.2 </span><span class="name">Configuring <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> without DVR</span></a></span></dt><dt><span class="section"><a href="#without-l3agent"><span class="number">12.3 </span><span class="name">Configuring <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> with Provider VLANs and Physical Routers Only</span></a></span></dt><dt><span class="section"><a href="#twosystems"><span class="number">12.4 </span><span class="name">Considerations When Installing Two Systems on One Subnet</span></a></span></dt></dl></div></div><div class="sect1" id="standalone-deployer"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using a Dedicated Cloud Lifecycle Manager Node</span> <a title="Permalink" class="permalink" href="#standalone-deployer">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-alternative-standalone_deployer.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-alternative-standalone_deployer.xml</li><li><span class="ds-label">ID: </span>standalone-deployer</li></ul></div></div></div></div><p>
  All of the example configurations included host the Cloud Lifecycle Manager on the first
  Control Node. It is also possible to deploy this service on a dedicated
  node. One use case for wanting to run the dedicated Cloud Lifecycle Manager is to be able to
  test the deployment of different configurations without having to re-install
  the first server. Some administrators prefer the additional security of
  keeping all of the configuration data on a separate server from those that
  users of the cloud connect to (although all of the data can be encrypted and
  SSH keys can be password protected).
 </p><p>
  Here is a graphical representation of this setup:
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-examples-entry_scale_kvm.png" target="_blank"><img src="images/media-examples-entry_scale_kvm.png" width="" /></a></div></div><div class="sect2" id="sec-specify-lifecycle-manager"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Specifying a dedicated Cloud Lifecycle Manager in your input model</span> <a title="Permalink" class="permalink" href="#sec-specify-lifecycle-manager">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-alternative-standalone_deployer.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-alternative-standalone_deployer.xml</li><li><span class="ds-label">ID: </span>sec-specify-lifecycle-manager</li></ul></div></div></div></div><p>
   To specify a dedicated Cloud Lifecycle Manager in your input model, make the following edits
   to your configuration files.
  </p><div id="id-1.3.4.10.3.5.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    The indentation of each of the input files is important and will cause
    errors if not done correctly. Use the existing content in each of these
    files as a reference when adding additional content for your Cloud Lifecycle Manager.
   </p></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Update <code class="filename">control_plane.yml</code> to add the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Update <code class="filename">server_roles.yml</code> to add the Cloud Lifecycle Manager role.
    </p></li><li class="listitem "><p>
     Update <code class="filename">net_interfaces.yml</code> to add the interface
     definition for the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Create a <code class="filename">disks_lifecycle_manager.yml</code> file to define
     the disk layout for the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Update <code class="filename">servers.yml</code> to add the dedicated Cloud Lifecycle Manager node.
    </p></li></ul></div><p>
   <code class="filename">Control_plane.yml</code>: The snippet below shows the addition
   of a single node cluster into the control plane to host the Cloud Lifecycle Manager service.
   Note that, in addition to adding the new cluster, you also have to remove
   the Cloud Lifecycle Manager component from the <code class="literal">cluster1</code> in the examples:
  </p><div class="verbatim-wrap"><pre class="screen">  clusters:
<span class="bold"><strong>     - name: cluster0
       cluster-prefix: c0
       server-role: LIFECYCLE-MANAGER-ROLE
       member-count: 1
       allocation-policy: strict
       service-components:
         - lifecycle-manager</strong></span>
         - ntp-client
     - name: cluster1
       cluster-prefix: c1
       server-role: CONTROLLER-ROLE
       member-count: 3
       allocation-policy: strict
       service-components:
         - lifecycle-manager
         - ntp-server
         - tempest</pre></div><p>
   This specifies a single node of role
   <code class="literal">LIFECYCLE-MANAGER-ROLE</code> hosting the Cloud Lifecycle Manager.
  </p><p>
   <code class="filename">Server_roles.yml</code>: The snippet below shows the insertion
   of the new server roles definition:
  </p><div class="verbatim-wrap"><pre class="screen">   server-roles:

<span class="bold"><strong>      - name: LIFECYCLE-MANAGER-ROLE
        interface-model: LIFECYCLE-MANAGER-INTERFACES
        disk-model: LIFECYCLE-MANAGER-DISKS</strong></span>

      - name: CONTROLLER-ROLE</pre></div><p>
   This defines a new server role which references a new interface-model and
   disk-model to be used when configuring the server.
  </p><p>
   <code class="filename">net-interfaces.yml</code>: The snippet below shows the
   insertion of the network-interface info:
  </p><div class="verbatim-wrap"><pre class="screen"><span class="bold"><strong>    - name: LIFECYCLE-MANAGER-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
             name: bond0
          bond-data:
             options:
                 mode: active-backup
                 miimon: 200
                 primary: hed3
             provider: linux
             devices:
                 - name: hed3
                 - name: hed4
          network-groups:
             - MANAGEMENT</strong></span></pre></div><p>
   This assumes that the server uses the same physical networking layout as the
   other servers in the example.
   
   
  </p><p>
   <code class="filename">disks_lifecycle_manager.yml</code>: In the examples,
   disk-models are provided as separate files (this is just a convention, not a
   limitation) so the following should be added as a new file named
   <code class="filename">disks_lifecycle_manager.yml</code>:
  </p><div class="verbatim-wrap"><pre class="screen">---
   product:
      version: 2

   disk-models:
<span class="bold"><strong>   - name: LIFECYCLE-MANAGER-DISKS
     # Disk model to be used for Cloud Lifecycle Managers nodes
     # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
     # sda_root is a templated value to align with whatever partition is really used
     # This value is checked in os config and replaced by the partition actually used
     # on sda e.g. sda1 or sda5

     volume-groups:
       - name: ardana-vg
         physical-volumes:
           - /dev/sda_root

       logical-volumes:
       # The policy is not to consume 100% of the space of each volume group.
       # 5% should be left free for snapshots and to allow for some flexibility.
          - name: root
            size: 80%
            fstype: ext4
            mount: /
          - name: crash
            size: 15%
            mount: /var/crash
            fstype: ext4
            mkfs-opts: -O large_file
        consumer:
              name: os</strong></span></pre></div><p>
   <code class="filename">Servers.yml</code>: The snippet below shows the insertion of an
   additional server used for hosting the Cloud Lifecycle Manager. Provide the address
   information here for the server you are running on, that is, the node where
   you have installed the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> ISO.
  </p><div class="verbatim-wrap"><pre class="screen">  servers:
     # NOTE: Addresses of servers need to be changed to match your environment.
     #
     #       Add additional servers as required

<span class="bold"><strong>     #Lifecycle-manager
     - id: lifecycle-manager
       ip-addr: <em class="replaceable ">YOUR IP ADDRESS HERE</em>
       role: LIFECYCLE-MANAGER-ROLE
       server-group: RACK1
       nic-mapping: HP-SL230-4PORT
       mac-addr: 8c:dc:d4:b5:c9:e0
       # ipmi information is not needed </strong></span>

     # Controllers
     - id: controller1
       ip-addr: 192.168.10.3
       role: CONTROLLER-ROLE</pre></div><div id="id-1.3.4.10.3.5.18" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    With a stand-alone deployer, the OpenStack CLI and other clients will not
    be installed automatically. You need to install <span class="productname">OpenStack</span> clients to get the
    desired <span class="productname">OpenStack</span> capabilities. For more information and installation
    instructions, consult <a class="xref" href="#install-openstack-clients" title="Chapter 40. Installing OpenStack Clients">Chapter 40, <em>Installing OpenStack Clients</em></a>.
   </p></div></div></div><div class="sect1" id="without-dvr"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> without DVR</span> <a title="Permalink" class="permalink" href="#without-dvr">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-alternative-without_dvr.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-alternative-without_dvr.xml</li><li><span class="ds-label">ID: </span>without-dvr</li></ul></div></div></div></div><p>
  By default in the KVM model, the neutron service utilizes distributed routing
  (DVR). This is the recommended setup because it allows for high availability.
  However, if you would like to disable this feature, here are the steps to
  achieve this.
 </p><p>
  On your Cloud Lifecycle Manager, make the following changes:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    In the
    <code class="filename">~/openstack/my_cloud/config/neutron/neutron.conf.j2</code>
    file, change the line below from:
   </p><div class="verbatim-wrap"><pre class="screen">router_distributed = {{ router_distributed }}</pre></div><p>
    to:
   </p><div class="verbatim-wrap"><pre class="screen">router_distributed = False</pre></div></li><li class="step "><p>
    In the
    <code class="filename">~/openstack/my_cloud/config/neutron/ml2_conf.ini.j2</code>
    file, change the line below from:
   </p><div class="verbatim-wrap"><pre class="screen">enable_distributed_routing = {{ enable_distributed_routing }}</pre></div><p>
    to:
   </p><div class="verbatim-wrap"><pre class="screen">enable_distributed_routing = False</pre></div></li><li class="step "><p>
    In the
    <code class="filename">~/openstack/my_cloud/config/neutron/l3_agent.ini.j2</code>
    file, change the line below from:
   </p><div class="verbatim-wrap"><pre class="screen">agent_mode = {{ neutron_l3_agent_mode }}</pre></div><p>
    to:
   </p><div class="verbatim-wrap"><pre class="screen">agent_mode = legacy</pre></div></li><li class="step "><p>
    In the
    <code class="filename">~/openstack/my_cloud/definition/data/control_plane.yml</code>
    file, remove the following values from the Compute resource
    <code class="literal">service-components</code> list:
   </p><div class="verbatim-wrap"><pre class="screen">- neutron-l3-agent
   - neutron-metadata-agent</pre></div><div id="id-1.3.4.10.4.4.4.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
     If you fail to remove the above values from the Compute resource
     service-components list from file
     <code class="filename">~/openstack/my_cloud/definition/data/control_plane.yml</code>,
     you will end up with routers (non_DVR routers) being deployed in the
     compute host, even though the lifecycle manager is configured for
     non_distributed routers.
    </p></div></li><li class="step "><p>
    Commit your changes to your local git repository:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
    Run the configuration processor:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
    Run the ready deployment playbook:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
    Continue installation. More information on cloud deployments are available
    in the <a class="xref" href="#cloudinstallation-overview" title="Chapter 19. Overview">Chapter 19, <em>Overview</em></a>
   </p></li></ol></div></div></div><div class="sect1" id="without-l3agent"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> with Provider VLANs and Physical Routers Only</span> <a title="Permalink" class="permalink" href="#without-l3agent">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-alternative-without_l3agent.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-alternative-without_l3agent.xml</li><li><span class="ds-label">ID: </span>without-l3agent</li></ul></div></div></div></div><p>
  Another option for configuring neutron is to use provider VLANs and physical
  routers only, here are the steps to achieve this.
 </p><p>
  On your Cloud Lifecycle Manager, make the following changes:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    In the
    <code class="filename">~/openstack/my_cloud/config/neutron/neutron.conf.j2</code>
    file, change the line below from:
   </p><div class="verbatim-wrap"><pre class="screen">router_distributed = {{ router_distributed }}</pre></div><p>
    to:
   </p><div class="verbatim-wrap"><pre class="screen">router_distributed = False</pre></div></li><li class="step "><p>
    In the
    <code class="filename">~/openstack/my_cloud/config/neutron/ml2_conf.ini.j2</code>
    file, change the line below from:
   </p><div class="verbatim-wrap"><pre class="screen">enable_distributed_routing = {{ enable_distributed_routing }}</pre></div><p>
    to:
   </p><div class="verbatim-wrap"><pre class="screen">enable_distributed_routing = False</pre></div></li><li class="step "><p>
    In the
    <code class="filename">~/openstack/my_cloud/config/neutron/dhcp_agent.ini.j2</code>
    file, change the line below from:
   </p><div class="verbatim-wrap"><pre class="screen">enable_isolated_metadata = {{ neutron_enable_isolated_metadata }}</pre></div><p>
    to:
   </p><div class="verbatim-wrap"><pre class="screen">enable_isolated_metadata = True</pre></div></li><li class="step "><p>
    In the
    <code class="filename">~/openstack/my_cloud/definition/data/control_plane.yml</code>
    file, remove the following values from the Compute resource
    <code class="literal">service-components</code> list:
   </p><div class="verbatim-wrap"><pre class="screen">- neutron-l3-agent
  - neutron-metadata-agent</pre></div></li></ol></div></div></div><div class="sect1" id="twosystems"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Considerations When Installing Two Systems on One Subnet</span> <a title="Permalink" class="permalink" href="#twosystems">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/planning-architecture-alternative-twosystems.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>planning-architecture-alternative-twosystems.xml</li><li><span class="ds-label">ID: </span>twosystems</li></ul></div></div></div></div><p>
  If you wish to install two separate <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 systems using a single
  subnet, you will need to consider the following notes.
 </p><p>
  The <code class="literal">ip_cluster</code> service includes the
  <code class="literal">keepalived</code> daemon which maintains virtual IPs (VIPs) on
  cluster nodes. In order to maintain VIPs, it communicates between cluster
  nodes over the VRRP protocol.
 </p><p>
  A VRRP virtual routerid identifies a particular VRRP cluster and must be
  unique for a subnet. If you have two VRRP clusters with the same virtual
  routerid, causing a clash of VRRP traffic, the VIPs are unlikely to be up or
  pingable and you are likely to get the following signature in your
  <code class="literal">/etc/keepalived/keepalived.log</code>:
 </p><div class="verbatim-wrap"><pre class="screen">Dec 16 15:43:43 ardana-cp1-c1-m1-mgmt Keepalived_vrrp[2218]: ip address
  associated with VRID not present in received packet : 10.2.1.11
Dec 16 15:43:43 ardana-cp1-c1-m1-mgmt Keepalived_vrrp[2218]: one or more VIP
  associated with VRID mismatch actual MASTER advert
Dec 16 15:43:43 ardana-cp1-c1-m1-mgmt Keepalived_vrrp[2218]: bogus VRRP packet
  received on br-bond0 !!!
Dec 16 15:43:43 ardana-cp1-c1-m1-mgmt Keepalived_vrrp[2218]: VRRP_Instance(VI_2)
  ignoring received advertisment...</pre></div><p>
  To resolve this issue, our recommendation is to install your separate
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 systems with VRRP traffic on different subnets.
 </p><p>
  If this is not possible, you may also assign a unique routerid to your
  separate <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 system by changing the
  <code class="literal">keepalived_vrrp_offset</code> service configurable. The routerid
  is currently derived using the <code class="literal">keepalived_vrrp_index</code> which
  comes from a configuration processor variable and the
  <code class="literal">keepalived_vrrp_offset</code>.
 </p><p>
  For example,
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Log in to your Cloud Lifecycle Manager.
   </p></li><li class="step "><p>
    Edit your
    <code class="filename">~/openstack/my_cloud/config/keepalived/defaults.yml</code>
    file and change the value of the following line:
   </p><div class="verbatim-wrap"><pre class="screen">keepalived_vrrp_offset: 0</pre></div><p>
    Change the off value to a number that uniquely identifies a separate vrrp
    cluster. For example:
   </p><p>
    <code class="literal">keepalived_vrrp_offset: 0</code> for the 1st vrrp cluster on
    this subnet.
   </p><p>
    <code class="literal">keepalived_vrrp_offset: 1</code> for the 2nd vrrp cluster on
    this subnet.
   </p><p>
    <code class="literal">keepalived_vrrp_offset: 2</code> for the 3rd vrrp cluster on
    this subnet.
   </p><div id="symlinks" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
     You should be aware that the files in the
     <code class="filename">~/openstack/my_cloud/config/</code> directory are symlinks
     to the <code class="filename">~/openstack/ardana/ansible/</code> directory. For
     example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls -al ~/openstack/my_cloud/config/keepalived/defaults.yml
lrwxrwxrwx 1 stack stack 55 May 24 20:38 /var/lib/ardana/openstack/my_cloud/config/keepalived/defaults.yml -&gt;
    ../../../ardana/ansible/roles/keepalived/defaults/main.yml</pre></div><p>
     If you are using a tool like <code class="literal">sed</code> to make edits to files
     in this directory, you might break the symbolic link and create a new copy
     of the file. To maintain the link, you will need to force
     <code class="literal">sed</code> to follow the link:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sed -i <span class="bold"><strong>--follow-symlinks</strong></span> \
  's$keepalived_vrrp_offset: 0$keepalived_vrrp_offset: 2$' \
  ~/openstack/my_cloud/config/keepalived/defaults.yml</pre></div><p>
     Alternatively, directly edit the target of the link
     <code class="filename">~/openstack/ardana/ansible/roles/keepalived/defaults/main.yml</code>.
    </p></div></li><li class="step "><p>
    Commit your configuration to the Git repository (see
    <a class="xref" href="#using-git" title="Chapter 22. Using Git for Configuration Management">Chapter 22, <em>Using Git for Configuration Management</em></a>), as follows:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "changing Admin password"</pre></div></li><li class="step "><p>
    Run the configuration processor with this command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
    Use the playbook below to create a deployment directory:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
    If you are making this change after your initial install, run the following
    reconfigure playbook to make this change in your environment:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts FND-CLU-reconfigure.yml</pre></div></li></ol></div></div></div></div></div><div class="part" id="preinstall"><div class="titlepage"><div><div><h1 class="title"><span class="number">Part III </span><span class="name">Pre-Installation </span><a title="Permalink" class="permalink" href="#preinstall">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-preinstall_overview.xml" title="Edit the source file for this section">Edit source</a></h1></div></div></div><div class="toc"><dl><dt><span class="chapter"><a href="#preinstall-overview"><span class="number">13 </span><span class="name">Overview</span></a></span></dt><dd class="toc-abstract"><p>
   To ensure that your environment meets the requirements of the cloud model you
   choose, see the check list in <a class="xref" href="#preinstall-checklist" title="Chapter 14. Pre-Installation Checklist">Chapter 14, <em>Pre-Installation Checklist</em></a>.
  </p></dd><dt><span class="chapter"><a href="#preinstall-checklist"><span class="number">14 </span><span class="name">Pre-Installation Checklist</span></a></span></dt><dd class="toc-abstract"><p>
   The formatting of this page facilitates printing it out and using it to
   record details of your setup.
  </p></dd><dt><span class="chapter"><a href="#cha-depl-dep-inst"><span class="number">15 </span><span class="name">Installing the Cloud Lifecycle Manager server</span></a></span></dt><dd class="toc-abstract"><p>
    This chapter will show how to install the Cloud Lifecycle Manager from scratch. It will run
    on SUSE Linux Enterprise Server 12 SP4, include the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> extension, and, optionally, the
    Subscription Management Tool (SMT) server.
   </p></dd><dt><span class="chapter"><a href="#app-deploy-smt-lcm"><span class="number">16 </span><span class="name">Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</span></a></span></dt><dd class="toc-abstract"><p>
    One way to provide the repositories needed to set up the nodes in
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is to install a Subscription Management Tool (SMT) server on the Cloud Lifecycle Manager server, and then mirror all
    repositories from SUSE Customer Center via this server. Installing an SMT server
    on the Cloud Lifecycle Manager server is optional. If your organization already provides an
    SMT server or a SUSE Manager server that can be accessed from the
    Cloud Lifecycle Manager server, skip this step.
   </p></dd><dt><span class="chapter"><a href="#cha-depl-repo-conf-lcm"><span class="number">17 </span><span class="name">Software Repository Setup</span></a></span></dt><dd class="toc-abstract"><p>Software repositories containing products, extensions, and the respective updates for all software need to be available to all nodes in SUSE OpenStack Cloud in order to complete the deployment. These can be managed manually, or they can be hosted on the Cloud Lifecycle Manager server. In this config…</p></dd><dt><span class="chapter"><a href="#multipath-boot-from-san"><span class="number">18 </span><span class="name">Boot from SAN and Multipath Configuration</span></a></span></dt><dd class="toc-abstract"><p>
   For information about supported hardware for multipathing, see
   <a class="xref" href="#hw-support-hardwareconfig" title="2.2. Supported Hardware Configurations">Section 2.2, “Supported Hardware Configurations”</a>.
  </p></dd></dl></div><div class="chapter " id="preinstall-overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Overview</span> <a title="Permalink" class="permalink" href="#preinstall-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-preinstall_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-preinstall_overview.xml</li><li><span class="ds-label">ID: </span>preinstall-overview</li></ul></div></div></div></div><div class="line"></div><p>
   To ensure that your environment meets the requirements of the cloud model you
   choose, see the check list in <a class="xref" href="#preinstall-checklist" title="Chapter 14. Pre-Installation Checklist">Chapter 14, <em>Pre-Installation Checklist</em></a>.
  </p><p>
   After you have decided on a configuration to choose for your cloud
   and you have gone through the pre-installation steps, you will have two options
   for installation:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     You can use a graphical user interface (GUI) that runs in your Web browser.
    </p></li><li class="listitem "><p>
     You can install via the command line that gives you the flexibility and
     full control of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9.
    </p></li></ul></div><p>
   <span class="bold"><strong>Using the GUI</strong></span>
  </p><p>
   You should use the GUI if:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     You are not planning to deploy availability zones or use L3 segmentation
     in your initial deployment.
    </p></li><li class="listitem "><p>
     You are satisfied with the tuned SUSE-default <span class="productname">OpenStack</span> configuration.
    </p></li></ul></div><p>
   Instructions for GUI installation are in <a class="xref" href="#install-gui" title="Chapter 21. Installing with the Install UI">Chapter 21, <em>Installing with the Install UI</em></a>.
  </p><div id="id-1.3.5.2.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    Reconfiguring your cloud can only be done via the command line. The GUI
    installer is for initial installation only.
   </p></div><p>
   <span class="bold"><strong>Using the Command Line</strong></span>
  </p><p>
   You should use the command line if:
  </p><div class="itemizedlist " id="idg-installation-installation-installation-overview-xml-7"><ul class="itemizedlist"><li class="listitem "><p>
     You are installing a complex or large-scale cloud.
    </p></li><li class="listitem "><p>
     You need to use availability zones or the server groups functionality of
     the cloud model. For more information, see the <a class="xref" href="#cha-input-model-intro-concept" title="Chapter 5. Input Model">Chapter 5, <em>Input Model</em></a>.
    </p></li><li class="listitem "><p>
     You want to customize the cloud configuration beyond the tuned defaults
     that SUSE provides out of the box.
    </p></li><li class="listitem "><p>
     You need more extensive customizations than are possible using the GUI.
    </p></li></ul></div><p>
   Instructions for installing via the command line are in <a class="xref" href="#install-kvm" title="Chapter 24. Installing Mid-scale and Entry-scale KVM">Chapter 24, <em>Installing Mid-scale and Entry-scale KVM</em></a>.
  </p><div id="id-1.3.5.2.14" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    Ardana is an open-source project and a generalized lifecycle management
    framework.  Cloud Lifecycle Manager is based on Ardana, and delivers the lifecycle management
    functionality required by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9. Due to this
    relationship, some Cloud Lifecycle Manager commands refer to Ardana.
   </p></div></div><div class="chapter " id="preinstall-checklist"><div class="titlepage"><div><div><h2 class="title"><span class="number">14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Pre-Installation Checklist</span> <a title="Permalink" class="permalink" href="#preinstall-checklist">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-preinstall_checklist.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-preinstall_checklist.xml</li><li><span class="ds-label">ID: </span>preinstall-checklist</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#id-1.3.5.3.4"><span class="number">14.1 </span><span class="name">BIOS and IPMI Settings</span></a></span></dt><dt><span class="section"><a href="#id-1.3.5.3.5"><span class="number">14.2 </span><span class="name">Network Setup and Configuration</span></a></span></dt><dt><span class="section"><a href="#id-1.3.5.3.6"><span class="number">14.3 </span><span class="name">Cloud Lifecycle Manager</span></a></span></dt><dt><span class="section"><a href="#id-1.3.5.3.7"><span class="number">14.4 </span><span class="name">Information for the <code class="filename">nic_mappings.yml</code> Input File</span></a></span></dt><dt><span class="section"><a href="#id-1.3.5.3.8"><span class="number">14.5 </span><span class="name">Control Plane</span></a></span></dt><dt><span class="section"><a href="#id-1.3.5.3.9"><span class="number">14.6 </span><span class="name">Compute Hosts</span></a></span></dt><dt><span class="section"><a href="#id-1.3.5.3.10"><span class="number">14.7 </span><span class="name">Storage Hosts</span></a></span></dt><dt><span class="section"><a href="#id-1.3.5.3.11"><span class="number">14.8 </span><span class="name">Additional Comments</span></a></span></dt></dl></div></div><div id="id-1.3.5.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   The formatting of this page facilitates printing it out and using it to
   record details of your setup.
  </p></div><p>
  This checklist is focused on the Entry-scale KVM model but you can alter it
  to fit the example configuration you choose for your cloud.
 </p><div class="sect1" id="id-1.3.5.3.4"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">BIOS and IPMI Settings</span> <a title="Permalink" class="permalink" href="#id-1.3.5.3.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-preinstall_checklist.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-preinstall_checklist.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Ensure that the following BIOS and IPMI settings are applied to each
   bare-metal server:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>☐</th><th>Item</th></tr></thead><tbody><tr><td> </td><td>
       Choose either UEFI or Legacy BIOS in the BIOS settings
      </td></tr><tr><td> </td><td>
       <p>
        Verify the Date and Time settings in the BIOS.
       </p>
       <div id="id-1.3.5.3.4.3.1.4.2.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
         <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installs and runs with UTC, not local time.
        </p></div>
      </td></tr><tr><td> </td><td>Ensure that Wake-on-LAN is disabled in the BIOS</td></tr><tr><td> </td><td>
       Ensure that the NIC port to be used for PXE installation has PXE
       enabled in the BIOS
      </td></tr><tr><td> </td><td>Ensure that all other NIC ports have PXE disabled in the BIOS</td></tr><tr><td> </td><td>
       Ensure all hardware in the server not directly used by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is
       disabled
      </td></tr></tbody></table></div></div><div class="sect1" id="id-1.3.5.3.5"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Setup and Configuration</span> <a title="Permalink" class="permalink" href="#id-1.3.5.3.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-preinstall_checklist.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-preinstall_checklist.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Before installing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the following networks must be provisioned and
   tested. The networks are not installed or managed by the Cloud. You must
   install and manage the networks as documented in
   <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a>.
  </p><p>
   Note that if you want a pluggable IPAM driver, it must be specified at
   install time. Only with a clean install of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 can you specify a
   different IPAM driver. If upgrading, you must use the default driver.
   More information can be found in
   <span class="intraxref">Book “Operations Guide CLM”, Chapter 10 “Managing Networking”, Section 10.4 “Networking Service Overview”, Section 10.4.7 “Using IPAM Drivers in the Networking Service”</span>.
  </p><p>
   Use these checklists to confirm and record your network configuration
   information.
  </p><p>
   <span class="bold"><strong>Router</strong></span>
  </p><p>
   The IP router used with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> must support the updated of its ARP table
   through gratuitous ARP packets.
  </p><p>
   <span class="bold"><strong>PXE Installation Network</strong></span>
  </p><p>
   When provisioning the IP range, allocate sufficient IP addresses to cover
   both the current number of servers and any planned expansion. Use the
   following table to help calculate the requirements:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Instance</th><th>Description</th><th>IPs</th></tr></thead><tbody><tr><td>Deployer O/S</td><td> </td><td>1</td></tr><tr><td>Controller server O/S (x3)</td><td> </td><td>3</td></tr><tr><td>Compute servers (2nd thru 100th)</td><td>single IP per server</td><td> </td></tr><tr><td>block storage host servers</td><td>single IP per server</td><td> </td></tr></tbody></table></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>☐</th><th>Item</th><th>Value</th></tr></thead><tbody><tr><td> </td><td>Network is untagged</td><td> </td></tr><tr><td> </td><td>No DHCP servers other than <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are on the network</td><td> </td></tr><tr><td> </td><td>Switch PVID used to map any "internal" VLANs to untagged</td><td> </td></tr><tr><td> </td><td>Routable to the IPMI network</td><td> </td></tr><tr><td> </td><td>IP CIDR</td><td> </td></tr><tr><td> </td><td>IP Range (Usable IPs)</td><td>
       <p>
        begin:
       </p>
       <p>
        end:
       </p>
      </td></tr><tr><td> </td><td>Default IP Gateway</td><td> </td></tr></tbody></table></div><p>
   <span class="bold"><strong>Management Network</strong></span>
  </p><p>
   The management network is the backbone used for the majority of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   management communications. Control messages are exchanged between the
   Controllers, Compute hosts, and cinder backends through this
   network. In addition to the control flows, the management network is also
   used to transport swift and iSCSI based cinder block storage
   traffic between servers.
  </p><p>
   When provisioning the IP Range, allocate sufficient IP addresses to cover
   both the current number of servers and any planned expansion. Use the
   following table to help calculate the requirements:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Instance</th><th>Description</th><th>IPs</th></tr></thead><tbody><tr><td>Controller server O/S (x3)</td><td> </td><td>3</td></tr><tr><td>Controller VIP</td><td> </td><td>1</td></tr><tr><td>Compute servers (2nd through 100th)</td><td>single IP per server</td><td> </td></tr><tr><td>VM servers</td><td>single IP per server</td><td> </td></tr><tr><td>VIP per cluster</td><td> </td><td> </td></tr></tbody></table></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>☐</th><th>Item</th><th>Value</th></tr></thead><tbody><tr><td> </td><td>Network is untagged</td><td> </td></tr><tr><td> </td><td>No DHCP servers other than <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are on the network</td><td> </td></tr><tr><td> </td><td>Switch PVID used to map any "internal" VLANs to untagged</td><td> </td></tr><tr><td> </td><td>IP CIDR</td><td> </td></tr><tr><td> </td><td>IP Range (Usable IPs)</td><td>
       <p>
        begin:
       </p>
       <p>
        end:
       </p>
      </td></tr><tr><td> </td><td>Default IP Gateway</td><td> </td></tr><tr><td> </td><td>VLAN ID</td><td> </td></tr></tbody></table></div><p>
   <span class="bold"><strong>IPMI Network</strong></span>
  </p><p>
   The IPMI network is used to connect the IPMI interfaces on the servers that
   are assigned for use with implementing the cloud. This network is used by
   Cobbler to control the state of the servers
   during baremetal deployments.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>☐</th><th>Item</th><th>Value</th></tr></thead><tbody><tr><td> </td><td>Network is untagged</td><td> </td></tr><tr><td> </td><td>Routable to the Management Network</td><td> </td></tr><tr><td> </td><td>IP Subnet</td><td> </td></tr><tr><td> </td><td>Default IP Gateway</td><td> </td></tr></tbody></table></div><p>
   <span class="bold"><strong>External API Network</strong></span>
  </p><p>
   The External network is used to connect <span class="productname">OpenStack</span> endpoints to an external
   public network such as a company’s intranet or the public internet in the
   case of a public cloud provider.
  </p><p>
   When provisioning the IP Range, allocate sufficient IP addresses to cover
   both the current number of servers and any planned expansion. Use the
   following table to help calculate the requirements.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Instance</th><th>Description</th><th>IPs</th></tr></thead><tbody><tr><td>Controller server O/S (x3)</td><td> </td><td>3</td></tr><tr><td>Controller VIP</td><td> </td><td>1</td></tr></tbody></table></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>☐</th><th>Item</th><th>Value</th></tr></thead><tbody><tr><td> </td><td>VLAN Tag assigned:</td><td> </td></tr><tr><td> </td><td>IP CIDR</td><td> </td></tr><tr><td> </td><td>IP Range (Usable IPs)</td><td>
       <p>
        begin:
       </p>
       <p>
        end:
       </p>
      </td></tr><tr><td> </td><td>Default IP Gateway</td><td> </td></tr><tr><td> </td><td>VLAN ID</td><td> </td></tr></tbody></table></div><p>
   <span class="bold"><strong>External VM Network</strong></span>
  </p><p>
   The External VM network is used to connect cloud instances to an external
   public network such as a company’s intranet or the public internet in the
   case of a public cloud provider. The external network has a predefined range
   of Floating IPs which are assigned to individual instances to enable
   communications to and from the instance to the assigned corporate
   intranet/internet. There should be a route between the External VM and
   External API networks so that instances provisioned in the cloud, may access
   the Cloud API endpoints, using the instance floating IPs.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>☐</th><th>Item</th><th>Value</th></tr></thead><tbody><tr><td> </td><td>VLAN Tag assigned:</td><td> </td></tr><tr><td> </td><td>IP CIDR</td><td> </td></tr><tr><td> </td><td>IP Range (Usable IPs)</td><td>
       <p>
        begin:
       </p>
       <p>
        end:
       </p>
      </td></tr><tr><td> </td><td>Default IP Gateway</td><td> </td></tr><tr><td> </td><td>VLAN ID</td><td> </td></tr></tbody></table></div></div><div class="sect1" id="id-1.3.5.3.6"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cloud Lifecycle Manager</span> <a title="Permalink" class="permalink" href="#id-1.3.5.3.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-preinstall_checklist.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-preinstall_checklist.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   This server contains the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installer, which is based on Git, Ansible,
   and Cobbler.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>☐</th><th>Item</th><th>Value</th></tr></thead><tbody><tr><td> </td><td>
       Disk Requirement: Single 8GB disk needed per the
       <a class="xref" href="#min-hardware" title="Chapter 2. Hardware and Software Support Matrix">Chapter 2, <em>Hardware and Software Support Matrix</em></a>
      </td><td> </td></tr><tr><td> </td><td>
       <a class="xref" href="#sec-depl-adm-inst-add-on" title="15.5.2. Installing the SUSE OpenStack Cloud Extension">Section 15.5.2, “Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Extension”</a>
      </td><td> </td></tr><tr><td> </td><td>
       Ensure your local DNS nameserver is placed into your
       <code class="filename">/etc/resolv.conf</code> file
      </td><td> </td></tr><tr><td> </td><td>Install and configure NTP for your environment</td><td> </td></tr><tr><td> </td><td>
       Ensure your NTP server(s) is placed into your
       <code class="filename">/etc/ntp.conf</code> file
      </td><td> </td></tr><tr><td> </td><td>NTP time source:</td><td> </td></tr><tr><td> </td><td> </td><td> </td></tr></tbody></table></div></div><div class="sect1" id="id-1.3.5.3.7"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Information for the <code class="filename">nic_mappings.yml</code> Input File</span> <a title="Permalink" class="permalink" href="#id-1.3.5.3.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-preinstall_checklist.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-preinstall_checklist.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Log on to each type of physical server you have and issue platform-appropriate
   commands to identify the <code class="literal">bus-address</code> and
   <code class="literal">port-num</code> values that may be required. For example, run the
   following command:
  </p><div class="verbatim-wrap"><pre class="screen">sudo lspci -D | grep -i net</pre></div><p>
   and enter this information in the space below. Use this information for the
   <code class="literal">bus-address</code> value in your
   <code class="literal">nic_mappings.yml</code> file.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /></colgroup><thead><tr><th>NIC Adapter PCI Bus Address Output</th></tr></thead><tbody><tr><td>
<div class="verbatim-wrap"><pre class="screen">











</pre></div>
      </td></tr></tbody></table></div><p>
   To find the <code class="literal">port-num</code> use:
  </p><div class="verbatim-wrap"><pre class="screen">cat /sys/class/net/&lt;device name&gt;/dev_port</pre></div><p>
   where the 'device-name' is the name of the device <span class="bold"><strong>currently mapped</strong></span> to this address, not necessarily the
   name of the device <span class="bold"><strong>to be mapped</strong></span>. Enter the
   information for your system in the space below.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /></colgroup><thead><tr><th>Network Device Port Number Output</th></tr></thead><tbody><tr><td>
<div class="verbatim-wrap"><pre class="screen">











</pre></div>
      </td></tr></tbody></table></div></div><div class="sect1" id="id-1.3.5.3.8"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Control Plane</span> <a title="Permalink" class="permalink" href="#id-1.3.5.3.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-preinstall_checklist.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-preinstall_checklist.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The Control Plane consists of at least three servers in a highly available
   cluster that host the core <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services including nova, keystone,
   glance, cinder, heat, neutron, swift, ceilometer, and
   horizon. Additional services include mariadb, ip-cluster, apache2,
   rabbitmq, memcached, zookeeper, kafka, storm, monasca, logging, and cmc.
  </p><div id="id-1.3.5.3.8.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    To mitigate the <span class="quote">“<span class="quote ">split-brain</span>”</span> situation described in
    <span class="intraxref">Book “Operations Guide CLM”, Chapter 18 “Troubleshooting Issues”, Section 18.4 “Network Service Troubleshooting”</span> it is recommended
    that you have HA network configuration with Multi-Chassis Link Aggregation
    (MLAG) and NIC bonding configured for all the controllers to deliver
    system-level redundancy as well network-level resiliency. Also reducing
    the ARP timeout on the TOR switches will help.
   </p></div><div class="table" id="cp-1"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 14.1: </span><span class="name">Control Plane 1 </span><a title="Permalink" class="permalink" href="#cp-1">#</a></h6></div><div class="table-contents"><table class="table" summary="Control Plane 1" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>☐</th><th>Item</th><th>Value</th></tr></thead><tbody><tr><td> </td><td>Disk Requirement: 3x 512 GB disks (or enough space to create three
logical drives with that amount of space)
      </td><td> </td></tr><tr><td> </td><td>Ensure the disks are wiped</td><td> </td></tr><tr><td> </td><td>MAC address of first NIC</td><td> </td></tr><tr><td> </td><td>A second NIC, or a set of bonded NICs are required</td><td> </td></tr><tr><td> </td><td>IPMI IP address</td><td> </td></tr><tr><td> </td><td>IPMI Username/Password</td><td> </td></tr></tbody></table></div></div><div class="table" id="cp-2"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 14.2: </span><span class="name">Control Plane 2 </span><a title="Permalink" class="permalink" href="#cp-2">#</a></h6></div><div class="table-contents"><table class="table" summary="Control Plane 2" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>☐</th><th>Item</th><th>Value</th></tr></thead><tbody><tr><td> </td><td>
       Disk Requirement: 3x 512 GB disks (or enough space to create three
       logical drives with that amount of space)
      </td><td> </td></tr><tr><td> </td><td>Ensure the disks are wiped</td><td> </td></tr><tr><td> </td><td>MAC address of first NIC</td><td> </td></tr><tr><td> </td><td>A second NIC, or a set of bonded NICs are required</td><td> </td></tr><tr><td> </td><td>IPMI IP address</td><td> </td></tr><tr><td> </td><td>IPMI Username/Password</td><td> </td></tr></tbody></table></div></div><div class="table" id="cp-3"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 14.3: </span><span class="name">Control Plane 3 </span><a title="Permalink" class="permalink" href="#cp-3">#</a></h6></div><div class="table-contents"><table class="table" summary="Control Plane 3" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>☐</th><th>Item</th><th>Value</th></tr></thead><tbody><tr><td> </td><td>
       Disk Requirement: 3x 512 GB disks (or enough space to create three
       logical drives with that amount of space)
      </td><td> </td></tr><tr><td> </td><td>Ensure the disks are wiped</td><td> </td></tr><tr><td> </td><td>MAC address of first NIC</td><td> </td></tr><tr><td> </td><td>A second NIC, or a set of bonded NICs are required</td><td> </td></tr><tr><td> </td><td>IPMI IP address</td><td> </td></tr><tr><td> </td><td>IPMI Username/Password</td><td> </td></tr></tbody></table></div></div></div><div class="sect1" id="id-1.3.5.3.9"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute Hosts</span> <a title="Permalink" class="permalink" href="#id-1.3.5.3.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-preinstall_checklist.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-preinstall_checklist.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   One or more KVM Compute servers will be used as the compute host targets for
   instances.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>☐</th><th>Item</th></tr></thead><tbody><tr><td> </td><td>
       Disk Requirement: 2x 512 GB disks (or enough space to create three
       logical drives with that amount of space)
      </td></tr><tr><td> </td><td>
       A NIC for PXE boot and a second NIC, or a NIC for PXE and a set of
       bonded NICs are required
      </td></tr><tr><td> </td><td>Ensure the disks are wiped</td></tr></tbody></table></div><p>
   Table to record your Compute host details:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /></colgroup><thead><tr><th>ID</th><th>NIC MAC Address</th><th>IPMI Username/Password</th><th>IMPI IP Address</th><th>CPU/Mem/Disk</th></tr></thead><tbody><tr><td> </td><td> </td><td> </td><td> </td><td> </td></tr><tr><td> </td><td> </td><td> </td><td> </td><td> </td></tr><tr><td> </td><td> </td><td> </td><td> </td><td> </td></tr></tbody></table></div></div><div class="sect1" id="id-1.3.5.3.10"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Storage Hosts</span> <a title="Permalink" class="permalink" href="#id-1.3.5.3.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-preinstall_checklist.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-preinstall_checklist.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Three or more servers with local disk volumes to provide cinder
   block storage resources.
  </p><div id="id-1.3.5.3.10.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    The cluster created from block storage nodes must allow for quorum. In
    other words, the node count of the cluster must be 3, 5, 7, or another odd
    number.
   </p></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>☐</th><th>Item</th></tr></thead><tbody><tr><td> </td><td>
       <p>
        Disk Requirement: 3x 512 GB disks (or enough space to create three
        logical drives with that amount of space)
       </p>
       <p>
        The block storage appliance deployed on a host is expected to consume
        ~40 GB of disk space from the host root disk for ephemeral storage to
        run the block storage virtual machine.
       </p>
      </td></tr><tr><td> </td><td>
       A NIC for PXE boot and a second NIC, or a NIC for PXE and a set of
       bonded NICs are required
      </td></tr><tr><td> </td><td>Ensure the disks are wiped</td></tr></tbody></table></div><p>
   Table to record your block storage host details:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /><col class="c6" /></colgroup><thead><tr><th>ID</th><th>NIC MAC Address</th><th>IPMI Username/Password</th><th>IPMI IP Address</th><th>CPU/Mem/Disk</th><th>Data Volume</th></tr></thead><tbody><tr><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td></tr><tr><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td></tr><tr><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td></tr></tbody></table></div></div><div class="sect1" id="id-1.3.5.3.11"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Additional Comments</span> <a title="Permalink" class="permalink" href="#id-1.3.5.3.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-preinstall_checklist.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-preinstall_checklist.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   This section is for any additional information that you deem necessary.
  </p><div class="verbatim-wrap"><pre class="screen">













</pre></div></div></div><div class="chapter " id="cha-depl-dep-inst"><div class="titlepage"><div><div><h2 class="title"><span class="number">15 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing the Cloud Lifecycle Manager server</span> <a title="Permalink" class="permalink" href="#cha-depl-dep-inst">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/preinstall-prepare-deployer.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>preinstall-prepare-deployer.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div><div><div class="abstract"><p>
    This chapter will show how to install the Cloud Lifecycle Manager from scratch. It will run
    on SUSE Linux Enterprise Server 12 SP4, include the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> extension, and, optionally, the
    Subscription Management Tool (SMT) server.
   </p></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="#sec-depl-adm-inst-online-update"><span class="number">15.1 </span><span class="name">Registration and Online Updates</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-adm-inst-os"><span class="number">15.2 </span><span class="name">Starting the Operating System Installation</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-adm-inst-partitioning"><span class="number">15.3 </span><span class="name">Partitioning</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-adm-inst-user"><span class="number">15.4 </span><span class="name">Creating a User</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-adm-inst-settings"><span class="number">15.5 </span><span class="name">Installation Settings</span></a></span></dt></dl></div></div><div class="sect1 " id="sec-depl-adm-inst-online-update"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Registration and Online Updates</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-online-update">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/preinstall-prepare-deployer.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>preinstall-prepare-deployer.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-inst-online-update</li></ul></div></div></div></div><p>
   Registering SUSE Linux Enterprise Server 12 SP4 during the installation process is required for
   getting product updates and for installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   extension.</p><p>
   After a successful registration you will be asked whether
   to add the update repositories. If you agree, the latest updates will
   automatically be installed, ensuring that your system is on the latest
   patch level after the initial installation. We strongly recommend adding the
   update repositories immediately. If you choose to skip this step you need to
   perform an online update later, before starting the installation.
  </p><div id="id-1.3.5.4.2.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: SUSE Login Required</h6><p>
    To register a product, you need to have a SUSE login.
    If you do not have such a login, create it at
    <a class="link" href="http://www.suse.com/" target="_blank">http://www.suse.com/</a>.
   </p></div></div><div class="sect1 " id="sec-depl-adm-inst-os"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Starting the Operating System Installation</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-os">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/preinstall-prepare-deployer.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>preinstall-prepare-deployer.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-inst-os</li></ul></div></div></div></div><div id="id-1.3.5.4.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    Installing SUSE Linux Enterprise Server for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> requires the following steps, which are
    different from the default SUSE Linux Enterprise Server installation process.
   </p></div><div id="id-1.3.5.4.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    For an overview of a default SUSE Linux Enterprise Server installation, refer to <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-installquick/#art-sle-installquick" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-installquick/#art-sle-installquick</a>.
   </p></div><p>
   Start the installation by booting into the SUSE Linux Enterprise Server 12 SP4 installation system.
  </p></div><div class="sect1 " id="sec-depl-adm-inst-partitioning"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Partitioning</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-partitioning">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/preinstall-prepare-deployer.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>preinstall-prepare-deployer.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-inst-partitioning</li></ul></div></div></div></div><p>
   Create a custom partition setup using the <span class="guimenu ">Expert
   Partitioner</span>. The following setup is required:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Two partitions are needed: one for boot, EFI or UEFI, and one for
     everything else.
    </p></li><li class="listitem "><p>
     If the system is using a UEFI BIOS, there must be a UEFI boot
     partition.
    </p></li><li class="listitem "><p>
     An LVM setup with no encryption is recommended, Btrfs will work. The file
     system must contain:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       a volume group named <code class="literal">ardana-vg</code> on the first disk
       (<code class="systemitem">/dev/sda</code>)
      </p></li><li class="listitem "><p>
       a volume named <code class="literal">root</code> with a size of 50GB and an ext4 filesystem
      </p></li></ul></div></li><li class="listitem "><p>
     no separate mount point for <code class="filename">/home</code>
    </p></li><li class="listitem "><p>
     no swap partition or file (No swap is a general <span class="productname">OpenStack</span>
     recommendation. Some services such as rabbit and cassandra do not perform
     well with swapping.)
    </p></li></ul></div></div><div class="sect1 " id="sec-depl-adm-inst-user"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a User</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-user">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/preinstall-prepare-deployer.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>preinstall-prepare-deployer.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-inst-user</li></ul></div></div></div></div><p>
   Setting up Cloud Lifecycle Manager requires a regular user which you can set up during the
   installation. You are free to choose any available user name except for
   <code class="systemitem">ardana</code>, because the <code class="systemitem">ardana</code> user is reserved by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  </p></div><div class="sect1 " id="sec-depl-adm-inst-settings"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installation Settings</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-settings">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/preinstall-prepare-deployer.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>preinstall-prepare-deployer.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-inst-settings</li></ul></div></div></div></div><p>
   With <span class="guimenu ">Installation Settings</span>, you need to adjust the
   software selection for your Cloud Lifecycle Manager setup. For more information refer to the
   <a class="link" href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-deployment/#sec-yast-install-perform" target="_blank">https://documentation.suse.com/sles/15-SP1/single-html/SLES-deployment/#sec-yast-install-perform</a>.
  </p><div id="id-1.3.5.4.6.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Additional Installation Settings</h6><p>
    The default firewall must be disabled, as <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> enables its own
    firewall during deployment.
   </p><p>
    SSH must be enabled.
   </p><p>
    Set <code class="literal">text</code> as the <span class="guimenu ">Default systemd
    target</span>.
   </p></div><div class="sect2 " id="sec-depl-adm-inst-settings-software"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Software Selection</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-settings-software">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/preinstall-prepare-deployer.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>preinstall-prepare-deployer.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-inst-settings-software</li></ul></div></div></div></div><p>
    Installing a minimal base system is sufficient to set up the
    Cloud Lifecycle Manager. The following patterns are the minimum required:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      <span class="guimenu ">Base System</span>
     </p></li><li class="listitem "><p>
      <span class="guimenu ">Minimal System (Appliances)</span>
     </p></li><li class="listitem "><p>
      <span class="guimenu ">Meta Package for Pattern cloud-ardana</span> (in case you have
      chosen to install the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Extension)
     </p></li><li class="listitem "><p>
      <span class="guimenu ">Subscription Management Tool</span> (optional, also see <a class="xref" href="#tip-depl-adm-inst-settings-smt" title="Tip: Installing a Local SMT Server (Optional)">Tip: Installing a Local SMT Server (Optional)</a>)
     </p></li><li class="listitem "><p>
      <span class="guimenu ">YaST2 configuration packages</span>
     </p></li></ul></div><div id="tip-depl-adm-inst-settings-smt" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip: Installing a Local SMT Server (Optional)</h6><p>
     If you do not have a SUSE Manager or SMT server in your organization, or
     are planning to manually update the repositories required for deployment
     of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> nodes, you need to set up an SMT server on the
     Cloud Lifecycle Manager. Choose the pattern <span class="guimenu ">Subscription Management
     Tool</span> in addition to the patterns listed above to install the
     SMT server software.
    </p></div></div><div class="sect2 " id="sec-depl-adm-inst-add-on"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Extension</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-inst-add-on">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/preinstall-prepare-deployer.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>preinstall-prepare-deployer.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-inst-add-on</li></ul></div></div></div></div><p>
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is an extension to SUSE Linux Enterprise Server. Installing it during the SUSE Linux Enterprise Server
    installation is the easiest and recommended way to set up the Cloud Lifecycle Manager. To get
    access to the extension selection dialog, you need to register SUSE Linux Enterprise Server 12 SP4
    during the installation. After a successful registration, the SUSE Linux Enterprise Server 12 SP4
    installation continues with the <span class="guimenu ">Extension &amp; Module
    Selection</span>. Choose <span class="guimenu "><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
    9</span> and provide the registration key you obtained by
    purchasing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. The registration and the extension installation
    require an Internet connection.
   </p><p>
    If you do not have Internet access or are not able to register during
    installation, then once Internet access is available for the Cloud Lifecycle Manager do the
    following steps:
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      <code class="prompt user">tux &gt; </code>sudo SUSEConnect -r
      <em class="replaceable ">SLES_REGISTRATION_CODE</em>
     </p></li><li class="step "><p>
      List repositories to verify:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>zypper lr</pre></div></li><li class="step "><p>
      Refresh the repositories:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper ref</pre></div></li></ol></div></div><p>
    Alternatively, install the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> after the
   SUSE Linux Enterprise Server 12 SP4 installation via <span class="guimenu ">YaST</span> › <span class="guimenu ">Software</span> › <span class="guimenu ">Add-On Products</span>.
   For details, refer to <a class="link" href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-deployment/#sec-add-ons-extensions" target="_blank">https://documentation.suse.com/sles/15-SP1/single-html/SLES-deployment/#sec-add-ons-extensions</a>.
   </p></div></div></div><div class="chapter " id="app-deploy-smt-lcm"><div class="titlepage"><div><div><h2 class="title"><span class="number">16 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</span> <a title="Permalink" class="permalink" href="#app-deploy-smt-lcm">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/preinstall-smt-setup.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>preinstall-smt-setup.xml</li><li><span class="ds-label">ID: </span>app-deploy-smt-lcm</li></ul></div></div><div><div class="abstract"><p>
    One way to provide the repositories needed to set up the nodes in
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is to install a Subscription Management Tool (SMT) server on the Cloud Lifecycle Manager server, and then mirror all
    repositories from SUSE Customer Center via this server. Installing an SMT server
    on the Cloud Lifecycle Manager server is optional. If your organization already provides an
    SMT server or a SUSE Manager server that can be accessed from the
    Cloud Lifecycle Manager server, skip this step.
   </p></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="#app-deploy-smt-install"><span class="number">16.1 </span><span class="name">SMT Installation</span></a></span></dt><dt><span class="sect1"><a href="#clm-app-deploy-smt-config"><span class="number">16.2 </span><span class="name">SMT Configuration</span></a></span></dt><dt><span class="sect1"><a href="#app-deploy-smt-repos"><span class="number">16.3 </span><span class="name">Setting up Repository Mirroring on the SMT Server</span></a></span></dt><dt><span class="sect1"><a href="#app-deploy-smt-info"><span class="number">16.4 </span><span class="name">For More Information</span></a></span></dt></dl></div></div><div id="id-1.3.5.5.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Use of SMT Server and Ports</h6><p>
   When installing an SMT server on the Cloud Lifecycle Manager server, use it exclusively
   for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. To use the SMT server for other
   products, run it outside of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. Make sure it can be accessed
   from the Cloud Lifecycle Manager for mirroring the repositories needed for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  </p><p>
   When the SMT server is installed on the Cloud Lifecycle Manager server, Cloud Lifecycle Manager
   provides the mirrored repositories on port <code class="literal">79</code>.
  </p></div><div class="sect1 " id="app-deploy-smt-install"><div class="titlepage"><div><div><h2 class="title"><span class="number">16.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SMT Installation</span> <a title="Permalink" class="permalink" href="#app-deploy-smt-install">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/preinstall-smt-setup.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>preinstall-smt-setup.xml</li><li><span class="ds-label">ID: </span>app-deploy-smt-install</li></ul></div></div></div></div><p>
   If you have not installed the SMT server during the initial Cloud Lifecycle Manager server
   installation as suggested in <a class="xref" href="#sec-depl-adm-inst-settings-software" title="15.5.1. Software Selection">Section 15.5.1, “Software Selection”</a>, run the following command
   to install it:
  </p><div class="verbatim-wrap"><pre class="screen">sudo zypper in -t pattern smt</pre></div></div><div class="sect1 " id="clm-app-deploy-smt-config"><div class="titlepage"><div><div><h2 class="title"><span class="number">16.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SMT Configuration</span> <a title="Permalink" class="permalink" href="#clm-app-deploy-smt-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/preinstall-smt-setup.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>preinstall-smt-setup.xml</li><li><span class="ds-label">ID: </span>clm-app-deploy-smt-config</li></ul></div></div></div></div><p>
   No matter whether the SMT server was installed during the initial
   installation or in the running system, it needs to be configured with the
   following steps.
  </p><div id="id-1.3.5.5.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Prerequisites</h6><p>
    To configure the SMT server, a SUSE account is required. If you do not
    have such an account, register at <a class="link" href="http://www.suse.com/" target="_blank">http://www.suse.com/</a>. All products and
    extensions for which you want to mirror updates with the SMT
    server should be registered at the SUSE Customer Center (<a class="link" href="http://scc.suse.com/" target="_blank">http://scc.suse.com/</a>).
   </p><p>
    If you did not register with the SUSE Customer Center during installation, then at this
    point you will need to register in order to proceed. Ensure that the Cloud Lifecycle Manager has external
    network access and then run the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo SUSEConnect -r
     <em class="replaceable ">SLES_REGISTRATION_CODE</em></pre></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Configuring the SMT server requires you to have your mirroring
     credentials (user name and password) and your registration e-mail
     address at hand. To access them, proceed as follows:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Open a Web browser and log in to the SUSE Customer Center at
       <a class="link" href="http://scc.suse.com/" target="_blank">http://scc.suse.com/</a>.
      </p></li><li class="step "><p>
       Click your name to see the e-mail address which you have registered.
      </p></li><li class="step "><p>
       Click <span class="guimenu ">Organization</span> › <span class="guimenu ">Organization Credentials</span> to obtain
       your mirroring credentials (user name and password).
      </p></li></ol></li><li class="step "><p>
     Start <span class="guimenu ">YaST</span> › <span class="guimenu ">Network
     Services</span> › <span class="guimenu ">SMT Configuration
     Wizard</span>.
    </p></li><li class="step "><p>
     Activate <span class="guimenu ">Enable Subscription Management Tool Service
     (SMT)</span>.
    </p></li><li class="step "><p>
     Enter the <span class="guimenu ">Customer Center Configuration</span> data as
     follows:
    </p><table border="0" summary="Simple list" class="simplelist "><tr><td><span class="guimenu ">Use Custom Server</span>:
     Do <span class="emphasis"><em>not</em></span> activate this option</td></tr><tr><td><span class="guimenu ">User</span>: The user name you retrieved from the
     SUSE Customer Center</td></tr><tr><td><span class="guimenu ">Password</span>: The password you retrieved from the
     SUSE Customer Center</td></tr></table><p>
     Check your input with <span class="guimenu ">Test</span>. If the test does not
     return <code class="literal">success</code>, check the credentials you entered.
    </p></li><li class="step "><p>
     Enter the e-mail address you retrieved from the SUSE Customer Center at
     <span class="guimenu ">SCC E-Mail Used for Registration</span>.
    </p></li><li class="step "><p>
     <span class="guimenu ">Your SMT Server URL</span> shows the HTTP address of your
     server. Usually it should not be necessary to change it.
    </p></li><li class="step "><p>
     Select <span class="guimenu ">Next</span> to proceed to step two of the <span class="guimenu ">SMT Configuration Wizard</span>.
    </p></li><li class="step "><p>
     Enter a <span class="guimenu ">Database Password for SMT User</span> and confirm
     it by entering it once again.
    </p></li><li class="step "><p>
     Enter one or more e-mail addresses to which SMT status reports are
     sent by selecting <span class="guimenu ">Add</span>.
    </p></li><li class="step "><p>
     Select <span class="guimenu ">Next</span> to save your SMT configuration. When
     setting up the database you will be prompted for the MariaDB root
     password. If you have not already created one then create it in this step. Note that this is
     the global MariaDB root password, not the database password for the SMT
     user you specified before.
    </p><p>
     The SMT server requires a server certificate at
     <code class="filename">/etc/pki/trust/anchors/YaST-CA.pem</code>. Choose
     <span class="guimenu ">Run CA Management</span>, provide a password and choose
     <span class="guimenu ">Next</span> to create such a certificate. If your
     organization already provides a CA certificate, <span class="guimenu ">Skip</span>
     this step and import the certificate via <span class="guimenu ">YaST</span> › <span class="guimenu ">Security and Users</span> › <span class="guimenu ">CA Management</span> after the SMT
     configuration is done. See
   <a class="link" href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-security/#cha-security-yast-security" target="_blank">https://documentation.suse.com/sles/15-SP1/single-html/SLES-security/#cha-security-yast-security</a>
   for more information.
    </p><p>
     After you complete your configuration a synchronization check with the SUSE Customer Center will run, which may take several minutes.
    </p></li></ol></div></div></div><div class="sect1 " id="app-deploy-smt-repos"><div class="titlepage"><div><div><h2 class="title"><span class="number">16.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting up Repository Mirroring on the SMT Server</span> <a title="Permalink" class="permalink" href="#app-deploy-smt-repos">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/preinstall-smt-setup.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>preinstall-smt-setup.xml</li><li><span class="ds-label">ID: </span>app-deploy-smt-repos</li></ul></div></div></div></div><p>
   The final step in setting up the SMT server is configuring it to
   mirror the repositories needed for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. The SMT server
   mirrors the repositories from the SUSE Customer Center. Make
   sure to have the appropriate subscriptions registered in SUSE Customer Center with the
   same e-mail address you specified when configuring SMT.
  </p><div class="sect2 " id="app-deploy-smt-repos-mandatory"><div class="titlepage"><div><div><h3 class="title"><span class="number">16.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding Mandatory Repositories</span> <a title="Permalink" class="permalink" href="#app-deploy-smt-repos-mandatory">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/preinstall-smt-setup.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>preinstall-smt-setup.xml</li><li><span class="ds-label">ID: </span>app-deploy-smt-repos-mandatory</li></ul></div></div></div></div><p>
    Mirroring the SUSE Linux Enterprise Server 12 SP4 and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9
    repositories is mandatory. Run the following commands as user
    <code class="systemitem">root</code> to add them to the list of mirrored repositories:
   </p><div class="verbatim-wrap"><pre class="screen">for REPO in SLES12-SP4-{Pool,Updates} <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-9</span></span>-{Pool,Updates}; do
  smt-repos $REPO sle-12-x86_64 -e
done</pre></div></div><div class="sect2 " id="app-deploy-smt-repos-mirror"><div class="titlepage"><div><div><h3 class="title"><span class="number">16.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Updating the Repositories</span> <a title="Permalink" class="permalink" href="#app-deploy-smt-repos-mirror">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/preinstall-smt-setup.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>preinstall-smt-setup.xml</li><li><span class="ds-label">ID: </span>app-deploy-smt-repos-mirror</li></ul></div></div></div></div><p>
    New repositories added to SMT must be updated immediately by running the following command as user <code class="systemitem">root</code>:
   </p><div class="verbatim-wrap"><pre class="screen">smt-mirror -L /var/log/smt/smt-mirror.log</pre></div><p>
    This command will download several GB of patches. This process may last
    up to several hours. A log file is written to
    <code class="filename">/var/log/smt/smt-mirror.log</code>. After this first manual update the repositories are updated automatically via cron
    job. A list of all
    repositories and their location in the file system on the Cloud Lifecycle Manager server can be
    found at <a class="xref" href="#tab-smt-repos-local" title="SMT Repositories Hosted on the Cloud Lifecycle Manager">Table 17.2, “SMT Repositories Hosted on the Cloud Lifecycle Manager”</a>.
   </p></div></div><div class="sect1 " id="app-deploy-smt-info"><div class="titlepage"><div><div><h2 class="title"><span class="number">16.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="#app-deploy-smt-info">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/preinstall-smt-setup.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>preinstall-smt-setup.xml</li><li><span class="ds-label">ID: </span>app-deploy-smt-info</li></ul></div></div></div></div><p>
   For detailed information about SMT refer to the Subscription Management Tool manual at
   <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-smt/" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-smt/</a>.
  </p></div></div><div class="chapter " id="cha-depl-repo-conf-lcm"><div class="titlepage"><div><div><h2 class="title"><span class="number">17 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Software Repository Setup</span> <a title="Permalink" class="permalink" href="#cha-depl-repo-conf-lcm">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/preinstall-configure-deployer-repos.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>preinstall-configure-deployer-repos.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="#sec-depl-adm-conf-repos-product"><span class="number">17.1 </span><span class="name">Copying the Product Media Repositories</span></a></span></dt><dt><span class="sect1"><a href="#sec-depl-adm-conf-repos-scc"><span class="number">17.2 </span><span class="name">Update and Pool Repositories</span></a></span></dt><dt><span class="sect1"><a href="#sec-deploy-repo-locations"><span class="number">17.3 </span><span class="name">Repository Locations</span></a></span></dt></dl></div></div><p>
  Software repositories containing products, extensions, and the respective
  updates for all software need to be available to all nodes in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  in order to complete the deployment. These can be managed manually, or they
  can be hosted on the Cloud Lifecycle Manager server. In this configuration step, these
  repositories are made available on the Cloud Lifecycle Manager server. There are two types of
  repositories:
 </p><p>
  <span class="bold"><strong>Product Media Repositories</strong></span>: Product media
  repositories are copies of the installation media. They need to be
  directly copied to the Cloud Lifecycle Manager server, <span class="quote">“<span class="quote ">loop-mounted</span>”</span> from an iso
  image, or mounted from a remote server via NFS. Affected are SUSE Linux Enterprise Server 12 SP4 and
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9. These are static repositories; they do not
  change or receive updates. See <a class="xref" href="#sec-depl-adm-conf-repos-product" title="17.1. Copying the Product Media Repositories">Section 17.1, “Copying the Product Media Repositories”</a> for setup instructions.
 </p><p>
  <span class="bold"><strong>Update and Pool Repositories</strong></span>: Update and
  Pool repositories are provided by the SUSE Customer Center. They contain all updates and
  patches for the products and extensions. To make them available for
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> they need to be mirrored from the SUSE Customer Center. Their content is
  regularly updated, so they must be kept in synchronization with SUSE Customer Center. For
  these purposes, SUSE provides the Subscription Management Tool (SMT). See <a class="xref" href="#sec-depl-adm-conf-repos-scc" title="17.2. Update and Pool Repositories">Section 17.2, “Update and Pool Repositories”</a> for setup instructions.
 </p><div class="sect1 " id="sec-depl-adm-conf-repos-product"><div class="titlepage"><div><div><h2 class="title"><span class="number">17.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Copying the Product Media Repositories</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-conf-repos-product">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/preinstall-configure-deployer-repos.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>preinstall-configure-deployer-repos.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-conf-repos-product</li></ul></div></div></div></div><p>
   The files in the product repositories for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> do not
   change, therefore they do not need to be synchronized with a remote
   source. If you have installed the product media from a downloaded ISO image,
   the product repositories will automatically be made available to the client
   nodes and these steps can be skipped. These steps can also be skipped if you
   prefer to install from the Pool repositories provided by SUSE Customer Center. Otherwise,
   it is sufficient to either copy the data (from a remote host or the
   installation media), to mount the product repository from a remote server
   via <code class="literal">NFS</code>, or to loop mount a copy of the installation
   images.
   </p><p>
    If you choose to install from the product media rather than from the SUSE Customer Center
    repositories, the following product media needs to reside in the specified
    directories:
   </p><div class="table" id="id-1.3.5.6.5.4"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 17.1: </span><span class="name">Local Product Repositories for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> </span><a title="Permalink" class="permalink" href="#id-1.3.5.6.5.4">#</a></h6></div><div class="table-contents"><table class="table" summary="Local Product Repositories for SUSE OpenStack Cloud" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th>
        <p>
         Repository
        </p>
       </th><th>
        <p>
         Directory
        </p>
       </th></tr></thead><tbody><tr><td>
        <p>
         <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 DVD #1
        </p>
       </td><td>
        <p>
         <code class="filename">/srv/www/suse-12.4/x86_64/repos/Cloud</code>
        </p>
       </td></tr></tbody></table></div></div><p>
    The data can be copied by a variety of methods:
   </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.5.6.5.6.1"><span class="term ">Copying from the Installation Media</span></dt><dd><p>
      We recommend using <code class="command">rsync</code> for copying. If the
      installation data is located on a removable device, make sure to mount
      it first (for example, after inserting the DVD1 in the Cloud Lifecycle Manager and
      waiting for the device to become ready):
     </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.5.6.5.6.1.2.2"><span class="name">
      <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 DVD#1
     </span><a title="Permalink" class="permalink" href="#id-1.3.5.6.5.6.1.2.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/preinstall-configure-deployer-repos.xml" title="Edit the source file for this section">Edit source</a></h5></div><div class="verbatim-wrap"><pre class="screen">mkdir -p /srv/www/suse-12.4/x86_64/repos/Cloud
mount /dev/dvd /mnt
rsync -avP /mnt/ /srv/www/suse-12.4/x86_64/repos/Cloud/
umount /mnt</pre></div></dd><dt id="id-1.3.5.6.5.6.2"><span class="term ">Copying from a Remote Host</span></dt><dd><p>
       If the data is provided by a remote machine, log in to that machine and
       push the data to the Cloud Lifecycle Manager (which has the IP address <code class="systemitem">192.168.245.10</code> in the following
       example):
      </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.5.6.5.6.2.2.2"><span class="name">
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 DVD#1
      </span><a title="Permalink" class="permalink" href="#id-1.3.5.6.5.6.2.2.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/preinstall-configure-deployer-repos.xml" title="Edit the source file for this section">Edit source</a></h5></div><div class="verbatim-wrap"><pre class="screen">mkdir -p /srv/www/suse-12.4/x86_64/repos/Cloud
rsync -avPz <em class="replaceable ">/data/SUSE-OPENSTACK-CLOUD//DVD1/</em> <em class="replaceable ">192.168.245.10</em>:/srv/www/suse-12.4/x86_64/repos/Cloud/</pre></div></dd><dt id="id-1.3.5.6.5.6.3"><span class="term ">Mounting from an NFS Server</span></dt><dd><p>
       If the installation data is provided via NFS by a remote machine, mount
       the respective shares as follows. To automatically mount these
       directories either create entries in <code class="filename">/etc/fstab</code> or
       set up the automounter.
      </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.5.6.5.6.3.2.2"><span class="name">
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 DVD#1
      </span><a title="Permalink" class="permalink" href="#id-1.3.5.6.5.6.3.2.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/preinstall-configure-deployer-repos.xml" title="Edit the source file for this section">Edit source</a></h5></div><div class="verbatim-wrap"><pre class="screen">mkdir -p /srv/www/suse-12.4/x86_64/repos/Cloud/
mount -t nfs <em class="replaceable ">nfs.example.com:/exports/SUSE-OPENSTACK-CLOUD/DVD1/</em> /srv/www/suse-12.4/x86_64/repos/Cloud</pre></div></dd></dl></div></div><div class="sect1 " id="sec-depl-adm-conf-repos-scc"><div class="titlepage"><div><div><h2 class="title"><span class="number">17.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Update and Pool Repositories</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-conf-repos-scc">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/preinstall-configure-deployer-repos.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>preinstall-configure-deployer-repos.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-conf-repos-scc</li></ul></div></div></div></div><p>
    Update and Pool Repositories are required on the Cloud Lifecycle Manager server to set up and
    maintain the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> nodes. They are provided by SUSE Customer Center and contain all
    software packages needed to install SUSE Linux Enterprise Server 12 SP4 and the extensions (pool
    repositories). In addition, they contain all updates and patches (update
    repositories).
   </p><p>
    The repositories can be made available on the Cloud Lifecycle Manager server using one or more of the
    following methods:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      <a class="xref" href="#sec-depl-adm-conf-repos-scc-local-smt" title="17.2.1.  Repositories Hosted on an SMT Server Installed on the Cloud Lifecycle Manager">Section 17.2.1, “
     Repositories Hosted on an SMT Server Installed on the Cloud Lifecycle Manager
    ”</a>
     </p></li><li class="listitem "><p>
      <a class="xref" href="#sec-depl-adm-conf-repos-scc-alternatives" title="17.2.2. Alternative Ways to Make the Repositories Available">Section 17.2.2, “Alternative Ways to Make the Repositories Available”</a>
     </p></li></ul></div><div class="sect2 " id="sec-depl-adm-conf-repos-scc-local-smt"><div class="titlepage"><div><div><h3 class="title"><span class="number">17.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">
     Repositories Hosted on an SMT Server Installed on the Cloud Lifecycle Manager
    </span> <a title="Permalink" class="permalink" href="#sec-depl-adm-conf-repos-scc-local-smt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/preinstall-configure-deployer-repos.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>preinstall-configure-deployer-repos.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-conf-repos-scc-local-smt</li></ul></div></div></div></div><p>
     When all update and pool repositories are managed by an SMT server
     installed on the Cloud Lifecycle Manager server (see <a class="xref" href="#app-deploy-smt-lcm" title="Chapter 16. Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)">Chapter 16, <em>Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</em></a>),
     the Cloud Lifecycle Manager automatically detects all available repositories. No further
     action is required.
    </p></div><div class="sect2 " id="sec-depl-adm-conf-repos-scc-alternatives"><div class="titlepage"><div><div><h3 class="title"><span class="number">17.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Alternative Ways to Make the Repositories Available</span> <a title="Permalink" class="permalink" href="#sec-depl-adm-conf-repos-scc-alternatives">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/preinstall-configure-deployer-repos.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>preinstall-configure-deployer-repos.xml</li><li><span class="ds-label">ID: </span>sec-depl-adm-conf-repos-scc-alternatives</li></ul></div></div></div></div><p>
     If you want to keep your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> network as isolated from the company
     network as possible, or your infrastructure does not allow accessing a
     SUSE Manager or an SMT server, you can alternatively provide access to the
     required repositories by one of the following methods:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Mount the repositories from a remote server.
      </p></li><li class="listitem "><p>
       Synchronize the repositories from a remote server (for example via
       <code class="command">rsync</code> and cron).
      </p></li><li class="listitem "><p>
        Manually synchronize the update repositories from removable media.
      </p></li></ul></div><p>
     The repositories must be made available at the
     default locations on the Cloud Lifecycle Manager server as listed in <a class="xref" href="#tab-depl-adm-conf-local-repos" title="Repository Locations on the Cloud Lifecycle Manager server">Table 17.3, “Repository Locations on the Cloud Lifecycle Manager server”</a>.
    </p></div></div><div class="sect1 " id="sec-deploy-repo-locations"><div class="titlepage"><div><div><h2 class="title"><span class="number">17.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Repository Locations</span> <a title="Permalink" class="permalink" href="#sec-deploy-repo-locations">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/preinstall-configure-deployer-repos.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>preinstall-configure-deployer-repos.xml</li><li><span class="ds-label">ID: </span>sec-deploy-repo-locations</li></ul></div></div></div></div><p>
The following tables show the locations of all repositories that can be used for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p><div class="table" id="tab-smt-repos-local"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 17.2: </span><span class="name">SMT Repositories Hosted on the Cloud Lifecycle Manager </span><a title="Permalink" class="permalink" href="#tab-smt-repos-local">#</a></h6></div><div class="table-contents"><table class="table" summary="SMT Repositories Hosted on the Cloud Lifecycle Manager" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th>
      <p>
       Repository
      </p>
     </th><th>
      <p>
       Directory
      </p>
     </th></tr></thead><tbody><tr><td colspan="2">
      <p>
       Mandatory Repositories
      </p>
     </td></tr><tr><td>
      <p>
       SLES12-SP4-Pool
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Products/SLE-SERVER/12-SP4/x86_64/product/</code>
      </p>
     </td></tr><tr><td>
      <p>
       SLES12-SP4-Updates
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Updates/SLE-SERVER/12-SP4/x86_64/update/</code>
      </p>
     </td></tr><tr><td>
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-9</span></span>-Pool
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Products/<span class="phrase"><span class="phrase">OpenStack-Cloud</span></span>/9/x86_64/product/</code>
      </p>
     </td></tr><tr><td>
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-9</span></span>-Updates
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Updates/<span class="phrase"><span class="phrase">OpenStack-Cloud</span></span>/9/x86_64/update/</code>
      </p>
     </td></tr></tbody></table></div></div><p>
  The following table shows the required repository locations  to use when manually copying, synchronizing, or mounting the
  repositories.
 </p><div class="table" id="tab-depl-adm-conf-local-repos"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 17.3: </span><span class="name">Repository Locations on the Cloud Lifecycle Manager server </span><a title="Permalink" class="permalink" href="#tab-depl-adm-conf-local-repos">#</a></h6></div><div class="table-contents"><table class="table" summary="Repository Locations on the Cloud Lifecycle Manager server" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th>
      <p>
       Channel
      </p>
     </th><th>
      <p>
       Directory on the Cloud Lifecycle Manager
      </p>
     </th></tr></thead><tbody><tr><td colspan="2">
      <p>
       Mandatory Repositories
      </p>
     </td></tr><tr><td>
      <p>
       SLES12-SP4-Pool
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/www/suse-12.4/x86_64/repos/SLES12-SP4-Pool/</code>
      </p>
     </td></tr><tr><td>
      <p>
       SLES12-SP4-Updates
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/www/suse-12.4/x86_64/repos/SLES12-SP4-Updates/</code>
      </p>
     </td></tr><tr><td>
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-9</span></span>-Pool
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/www/suse-12.4/x86_64/repos/<span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-9</span></span>-Pool/</code>
      </p>
     </td></tr><tr><td>
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-9</span></span>-Updates
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/www/suse-12.4/x86_64/repos/<span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-9</span></span>-Updates</code>
      </p>
     </td></tr></tbody></table></div></div></div></div><div class="chapter " id="multipath-boot-from-san"><div class="titlepage"><div><div><h2 class="title"><span class="number">18 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Boot from SAN and Multipath Configuration</span> <a title="Permalink" class="permalink" href="#multipath-boot-from-san">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-multipath_boot_from_san.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-multipath_boot_from_san.xml</li><li><span class="ds-label">ID: </span>multipath-boot-from-san</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#multipath-overview"><span class="number">18.1 </span><span class="name">Introduction</span></a></span></dt><dt><span class="section"><a href="#id-1.3.5.7.3"><span class="number">18.2 </span><span class="name">Install Phase Configuration</span></a></span></dt></dl></div></div><div class="sect1" id="multipath-overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">18.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Introduction</span> <a title="Permalink" class="permalink" href="#multipath-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-multipath_boot_from_san.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-multipath_boot_from_san.xml</li><li><span class="ds-label">ID: </span>multipath-overview</li></ul></div></div></div></div><p>
   For information about supported hardware for multipathing, see
   <a class="xref" href="#hw-support-hardwareconfig" title="2.2. Supported Hardware Configurations">Section 2.2, “Supported Hardware Configurations”</a>.
  </p><div id="boot-from-san-LUN0" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    When exporting a LUN to a node for boot from SAN, you should ensure that
    <span class="emphasis"><em>LUN 0</em></span> is assigned to the LUN and configure any setup
    dialog that is necessary in the firmware to consume this LUN 0 for OS boot.
   </p></div><div id="boot-from-san-host-persona" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    Any hosts that are connected to 3PAR storage must have a <code class="literal">host
    persona</code> of <code class="literal">2-generic-alua</code> set on the 3PAR.
    Refer to the 3PAR documentation for the steps necessary to check this and
    change if necessary.
   </p></div><p>
   iSCSI boot from SAN is not supported. For more information on the use of
   cinder with multipath, see <a class="xref" href="#sec-3par-multipath" title="35.1.3. Multipath Support">Section 35.1.3, “Multipath Support”</a>.
  </p><p>
   To allow <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 to use volumes from a SAN, you have to specify
   configuration options for both the installation and the OS configuration
   phase. In all cases, the devices that are utilized are devices for which
   multipath is configured.
  </p></div><div class="sect1" id="id-1.3.5.7.3"><div class="titlepage"><div><div><h2 class="title"><span class="number">18.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Install Phase Configuration</span> <a title="Permalink" class="permalink" href="#id-1.3.5.7.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-multipath_boot_from_san.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-multipath_boot_from_san.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   For FC connected nodes and for FCoE nodes where the network processor used
   is from the Emulex family such as for the 650FLB, the following changes are
   required.
  </p><p>
   Instead of using Cobbler, you need to provision a baremetal node manually
   using the following procedure.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     During manual installation of SUSE Linux Enterprise Server 12 SP4, select the desired SAN disk and
     create an LVM partitioning scheme that meets SUSE <span class="productname">OpenStack</span> Cloud requirements: it has
     an <code class="literal">ardana-vg</code> volume group and an
     <code class="literal">ardana-vg-root</code> logical volume. For more information on
     partitioning, see <a class="xref" href="#sec-depl-adm-inst-partitioning" title="15.3. Partitioning">Section 15.3, “Partitioning”</a>.
    </p></li><li class="step "><p>
     Open the <code class="filename">/etc/multipath/bindings</code> file and map the
     expected device name to the SAN disk selected during installation. In
     SUSE <span class="productname">OpenStack</span> Cloud, the naming convention is mpatha, mpathb, and so on. For example:
    </p><div class="verbatim-wrap"><pre class="screen">mpatha-part1 360000000030349030-part1
mpatha-part2 360000000030349030-part2
mpatha-part3 360000000030349030-part3

mpathb-part1 360000000030349000-part1
mpathb-part2 360000000030349000-part2</pre></div></li><li class="step "><p>
     Reboot to enable the changes.
    </p></li><li class="step "><p>
     Assign a static IP to the node:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Use the <code class="command">ip addr</code> command to list active network
       interfaces on your system:
      </p><div class="verbatim-wrap"><pre class="screen">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eno1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000
    link/ether f0:92:1c:05:89:70 brd ff:ff:ff:ff:ff:ff
    inet 10.13.111.178/26 brd 10.13.111.191 scope global eno1
       valid_lft forever preferred_lft forever
    inet6 fe80::f292:1cff:fe05:8970/64 scope link
       valid_lft forever preferred_lft forever
3: eno2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000
    link/ether f0:92:1c:05:89:74 brd ff:ff:ff:ff:ff:ff</pre></div></li><li class="step "><p>
       Identify the network interface that matches the MAC address of your
       server and edit the corresponding configuration file in
       <code class="filename">/etc/sysconfig/network-scripts</code>. For example, for
       the <code class="systemitem">eno1</code> interface, open the
       <code class="systemitem">/etc/sysconfig/network-scripts/ifcfg-eno1</code> file
       and edit <em class="replaceable ">IPADDR</em> and
       <em class="replaceable ">NETMASK</em> values to match your environment.
       The <em class="replaceable ">IPADDR</em> is used in the corresponding
       stanza in <code class="filename">servers.yml</code>. You may also need to set
       <code class="literal">BOOTPROTO</code> to <code class="literal">none</code>:
      </p><div class="verbatim-wrap"><pre class="screen">TYPE=Ethernet
BOOTPROTO=none
DEFROUTE=yes
PEERDNS=yes
PEERROUTES=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_PEERDNS=yes
IPV6_PEERROUTES=yes
IPV6_FAILURE_FATAL=no
NAME=eno1
UUID=360360aa-12aa-444a-a1aa-ee777a3a1a7a
DEVICE=eno1
ONBOOT=yes
NETMASK=255.255.255.192
IPADDR=10.10.100.10</pre></div></li><li class="step "><p>
       Reboot the SLES node and ensure that it can be accessed from the
       Cloud Lifecycle Manager.
      </p></li></ol></li><li class="step "><p>
     Add the <code class="literal">ardana</code> user and home directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>useradd -m -d /var/lib/ardana -U ardana</pre></div></li><li class="step "><p>
     Allow the user <code class="literal">ardana</code> to run <code class="command">sudo</code>
     without a password by creating the
     <code class="filename">/etc/sudoers.d/ardana</code> file with the following
     configuration:
    </p><div class="verbatim-wrap"><pre class="screen">ardana ALL=(ALL) NOPASSWD:ALL</pre></div></li><li class="step "><p>
     When you start installation using the Cloud Lifecycle Manager, or if you are adding a SLES
     node to an existing cloud, copy the Cloud Lifecycle Manager public key to the SLES node to
     enable passwordless SSH access. One way of doing this is to copy the file
     <code class="filename">~/.ssh/authorized_keys</code> from another node in the cloud
     to the same location on the SLES node. If you are installing a new
     cloud, this file will be available on the nodes after running the
     <code class="filename">bm-reimage.yml</code> playbook. Ensure that there is global
     read access to the file
     <code class="filename">/var/lib/ardana/.ssh/authorized_keys</code>.
    </p><p>
     Use the following command to test passwordless SSH from the deployer and
     check the ability to remotely execute sudo commands:
    </p><div class="verbatim-wrap"><pre class="screen">ssh stack@<em class="replaceable ">SLES_NODE_IP</em> "sudo tail -5 /var/log/messages"</pre></div></li></ol></div></div><div class="sect2" id="depl-cloud"><div class="titlepage"><div><div><h3 class="title"><span class="number">18.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying the Cloud</span> <a title="Permalink" class="permalink" href="#depl-cloud">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-multipath_boot_from_san.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-multipath_boot_from_san.xml</li><li><span class="ds-label">ID: </span>depl-cloud</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     In
     <code class="filename">openstack/my_cloud/config/multipath/multipath_settings.yml</code>,
     set <code class="literal">manual_multipath_conf</code> to <code class="literal">True</code> so
     that <code class="literal">multipath.conf</code> on manually installed nodes is not
     overwritten.
    </p></li><li class="step "><p>
     Commit the changes.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>cd ~/openstack
<code class="prompt user">tux &gt; </code>git add -A
<code class="prompt user">tux &gt; </code>git commit -m "multipath config"</pre></div></li><li class="step "><p>
     Run <code class="literal">config-processor</code> and
     <code class="literal">ready-deployment</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">tux &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Ensure that all existing non-OS partitions on the nodes are wiped prior to
     installation by running the <code class="filename">wipe_disks.yml</code> playbook.
    </p><div id="id-1.3.5.7.3.5.2.4.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      Confirm that your root partition disk is not listed in disks to be wiped.
     </p></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">tux &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml</pre></div></li><li class="step "><p>
     Run the <code class="filename">site.yml</code> playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">tux &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml</pre></div></li></ol></div></div></div></div></div></div><div class="part" id="cloudinstallation"><div class="titlepage"><div><div><h1 class="title"><span class="number">Part IV </span><span class="name">Cloud Installation </span><a title="Permalink" class="permalink" href="#cloudinstallation">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-cloudinstallation_overview.xml" title="Edit the source file for this section">Edit source</a></h1></div></div></div><div class="toc"><dl><dt><span class="chapter"><a href="#cloudinstallation-overview"><span class="number">19 </span><span class="name">Overview</span></a></span></dt><dd class="toc-abstract"><p>
   Before starting the installation, review the sample configurations offered
   by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> in <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a>.
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> ships with highly tuned example configurations for each of
   these cloud models:
  </p></dd><dt><span class="chapter"><a href="#preparing-standalone"><span class="number">20 </span><span class="name">Preparing for Stand-Alone Deployment</span></a></span></dt><dd class="toc-abstract"><p>
  The Cloud Lifecycle Manager can be installed on a Control Plane or on a stand-alone
  server.
 </p></dd><dt><span class="chapter"><a href="#install-gui"><span class="number">21 </span><span class="name">Installing with the Install UI</span></a></span></dt><dd class="toc-abstract"><p>
 </p></dd><dt><span class="chapter"><a href="#using-git"><span class="number">22 </span><span class="name">Using Git for Configuration Management</span></a></span></dt><dd class="toc-abstract"><p>In SUSE OpenStack Cloud 9, a local git repository is used to track configuration changes; the Configuration Processor (CP) uses this repository. Use of a git workflow means that your configuration history is maintained, making rollbacks easier and keeping a record of previous configuration settings.…</p></dd><dt><span class="chapter"><a href="#install-standalone"><span class="number">23 </span><span class="name">Installing a Stand-Alone Cloud Lifecycle Manager</span></a></span></dt><dd class="toc-abstract"><p>
     For information about when to use the GUI installer and when to use the
     command line (CLI), see <a class="xref" href="#preinstall-overview" title="Chapter 13. Overview">Chapter 13, <em>Overview</em></a>.
    </p></dd><dt><span class="chapter"><a href="#install-kvm"><span class="number">24 </span><span class="name">Installing Mid-scale and Entry-scale KVM</span></a></span></dt><dd class="toc-abstract"><p>
     For information about when to use the GUI installer and when to use the
     command line (CLI), see <a class="xref" href="#preinstall-overview" title="Chapter 13. Overview">Chapter 13, <em>Overview</em></a>.
    </p></dd><dt><span class="chapter"><a href="#DesignateInstallOverview"><span class="number">25 </span><span class="name">DNS Service Installation Overview</span></a></span></dt><dd class="toc-abstract"><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> DNS Service supports several different backends for domain name
  service. The choice of backend must be included in the deployment model
  before the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> install is completed.
 </p></dd><dt><span class="chapter"><a href="#MagnumOverview"><span class="number">26 </span><span class="name">Magnum Overview</span></a></span></dt><dd class="toc-abstract"><p>The SUSE OpenStack Cloud Magnum Service provides container orchestration engines such as Docker Swarm, Kubernetes, and Apache Mesos available as first class resources. SUSE OpenStack Cloud Magnum uses heat to orchestrate an OS image which contains Docker and Kubernetes and runs that image in either …</p></dd><dt><span class="chapter"><a href="#install-esx-ovsvapp"><span class="number">27 </span><span class="name">Installing ESX Computes and OVSvAPP</span></a></span></dt><dd class="toc-abstract"><p>
  This section describes the installation step requirements for ESX
  Computes (nova-proxy) and OVSvAPP.
 </p></dd><dt><span class="chapter"><a href="#integrate-nsx-vsphere"><span class="number">28 </span><span class="name">Integrating NSX for vSphere</span></a></span></dt><dd class="toc-abstract"><p>
  This section describes the installation and integration of NSX-v, a Software
  Defined Networking (SDN) network virtualization and security platform for
  VMware's vSphere.
 </p></dd><dt><span class="chapter"><a href="#install-ironic-overview"><span class="number">29 </span><span class="name">Installing Baremetal (Ironic)</span></a></span></dt><dd class="toc-abstract"><p>
  Bare Metal as a Service is enabled in this release for deployment of nova
  instances on bare metal nodes using flat networking.
 </p></dd><dt><span class="chapter"><a href="#install-swift"><span class="number">30 </span><span class="name">Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Swift Only</span></a></span></dt><dd class="toc-abstract"><p>
  This page describes the installation step requirements for the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  Entry-scale Cloud with swift Only model.
 </p></dd><dt><span class="chapter"><a href="#install-sles-compute"><span class="number">31 </span><span class="name">Installing SLES Compute</span></a></span></dt><dd class="toc-abstract"><p>SUSE OpenStack Cloud 9 supports SLES compute nodes, specifically SUSE Linux Enterprise Server 12 SP4. SUSE does not ship a SLES ISO with SUSE OpenStack Cloud so you will need to download a copy of the SLES ISO (SLE-12-SP4-Server-DVD-x86_64-GM-DVD1.iso) from SUSE. You can use the following link to do…</p></dd><dt><span class="chapter"><a href="#install-ardana-manila"><span class="number">32 </span><span class="name">Installing manila and Creating manila Shares</span></a></span></dt><dd class="toc-abstract"><p>The OpenStack Shared File Systems service (manila) provides file storage to a virtual machine. The Shared File Systems service provides a storage provisioning control plane for shared or distributed file systems. The service enables management of share types and share snapshots if you have a driver …</p></dd><dt><span class="chapter"><a href="#install-heat-templates"><span class="number">33 </span><span class="name">Installing SUSE CaaS Platform heat Templates</span></a></span></dt><dd class="toc-abstract"><p>
  This chapter describes how to install SUSE CaaS Platform v3 using heat template on
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p></dd><dt><span class="chapter"><a href="#install-caasp-terraform"><span class="number">34 </span><span class="name">Installing SUSE CaaS Platform v4 using terraform</span></a></span></dt><dd class="toc-abstract"><p>
   More information about the SUSE CaaS Platform v4 is available at <a class="link" href="https://documentation.suse.com/suse-caasp/4.0/html/caasp-deployment/_deployment_instructions.html#_deployment_on_suse_openstack_cloud" target="_blank">https://documentation.suse.com/suse-caasp/4.0/html/caasp-deployment/_deployment_instructions.html#_deployment_on_suse_openstack_cloud</a>
  </p></dd><dt><span class="chapter"><a href="#integrations"><span class="number">35 </span><span class="name">Integrations</span></a></span></dt><dd class="toc-abstract"><p>
  Once you have completed your cloud installation, these are some of the common
  integrations you may want to perform.
 </p></dd><dt><span class="chapter"><a href="#troubleshooting-installation"><span class="number">36 </span><span class="name">Troubleshooting the Installation</span></a></span></dt><dd class="toc-abstract"><p>
  We have gathered some of the common issues that occur during installation and
  organized them by when they occur during the installation. These sections
  will coincide with the steps labeled in the installation instructions.
 </p></dd><dt><span class="chapter"><a href="#esx-troubleshooting-installation"><span class="number">37 </span><span class="name">Troubleshooting the ESX</span></a></span></dt><dd class="toc-abstract"><p>
  This section contains troubleshooting tasks for your <span class="phrase"><span class="phrase">SUSE® <span class="productname">OpenStack</span> Cloud</span></span>
  9 for ESX.
 </p></dd></dl></div><div class="chapter " id="cloudinstallation-overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">19 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Overview</span> <a title="Permalink" class="permalink" href="#cloudinstallation-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-cloudinstallation_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-cloudinstallation_overview.xml</li><li><span class="ds-label">ID: </span>cloudinstallation-overview</li></ul></div></div></div></div><div class="line"></div><p>
   Before starting the installation, review the sample configurations offered
   by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> in <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a>.
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> ships with highly tuned example configurations for each of
   these cloud models:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Name</th><th>Location</th></tr></thead><tbody><tr><td>
      <a class="xref" href="#entry-scale-kvm" title="9.3.1. Entry-Scale Cloud">Section 9.3.1, “Entry-Scale Cloud”</a>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-kvm</code>
     </td></tr><tr><td>
      <a class="xref" href="#entry-scale-kvm-mml" title="9.3.2. Entry Scale Cloud with Metering and Monitoring Services">Section 9.3.2, “Entry Scale Cloud with Metering and Monitoring Services”</a>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-kvm-mml</code>
     </td></tr><tr><td><a class="xref" href="#entry-scale-kvm-esx" title="9.4.1. Single-Region Entry-Scale Cloud with a Mix of KVM and ESX Hypervisors">Section 9.4.1, “Single-Region Entry-Scale Cloud with a Mix of KVM and ESX Hypervisors”</a>
     </td><td><code class="filename">~/openstack/examples/entry-scale-kvm-esx</code>
     </td></tr><tr><td>
      <a class="xref" href="#entry-scale-kvm-esx-mml" title="9.4.2. Single-Region Entry-Scale Cloud with Metering and Monitoring Services, and a Mix of KVM and ESX Hypervisors">Section 9.4.2, “Single-Region Entry-Scale Cloud with Metering and Monitoring Services, and a Mix of KVM and ESX Hypervisors”</a>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-kvm-esx-mml</code>
     </td></tr><tr><td>
      <a class="xref" href="#entryscale-swift" title="9.5.1. Entry-scale swift Model">Section 9.5.1, “Entry-scale swift Model”</a>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-swift</code>
     </td></tr><tr><td>
      <a class="xref" href="#entryscale-ironic" title="9.6.1. Entry-Scale Cloud with Ironic Flat Network">Section 9.6.1, “Entry-Scale Cloud with Ironic Flat Network”</a>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-ironic-flat-network</code>
     </td></tr><tr><td>
      <a class="xref" href="#entryscale-ironic-multi-tenancy" title="9.6.2. Entry-Scale Cloud with Ironic Multi-Tenancy">Section 9.6.2, “Entry-Scale Cloud with Ironic Multi-Tenancy”</a>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-ironic-multi-tenancy</code>
     </td></tr><tr><td><a class="xref" href="#mid-scale-kvm" title="9.3.3. Single-Region Mid-Size Model">Section 9.3.3, “Single-Region Mid-Size Model”</a>
     </td><td>
      <code class="filename">~/openstack/examples/mid-scale-kvm</code>
     </td></tr></tbody></table></div><p>
   There are two methods for installation to choose from:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     You can use a GUI that runs in your Web browser.
    </p></li><li class="listitem "><p>
     You can install via the command line that gives you the flexibility and
     full control of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9.
    </p></li></ul></div><p>
   <span class="bold"><strong>Using the GUI</strong></span>
  </p><p>
   You should use the GUI if:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     You are not planning to deploy availability zones, L3 segmentation, or
     server groups functionality in your initial deployment.
    </p></li><li class="listitem "><p>
     You are satisfied with the tuned SUSE-default <span class="productname">OpenStack</span> configuration.
    </p></li></ul></div><p>
   Instructions for GUI installation are in <a class="xref" href="#install-gui" title="Chapter 21. Installing with the Install UI">Chapter 21, <em>Installing with the Install UI</em></a>.
  </p><div id="id-1.3.6.2.10" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    Reconfiguring your cloud can only be done via the command line. The GUI
    installer is for initial installation only.
   </p></div><p>
   <span class="bold"><strong>Using the Command Line</strong></span>
  </p><p>
   You should use the command line if:
  </p><div class="itemizedlist " id="idg-installation-installation-cloudinstallation-overview-xml-7"><ul class="itemizedlist"><li class="listitem "><p>
     You are installing a complex or large-scale cloud.
    </p></li><li class="listitem "><p>
     You are planning to deploy availability zones, L3 segmentation, or server
     groups functionality. For more information, see the <a class="xref" href="#cha-input-model-intro-concept" title="Chapter 5. Input Model">Chapter 5, <em>Input Model</em></a>.
    </p></li><li class="listitem "><p>
     You want to customize the cloud configuration beyond the tuned defaults
     that SUSE provides out of the box.
    </p></li><li class="listitem "><p>
     You need more extensive customizations than are possible using the GUI.
    </p></li></ul></div><p>
   Instructions for installing via the command line are in <a class="xref" href="#install-kvm" title="Chapter 24. Installing Mid-scale and Entry-scale KVM">Chapter 24, <em>Installing Mid-scale and Entry-scale KVM</em></a>.
  </p></div><div class="chapter " id="preparing-standalone"><div class="titlepage"><div><div><h2 class="title"><span class="number">20 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Preparing for Stand-Alone Deployment</span> <a title="Permalink" class="permalink" href="#preparing-standalone">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-preparing-standalone.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-preparing-standalone.xml</li><li><span class="ds-label">ID: </span>preparing-standalone</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#id-1.3.6.3.2"><span class="number">20.1 </span><span class="name">Cloud Lifecycle Manager Installation Alternatives</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.3.3"><span class="number">20.2 </span><span class="name">Installing a Stand-Alone Deployer</span></a></span></dt></dl></div></div><div class="sect1" id="id-1.3.6.3.2"><div class="titlepage"><div><div><h2 class="title"><span class="number">20.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cloud Lifecycle Manager Installation Alternatives</span> <a title="Permalink" class="permalink" href="#id-1.3.6.3.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-preparing-standalone.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-preparing-standalone.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
  The Cloud Lifecycle Manager can be installed on a Control Plane or on a stand-alone
  server.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Installing the Cloud Lifecycle Manager on a Control Plane is done during the process of
    deploying your Cloud. Your Cloud and the Cloud Lifecycle Manager are deployed together.
   </p></li><li class="listitem "><p>
    With a standalone Cloud Lifecycle Manager, you install the deployer first and then deploy
    your Cloud in a separate process. Either the Install UI or command line
    can be used to deploy a stand-alone Cloud Lifecycle Manager.
   </p></li></ul></div><p>
  Each method is suited for particular needs. The best choice depends on your
  situation.
 </p><p>
  <span class="bold"><strong>Stand-alone Deployer</strong></span>
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    + Compared to a Control Plane deployer, a stand-alone deployer is easier to
    backup and redeploy in case of disaster
   </p></li><li class="listitem "><p>
    + Separates cloud management from components being managed
   </p></li><li class="listitem "><p>
    + Does not use Control Plane resources
   </p></li><li class="listitem "><p>
    - Another server is required (less of a disadvantage if using a VM)
   </p></li><li class="listitem "><p>
    - Installation may be more complex than a Control Plane Cloud Lifecycle Manager
   </p></li></ul></div><p>
  <span class="bold"><strong>Control Plane Deployer</strong></span>
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    + Installation is usually simpler than installing a stand-alone deployer
   </p></li><li class="listitem "><p>
    + Requires fewer servers or VMs
   </p></li><li class="listitem "><p>
    - Could contend with workloads for resources
   </p></li><li class="listitem "><p>
    - Harder to redeploy in case of failure compared to stand-alone deployer
   </p></li><li class="listitem "><p>
    - There is a risk to the Cloud Lifecycle Manager when updating or modifying controllers
   </p></li><li class="listitem "><p>
    - Runs on one of the servers that is deploying or managing your Cloud
   </p></li></ul></div><p>
  <span class="bold"><strong>Summary</strong></span>
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    A Control Plane Cloud Lifecycle Manager is best for small, simple Cloud deployments.
   </p></li><li class="listitem "><p>
    With a larger, more complex cloud, a stand-alone deployer provides better
    recoverability and the separation of manager from managed components.
   </p></li></ul></div></div><div class="sect1" id="id-1.3.6.3.3"><div class="titlepage"><div><div><h2 class="title"><span class="number">20.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing a Stand-Alone Deployer</span> <a title="Permalink" class="permalink" href="#id-1.3.6.3.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-preparing-standalone.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-preparing-standalone.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If you do not intend to install a stand-alone deployer, proceed to
   installing the Cloud Lifecycle Manager on a Control Plane.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Instructions for GUI installation are in <a class="xref" href="#install-gui" title="Chapter 21. Installing with the Install UI">Chapter 21, <em>Installing with the Install UI</em></a>.
    </p></li><li class="listitem "><p>
     Instructions for installing via the command line are in <a class="xref" href="#install-kvm" title="Chapter 24. Installing Mid-scale and Entry-scale KVM">Chapter 24, <em>Installing Mid-scale and Entry-scale KVM</em></a>.
    </p></li></ul></div><div class="sect2" id="id-1.3.6.3.3.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">20.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prepare for Cloud Installation</span> <a title="Permalink" class="permalink" href="#id-1.3.6.3.3.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-preparing-standalone.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-preparing-standalone.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Review the <a class="xref" href="#preinstall-checklist" title="Chapter 14. Pre-Installation Checklist">Chapter 14, <em>Pre-Installation Checklist</em></a> about recommended
     pre-installation tasks.
    </p></li><li class="step "><p>
     Prepare the Cloud Lifecycle Manager node. The Cloud Lifecycle Manager must be accessible either directly or via
     <code class="literal">ssh</code>, and have SUSE Linux Enterprise Server 12 SP4 installed. All nodes must
     be accessible to the Cloud Lifecycle Manager. If the nodes do not have direct access to
     online Cloud subscription channels, the Cloud Lifecycle Manager node will need to host the
     Cloud repositories.
    </p><ol type="a" class="substeps "><li class="step "><p>
       If you followed the installation instructions for Cloud Lifecycle Manager server (see <a class="xref" href="#cha-depl-dep-inst" title="Chapter 15. Installing the Cloud Lifecycle Manager server">Chapter 15, <em>Installing the Cloud Lifecycle Manager server</em></a>), <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> software should already be
       installed. Double-check whether SUSE Linux Enterprise and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are properly
       registered at the SUSE Customer Center by starting YaST and running <span class="guimenu ">Software</span> › <span class="guimenu ">Product
       Registration</span>.
      </p><p>
       If you have not yet installed <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, do so by starting YaST and
       running <span class="guimenu ">Software</span> › <span class="guimenu ">Product
       Registration</span> › <span class="guimenu ">Select
       Extensions</span>. Choose <span class="guimenu "><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span>
       and follow the on-screen instructions. Make sure to register
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> during the installation process and to install the software
       pattern <code class="literal">patterns-cloud-ardana</code>.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper -n in patterns-cloud-ardana</pre></div></li><li class="step "><p>
       Ensure the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> media repositories and updates repositories are
       made available to all nodes in your deployment. This can be accomplished
       either by configuring the Cloud Lifecycle Manager server as an SMT mirror as described in
       <a class="xref" href="#app-deploy-smt-lcm" title="Chapter 16. Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)">Chapter 16, <em>Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</em></a> or by syncing or mounting the
       Cloud and updates repositories to the Cloud Lifecycle Manager server as described in <a class="xref" href="#cha-depl-repo-conf-lcm" title="Chapter 17. Software Repository Setup">Chapter 17, <em>Software Repository Setup</em></a>.
      </p></li><li class="step "><p>
       Configure passwordless <code class="command">sudo</code> for the user created when
       setting up the node (as described in <a class="xref" href="#sec-depl-adm-inst-user" title="15.4. Creating a User">Section 15.4, “Creating a User”</a>). Note that this is
       <span class="emphasis"><em>not</em></span> the user <code class="systemitem">ardana</code> that will be used later in this
       procedure. In the following we assume you named the user <code class="systemitem">cloud</code>. Run the command
       <code class="command">visudo</code> as user <code class="systemitem">root</code> and add the following line
       to the end of the file:
      </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable ">CLOUD</em> ALL = (root) NOPASSWD:ALL</pre></div><p>
       Make sure to replace <em class="replaceable ">CLOUD</em> with your user name
       choice.
      </p></li><li class="step "><p>
       Set the password for the user
       <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo passwd ardana</pre></div></li><li class="step "><p>
       Become the user <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>su - ardana</pre></div></li><li class="step "><p>
       Place a copy of the SUSE Linux Enterprise Server 12 SP4 <code class="filename">.iso</code> in the
       <code class="systemitem">ardana</code> home directory,
       <code class="filename">var/lib/ardana</code>, and rename it to
       <code class="filename">sles12sp4.iso</code>.
      </p></li><li class="step "><p>
       Install the templates, examples, and working model directories:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>/usr/bin/ardana-init</pre></div></li></ol></li></ol></div></div></div><div class="sect2" id="id-1.3.6.3.3.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">20.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring for a Stand-Alone Deployer</span> <a title="Permalink" class="permalink" href="#id-1.3.6.3.3.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-preparing-standalone.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-preparing-standalone.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following steps are necessary to set up a stand-alone deployer whether
   you will be using the Install UI or command line.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Copy the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale KVM example input model to a
     stand-alone input model. This new input model will be edited so that it
     can be used as a stand-alone Cloud Lifecycle Manager installation.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>cp -r ~/openstack/examples/entry-scale-kvm/* \
~/openstack/examples/entry-scale-kvm-stand-alone-deployer</pre></div></li><li class="step "><p>
     Change to the new directory
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>cd ~/openstack/examples/entry-scale-kvm-stand-alone-deployer</pre></div></li><li class="step "><p>
     Edit the cloudConfig.yml file to change the name of the input model. This
     will make the model available both to the Install UI and to the command
     line installation process.
    </p><p>
     Change <code class="literal">name: entry-scale-kvm</code> to <code class="literal">name:
     entry-scale-kvm-stand-alone-deployer</code>
    </p></li><li class="step "><p>
     Change to the <code class="filename">data</code> directory.
    </p></li><li class="step "><p>
     Make the following edits to your configuration files.
    </p><div id="id-1.3.6.3.3.5.3.5.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      The indentation of each of the input files is important and will cause
      errors if not done correctly. Use the existing content in each of these
      files as a reference when adding additional content for your Cloud Lifecycle Manager.
     </p></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Update <code class="filename">control_plane.yml</code> to add the Cloud Lifecycle Manager.
      </p></li><li class="listitem "><p>
       Update <code class="filename">server_roles.yml</code> to add the Cloud Lifecycle Manager role.
      </p></li><li class="listitem "><p>
       Update <code class="filename">net_interfaces.yml</code> to add the interface
       definition for the Cloud Lifecycle Manager.
      </p></li><li class="listitem "><p>
       Create a <code class="filename">disks_lifecycle_manager.yml</code> file to define
       the disk layout for the Cloud Lifecycle Manager.
      </p></li><li class="listitem "><p>
       Update <code class="filename">servers.yml</code> to add the dedicated Cloud Lifecycle Manager node.
      </p></li></ul></div><p>
     <code class="filename">Control_plane.yml</code>: The snippet below shows the
     addition of a single node cluster into the control plane to host the Cloud Lifecycle Manager
     service.
    </p><div id="id-1.3.6.3.3.5.3.5.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      In addition to adding the new cluster, you also have to remove the Cloud Lifecycle Manager
      component from the <code class="literal">cluster1</code> in the examples.
     </p></div><div class="verbatim-wrap"><pre class="screen">  clusters:
<span class="bold"><strong>     - name: cluster0
       cluster-prefix: c0
       server-role: LIFECYCLE-MANAGER-ROLE
       member-count: 1
       allocation-policy: strict
       service-components:
         - lifecycle-manager</strong></span>
         - ntp-client
     - name: cluster1
       cluster-prefix: c1
       server-role: CONTROLLER-ROLE
       member-count: 3
       allocation-policy: strict
       service-components:
         - ntp-server</pre></div><p>
     This specifies a single node of role
     <code class="literal">LIFECYCLE-MANAGER-ROLE</code> hosting the Cloud Lifecycle Manager.
    </p><p>
     <code class="filename">Server_roles.yml</code>: The snippet below shows the
     insertion of the new server roles definition:
    </p><div class="verbatim-wrap"><pre class="screen">   server-roles:

<span class="bold"><strong>      - name: LIFECYCLE-MANAGER-ROLE
        interface-model: LIFECYCLE-MANAGER-INTERFACES
        disk-model: LIFECYCLE-MANAGER-DISKS</strong></span>

      - name: CONTROLLER-ROLE</pre></div><p>
     This defines a new server role which references a new interface-model and
     disk-model to be used when configuring the server.
    </p><p>
     <code class="filename">net-interfaces.yml</code>: The snippet below shows the
     insertion of the network-interface info:
    </p><div class="verbatim-wrap"><pre class="screen"><span class="bold"><strong>    - name: LIFECYCLE-MANAGER-INTERFACES
      network-interfaces:
        - name: BOND0
          device:
             name: bond0
          bond-data:
             options:
                 mode: active-backup
                 miimon: 200
                 primary: hed3
             provider: linux
             devices:
                 - name: hed3
                 - name: hed4
          network-groups:
             - MANAGEMENT</strong></span></pre></div><p>
     This assumes that the server uses the same physical networking layout as
     the other servers in the example.
    </p><p>
     <code class="filename">disks_lifecycle_manager.yml</code>: In the examples,
     disk-models are provided as separate files (this is just a convention, not
     a limitation) so the following should be added as a new file named
     <code class="filename">disks_lifecycle_manager.yml</code>:
    </p><div class="verbatim-wrap"><pre class="screen">---
   product:
      version: 2

   disk-models:
<span class="bold"><strong>   - name: LIFECYCLE-MANAGER-DISKS
     # Disk model to be used for Cloud Lifecycle Managers nodes
     # /dev/sda_root is used as a volume group for /, /var/log and /var/crash
     # sda_root is a templated value to align with whatever partition is really used
     # This value is checked in os config and replaced by the partition actually used
     # on sda e.g. sda1 or sda5

     volume-groups:
       - name: ardana-vg
         physical-volumes:
           - /dev/sda_root

       logical-volumes:
       # The policy is not to consume 100% of the space of each volume group.
       # 5% should be left free for snapshots and to allow for some flexibility.
          - name: root
            size: 80%
            fstype: ext4
            mount: /
          - name: crash
            size: 15%
            mount: /var/crash
            fstype: ext4
            mkfs-opts: -O large_file
        consumer:
              name: os</strong></span></pre></div><p>
     <code class="filename">Servers.yml</code>: The snippet below shows the insertion of
     an additional server used for hosting the Cloud Lifecycle Manager. Provide the address
     information here for the server you are running on, that is, the node
     where you have installed the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> ISO.
    </p><div class="verbatim-wrap"><pre class="screen">  servers:
     # NOTE: Addresses of servers need to be changed to match your environment.
     #
     #       Add additional servers as required

<span class="bold"><strong>     #Lifecycle-manager
     - id: lifecycle-manager
       ip-addr: <em class="replaceable ">YOUR IP ADDRESS HERE</em>
       role: LIFECYCLE-MANAGER-ROLE
       server-group: RACK1
       nic-mapping: HP-SL230-4PORT
       mac-addr: 8c:dc:d4:b5:c9:e0
       # ipmi information is not needed </strong></span>

     # Controllers
     - id: controller1
       ip-addr: 192.168.10.3
       role: CONTROLLER-ROLE</pre></div></li></ol></div></div><p>
   With the stand-alone input model complete, you are ready to proceed to
   installing the stand-alone deployer with either the Install UI or the
   command line.
  </p></div></div></div><div class="chapter " id="install-gui"><div class="titlepage"><div><div><h2 class="title"><span class="number">21 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing with the Install UI</span> <a title="Permalink" class="permalink" href="#install-gui">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-gui_installer.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-gui_installer.xml</li><li><span class="ds-label">ID: </span>install-gui</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#id-1.3.6.4.8"><span class="number">21.1 </span><span class="name">Prepare for Cloud Installation</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.4.9"><span class="number">21.2 </span><span class="name">Preparing to Run the Install UI</span></a></span></dt><dt><span class="section"><a href="#create-csv-file"><span class="number">21.3 </span><span class="name">Optional: Creating a CSV File to Import Server Data</span></a></span></dt><dt><span class="section"><a href="#discover-servers"><span class="number">21.4 </span><span class="name">Optional: Importing Certificates for SUSE Manager and HPE OneView</span></a></span></dt><dt><span class="section"><a href="#running-install-ui"><span class="number">21.5 </span><span class="name">Running the Install UI</span></a></span></dt></dl></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> comes with a GUI-based installation wizard for first-time cloud
  installations. It will guide you through the configuration process and deploy
  your cloud based on the custom configuration you provide.  The Install UI
  will start with a set of example cloud configurations for you to choose
  from. Based on your cloud choice, you can refine your configuration to match
  your needs using Install UI widgets. You can also directly edit your model
  configuration files.
 </p><div id="id-1.3.6.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   The Install UI is only for initial deployments. It will not function
   properly after your cloud has been deployed successfully, whether it was
   from the CLI or with the Install UI.
  </p></div><p>
  When you are satisfied with your configuration and the Install UI has
  validated your configuration successfully, you can then deploy the cloud into
  your environment. Deploying the cloud will version-control your configuration
  into a git repository and provide you with live progress of your deployment.
 </p><p>
  With the Install UI, you have the option of provisioning SLES12-SP4 to
  IPMI-capable machines described in your configuration files. Provisioning
  machines with the Install UI will also properly configure them for Ansible
  access.
 </p><p>
  The Install UI is designed to make the initial installation process
  simpler, more accurate, and faster than manual installation.
 </p><div class="sect1" id="id-1.3.6.4.8"><div class="titlepage"><div><div><h2 class="title"><span class="number">21.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prepare for Cloud Installation</span> <a title="Permalink" class="permalink" href="#id-1.3.6.4.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-gui_installer.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-gui_installer.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Review the <a class="xref" href="#preinstall-checklist" title="Chapter 14. Pre-Installation Checklist">Chapter 14, <em>Pre-Installation Checklist</em></a> about recommended
     pre-installation tasks.
    </p></li><li class="step "><p>
     Prepare the Cloud Lifecycle Manager node. The Cloud Lifecycle Manager must be accessible either directly or via
     <code class="literal">ssh</code>, and have SUSE Linux Enterprise Server 12 SP4 installed. All nodes must
     be accessible to the Cloud Lifecycle Manager. If the nodes do not have direct access to
     online Cloud subscription channels, the Cloud Lifecycle Manager node will need to host the
     Cloud repositories.
    </p><ol type="a" class="substeps "><li class="step "><p>
       If you followed the installation instructions for Cloud Lifecycle Manager server (see <a class="xref" href="#cha-depl-dep-inst" title="Chapter 15. Installing the Cloud Lifecycle Manager server">Chapter 15, <em>Installing the Cloud Lifecycle Manager server</em></a>), <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> software should already be
       installed. Double-check whether SUSE Linux Enterprise and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are properly
       registered at the SUSE Customer Center by starting YaST and running <span class="guimenu ">Software</span> › <span class="guimenu ">Product
       Registration</span>.
      </p><p>
       If you have not yet installed <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, do so by starting YaST and
       running <span class="guimenu ">Software</span> › <span class="guimenu ">Product
       Registration</span> › <span class="guimenu ">Select
       Extensions</span>. Choose <span class="guimenu "><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span>
       and follow the on-screen instructions. Make sure to register
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> during the installation process and to install the software
       pattern <code class="literal">patterns-cloud-ardana</code>.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper -n in patterns-cloud-ardana</pre></div></li><li class="step "><p>
       Ensure the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> media repositories and updates repositories are
       made available to all nodes in your deployment. This can be accomplished
       either by configuring the Cloud Lifecycle Manager server as an SMT mirror as described in
       <a class="xref" href="#app-deploy-smt-lcm" title="Chapter 16. Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)">Chapter 16, <em>Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</em></a> or by syncing or mounting the
       Cloud and updates repositories to the Cloud Lifecycle Manager server as described in <a class="xref" href="#cha-depl-repo-conf-lcm" title="Chapter 17. Software Repository Setup">Chapter 17, <em>Software Repository Setup</em></a>.
      </p></li><li class="step "><p>
       Configure passwordless <code class="command">sudo</code> for the user created when
       setting up the node (as described in <a class="xref" href="#sec-depl-adm-inst-user" title="15.4. Creating a User">Section 15.4, “Creating a User”</a>). Note that this is
       <span class="emphasis"><em>not</em></span> the user <code class="systemitem">ardana</code> that will be used later in this
       procedure. In the following we assume you named the user <code class="systemitem">cloud</code>. Run the command
       <code class="command">visudo</code> as user <code class="systemitem">root</code> and add the following line
       to the end of the file:
      </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable ">CLOUD</em> ALL = (root) NOPASSWD:ALL</pre></div><p>
       Make sure to replace <em class="replaceable ">CLOUD</em> with your user name
       choice.
      </p></li><li class="step "><p>
       Set the password for the user
       <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo passwd ardana</pre></div></li><li class="step "><p>
       Become the user <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>su - ardana</pre></div></li><li class="step "><p>
       Place a copy of the SUSE Linux Enterprise Server 12 SP4 <code class="filename">.iso</code> in the
       <code class="systemitem">ardana</code> home directory,
       <code class="filename">var/lib/ardana</code>, and rename it to
       <code class="filename">sles12sp4.iso</code>.
      </p></li><li class="step "><p>
       Install the templates, examples, and working model directories:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>/usr/bin/ardana-init</pre></div></li></ol></li></ol></div></div></div><div class="sect1" id="id-1.3.6.4.9"><div class="titlepage"><div><div><h2 class="title"><span class="number">21.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Preparing to Run the Install UI</span> <a title="Permalink" class="permalink" href="#id-1.3.6.4.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-gui_installer.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-gui_installer.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Before you launch the Install UI to install your cloud, do the following:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Gather the following details from the servers that will make up your
     cloud:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Server names
      </p></li><li class="listitem "><p>
       IP addresses
      </p></li><li class="listitem "><p>
       Server roles
      </p></li><li class="listitem "><p>
       PXE MAC addresses
      </p></li><li class="listitem "><p>
       PXE IP addresses
      </p></li><li class="listitem "><p>
       PXE interfaces
      </p></li><li class="listitem "><p>
       IPMI IP address, username, password
      </p></li></ul></div></li><li class="step "><p>
     Choose an input model from <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a>. No
     action other than an understanding of your needs is necessary at this
     point. In the Install UI you will indicate which input model you wish to
     deploy.
    </p></li><li class="step "><p>
     Before you use the Install UI to install your cloud, you may install the
     operating system, SLES, on your nodes (servers) if you choose.
     Otherwise, the Install UI will install it for you.
    </p><p> If you are installing the operating system on all nodes yourself,
    you must do so using the SLES image included in the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9
    package.
    </p></li></ol></div></div><p>
  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9, a local git repository is used to track configuration
  changes; the Configuration Processor (CP) uses this repository. Use of a git
  workflow means that your configuration history is maintained, making
  rollbacks easier and keeping a record of previous configuration settings. The
  git repository also provides a way for you to merge changes that you pull down as
  <span class="quote">“<span class="quote ">upstream</span>”</span> updates (that is, updates from SUSE). It also
  allows you to manage your own configuration changes.
 </p><p>
  The git repository is installed by the Cloud Lifecycle Manager on the Cloud Lifecycle Manager node.
 </p><p>
  Using the Install UI does not require the use of the git repository. After
  the installation, it may be useful to know more about
  <a class="xref" href="#using-git" title="Chapter 22. Using Git for Configuration Management">Chapter 22, <em>Using Git for Configuration Management</em></a>.
 </p></div><div class="sect1" id="create-csv-file"><div class="titlepage"><div><div><h2 class="title"><span class="number">21.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Optional: Creating a CSV File to Import Server Data</span> <a title="Permalink" class="permalink" href="#create-csv-file">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-gui_installer.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-gui_installer.xml</li><li><span class="ds-label">ID: </span>create-csv-file</li></ul></div></div></div></div><p>
   Before beginning the installation, you can create a CSV file with your
   server information to import directly into the Install UI to avoid
   entering it manually on the <span class="guimenu ">Assign Servers</span> page.
  </p><p>
   The following table shows the fields needed for your CSV file.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col align="center" class="col2" /><col align="center" class="col3" /><col class="col4" /></colgroup><tbody><tr><td><span class="bold"><strong>Field</strong></span>
      </td><td align="center"><span class="bold"><strong>Required</strong></span>
      </td><td align="center"><span class="bold"><strong>Required for OS Provisioning</strong></span>
      </td><td><span class="bold"><strong>Aliases</strong></span>
      </td></tr><tr><td>Server ID</td><td align="center">Yes</td><td align="center">Yes</td><td>server_id, id</td></tr><tr><td>IP Address</td><td align="center">Yes</td><td align="center">Yes</td><td>ip, ip_address, ip_addr</td></tr><tr><td>MAC Address</td><td align="center">Yes</td><td align="center">Yes</td><td>mac, mac_address, mac_addr</td></tr><tr><td>IPMI IP Address</td><td align="center">No</td><td align="center">Yes</td><td>ipmi_ip, ipmi_ip_address</td></tr><tr><td>IPMI User</td><td align="center">No</td><td align="center">Yes</td><td>ipmi_user, user</td></tr><tr><td>IPMI Password</td><td align="center">No</td><td align="center">Yes</td><td>ipmi_password, password</td></tr><tr><td>Server Role</td><td align="center">No</td><td align="center">No</td><td>server_role, role</td></tr><tr><td>Server Group</td><td align="center">No</td><td align="center">No</td><td>server_group, group</td></tr><tr><td>NIC Mapping</td><td align="center">No</td><td align="center">No</td><td>server_nic_map, nic_map, nic_mapping</td></tr></tbody></table></div><p>
   The aliases are all the valid names that can be used in the CSV file for the
   column header for a given field. Field names are not case sensitive. You can
   use either <code class="literal"> </code> (space) or <code class="literal">-</code> (hyphen) in
   place of underscore for a field name.
  </p><p>
   An example CSV file could be:
  </p><div class="verbatim-wrap"><pre class="screen">id,ip-addr,mac-address,server-group,nic-mapping,server-role,ipmi-ip,ipmi-user
controller1,192.168.110.3,b2:72:8d:ac:7c:6f,RACK1,HP-DL360-4PORT,CONTROLLER-ROLE,192.168.109.3,admin
myserver4,10.2.10.24,00:14:22:01:23:44,AZ1,,,,</pre></div></div><div class="sect1" id="discover-servers"><div class="titlepage"><div><div><h2 class="title"><span class="number">21.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Optional: Importing Certificates for SUSE Manager and HPE OneView</span> <a title="Permalink" class="permalink" href="#discover-servers">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-gui_installer.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-gui_installer.xml</li><li><span class="ds-label">ID: </span>discover-servers</li></ul></div></div></div></div><p>
   If you intend to use SUSE Manager or HPE OneView to add servers, certificates for
   those services must be accessible to the Install UI.
  </p><p>
   Use the following steps to import a SUSE Manager certificate.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Retrieve the <code class="filename">.pem</code> file from the SUSE Manager.
    </p><div class="verbatim-wrap"><pre class="screen">curl -k https://<em class="replaceable ">SUSE_MANAGER_IP</em>:<em class="replaceable ">PORT</em>/pub/RHN-ORG-TRUSTED-SSL-CERT &gt; <em class="replaceable ">PEM_NAME</em>.pem</pre></div></li><li class="step "><p>
     Copy the <code class="filename">.pem</code> file to the proper location on the
     Cloud Lifecycle Manager.
    </p><div class="verbatim-wrap"><pre class="screen">cd /etc/pki/trust/anchors
sudo cp ~/<em class="replaceable ">PEM_NAME</em>.pem .</pre></div></li><li class="step "><p>
     Install the certificate.
    </p><div class="verbatim-wrap"><pre class="screen">sudo update-ca-certificates</pre></div></li><li class="step "><p>
     Add <em class="replaceable ">SUSE Manager host IP address</em> if
     <em class="replaceable ">SUSE Manager.test.domain</em> is not reachable by DNS.
    </p><div class="verbatim-wrap"><pre class="screen">sudo vi /etc/hosts</pre></div><p>
     Add <em class="replaceable ">SUSE Manager host IP address</em>
     <em class="replaceable ">SUSE Manager.test.domain</em>. For example:
    </p><div class="verbatim-wrap"><pre class="screen">10.10.10.10 SUSE Manager.test.domain</pre></div></li></ol></div></div><p>
   Use the following steps to import an HPE OneView certificate.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step " id="st-oneview-retrieve-id"><p>
     Retrieve the <code class="literal">sessionID</code>.
    </p><div class="verbatim-wrap"><pre class="screen">curl -k -H "X-Api-Version:500" -H "Content-Type: application/json" \
-d '{"userName":<em class="replaceable ">ONEVIEW_USER</em>, "password":<em class="replaceable ">ONEVIEW_PASSWORD</em>, \
"loginMsgAck":"true"}' https://<em class="replaceable ">ONEVIEW_MANAGER_URL</em>:<em class="replaceable ">PORT</em>/rest/login-sessions</pre></div><p>
     The response will be similar to:
    </p><div class="verbatim-wrap"><pre class="screen">{"partnerData":{},"sessionID":"LTYxNjA1O1NjkxMHcI1b2ypaGPscErUOHrl7At3-odHPmR"}</pre></div></li><li class="step "><p>
     Retrieve a Certificate Signing Request (CSR) using the
     <em class="replaceable ">sessionID</em> from
     <a class="xref" href="#st-oneview-retrieve-id" title="Step 1">Step 1</a>.
    </p><div class="verbatim-wrap"><pre class="screen">curl -k -i -H "X-Api-Version:500" -H <em class="replaceable ">sessionID</em> \
<em class="replaceable ">ONEVIEW_MANAGER_URL</em>/rest/certificates/ca \
&gt; <em class="replaceable ">CA_NAME</em>.csr</pre></div></li><li class="step "><p>
     Follow instructions in the HPE OneView User Guide to validate the CSR and
     obtain a signed certificate
     (<em class="replaceable ">CA_NAME</em><code class="filename">.crt</code>).
    </p></li><li class="step "><p>
     Copy the <code class="filename">.crt</code> file to the proper location on the
     Cloud Lifecycle Manager.
    </p><div class="verbatim-wrap"><pre class="screen">cd /etc/pki/trust/anchors
sudo cp ~/data/<em class="replaceable ">CA_NAME</em>.crt .</pre></div></li><li class="step "><p>
     Install the certificate.
    </p><div class="verbatim-wrap"><pre class="screen">sudo update-ca-certificates</pre></div></li><li class="step "><p>
     Follow instructions in your HPE OneView User Guide to import the
     <em class="replaceable ">CA_NAME</em>.crt certificate into HPE OneView.
    </p></li><li class="step "><p>
     Add <em class="replaceable ">HPE OneView host IP address</em> if
     <em class="replaceable ">HPE OneView.test.domain</em> is not reachable by DNS.
    </p><div class="verbatim-wrap"><pre class="screen">sudo vi /etc/hosts</pre></div><p>
     Add <em class="replaceable ">HPE OneView host IP address</em>
     <em class="replaceable ">HPE OneView.test.domain</em> For example:
    </p><div class="verbatim-wrap"><pre class="screen">10.84.84.84  HPE OneView.test.domain</pre></div></li></ol></div></div></div><div class="sect1" id="running-install-ui"><div class="titlepage"><div><div><h2 class="title"><span class="number">21.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Running the Install UI</span> <a title="Permalink" class="permalink" href="#running-install-ui">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-gui_installer.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-gui_installer.xml</li><li><span class="ds-label">ID: </span>running-install-ui</li></ul></div></div></div></div><div id="id-1.3.6.4.12.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    The Install UI must run continuously without stopping for authentication
    at any step. When using the Install UI it is required to launch the Cloud Lifecycle Manager
    with the following command:
   </p><div class="verbatim-wrap"><pre class="screen">ARDANA_INIT_AUTO=1 /usr/bin/ardana-init</pre></div></div><p>
   Deploying the cloud to your servers will reconfigure networking and firewall
   rules on your cloud servers. To avoid problems with these networking changes
   when using the Install UI, we recommend you run a browser directly on your
   Cloud Lifecycle Manager node and point it to <code class="uri">http://localhost:9085</code>.
  </p><p>
   If you cannot run a browser on the Cloud Lifecycle Manager node to perform the install, you
   can run a browser from a Linux-based computer in your
   <span class="bold"><strong>MANAGEMENT</strong></span> network. However, firewall rules
   applied during cloud deployment will block access to the Install UI. To
   avoid blocking the connection, you can use the Install UI via an SSH
   tunnel to the Cloud Lifecycle Manager server. This will allow SSH connections through the
   <span class="bold"><strong>MANAGEMENT</strong></span> network when you reach the
   "Review Configuration Files" step of the install process.
  </p><p>
   To open an SSH tunnel from your Linux-based computer in your
   <span class="bold"><strong>MANAGEMENT</strong></span> network to the Cloud Lifecycle Manager:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Open a new terminal and enter the following command:
    </p><div class="verbatim-wrap"><pre class="screen">ssh -N -L 8080:localhost:9085 ardana@<em class="replaceable ">MANAGEMENT IP address of Cloud Lifecycle Manager</em></pre></div><p>
     The user name and password should be what was set in
     <a class="xref" href="#sec-depl-adm-inst-add-on" title="15.5.2. Installing the SUSE OpenStack Cloud Extension">Section 15.5.2, “Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Extension”</a>. There will be no prompt after
     you have logged in.
    </p></li><li class="step "><p>
     Leave this terminal session open to keep the SSH tunnel open and running.
     This SSH tunnel will forward connections from your Linux-based computer
     directly to the Cloud Lifecycle Manager, bypassing firewall restrictions.
    </p></li><li class="step "><p>
     On your local computer (the one you are tunneling from), point your
     browser to <code class="uri">http://localhost:8080</code>.
    </p></li><li class="step "><p>
     If the connection is interrupted, refresh your browser.
    </p></li></ol></div></div><div id="id-1.3.6.4.12.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    If you use an SSH tunnel to connect to the Install UI, there is an
    important note in the "Review Configuration Files" step about modifying
    <code class="filename">firewall_rules.yml</code> to allow SSH connections on the
    <span class="bold"><strong>MANAGEMENT</strong></span> network.
   </p></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.6.4.12.8"><span class="name">Overview</span><a title="Permalink" class="permalink" href="#id-1.3.6.4.12.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-gui_installer.xml" title="Edit the source file for this section">Edit source</a></h5></div><p>
   The first page of the Install UI shows the general installation process
   and a reminder to gather some information before beginning. Clicking the
   <span class="guimenu ">Next</span> button brings up the <span class="guimenu ">Model
   Selection</span> page.
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_intro.png" target="_blank"><img src="images/installer_ui_intro.png" width="" /></a></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.6.4.12.11"><span class="name">Choose an <span class="productname">OpenStack</span> Cloud Model</span><a title="Permalink" class="permalink" href="#id-1.3.6.4.12.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-gui_installer.xml" title="Edit the source file for this section">Edit source</a></h5></div><p>
   The input model choices are displayed on this page. Details of each model
   can be seen on the right by clicking the model name on the left. If you have
   already decided some aspects of your cloud environment, models can be
   filtered using the dropdown selections. Narrowing a parameter affects the
   range of choices of models and changes other dropdown choices to only those
   that are compatible.
  </p><p>
   Selecting a model will determine the base template from which the cloud will
   be deployed. Models can be adjusted later in the process, though selecting
   the closest match to your requirements reduces the effort required to deploy
   your cloud.
  </p><div id="id-1.3.6.4.12.14" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
    If you select any ESX model, extra manual steps are required to avoid
    configuration failures. While installing an ESX model with the
    Install UI, you will be asked for interfaces related to ESX and
    OVSvApp. Those interfaces must be defined before being entered in the
    Install UI.  Instructions are available at <a class="xref" href="#esxi-overview" title="27.3. Overview of ESXi and OVSvApp">Section 27.3, “Overview of ESXi and OVSvApp”</a>.
   </p></div><div id="id-1.3.6.4.12.15" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    <span class="bold"><strong>Installing a Stand-alone Deployer</strong></span>
   </p><p>
    If you are using the Install UI to install a stand-alone deployer, select
    that model, which was created previously in <a class="xref" href="#preparing-standalone" title="Chapter 20. Preparing for Stand-Alone Deployment">Chapter 20, <em>Preparing for Stand-Alone Deployment</em></a>.
   </p><p>
    Continue with the remaining Install UI steps to finish installing the
    stand-alone deployer.
   </p></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_select_model.png" target="_blank"><img src="images/installer_ui_select_model.png" width="" /></a></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.6.4.12.17"><span class="name">Cloud Model to Deploy</span><a title="Permalink" class="permalink" href="#id-1.3.6.4.12.17">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-gui_installer.xml" title="Edit the source file for this section">Edit source</a></h5></div><p>
   Based on the cloud example selected on the previous page, more detail is
   shown about that cloud configuration and the components that will be
   deployed. If you go back and select a different model, the deployment
   process restarts from the beginning. Any configuration changes you have made
   will be deleted.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="emphasis"><em>Mandatory components</em></span> have assigned quantities. We
     strongly suggest not changing those quantities to avoid potential problems
     later in the installation process.
    </p></li><li class="listitem "><p>
     <span class="emphasis"><em>Additional components</em></span> can be adjusted within the
     parameters shown.
    </p></li></ul></div><p>
   The number of nodes (servers) dedicated to each server role can be adjusted.
   Most input models are designed to support High Availability and to
   distribute <span class="productname">OpenStack</span> services appropriately.
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_model_details.png" target="_blank"><img src="images/installer_ui_model_details.png" width="" /></a></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.6.4.12.22"><span class="name">Adding Servers and Assigning Server Roles</span><a title="Permalink" class="permalink" href="#id-1.3.6.4.12.22">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-gui_installer.xml" title="Edit the source file for this section">Edit source</a></h5></div><p>
   This page provides more detail about the number and assignment of each type
   of node based on the information from the previous page (any changes must be
   made there).
  </p><p>
   Components that do not meet the required parameters will be shown in red in
   the accordion bar. Missing required fields and duplicate server names will
   also be red, as will the accordion bar. The <span class="guimenu ">Next</span> button
   will be disabled.
  </p><p>
   Servers may be discovered using SUSE Manager, HPE OneView, or both. Ensure that
   the certificates are accessible, as described in
   <a class="xref" href="#discover-servers" title="21.4. Optional: Importing Certificates for SUSE Manager and HPE OneView">Section 21.4, “Optional: Importing Certificates for SUSE Manager and HPE OneView”</a>. Clicking the <span class="guimenu ">Discover</span>
   button will prompt for access credentials to the system management software
   to be used for discovery. Certificates can be verified by checking
   <span class="guimenu ">Verify SSL certificate</span>. After validating credentials,
   Discovery will retrieve a list of known servers from SUSE Manager and/or
   HPE OneView and allow access to server details on those management platforms.
  </p><p>
   You can drag and drop to move servers from the left to the right in order to
   assign server roles, from right to left, or up and down in the accordion
   bar.
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_assign_servers.png" target="_blank"><img src="images/installer_ui_assign_servers.png" width="" /></a></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_discovery.png" target="_blank"><img src="images/installer_ui_discovery.png" width="" /></a></div></div><p>
   Server information may also be entered manually or imported via CSV in the
   <span class="guimenu ">Manual Entry</span> tab. The format for CSV entry is described
   in <a class="xref" href="#create-csv-file" title="21.3. Optional: Creating a CSV File to Import Server Data">Section 21.3, “Optional: Creating a CSV File to Import Server Data”</a>. The server assignment list includes
   placeholder server details that can be edited to reflect real hardware, or
   can be removed and replaced with discovered or manually added systems.
  </p><p>
   For more information about server roles, see
   <a class="xref" href="#concept-serverroles" title="5.2.4. Server Roles">Section 5.2.4, “Server Roles”</a>.
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_add_servers_manually.png" target="_blank"><img src="images/installer_ui_add_servers_manually.png" width="" /></a></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_add_server_manually.png" target="_blank"><img src="images/installer_ui_add_server_manually.png" width="" /></a></div></div><p>
   Subnet and netmask values should be set on this page as they may impact the
   IP addresses being assigned to various servers.
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.6.4.12.34"><span class="name">Choose servers on which SLES will be installed</span><a title="Permalink" class="permalink" href="#id-1.3.6.4.12.34">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-gui_installer.xml" title="Edit the source file for this section">Edit source</a></h5></div><p>
   If an OS has not previously been installed on the servers that make up the
   cloud configuration, the OS installation page allows for Cobbler to deploy
   SLES on servers in the cloud configuration. Enter password, select
   servers and click <span class="guimenu ">Install</span> to deploy SLES to these
   servers. An installation log and progress indicators will be displayed.
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_install_os.png" target="_blank"><img src="images/installer_ui_install_os.png" width="" /></a></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_install_os_inprogress.png" target="_blank"><img src="images/installer_ui_install_os_inprogress.png" width="" /></a></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.6.4.12.38"><span class="name">Server and Role Summary</span><a title="Permalink" class="permalink" href="#id-1.3.6.4.12.38">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-gui_installer.xml" title="Edit the source file for this section">Edit source</a></h5></div><p>
   When the OS installation is complete, a Server and Server Role Summary page
   is displayed. It shows which servers have been assigned to each role, and
   provides an opportunity to edit the server configurations. Various cloud
   components can be configured by clicking on the <span class="guimenu ">Manage Cloud
   Settings</span> button. Incorrect information will be shown in red.
  </p><p>
   Below is the list of what can be changed within the Install UI, followed
   by a list of customizations that can only be changed by directly editing the
   files on the <span class="guimenu ">Review Configuration Files</span> page. Anything
   changed directly in the files themselves during the Install UI process
   will be overwritten by values you have entered with the Install UI.
  </p><p>
   Changes to the following items can be made:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     servers (including SLES installation configuration)
    </p></li><li class="listitem "><p>
     networks
    </p></li><li class="listitem "><p>
     disk models
    </p></li><li class="listitem "><p>
     interface models
    </p></li><li class="listitem "><p>
     NIC mappings
    </p></li><li class="listitem "><p>
     NTP servers
    </p></li><li class="listitem "><p>
     name servers
    </p></li><li class="listitem "><p>
     tags in network groups
    </p></li></ul></div><p>
   Changes to the following items can only be made by manually editing the
   associated <code class="filename">.yml</code> files on the <span class="guimenu ">Review
   Configuration</span> page of the Install UI:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     server groups
    </p></li><li class="listitem "><p>
     server roles
    </p></li><li class="listitem "><p>
     network groups
    </p></li><li class="listitem "><p>
     firewall rules
    </p></li><li class="listitem "><p>
     DNS, SMTP, firewall settings (<code class="filename">cloudConfig.yml</code>)
    </p></li><li class="listitem "><p>
     control planes
    </p></li></ul></div><div id="id-1.3.6.4.12.45" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    Directly changing files may cause the configuration to fail validation.
    During the process of installing with the Install UI, any changes should
    be made with the tools provided within the Install UI.
   </p></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_server_summary.png" target="_blank"><img src="images/installer_ui_server_summary.png" width="" /></a></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_edit_server.png" target="_blank"><img src="images/installer_ui_edit_server.png" width="" /></a></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_manage_cloud_settings.png" target="_blank"><img src="images/installer_ui_manage_cloud_settings.png" width="" /></a></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.6.4.12.49"><span class="name">Review Configuration Files</span><a title="Permalink" class="permalink" href="#id-1.3.6.4.12.49">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-gui_installer.xml" title="Edit the source file for this section">Edit source</a></h5></div><p>
   Advanced editing of the cloud configuration can be done on the
   <code class="literal">Review Configuration Files</code> page. Individual
   <code class="filename">.yml</code> and <code class="filename">.j2</code> files can be edited
   directly with the embedded editor in the <span class="guimenu ">Model</span> and
   <span class="guimenu ">Templates and Services</span> tabs. The
   <span class="guimenu ">Deployment</span> tab contains the items <span class="guimenu ">Wipe Data
   Disks</span>, <span class="guimenu ">Encryption Key</span> and <span class="guimenu ">Verbosity
   Level</span>.
  </p><p>
   This page also provides the ability to set up SUSE Enterprise Storage Integration during
   initial deployment.
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_review_configuration.png" target="_blank"><img src="images/installer_ui_review_configuration.png" width="" /></a></div></div><div id="id-1.3.6.4.12.53" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    If you are using an SSH tunnel to connect to the Install UI, you will
    need to make an extra modification here to allow SSH connections through
    the firewall:
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      While on the Review Configuration Files page, click on the
      <span class="guimenu ">Model</span> tab.
     </p></li><li class="step "><p>
      Click <span class="guimenu ">Firewall Rules</span>.
     </p></li><li class="step "><p>
      Uncomment the <code class="literal">SSH</code> section (remove the
      <code class="literal">#</code> at the beginning of the line for the <code class="literal">-
      name: SSH</code> section).
     </p></li><li class="step "><p>
      If you do not have such a <code class="literal">- name: SSH</code> section,
      manually add the following under the <code class="literal">firewall-rules:</code>
      section:
     </p><div class="verbatim-wrap"><pre class="screen">name: SSH
network-groups:
- MANAGEMENT
rules:
- type: allow
  remote-ip-prefix: 0.0.0.0/0
  port-range-min: 22
  port-range-max: 22
  protocol: tcp</pre></div></li></ol></div></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_review_configuration_edit_yml.png" target="_blank"><img src="images/installer_ui_review_configuration_edit_yml.png" width="" /></a></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_review_configuration_edit_services.png" target="_blank"><img src="images/installer_ui_review_configuration_edit_services.png" width="" /></a></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_review_configuration_deployment.png" target="_blank"><img src="images/installer_ui_review_configuration_deployment.png" width="" /></a></div></div><p>
   <span class="bold"><strong>Configure SUSE Enterprise Storage During Initial Deployment</strong></span>
  </p><p>
   A link to the <code class="filename">settings.yml</code> file is available under the
   <code class="literal">ses</code> selection on the <span class="guimenu ">Templates and
   services</span> tab.
  </p><p>
   To set up SUSE Enterprise Storage Integration:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Click on the link to edit the <code class="filename">settings.yml</code> file.
    </p></li><li class="step "><p>
     Uncomment the <code class="literal">ses_config_path</code> parameter, specify the
     location on the deployer host containing the
     <code class="filename">ses_config.yml</code> file, and save the
     <code class="filename">settings.yml</code> file.
    </p></li><li class="step "><p>
     If the <code class="filename">ses_config.yml</code> file does not yet exist in that
     location on the deployer host, a new link will appear for uploading a file
     from your local workstation.
    </p></li><li class="step "><p>
     When <code class="filename">ses_config.yml</code> is present on the deployer host,
     it will appear in the <code class="literal">ses</code> section of the
     <span class="guimenu ">Templates and services</span> tab and can be edited directly
     there.
    </p></li></ol></div></div><div id="id-1.3.6.4.12.61" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   Manual edits to your configuration files outside of the Install UI may not
   be reflected in the Install UI. If you make changes to any files directly,
   refresh the browser to make sure changes are seen by the Install UI.
  </p></div><p>
   Before performing the deployment, the configuration must be validated by
   clicking the <span class="guimenu ">Validate</span> button below the list of
   configuration files on the <span class="guimenu ">Model</span> tab. This ensures the
   configuration will be successful <span class="bold"><strong>before</strong></span> the
   actual configuration process runs and possibly fails. The
   <span class="guimenu ">Validate</span> button also commits any changes. If there are
   issues with the validation, the configuration processor will provide
   detailed information about the causes. When validation completes
   successfully, a message will be displayed that the model is valid. If either
   validation or commit fail, the <span class="guimenu ">Next</span> button is disabled.
  </p><p>
   Clicking the <span class="guimenu ">Deploy</span> button starts the actual deployment
   process.
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.6.4.12.64"><span class="name">Cloud Deployment in Progress</span><a title="Permalink" class="permalink" href="#id-1.3.6.4.12.64">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-gui_installer.xml" title="Edit the source file for this section">Edit source</a></h5></div><p>
   General progress steps are shown on the left. Detailed activity is shown on
   the right.
  </p><p>
   To start the deployment process, the Install UI runs scripts and playbooks
   based on the actual final configuration. Completed operations are green,
   black means in process, gray items are not started yet.
  </p><p>
   The log stream on the right shows finished states. If there are any
   failures, the log stream will show the errors and the
   <span class="guimenu ">Next</span> button will be disabled. The <span class="guimenu ">Back</span>
   and <span class="guimenu ">Next</span> buttons are disabled during the deployment
   process.
  </p><p>
   The log files in <code class="filename">~/ardana/.ansible/ansible.log</code> and
   <code class="filename">/var/cache/ardana_installer/</code> have debugging
   information.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <code class="filename">/var/cache/ardana_installer/log/ardana-service/ardana-service.log</code>
     is created and used during the deployment step.
    </p></li><li class="listitem "><p>
     Each of the time-stamped files in
     <code class="filename">/var/cache/ardana_installer/log/ardana-service/logs/*.log</code>
     shows the output of a single Ansible playbook run invoked during the UI
     installation process and the log output for each of those runs.
    </p></li><li class="listitem "><p>
     The <code class="filename">~/ardana/.ansible/ansible.log</code> file is the output
     of all Ansible playbook runs. This includes the logs from
     <code class="filename">/var/cache/ardana_installer/log/ardana-service/logs/*.log</code>.
    </p></li></ul></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_deploy_inprogress.png" target="_blank"><img src="images/installer_ui_deploy_inprogress.png" width="" /></a></div></div><p>
   When the deployment process is complete, all items on the left will be
   green. Some deployments will not include all steps shown if they do not apply
   to the selected input model. In such a situation, those unneeded steps will
   remain gray.
  </p><p>
   The <span class="guimenu ">Next</span> button will be enabled when deployment is
   successful.
  </p><p>
   Clicking <span class="guimenu ">Next</span> will display the <code class="literal">Cloud Deployment
   Successful</code> page with information about the deployment, including
   the chosen input model and links to cloud management tools.
  </p><div class="figure" id="ui-deploy-successful"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/installer_ui_deploy_successful.png" target="_blank"><img src="images/installer_ui_deploy_successful.png" width="" alt="Cloud Deployment Successful" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 21.1: </span><span class="name">Cloud Deployment Successful </span><a title="Permalink" class="permalink" href="#ui-deploy-successful">#</a></h6></div></div><p>
   After installation is complete, shutdown the Install UI by logging into
   the Cloud Lifecycle Manager and running the following commands:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost installui-stop.yml</pre></div><p>
   After deployment, continue to <a class="xref" href="#cloud-verification" title="Chapter 38. Post Installation Tasks">Chapter 38, <em>Post Installation Tasks</em></a> and
   <a class="xref" href="#postinstall-checklist" title="Chapter 44. Other Common Post-Installation Tasks">Chapter 44, <em>Other Common Post-Installation Tasks</em></a>.
  </p><p>
   To understand cloud configuration more thoroughly and to learn how to make
   changes later, see:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <a class="xref" href="#cha-input-model-intro-concept" title="Chapter 5. Input Model">Chapter 5, <em>Input Model</em></a>
    </p></li><li class="listitem "><p>
     <a class="xref" href="#using-git" title="Chapter 22. Using Git for Configuration Management">Chapter 22, <em>Using Git for Configuration Management</em></a>
    </p></li></ul></div></div></div><div class="chapter " id="using-git"><div class="titlepage"><div><div><h2 class="title"><span class="number">22 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using Git for Configuration Management</span> <a title="Permalink" class="permalink" href="#using-git">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-using_git.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-using_git.xml</li><li><span class="ds-label">ID: </span>using-git</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#id-1.3.6.5.4"><span class="number">22.1 </span><span class="name">Initialization on a new deployment</span></a></span></dt><dt><span class="section"><a href="#updating-configuration-including-default-config"><span class="number">22.2 </span><span class="name">Updating any configuration, including the default configuration</span></a></span></dt><dt><span class="section"><a href="#git-merge"><span class="number">22.3 </span><span class="name">Resolving Git merge conflicts</span></a></span></dt></dl></div></div><p>
  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9, a local git repository is used to track configuration
  changes; the Configuration Processor (CP) uses this repository. Use of a git
  workflow means that your configuration history is maintained, making
  rollbacks easier and keeping a record of previous configuration settings. The
  git repository also provides a way for you to merge changes that you pull down as
  <span class="quote">“<span class="quote ">upstream</span>”</span> updates (that is, updates from SUSE). It also
  allows you to manage your own configuration changes.
 </p><p>
  The git repository is installed by the Cloud Lifecycle Manager on the Cloud Lifecycle Manager node.
 </p><div class="sect1" id="id-1.3.6.5.4"><div class="titlepage"><div><div><h2 class="title"><span class="number">22.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Initialization on a new deployment</span> <a title="Permalink" class="permalink" href="#id-1.3.6.5.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-using_git.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-using_git.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   On a system new to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9, the Cloud Lifecycle Manager will prepare a git repository
   under <code class="literal">~/openstack</code>. The Cloud Lifecycle Manager provisioning runs the
   <code class="literal">ardana-init-deployer</code> script automatically. This calls
   <code class="literal">ansible-playbook -i hosts/localhost git-00-initialise.yml</code>.
  </p><p>
   As a result, the <code class="literal">~/openstack</code> directory is initialized as
   a git repo (if it is empty). It is initialized with four empty branches:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.6.5.4.4.1"><span class="term ">ardana</span></dt><dd><p>
      This holds the upstream source code corresponding to the contents of the
      <code class="literal">~/openstack</code> directory on a pristine installation.
      Every source code release that is downloaded from SUSE is applied as
      a fresh commit to this branch. This branch contains no customization by
      the end user.
     </p></dd><dt id="id-1.3.6.5.4.4.2"><span class="term ">site</span></dt><dd><p>
      This branch begins life as a copy of the first <code class="literal">ardana</code>
      drop. It is onto this branch that you commit your configuration changes.
      It is the branch most visible to the end user.
     </p></dd><dt id="id-1.3.6.5.4.4.3"><span class="term ">ansible</span></dt><dd><p>
      This branch contains the variable definitions generated by the CP that
      our main ansible playbooks need. This includes the
      <code class="literal">verb_hosts</code> file that describes to ansible what servers
      are playing what roles. The <code class="literal">ready-deployment</code> playbook
      takes this output and assembles a <code class="literal">~/scratch</code> directory
      containing the ansible playbooks together with the variable definitions
      in this branch. The result is a working ansible directory
      <code class="literal">~/scratch/ansible/next/ardana/ansible</code> from which the
      main deployment playbooks may be successfully run.
     </p></dd><dt id="id-1.3.6.5.4.4.4"><span class="term ">cp-persistent</span></dt><dd><p>
      This branch contains the persistent state that the CP needs to maintain.
      That state is mostly the assignment of IP addresses and roles to
      particular servers. Some operational procedures may involve editing the
      contents of this branch: for example, retiring a machine from service or
      repurposing it.
     </p></dd></dl></div><p>
   Two temporary branches are created and populated at run time:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.6.5.4.6.1"><span class="term ">staging-ansible</span></dt><dd><p>
      This branch hosts the most recent commit that will be appended to the
      Ansible branch.
     </p></dd><dt id="id-1.3.6.5.4.6.2"><span class="term ">staging-cp-persistent</span></dt><dd><p>
      This branch hosts the most recent commit that will be appended to the
      cp-persistent branch.
     </p></dd></dl></div><div id="id-1.3.6.5.4.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    The information above provides insight into the workings of the
    configuration processor and the git repository. However, in practice you
    can simply follow the steps below to make configuration changes.
   </p></div></div><div class="sect1" id="updating-configuration-including-default-config"><div class="titlepage"><div><div><h2 class="title"><span class="number">22.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Updating any configuration, including the default configuration</span> <a title="Permalink" class="permalink" href="#updating-configuration-including-default-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-using_git.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-using_git.xml</li><li><span class="ds-label">ID: </span>updating-configuration-including-default-config</li></ul></div></div></div></div><p>
   When you need to make updates to a configuration you must:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Check out the <code class="literal">site</code> branch. You may already be on that
     branch. If so, git will tell you that and the command will leave you
     there.
    </p><div class="verbatim-wrap"><pre class="screen">git checkout site</pre></div></li><li class="step "><p>
     Edit the YAML file or files that contain the configuration you want to
     change.
    </p></li><li class="step "><p>
     Commit the changes to the <code class="literal">site</code> branch.
    </p><div class="verbatim-wrap"><pre class="screen">git add -A
git commit -m "your commit message goes here in quotes"</pre></div><p>
     If you want to add a single file to your git repository, you can use the
     command below, as opposed to using <code class="command">git add -A</code>.
    </p><div class="verbatim-wrap"><pre class="screen">git add PATH_TO_FILE</pre></div><p>
     For example, if you made a change to your <code class="command">servers.yml</code>
     file and wanted to only commit that change, you would use this command:
    </p><div class="verbatim-wrap"><pre class="screen">git add ~/openstack/my_cloud/definition/data/servers.yml</pre></div></li><li class="step "><p>
     To produce the required configuration processor output from those changes.
     Review the output files manually if required, run the configuration
     processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Ready the deployment area
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the deployment playbooks from the resulting scratch directory.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml</pre></div></li></ol></div></div></div><div class="sect1" id="git-merge"><div class="titlepage"><div><div><h2 class="title"><span class="number">22.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Resolving Git merge conflicts</span> <a title="Permalink" class="permalink" href="#git-merge">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-using_git.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-using_git.xml</li><li><span class="ds-label">ID: </span>git-merge</li></ul></div></div></div></div><p>
   When you make changes, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> attempts to incorporate new or updated
   configuration information on top of your existing environment. However, with
   some changes to your environment, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cannot automatically
   determine whether to keep your changes or drop them in favor of the new or
   updated configurations. This will result in <code class="literal">merge
   conflicts</code>, and it will be up to you to manually resolve the
   conflicts before you can proceed. This is common, but it can be
   confusing. Git provides a way to resolve these situations.
  </p><p>
   This section gives an overview of how to approach the issue of merge
   conflicts, showing general procedures for determining where the conflict is
   occurring, alternative methods for resolution, and a fallback procedure for
   the case where the resolution goes wrong and you need to start changes from
   the beginning.
  </p><p>
   For a general overview of how <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses Git, see the introductory
   article <a class="xref" href="#using-git" title="Chapter 22. Using Git for Configuration Management">Chapter 22, <em>Using Git for Configuration Management</em></a>. In particular, note how the
   <code class="literal">site</code> branch is the one most used by the end-user, while
   the <code class="literal">ardana</code> branch contains the "upstream" source code
   corresponding to the contents of the <code class="literal">~/openstack</code>
   directory on a pristine fresh installation.
  </p><div class="sect2" id="id-1.3.6.5.6.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">22.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Identifying the occurrence of a <code class="literal">merge conflict</code></span> <a title="Permalink" class="permalink" href="#id-1.3.6.5.6.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-using_git.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-using_git.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If you get a <code class="literal">merge conflict</code>, you will observe output
   similar to the following on the console:
  </p><div class="verbatim-wrap"><pre class="screen">Auto-merging ardana/ansible/roles/nova-compute-esx/defaults/main.yml
Auto-merging ardana/ansible/roles/nova-common/templates/nova.conf.j2
<span class="bold"><strong>CONFLICT (content): Merge conflict in ardana/ansible/roles/nova-common/templates/nova.conf.j2</strong></span>
Auto-merging ardana/ansible/roles/nova-cli/tasks/availability_zones.yml
Auto-merging ardana/ansible/roles/nova-api/templates/api-paste.ini.j2</pre></div></div><div class="sect2" id="id-1.3.6.5.6.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">22.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Examining Conflicts</span> <a title="Permalink" class="permalink" href="#id-1.3.6.5.6.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-using_git.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-using_git.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Use <code class="literal">git status</code> to discover the source of the problem:
  </p><div class="verbatim-wrap"><pre class="screen">...
        new file:   tech-preview/entry-scale-kvm-mml/data/swift/swift_config.yml
        modified:   tech-preview/mid-scale/README.md
        modified:   tech-preview/mid-scale/data/control_plane.yml

Unmerged paths:
  (use "git add/rm &lt;file&gt;..." as appropriate to mark resolution)

        <span class="bold"><strong>both modified:   ardana/ansible/roles/nova-common/templates/nova.conf.j2</strong></span></pre></div><p>
   Edit the file
   <code class="literal">ardana/ansible/roles/nova-common/templates/nova.conf.j2</code>
   to see the conflict markers:
  </p><div class="verbatim-wrap"><pre class="screen">[neutron]
admin_auth_url = {{ neutron_admin_auth_url }}
admin_password = {{ neutron_admin_password }}
admin_tenant_name = {{ neutron_admin_tenant_name }}
admin_username = {{ neutron_admin_username }}
<span class="bold"><strong>&lt;&lt;&lt;&lt;&lt;&lt;&lt;HEAD
metadata_proxy_shared_secret = top_secret_password111
=======
metadata_proxy_shared_secret = {{ neutron_metadata_proxy_shared_secret }}
&gt;&gt;&gt;&gt;&gt;&gt;&gt; ardana</strong></span>
neutron_auth_strategy = keystone
neutron_ca_certificates_file = /etc/ssl/certs/ca-certificates.crt
service_metadata_proxy = True</pre></div><p>
   This indicates that <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9is trying to set the value of
   <code class="literal">metadata_proxy_shared_secret</code> to be <code class="literal">{{
   neutron_metadata_proxy_shared_secret }}</code> whereas previously you
   have set its value to <code class="literal">top_secret_password111</code>.
  </p></div><div class="sect2" id="id-1.3.6.5.6.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">22.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Examining differences between your current version and the previous upstream version</span> <a title="Permalink" class="permalink" href="#id-1.3.6.5.6.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-using_git.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-using_git.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Typically, the previous upstream version will be the last-but-one commit on
   the <code class="literal">ardana</code> branch. This version will have been created
   during the initial installation of your cloud (or perhaps during a previous
   upgrade or configuration change). So in total, there are three significant
   versions of the file:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The previous "upstream" version on the <code class="literal">ardana</code> branch.
    </p></li><li class="listitem "><p>
     Your current version on the <code class="literal">site</code> branch.
    </p></li><li class="listitem "><p>
     The new "upstream" version on the <code class="literal">ardana</code> branch.
    </p></li></ul></div><p>
   You can identify the commit number of the previous "upstream" version using
   <code class="literal">git merge-base</code>.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git merge-base ardana site
<span class="bold"><strong>2eda1df48e2822533c50f80f5bfd7a9d788bdf73</strong></span></pre></div><p>
   And then use <code class="literal">git log</code> to see where this commit occurs in
   the history of the <code class="literal">ardana</code> branch.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git log ardana --
commit 22cfa83f3526baf30b3697e971a712930f86f611
Author: Openstack git user &lt;openstack@example.com&gt;
Date:   Mon Jan 18 00:30:33 2018 +0000

    New drop

commit <span class="bold"><strong>2eda1df48e2822533c50f80f5bfd7a9d788bdf73</strong></span>
Author: Openstack git user &lt;openstack@example.com&gt;
Date:   Sun Jan 17 19:14:01 2018 +0000

    New drop</pre></div><p>
   In this instance, we can see that the relevant commit is in fact the
   last-but-one commit. We can use the simplified name
   <code class="literal">ardana^1</code> to identify that commit.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git diff <span class="bold"><strong>ardana^1</strong></span> HEAD -- ardana/ansible/roles/nova-common/templates/nova.conf.j2</pre></div><p>
   In the diff output, you should be able to see how you changed the value for
   <code class="literal">metadata_proxy_shared_secret</code> from
   <code class="literal">unset</code> to <code class="literal">top_secret_password111</code>:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>diff --git a/ardana/ansible/roles/nova-common/templates/nova.conf.j2 b/ardana/ansible/roles/nova-common/templates/nova.conf.j2
index 597a982..05cb07c 100644
--- a/ardana/ansible/roles/nova-common/templates/nova.conf.j2
+++ b/ardana/ansible/roles/nova-common/templates/nova.conf.j2
@@ -132,7 +132,7 @@ admin_auth_url = {{ neutron_admin_auth_url }}
 admin_password = {{ neutron_admin_password }}
 admin_tenant_name = {{ neutron_admin_tenant_name }}
 admin_username = {{ neutron_admin_username }}
<span class="bold"><strong>-metadata_proxy_shared_secret = unset
+metadata_proxy_shared_secret = top_secret_password111</strong></span>
 neutron_auth_strategy = keystone
 neutron_ca_certificates_file = /etc/ssl/certs/ca-certificates.crt
 service_metadata_proxy = True</pre></div></div><div class="sect2" id="id-1.3.6.5.6.8"><div class="titlepage"><div><div><h3 class="title"><span class="number">22.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Examining differences between your current version and the new upstream version</span> <a title="Permalink" class="permalink" href="#id-1.3.6.5.6.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-using_git.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-using_git.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To compare your change with the new upstream version from <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 on
   the <code class="literal">ardana</code> branch you can use <code class="literal">git diff HEAD
   ardana -- &lt;&lt;path/to/file&gt;&gt;</code>:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git diff HEAD ardana -- ardana/ansible/roles/nova-common/templates/nova.conf.j2</pre></div><p>
   In the extract of output below, you can see your value
   <code class="literal">top_secret_password111</code> and the new value <code class="literal">{{
   neutron_metadata_proxy_shared_secret }}</code> that <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 wants
   to set.
  </p><div class="verbatim-wrap"><pre class="screen">..
 admin_username = {{ neutron_admin_username }}
<span class="bold"><strong>-metadata_proxy_shared_secret = top_secret_password111
+metadata_proxy_shared_secret = {{ neutron_metadata_proxy_shared_secret }}</strong></span>
 neutron_auth_strategy = keystone
 neutron_ca_certificates_file = /etc/ssl/certs/ca-certificates.crt</pre></div></div><div class="sect2" id="id-1.3.6.5.6.9"><div class="titlepage"><div><div><h3 class="title"><span class="number">22.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Examining differences between the new upstream version and the previous upstream version</span> <a title="Permalink" class="permalink" href="#id-1.3.6.5.6.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-using_git.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-using_git.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To compare the new upstream version from <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 with the previous
   upstream version from your initial install (or previous change):
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git diff $(git merge-base ardana HEAD) ardana -- ardana/ansible/roles/nova-common/templates/nova.conf.j2</pre></div><p>
   In the extract of output below, you can see the new upstream value
   <code class="literal">{{ neutron_metadata_proxy_shared_secret }}</code> that
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 wants to set and the previous upstream value
   <code class="literal">unset</code>.
  </p><div class="verbatim-wrap"><pre class="screen"> admin_password = {{ neutron_admin_password }}
 admin_tenant_name = {{ neutron_admin_tenant_name }}
 admin_username = {{ neutron_admin_username }}
<span class="bold"><strong>-metadata_proxy_shared_secret = unset
+metadata_proxy_shared_secret = {{ neutron_metadata_proxy_shared_secret }}</strong></span>
 neutron_auth_strategy = keystone
 neutron_ca_certificates_file = /etc/ssl/certs/ca-certificates.crt</pre></div></div><div class="sect2" id="id-1.3.6.5.6.10"><div class="titlepage"><div><div><h3 class="title"><span class="number">22.3.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using <code class="literal">stage markers</code> to view clean versions of files (without conflict markers)</span> <a title="Permalink" class="permalink" href="#id-1.3.6.5.6.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-using_git.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-using_git.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   You can use the <code class="literal">git show</code> command with stage markers to
   view files without having conflict markers embedded in them. Stage 1 is the
   previous upstream version (<span class="bold"><strong>:1</strong></span>), stage 2 is
   your current version (<span class="bold"><strong>:2</strong></span>) while stage 3 is
   the new upstream version (<span class="bold"><strong>:3</strong></span>).
  </p><p>
   <span class="bold"><strong>Stage 1</strong></span>
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git show <span class="bold"><strong>:1</strong></span>:ardana/ansible/roles/nova-common/templates/nova.conf.j2</pre></div><div class="verbatim-wrap"><pre class="screen">...
admin_username = {{ neutron_admin_username }}
<span class="bold"><strong>metadata_proxy_shared_secret = unset</strong></span>
neutron_auth_strategy = keystone
neutron_ca_certificates_file = /etc/ssl/certs/ca-certificates.crt
...</pre></div><p>
   <span class="bold"><strong>Stage 2</strong></span>
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git show <span class="bold"><strong>:2</strong></span>:ardana/ansible/roles/nova-common/templates/nova.conf.j2</pre></div><div class="verbatim-wrap"><pre class="screen">...
admin_username = {{ neutron_admin_username }}
<span class="bold"><strong>metadata_proxy_shared_secret = top_secret_password111</strong></span>
neutron_auth_strategy = keystone
neutron_ca_certificates_file = /etc/ssl/certs/ca-certificates.crt
...</pre></div><p>
   <span class="bold"><strong>Stage 3</strong></span>
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git show <span class="bold"><strong>:3</strong></span>:ardana/ansible/roles/nova-common/templates/nova.conf.j2</pre></div><div class="verbatim-wrap"><pre class="screen">...
admin_username = {{ neutron_admin_username }}
<span class="bold"><strong>metadata_proxy_shared_secret = {{ neutron_metadata_proxy_shared_secret }}</strong></span>
neutron_auth_strategy = keystone
neutron_ca_certificates_file = /etc/ssl/certs/ca-certificates.crt
...</pre></div></div><div class="sect2" id="id-1.3.6.5.6.11"><div class="titlepage"><div><div><h3 class="title"><span class="number">22.3.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Resolving the conflict</span> <a title="Permalink" class="permalink" href="#id-1.3.6.5.6.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-using_git.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-using_git.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   There are two approaches to resolving the conflict:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Edit the merged file containing the conflict markers, keeping the change
     you want to preserve and removing the conflict markers and any changes you
     want to discard.
    </p></li><li class="listitem "><p>
     Take the new upstream version of the file and re-apply any changes you
     would like to keep from your current version.
    </p></li></ol></div></div><div class="sect2" id="id-1.3.6.5.6.12"><div class="titlepage"><div><div><h3 class="title"><span class="number">22.3.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Resolving the conflict - editing the file containing the conflict markers</span> <a title="Permalink" class="permalink" href="#id-1.3.6.5.6.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-using_git.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-using_git.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Edit the file
   <code class="literal">ardana/ansible/roles/nova-common/templates/nova.conf.j2</code>
   and if you want to maintain your change, then delete the lines in bold
   below:
  </p><div class="verbatim-wrap"><pre class="screen">[neutron]
admin_auth_url = {{ neutron_admin_auth_url }}
admin_password = {{ neutron_admin_password }}
admin_tenant_name = {{ neutron_admin_tenant_name }}
admin_username = {{ neutron_admin_username }}
<span class="bold"><strong>&lt;&lt;&lt;&lt;&lt;&lt;&lt;HEAD</strong></span>
metadata_proxy_shared_secret = top_secret_password111
<span class="bold"><strong>=======
metadata_proxy_shared_secret = {{ neutron_metadata_proxy_shared_secret }}
&gt;&gt;&gt;&gt;&gt;&gt;&gt; ardana</strong></span>
neutron_auth_strategy = keystone
neutron_ca_certificates_file = /etc/ssl/certs/ca-certificates.crt
service_metadata_proxy = True</pre></div><p>
   Your file should now look like this:
  </p><div class="verbatim-wrap"><pre class="screen">[neutron]
admin_auth_url = {{ neutron_admin_auth_url }}
admin_password = {{ neutron_admin_password }}
admin_tenant_name = {{ neutron_admin_tenant_name }}
admin_username = {{ neutron_admin_username }}
metadata_proxy_shared_secret = top_secret_password111
neutron_auth_strategy = keystone
neutron_ca_certificates_file = /etc/ssl/certs/ca-certificates.crt
service_metadata_proxy = True</pre></div></div><div class="sect2" id="id-1.3.6.5.6.13"><div class="titlepage"><div><div><h3 class="title"><span class="number">22.3.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Resolving the conflict - re-applying your changes to new upstream version</span> <a title="Permalink" class="permalink" href="#id-1.3.6.5.6.13">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-using_git.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-using_git.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Create a copy of the new upstream version (see Stage 3 above) in your
   working directory:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git show <span class="bold"><strong>:3</strong></span>:ardana/ansible/roles/nova-common/templates/nova.conf.j2 &gt; \
ardana/ansible/roles/nova-common/templates/nova.conf.j2</pre></div><p>
   Now edit the file
   <code class="literal">ardana/ansible/roles/nova-common/templates/nova.conf.j2</code>
   and manually re-apply the changes you want.
  </p></div><div class="sect2" id="id-1.3.6.5.6.14"><div class="titlepage"><div><div><h3 class="title"><span class="number">22.3.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Completing the merge procedure</span> <a title="Permalink" class="permalink" href="#id-1.3.6.5.6.14">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-using_git.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-using_git.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   You may want to check that the changes you have applied are correct. Compare
   the new upstream version with the version in your working directory:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git diff ardana -- ardana/ansible/roles/nova-common/templates/nova.conf.j2</pre></div><p>
   If you are happy with the resolution, you can stage your changes using:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git add ardana/ansible/roles/nova-common/templates/nova.conf.j2</pre></div><p>
   Apply the above steps to all the merge conflicts you encounter, and when you
   have them resolved to your satisfaction, complete the merge:
  </p><div class="verbatim-wrap"><pre class="screen">git commit -m "complete merge"</pre></div></div><div class="sect2" id="id-1.3.6.5.6.15"><div class="titlepage"><div><div><h3 class="title"><span class="number">22.3.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recovering from Errors</span> <a title="Permalink" class="permalink" href="#id-1.3.6.5.6.15">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-using_git.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-using_git.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If you make a mistake during the resolution process, you can return your
   working directory to a clean copy of the <code class="literal">site</code> branch
   using:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git reset --hard</pre></div><p>
   If the new upstream version contains files that did not exist in the previous
   version, these files will be left behind - you can see them using
   <code class="literal">git status</code>. To clean up these files, remove them and then
   reset:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git rm -rf ardana
<code class="prompt user">tux &gt; </code>git reset --hard</pre></div><p>
   Alternatively, you can use <code class="literal">git stash</code> to save these files
   to a transient stash queue.
  </p></div></div></div><div class="chapter " id="install-standalone"><div class="titlepage"><div><div><h2 class="title"><span class="number">23 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing a Stand-Alone Cloud Lifecycle Manager</span> <a title="Permalink" class="permalink" href="#install-standalone">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_standalone.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_standalone.xml</li><li><span class="ds-label">ID: </span>install-standalone</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#id-1.3.6.6.2"><span class="number">23.1 </span><span class="name">Important Notes</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.6.3"><span class="number">23.2 </span><span class="name">Prepare for Cloud Installation</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.6.4"><span class="number">23.3 </span><span class="name">Configuring Your Environment</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.6.5"><span class="number">23.4 </span><span class="name">Running the Configuration Processor</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.6.6"><span class="number">23.5 </span><span class="name">Configuring TLS</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.6.7"><span class="number">23.6 </span><span class="name">Deploying the Cloud</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.6.8"><span class="number">23.7 </span><span class="name">Installing <span class="productname">OpenStack</span> Assets on the Stand-alone Deployer</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.6.9"><span class="number">23.8 </span><span class="name">Post-Installation Verification and Administration</span></a></span></dt></dl></div></div><div class="sect1" id="id-1.3.6.6.2"><div class="titlepage"><div><div><h2 class="title"><span class="number">23.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Important Notes</span> <a title="Permalink" class="permalink" href="#id-1.3.6.6.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_standalone.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_standalone.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     For information about when to use the GUI installer and when to use the
     command line (CLI), see <a class="xref" href="#preinstall-overview" title="Chapter 13. Overview">Chapter 13, <em>Overview</em></a>.
    </p></li><li class="listitem "><p>
     Review the <a class="xref" href="#min-hardware" title="Chapter 2. Hardware and Software Support Matrix">Chapter 2, <em>Hardware and Software Support Matrix</em></a> that we have listed.
    </p></li><li class="listitem "><p>
     Review the release notes to make yourself aware of any known issues and
     limitations.
    </p></li><li class="listitem "><p>
     The installation process can occur in different phases. For example, you
     can install the control plane only and then add Compute nodes afterwards
     if you would like.
    </p></li><li class="listitem "><p>
     If you run into issues during installation, we have put together a list of
     <a class="xref" href="#troubleshooting-installation" title="Chapter 36. Troubleshooting the Installation">Chapter 36, <em>Troubleshooting the Installation</em></a> you can reference.
    </p></li><li class="listitem "><p>
     Make sure all disks on the system(s) are wiped before you begin the
     install. (For swift, refer to <a class="xref" href="#topic-d1s-hht-tt" title="11.6. Swift Requirements for Device Group Drives">Section 11.6, “Swift Requirements for Device Group Drives”</a>.)
    </p></li><li class="listitem "><p>
     There is no requirement to have a dedicated network for OS-install and
     system deployment, this can be shared with the management network. More
     information can be found in <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a>.
    </p></li><li class="listitem "><p>
     The terms deployer and Cloud Lifecycle Manager are used interchangeably. They refer to the
     same nodes in your cloud environment.
    </p></li><li class="listitem "><p>
     When running the Ansible playbook in this installation guide, if a runbook
     fails you will see in the error response to use the
     <code class="literal">--limit</code> switch when retrying a playbook. This should be
     avoided. You can simply re-run any playbook without this switch.
    </p></li><li class="listitem "><p>
     DVR is not supported with ESX compute.
    </p></li><li class="listitem "><p>
     When you attach a cinder volume to the VM running on the ESXi host, the
     volume will not get detected automatically. Make sure to set the image
     metadata <span class="bold"><strong>vmware_adaptertype=lsiLogicsas</strong></span>
     for image before launching the instance. This will help to discover the
     volume change appropriately.
    </p></li><li class="listitem "><p>
     The installation process will create several <span class="productname">OpenStack</span> roles. Not all roles
     will be relevant for a cloud with swift only, but they will not cause
     problems.
    </p></li></ul></div></div><div class="sect1" id="id-1.3.6.6.3"><div class="titlepage"><div><div><h2 class="title"><span class="number">23.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prepare for Cloud Installation</span> <a title="Permalink" class="permalink" href="#id-1.3.6.6.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_standalone.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_standalone.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Review the <a class="xref" href="#preinstall-checklist" title="Chapter 14. Pre-Installation Checklist">Chapter 14, <em>Pre-Installation Checklist</em></a> about recommended
     pre-installation tasks.
    </p></li><li class="step "><p>
     Prepare the Cloud Lifecycle Manager node. The Cloud Lifecycle Manager must be accessible either directly or via
     <code class="literal">ssh</code>, and have SUSE Linux Enterprise Server 12 SP4 installed. All nodes must
     be accessible to the Cloud Lifecycle Manager. If the nodes do not have direct access to
     online Cloud subscription channels, the Cloud Lifecycle Manager node will need to host the
     Cloud repositories.
    </p><ol type="a" class="substeps "><li class="step "><p>
       If you followed the installation instructions for Cloud Lifecycle Manager server (see <a class="xref" href="#cha-depl-dep-inst" title="Chapter 15. Installing the Cloud Lifecycle Manager server">Chapter 15, <em>Installing the Cloud Lifecycle Manager server</em></a>), <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> software should already be
       installed. Double-check whether SUSE Linux Enterprise and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are properly
       registered at the SUSE Customer Center by starting YaST and running <span class="guimenu ">Software</span> › <span class="guimenu ">Product
       Registration</span>.
      </p><p>
       If you have not yet installed <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, do so by starting YaST and
       running <span class="guimenu ">Software</span> › <span class="guimenu ">Product
       Registration</span> › <span class="guimenu ">Select
       Extensions</span>. Choose <span class="guimenu "><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span>
       and follow the on-screen instructions. Make sure to register
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> during the installation process and to install the software
       pattern <code class="literal">patterns-cloud-ardana</code>.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper -n in patterns-cloud-ardana</pre></div></li><li class="step "><p>
       Ensure the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> media repositories and updates repositories are
       made available to all nodes in your deployment. This can be accomplished
       either by configuring the Cloud Lifecycle Manager server as an SMT mirror as described in
       <a class="xref" href="#app-deploy-smt-lcm" title="Chapter 16. Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)">Chapter 16, <em>Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</em></a> or by syncing or mounting the
       Cloud and updates repositories to the Cloud Lifecycle Manager server as described in <a class="xref" href="#cha-depl-repo-conf-lcm" title="Chapter 17. Software Repository Setup">Chapter 17, <em>Software Repository Setup</em></a>.
      </p></li><li class="step "><p>
       Configure passwordless <code class="command">sudo</code> for the user created when
       setting up the node (as described in <a class="xref" href="#sec-depl-adm-inst-user" title="15.4. Creating a User">Section 15.4, “Creating a User”</a>). Note that this is
       <span class="emphasis"><em>not</em></span> the user <code class="systemitem">ardana</code> that will be used later in this
       procedure. In the following we assume you named the user <code class="systemitem">cloud</code>. Run the command
       <code class="command">visudo</code> as user <code class="systemitem">root</code> and add the following line
       to the end of the file:
      </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable ">CLOUD</em> ALL = (root) NOPASSWD:ALL</pre></div><p>
       Make sure to replace <em class="replaceable ">CLOUD</em> with your user name
       choice.
      </p></li><li class="step "><p>
       Set the password for the user
       <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo passwd ardana</pre></div></li><li class="step "><p>
       Become the user <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>su - ardana</pre></div></li><li class="step "><p>
       Place a copy of the SUSE Linux Enterprise Server 12 SP4 <code class="filename">.iso</code> in the
       <code class="systemitem">ardana</code> home directory,
       <code class="filename">var/lib/ardana</code>, and rename it to
       <code class="filename">sles12sp4.iso</code>.
      </p></li><li class="step "><p>
       Install the templates, examples, and working model directories:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>/usr/bin/ardana-init</pre></div></li></ol></li></ol></div></div></div><div class="sect1" id="id-1.3.6.6.4"><div class="titlepage"><div><div><h2 class="title"><span class="number">23.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Your Environment</span> <a title="Permalink" class="permalink" href="#id-1.3.6.6.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_standalone.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_standalone.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      You have already configured an input model for a stand-alone deployer in
      a previous step (<a class="xref" href="#preparing-standalone" title="Chapter 20. Preparing for Stand-Alone Deployment">Chapter 20, <em>Preparing for Stand-Alone Deployment</em></a>). Now that input
      model needs to be moved into the setup directory.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cp -r ~/openstack/examples/entry-scale-kvm-stand-alone-deployer/* \
~/openstack/my_cloud/definition/</pre></div></li><li class="step "><p><span class="step-optional">(Optional)</span> 
      You can use the <code class="literal">ardanaencrypt.py</code> script to encrypt
      your IPMI passwords. This script uses OpenSSL.
     </p><ol type="a" class="substeps "><li class="step "><p>
        Change to the Ansible directory:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible</pre></div></li><li class="step "><p>
        Enter the encryption key into the following environment variable:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>export ARDANA_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</pre></div></li><li class="step "><p>
        Run the python script below and follow the instructions. Enter a
        password that you want to encrypt.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>./ardanaencrypt.py</pre></div></li><li class="step "><p>
        Take the string generated and place it in the
        <code class="literal">ilo-password</code> field in your
        <code class="filename">~/openstack/my_cloud/definition/data/servers.yml</code>
        file, remembering to enclose it in quotes.
       </p></li><li class="step "><p>
        Repeat the above for each server.
       </p><div id="id-1.3.6.6.4.2.2.2.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
         Before you run any playbooks, remember that you need to export the
         encryption key in the following environment variable: <code class="literal">export
         ARDANA_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</code>
        </p></div></li></ol></li><li class="step "><p>
      Commit your configuration to the local git repo (<a class="xref" href="#using-git" title="Chapter 22. Using Git for Configuration Management">Chapter 22, <em>Using Git for Configuration Management</em></a>), as follows:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div><div id="id-1.3.6.6.4.2.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
       This step needs to be repeated any time you make changes to your
       configuration files before you move on to the following steps. See <a class="xref" href="#using-git" title="Chapter 22. Using Git for Configuration Management">Chapter 22, <em>Using Git for Configuration Management</em></a> for more information.
      </p></div></li></ol></div></div></div><div class="sect1" id="id-1.3.6.6.5"><div class="titlepage"><div><div><h2 class="title"><span class="number">23.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Running the Configuration Processor</span> <a title="Permalink" class="permalink" href="#id-1.3.6.6.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_standalone.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_standalone.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Once you have your configuration files setup, you need to run the
   configuration processor to complete your configuration.
  </p><p>
   When you run the configuration processor, you will be prompted for two
   passwords. Enter the first password to make the configuration processor
   encrypt its sensitive data, which consists of the random inter-service
   passwords that it generates and the ansible <code class="literal">group_vars</code>
   and <code class="literal">host_vars</code> that it produces for subsequent deploy
   runs. You will need this password for subsequent Ansible deploy and
   configuration processor runs. If you wish to change an encryption password
   that you have already used when running the configuration processor then
   enter the new password at the second prompt, otherwise just press
   <span class="keycap">Enter</span> to bypass this.
  </p><p>
   Run the configuration processor with this command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
   For automated installation (for example CI), you can specify the required
   passwords on the ansible command line. For example, the command below will
   disable encryption by the configuration processor:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml \
  -e encrypt="" -e rekey=""</pre></div><p>
   If you receive an error during this step, there is probably an issue with
   one or more of your configuration files. Verify that all information in each
   of your configuration files is correct for your environment. Then commit
   those changes to Git using the instructions in the previous section before
   re-running the configuration processor again.
  </p><p>
   For any troubleshooting information regarding these steps, see
   <a class="xref" href="#sec-trouble-config-processor" title="36.2. Issues while Updating Configuration Files">Section 36.2, “Issues while Updating Configuration Files”</a>.
  </p></div><div class="sect1" id="id-1.3.6.6.6"><div class="titlepage"><div><div><h2 class="title"><span class="number">23.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring TLS</span> <a title="Permalink" class="permalink" href="#id-1.3.6.6.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_standalone.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_standalone.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div id="id-1.3.6.6.6.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    This section is optional, but recommended, for a <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installation.
   </p></div><p>
   After you run the configuration processor the first time, the IP addresses
   for your environment will be generated and populated in the
   <code class="filename">~/openstack/my_cloud/info/address_info.yml</code> file. At
   this point, consider whether to configure TLS and set up an SSL certificate
   for your environment. Please read <a class="xref" href="#tls30" title="Chapter 41. Configuring Transport Layer Security (TLS)">Chapter 41, <em>Configuring Transport Layer Security (TLS)</em></a> before proceeding
   for how to achieve this.
  </p></div><div class="sect1" id="id-1.3.6.6.7"><div class="titlepage"><div><div><h2 class="title"><span class="number">23.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying the Cloud</span> <a title="Permalink" class="permalink" href="#id-1.3.6.6.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_standalone.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_standalone.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Use the playbook below to create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     [OPTIONAL] - Run the <code class="literal">wipe_disks.yml</code> playbook to ensure
     all of your non-OS partitions on your nodes are completely wiped before
     continuing with the installation. The <code class="filename">wipe_disks.yml</code>
     playbook is only meant to be run on systems immediately after running
     <code class="filename">bm-reimage.yml</code>. If used for any other case, it may
     not wipe all of the expected partitions.
    </p><p>
     If you are using fresh machines this step may not be necessary.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml</pre></div><p>
     If you have used an encryption password when running the configuration
     processor use the command below and enter the encryption password when
     prompted:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --ask-vault-pass</pre></div></li><li class="step "><p>
     Run the <code class="literal">site.yml</code> playbook below:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml</pre></div><p>
     If you have used an encryption password when running the configuration
     processor use the command below and enter the encryption password when
     prompted:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --ask-vault-pass</pre></div><div id="id-1.3.6.6.7.2.3.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The step above runs <code class="literal">osconfig</code> to configure the cloud
      and <code class="literal">ardana-deploy</code> to deploy the cloud. Therefore, this
      step may run for a while, perhaps 45 minutes or more, depending on the
      number of nodes in your environment.
     </p></div></li><li class="step "><p>
     Verify that the network is working correctly. Ping each IP in the
     <code class="literal">/etc/hosts</code> file from one of the controller nodes.
    </p></li></ol></div></div><p>
   For any troubleshooting information regarding these steps, see
   <a class="xref" href="#sec-trouble-deploy-cloud" title="36.3. Issues while Deploying the Cloud">Section 36.3, “Issues while Deploying the Cloud”</a>.
  </p></div><div class="sect1" id="id-1.3.6.6.8"><div class="titlepage"><div><div><h2 class="title"><span class="number">23.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing <span class="productname">OpenStack</span> Assets on the Stand-alone Deployer</span> <a title="Permalink" class="permalink" href="#id-1.3.6.6.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_standalone.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_standalone.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The <span class="productname">OpenStack</span> CLI and <span class="productname">OpenStack</span> clients will not be installed
   automatically. If you require access to these clients, you will need to
   follow the procedure below to add the appropriate software.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     [OPTIONAL] To confirm that <span class="productname">OpenStack</span> clients have not been installed,
     connect to your stand-alone deployer and try to use the OpenStack CLI:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/keystone.osrc
<span class="bold"><strong><code class="prompt user">ardana &gt; </code>openstack project list</strong></span>

-bash: openstack: command not found</pre></div></li><li class="step "><p>
     Edit the configuration file containing details of your Control Plane,
     <code class="filename">~/openstack/my_cloud/definition/data/control_plane.yml</code>
    </p></li><li class="step "><p>
     Locate the stanza for the cluster where you want to install the
     client(s). This will look like the following extract:
    </p><div class="verbatim-wrap"><pre class="screen">      clusters:
        - name: cluster0
          cluster-prefix: c0
          server-role: LIFECYCLE-MANAGER-ROLE
          member-count: 1
          allocation-policy: strict
          service-components:
            - ntp-server
            - lifecycle-manager</pre></div></li><li class="step "><p>
     Choose the client(s) you wish to install from the following list of
     available clients:
    </p><div class="verbatim-wrap"><pre class="screen"> - barbican-client
 - ceilometer-client
 - cinder-client
 - designate-client
 - glance-client
 - heat-client
 - ironic-client
 - keystone-client
 - magnum-client
 - manila-client
 - monasca-client
 - neutron-client
 - nova-client
 - ntp-client
 - octavia-client
 - openstack-client
 - swift-client</pre></div></li><li class="step "><p>
     Add the client(s) to the list of <code class="literal">service-components</code> -
     in the following example, several <span class="productname">OpenStack</span> clients are added to the
     stand-alone deployer:
      </p><div class="verbatim-wrap"><pre class="screen">      clusters:
        - name: cluster0
          cluster-prefix: c0
          server-role: LIFECYCLE-MANAGER-ROLE
          member-count: 1
          allocation-policy: strict
          service-components:
            - ntp-server
            - lifecycle-manager
            <span class="bold"><strong>- openstack-client
            - cinder-client
            - designate-client
            - glance-client
            - heat-client
            - ironic-client
            - keystone-client
            - neutron-client
            - nova-client
            - swift-client
            - monasca-client
            - barbican-client
</strong></span></pre></div></li><li class="step "><p>
     Commit the configuration changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Add explicit client service deployment"</pre></div></li><li class="step "><p>
     Run the configuration processor, followed by the
     <code class="literal">ready-deployment</code> playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml -e encrypt="" \
  -e rekey=""
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Add the software for the clients using the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts clients-upgrade.yml</pre></div></li><li class="step "><p>
     Check that the software has been installed correctly. Using the same test
     that was unsuccessful before, connect to your stand-alone deployer and try
     to use the OpenStack CLI:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/keystone.osrc
<code class="prompt user">ardana &gt; </code>openstack project list</pre></div><p>
     You should now see a list of projects returned:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code><span class="bold"><strong>openstack project list</strong></span>

+----------------------------------+------------------+
| ID                               | Name             |
+----------------------------------+------------------+
| 076b6e879f324183bbd28b46a7ee7826 | kronos           |
| 0b81c3a9e59c47cab0e208ea1bb7f827 | backup           |
| 143891c2a6094e2988358afc99043643 | octavia          |
| 1d3972a674434f3c95a1d5ed19e0008f | glance-swift     |
| 2e372dc57cac4915bf06bbee059fc547 | glance-check     |
| 383abda56aa2482b95fb9da0b9dd91f4 | monitor          |
| 606dd3b1fa6146668d468713413fb9a6 | swift-monitor    |
| 87db9d1b30044ea199f0293f63d84652 | admin            |
| 9fbb7494956a483ca731748126f50919 | demo             |
| a59d0c682474434a9ddc240ddfe71871 | services         |
| a69398f0f66a41b2872bcf45d55311a7 | swift-dispersion |
| f5ec48d0328d400992c1c5fb44ec238f | cinderinternal   |
+----------------------------------+------------------+</pre></div></li></ol></div></div></div><div class="sect1" id="id-1.3.6.6.9"><div class="titlepage"><div><div><h2 class="title"><span class="number">23.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Post-Installation Verification and Administration</span> <a title="Permalink" class="permalink" href="#id-1.3.6.6.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_standalone.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_standalone.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   We recommend verifying the installation using the instructions in
   <a class="xref" href="#cloud-verification" title="Chapter 38. Post Installation Tasks">Chapter 38, <em>Post Installation Tasks</em></a>.
  </p><p>
   There are also a list of other common post-installation administrative tasks
   listed in the <a class="xref" href="#postinstall-checklist" title="Chapter 44. Other Common Post-Installation Tasks">Chapter 44, <em>Other Common Post-Installation Tasks</em></a> list.
  </p></div></div><div class="chapter " id="install-kvm"><div class="titlepage"><div><div><h2 class="title"><span class="number">24 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing Mid-scale and Entry-scale KVM</span> <a title="Permalink" class="permalink" href="#install-kvm">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_kvm.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_kvm.xml</li><li><span class="ds-label">ID: </span>install-kvm</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#sec-kvm-important-notes"><span class="number">24.1 </span><span class="name">Important Notes</span></a></span></dt><dt><span class="section"><a href="#sec-kvm-prereqs"><span class="number">24.2 </span><span class="name">Prepare for Cloud Installation</span></a></span></dt><dt><span class="section"><a href="#sec-kvm-configuration"><span class="number">24.3 </span><span class="name">Configuring Your Environment</span></a></span></dt><dt><span class="section"><a href="#sec-kvm-provision"><span class="number">24.4 </span><span class="name">Provisioning Your Baremetal Nodes</span></a></span></dt><dt><span class="section"><a href="#sec-kvm-config-processor"><span class="number">24.5 </span><span class="name">Running the Configuration Processor</span></a></span></dt><dt><span class="section"><a href="#sec-kvm-security"><span class="number">24.6 </span><span class="name">Configuring TLS</span></a></span></dt><dt><span class="section"><a href="#sec-kvm-deploy"><span class="number">24.7 </span><span class="name">Deploying the Cloud</span></a></span></dt><dt><span class="section"><a href="#sec-kvm-configure-backend"><span class="number">24.8 </span><span class="name">Configuring a Block Storage Backend</span></a></span></dt><dt><span class="section"><a href="#sec-kvm-post-installation"><span class="number">24.9 </span><span class="name">Post-Installation Verification and Administration</span></a></span></dt></dl></div></div><div class="sect1" id="sec-kvm-important-notes"><div class="titlepage"><div><div><h2 class="title"><span class="number">24.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Important Notes</span> <a title="Permalink" class="permalink" href="#sec-kvm-important-notes">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_kvm.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_kvm.xml</li><li><span class="ds-label">ID: </span>sec-kvm-important-notes</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     For information about when to use the GUI installer and when to use the
     command line (CLI), see <a class="xref" href="#preinstall-overview" title="Chapter 13. Overview">Chapter 13, <em>Overview</em></a>.
    </p></li><li class="listitem "><p>
     Review the <a class="xref" href="#min-hardware" title="Chapter 2. Hardware and Software Support Matrix">Chapter 2, <em>Hardware and Software Support Matrix</em></a> that we have listed.
    </p></li><li class="listitem "><p>
     Review the release notes to make yourself aware of any known issues and
     limitations.
    </p></li><li class="listitem "><p>
     The installation process can occur in different phases. For example, you
     can install the control plane only and then add Compute nodes afterwards
     if you would like.
    </p></li><li class="listitem "><p>
     If you run into issues during installation, we have put together a list of
     <a class="xref" href="#troubleshooting-installation" title="Chapter 36. Troubleshooting the Installation">Chapter 36, <em>Troubleshooting the Installation</em></a> you can reference.
    </p></li><li class="listitem "><p>
     Make sure all disks on the system(s) are wiped before you begin the
     install. (For swift, refer to <a class="xref" href="#topic-d1s-hht-tt" title="11.6. Swift Requirements for Device Group Drives">Section 11.6, “Swift Requirements for Device Group Drives”</a>.)
    </p></li><li class="listitem "><p>
     There is no requirement to have a dedicated network for OS-install and
     system deployment, this can be shared with the management network. More
     information can be found in <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a>.
    </p></li><li class="listitem "><p>
     The terms deployer and Cloud Lifecycle Manager are used interchangeably. They refer to the
     same nodes in your cloud environment.
    </p></li><li class="listitem "><p>
     When running the Ansible playbook in this installation guide, if a runbook
     fails you will see in the error response to use the
     <code class="literal">--limit</code> switch when retrying a playbook. This should be
     avoided. You can simply re-run any playbook without this switch.
    </p></li><li class="listitem "><p>
     DVR is not supported with ESX compute.
    </p></li><li class="listitem "><p>
     When you attach a cinder volume to the VM running on the ESXi host, the
     volume will not get detected automatically. Make sure to set the image
     metadata <span class="bold"><strong>vmware_adaptertype=lsiLogicsas</strong></span>
     for image before launching the instance. This will help to discover the
     volume change appropriately.
    </p></li><li class="listitem "><p>
     The installation process will create several <span class="productname">OpenStack</span> roles. Not all roles
     will be relevant for a cloud with swift only, but they will not cause
     problems.
    </p></li></ul></div></div><div class="sect1" id="sec-kvm-prereqs"><div class="titlepage"><div><div><h2 class="title"><span class="number">24.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prepare for Cloud Installation</span> <a title="Permalink" class="permalink" href="#sec-kvm-prereqs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_kvm.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_kvm.xml</li><li><span class="ds-label">ID: </span>sec-kvm-prereqs</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Review the <a class="xref" href="#preinstall-checklist" title="Chapter 14. Pre-Installation Checklist">Chapter 14, <em>Pre-Installation Checklist</em></a> about recommended
     pre-installation tasks.
    </p></li><li class="step "><p>
     Prepare the Cloud Lifecycle Manager node. The Cloud Lifecycle Manager must be accessible either directly or via
     <code class="literal">ssh</code>, and have SUSE Linux Enterprise Server 12 SP4 installed. All nodes must
     be accessible to the Cloud Lifecycle Manager. If the nodes do not have direct access to
     online Cloud subscription channels, the Cloud Lifecycle Manager node will need to host the
     Cloud repositories.
    </p><ol type="a" class="substeps "><li class="step "><p>
       If you followed the installation instructions for Cloud Lifecycle Manager server (see <a class="xref" href="#cha-depl-dep-inst" title="Chapter 15. Installing the Cloud Lifecycle Manager server">Chapter 15, <em>Installing the Cloud Lifecycle Manager server</em></a>), <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> software should already be
       installed. Double-check whether SUSE Linux Enterprise and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are properly
       registered at the SUSE Customer Center by starting YaST and running <span class="guimenu ">Software</span> › <span class="guimenu ">Product
       Registration</span>.
      </p><p>
       If you have not yet installed <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, do so by starting YaST and
       running <span class="guimenu ">Software</span> › <span class="guimenu ">Product
       Registration</span> › <span class="guimenu ">Select
       Extensions</span>. Choose <span class="guimenu "><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span>
       and follow the on-screen instructions. Make sure to register
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> during the installation process and to install the software
       pattern <code class="literal">patterns-cloud-ardana</code>.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper -n in patterns-cloud-ardana</pre></div></li><li class="step "><p>
       Ensure the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> media repositories and updates repositories are
       made available to all nodes in your deployment. This can be accomplished
       either by configuring the Cloud Lifecycle Manager server as an SMT mirror as described in
       <a class="xref" href="#app-deploy-smt-lcm" title="Chapter 16. Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)">Chapter 16, <em>Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</em></a> or by syncing or mounting the
       Cloud and updates repositories to the Cloud Lifecycle Manager server as described in <a class="xref" href="#cha-depl-repo-conf-lcm" title="Chapter 17. Software Repository Setup">Chapter 17, <em>Software Repository Setup</em></a>.
      </p></li><li class="step "><p>
       Configure passwordless <code class="command">sudo</code> for the user created when
       setting up the node (as described in <a class="xref" href="#sec-depl-adm-inst-user" title="15.4. Creating a User">Section 15.4, “Creating a User”</a>). Note that this is
       <span class="emphasis"><em>not</em></span> the user <code class="systemitem">ardana</code> that will be used later in this
       procedure. In the following we assume you named the user <code class="systemitem">cloud</code>. Run the command
       <code class="command">visudo</code> as user <code class="systemitem">root</code> and add the following line
       to the end of the file:
      </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable ">CLOUD</em> ALL = (root) NOPASSWD:ALL</pre></div><p>
       Make sure to replace <em class="replaceable ">CLOUD</em> with your user name
       choice.
      </p></li><li class="step "><p>
       Set the password for the user
       <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo passwd ardana</pre></div></li><li class="step "><p>
       Become the user <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>su - ardana</pre></div></li><li class="step "><p>
       Place a copy of the SUSE Linux Enterprise Server 12 SP4 <code class="filename">.iso</code> in the
       <code class="systemitem">ardana</code> home directory,
       <code class="filename">var/lib/ardana</code>, and rename it to
       <code class="filename">sles12sp4.iso</code>.
      </p></li><li class="step "><p>
       Install the templates, examples, and working model directories:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>/usr/bin/ardana-init</pre></div></li></ol></li></ol></div></div></div><div class="sect1" id="sec-kvm-configuration"><div class="titlepage"><div><div><h2 class="title"><span class="number">24.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Your Environment</span> <a title="Permalink" class="permalink" href="#sec-kvm-configuration">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_kvm.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_kvm.xml</li><li><span class="ds-label">ID: </span>sec-kvm-configuration</li></ul></div></div></div></div><p>
   During the configuration phase of the installation you will be making
   modifications to the example configuration input files to match your cloud
   environment. You should use the <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a>
   documentation for detailed information on how to do this. There is also a
   <code class="filename">README.md</code> file included in each of the example
   directories on the Cloud Lifecycle Manager that has useful information about the models.
  </p><p>
   In the steps below we show how to set up the directory structure with the
   example input files as well as use the optional encryption methods for your
   sensitive data.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Set up your configuration files, as follows:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Copy the example configuration files into the required setup directory
       and edit them to contain the details of your environment.
      </p><p>
       For example, if you want to use the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Mid-scale KVM model,
       you can use this command to copy the files to your cloud definition
       directory:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cp -r ~/openstack/examples/mid-scale-kvm/* \
~/openstack/my_cloud/definition/</pre></div><p>
       If you want to use the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale KVM model, you can use
       this command to copy the files to your cloud definition directory:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cp -r ~/openstack/examples/entry-scale-kvm/* \
~/openstack/my_cloud/definition/</pre></div></li><li class="step "><p>
       Begin inputting your environment information into the configuration
       files in the <code class="filename">~/openstack/my_cloud/definition</code>
       directory.
      </p></li></ol></li><li class="step "><p><span class="step-optional">(Optional)</span> 
     You can use the <code class="literal">ardanaencrypt.py</code> script to
     encrypt your IPMI passwords. This script uses OpenSSL.
    </p><ol type="a" class="substeps "><li class="step "><p>
       Change to the Ansible directory:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible</pre></div></li><li class="step "><p>
       Put the encryption key into the following environment variable:
      </p><div class="verbatim-wrap"><pre class="screen">export ARDANA_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</pre></div></li><li class="step "><p>
       Run the python script below and follow the instructions. Enter a
       password that you want to encrypt.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>./ardanaencrypt.py</pre></div></li><li class="step "><p>
       Take the string generated and place it in the
       <code class="literal">ilo-password</code> field in your
       <code class="filename">~/openstack/my_cloud/definition/data/servers.yml</code>
       file, remembering to enclose it in quotes.
      </p></li><li class="step "><p>
       Repeat the above for each server.
      </p><div id="id-1.3.6.7.4.4.2.2.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
        Before you run any playbooks, remember that you need to export the
        encryption key in the following environment variable: <code class="literal">export
        ARDANA_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</code>
       </p></div></li></ol></li><li class="step "><p>
     Commit your configuration to the local git repo
     (<a class="xref" href="#using-git" title="Chapter 22. Using Git for Configuration Management">Chapter 22, <em>Using Git for Configuration Management</em></a>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div><div id="id-1.3.6.7.4.4.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      This step needs to be repeated any time you make changes to your
      configuration files before you move on to the following steps. See
      <a class="xref" href="#using-git" title="Chapter 22. Using Git for Configuration Management">Chapter 22, <em>Using Git for Configuration Management</em></a> for more information.
     </p></div></li></ol></div></div></div><div class="sect1" id="sec-kvm-provision"><div class="titlepage"><div><div><h2 class="title"><span class="number">24.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Provisioning Your Baremetal Nodes</span> <a title="Permalink" class="permalink" href="#sec-kvm-provision">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_kvm.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_kvm.xml</li><li><span class="ds-label">ID: </span>sec-kvm-provision</li></ul></div></div></div></div><p>
   To provision the baremetal nodes in your cloud deployment you can either use
   the automated operating system installation process provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> or
   you can use the 3rd party installation tooling of your choice. We will
   outline both methods below:
  </p><div class="sect2" id="id-1.3.6.7.5.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">24.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using Third Party Baremetal Installers</span> <a title="Permalink" class="permalink" href="#id-1.3.6.7.5.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_kvm.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_kvm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    If you do not wish to use the automated operating system installation
    tooling included with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> then the requirements that have to be met
    using the installation tooling of your choice are:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      The operating system must be installed via the SLES ISO provided on
      the <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a>.
     </p></li><li class="listitem "><p>
      Each node must have SSH keys in place that allows the same user from the
      Cloud Lifecycle Manager node who will be doing the deployment to SSH to each node without a
      password.
     </p></li><li class="listitem "><p>
      Passwordless sudo needs to be enabled for the user.
     </p></li><li class="listitem "><p>
      There should be a LVM logical volume as <code class="literal">/root</code> on each
      node.
     </p></li><li class="listitem "><p>
      If the LVM volume group name for the volume group holding the
      <code class="literal">root</code> LVM logical volume is
      <code class="literal">ardana-vg</code>, then it will align with the disk input
      models in the examples.
     </p></li><li class="listitem "><p>
      <span class="phrase">Ensure that <code class="literal">openssh-server</code>,
      <code class="literal">python</code>, <code class="literal">python-apt</code>, and
      <code class="literal">rsync</code> are installed.</span>
     </p></li></ul></div><p>
    If you chose this method for installing your baremetal hardware, skip
    forward to the step
    <em class="citetitle ">Running the Configuration Processor</em>.
   </p></div><div class="sect2" id="id-1.3.6.7.5.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">24.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Automated Operating System Installation Provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <a title="Permalink" class="permalink" href="#id-1.3.6.7.5.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_kvm.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_kvm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    If you would like to use the automated operating system installation tools
    provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, complete the steps below.
   </p><div class="sect3" id="id-1.3.6.7.5.4.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">24.4.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Cobbler</span> <a title="Permalink" class="permalink" href="#id-1.3.6.7.5.4.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_kvm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_kvm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
     This phase of the install process takes the baremetal information that was
     provided in <code class="literal">servers.yml</code> and installs the Cobbler
     provisioning tool and loads this information into Cobbler. This sets each
     node to <code class="literal">netboot-enabled: true</code> in Cobbler. Each node
     will be automatically marked as <code class="literal">netboot-enabled: false</code>
     when it completes its operating system install successfully. Even if the
     node tries to PXE boot subsequently, Cobbler will not serve it. This is
     deliberate so that you cannot reimage a live node by accident.
    </p><p>
     The <code class="literal">cobbler-deploy.yml</code> playbook prompts for a password
     - this is the password that will be encrypted and stored in Cobbler, which
     is associated with the user running the command on the Cloud Lifecycle Manager, that you
     will use to log in to the nodes via their consoles after install. The
     username is the same as the user set up in the initial dialogue when
     installing the Cloud Lifecycle Manager from the ISO, and is the same user that is running
     the cobbler-deploy play.
    </p><div id="id-1.3.6.7.5.4.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      When imaging servers with your own tooling, it is still necessary to have
      ILO/IPMI settings for all nodes. Even if you are not using Cobbler, the
      username and password fields in <code class="filename">servers.yml</code> need to
      be filled in with dummy settings. For example, add the following to
      <code class="filename">servers.yml</code>:
     </p><div class="verbatim-wrap"><pre class="screen">ilo-user: manual
ilo-password: deployment</pre></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Run the following playbook which confirms that there is IPMI connectivity
       for each of your nodes so that they are accessible to be re-imaged in a
       later step:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-power-status.yml</pre></div></li><li class="step "><p>
       Run the following playbook to deploy Cobbler:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li></ol></div></div></div><div class="sect3" id="id-1.3.6.7.5.4.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">24.4.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Imaging the Nodes</span> <a title="Permalink" class="permalink" href="#id-1.3.6.7.5.4.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_kvm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_kvm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
     This phase of the install process goes through a number of distinct steps:
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Powers down the nodes to be installed
      </p></li><li class="step "><p>
       Sets the nodes hardware boot order so that the first option is a network
       boot.
      </p></li><li class="step "><p>
       Powers on the nodes. (The nodes will then boot from the network and be
       installed using infrastructure set up in the previous phase)
      </p></li><li class="step "><p>
       Waits for the nodes to power themselves down (this indicates a
       successful install). This can take some time.
      </p></li><li class="step "><p>
       Sets the boot order to hard disk and powers on the nodes.
      </p></li><li class="step "><p>
       Waits for the nodes to be reachable by SSH and verifies that they have the
       signature expected.
      </p></li></ol></div></div><p>
     Deploying nodes has been automated in the Cloud Lifecycle Manager and requires the
     following:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       All of your nodes using SLES must already be installed, either
       manually or via Cobbler.
      </p></li><li class="listitem "><p>
       Your input model should be configured for your SLES nodes.
      </p></li><li class="listitem "><p>
       You should have run the configuration processor and the
       <code class="filename">ready-deployment.yml</code> playbook.
      </p></li></ul></div><p>
     Execute the following steps to re-image one or more nodes after you have
     run the <code class="filename">ready-deployment.yml</code> playbook.
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Run the following playbook, specifying your SLES nodes using the
       nodelist. This playbook will reconfigure Cobbler for the nodes listed.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook prepare-sles-grub2.yml -e \
      nodelist=node1[,node2,node3]</pre></div></li><li class="step "><p>
       Re-image the node(s) with the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml \
      -e nodelist=node1[,node2,node3]</pre></div></li></ol></div></div><p>
     If a nodelist is not specified then the set of nodes in Cobbler with
     <code class="literal">netboot-enabled: True</code> is selected. The playbook pauses
     at the start to give you a chance to review the set of nodes that it is
     targeting and to confirm that it is correct.
    </p><p>
     You can use the command below which will list all of your nodes with the
     <code class="literal">netboot-enabled: True</code> flag set:
    </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system find --netboot-enabled=1</pre></div></div></div></div><div class="sect1" id="sec-kvm-config-processor"><div class="titlepage"><div><div><h2 class="title"><span class="number">24.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Running the Configuration Processor</span> <a title="Permalink" class="permalink" href="#sec-kvm-config-processor">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_kvm.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_kvm.xml</li><li><span class="ds-label">ID: </span>sec-kvm-config-processor</li></ul></div></div></div></div><p>
   Once you have your configuration files setup, you need to run the
   configuration processor to complete your configuration.
  </p><p>
   When you run the configuration processor, you will be prompted for two
   passwords. Enter the first password to make the configuration processor
   encrypt its sensitive data, which consists of the random inter-service
   passwords that it generates and the ansible <code class="literal">group_vars</code>
   and <code class="literal">host_vars</code> that it produces for subsequent deploy
   runs. You will need this password for subsequent Ansible deploy and
   configuration processor runs. If you wish to change an encryption password
   that you have already used when running the configuration processor then
   enter the new password at the second prompt, otherwise just press
   <span class="keycap">Enter</span> to bypass this.
  </p><p>
   Run the configuration processor with this command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
   For automated installation (for example CI), you can specify the required
   passwords on the ansible command line. For example, the command below will
   disable encryption by the configuration processor:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml \
  -e encrypt="" -e rekey=""</pre></div><p>
   If you receive an error during this step, there is probably an issue with
   one or more of your configuration files. Verify that all information in each
   of your configuration files is correct for your environment. Then commit
   those changes to Git using the instructions in the previous section before
   re-running the configuration processor again.
  </p><p>
   For any troubleshooting information regarding these steps, see
   <a class="xref" href="#sec-trouble-config-processor" title="36.2. Issues while Updating Configuration Files">Section 36.2, “Issues while Updating Configuration Files”</a>.
  </p></div><div class="sect1" id="sec-kvm-security"><div class="titlepage"><div><div><h2 class="title"><span class="number">24.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring TLS</span> <a title="Permalink" class="permalink" href="#sec-kvm-security">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_kvm.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_kvm.xml</li><li><span class="ds-label">ID: </span>sec-kvm-security</li></ul></div></div></div></div><div id="id-1.3.6.7.7.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    This section is optional, but recommended, for a <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installation.
   </p></div><p>
   After you run the configuration processor the first time, the IP addresses
   for your environment will be generated and populated in the
   <code class="filename">~/openstack/my_cloud/info/address_info.yml</code> file. At
   this point, consider whether to configure TLS and set up an SSL certificate
   for your environment. Please read <a class="xref" href="#tls30" title="Chapter 41. Configuring Transport Layer Security (TLS)">Chapter 41, <em>Configuring Transport Layer Security (TLS)</em></a> before proceeding
   for how to achieve this.
  </p></div><div class="sect1" id="sec-kvm-deploy"><div class="titlepage"><div><div><h2 class="title"><span class="number">24.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying the Cloud</span> <a title="Permalink" class="permalink" href="#sec-kvm-deploy">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_kvm.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_kvm.xml</li><li><span class="ds-label">ID: </span>sec-kvm-deploy</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Use the playbook below to create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     [OPTIONAL] - Run the <code class="literal">wipe_disks.yml</code> playbook to ensure
     all of your non-OS partitions on your nodes are completely wiped before
     continuing with the installation. The <code class="filename">wipe_disks.yml</code>
     playbook is only meant to be run on systems immediately after running
     <code class="filename">bm-reimage.yml</code>. If used for any other case, it may
     not wipe all of the expected partitions.
    </p><p>
     If you are using fresh machines this step may not be necessary.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml</pre></div><p>
     If you have used an encryption password when running the configuration
     processor use the command below and enter the encryption password when
     prompted:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --ask-vault-pass</pre></div></li><li class="step "><p>
     Run the <code class="literal">site.yml</code> playbook below:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml</pre></div><p>
     If you have used an encryption password when running the configuration
     processor use the command below and enter the encryption password when
     prompted:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --ask-vault-pass</pre></div><div id="id-1.3.6.7.8.2.3.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The step above runs <code class="literal">osconfig</code> to configure the cloud
      and <code class="literal">ardana-deploy</code> to deploy the cloud. Therefore, this
      step may run for a while, perhaps 45 minutes or more, depending on the
      number of nodes in your environment.
     </p></div></li><li class="step "><p>
     Verify that the network is working correctly. Ping each IP in the
     <code class="literal">/etc/hosts</code> file from one of the controller nodes.
    </p></li></ol></div></div><p>
   For any troubleshooting information regarding these steps, see
   <a class="xref" href="#sec-trouble-deploy-cloud" title="36.3. Issues while Deploying the Cloud">Section 36.3, “Issues while Deploying the Cloud”</a>.
  </p></div><div class="sect1" id="sec-kvm-configure-backend"><div class="titlepage"><div><div><h2 class="title"><span class="number">24.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring a Block Storage Backend</span> <a title="Permalink" class="permalink" href="#sec-kvm-configure-backend">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_kvm.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_kvm.xml</li><li><span class="ds-label">ID: </span>sec-kvm-configure-backend</li></ul></div></div></div></div><p>
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> supports multiple Block Storage backend options. You can use one or
    more of these for setting up multiple Block Storage backends. Multiple
    volume types are also supported. For more information on configuring backends,
    see <a class="link" href="https://docs.openstack.org/cinder/latest/admin/blockstorage-multi-backend.html" target="_blank">Configure multiple-storage back ends</a>
   </p><div class="procedure " id="id-1.3.6.7.9.3"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 24.1: </span><span class="name">Specifying the <code class="literal">default_volume_type</code> </span><a title="Permalink" class="permalink" href="#id-1.3.6.7.9.3">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
        Create a volume type:
       </p><div class="verbatim-wrap"><pre class="screen">         openstack volume type create --public NAME</pre></div></li><li class="step "><p>
         Volume types do not have to be tied to a backend. They can contain
         attributes that can be applied to a volume during creation, these are referred
         to as extra specs. One of those attributes is the <code class="literal">volume_backend_name</code>.
         By setting the value to the same as the <code class="literal">volume_backend_name</code> of
         a specific backend as defined in <code class="literal">cinder.conf</code>, then
         volumes created with that type will always land on that backend.
         For example, if the <code class="literal">cinder.conf</code> has an LVM volume
         backend defined as:
       </p><div class="verbatim-wrap"><pre class="screen">         [lvmdriver-1]
         image_volume_cache_enabled = True
         volume_clear = zero
         lvm_type = auto
         target_helper = lioadm
         volume_group = stack-volumes-lvmdriver-1
         volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver
         volume_backend_name = lvmdriver-1</pre></div><p>
         You can tie a volume type to that specific backend by doing:
       </p><div class="verbatim-wrap"><pre class="screen">         $ openstack volume type create --public my_lvm_type
         $ openstack volume type set --property volume_backend_name=lvmdriver-1 my_lvm_type
         $ openstack volume type show my_lvm_type
         +--------------------+--------------------------------------+
         | Field              | Value                                |
         +--------------------+--------------------------------------+
         | access_project_ids | None                                 |
         | description        | None                                 |
         | id                 | a2049509-7789-4949-95e2-89cf7fd5792f |
         | is_public          | True                                 |
         | name               | my_lvm_type                          |
         | properties         | volume_backend_name='lvmdriver-1'    |
         | qos_specs_id       | None                                 |
         +--------------------+--------------------------------------+</pre></div></li></ol></div></div></div><div class="sect1" id="sec-kvm-post-installation"><div class="titlepage"><div><div><h2 class="title"><span class="number">24.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Post-Installation Verification and Administration</span> <a title="Permalink" class="permalink" href="#sec-kvm-post-installation">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_kvm.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_kvm.xml</li><li><span class="ds-label">ID: </span>sec-kvm-post-installation</li></ul></div></div></div></div><p>
   We recommend verifying the installation using the instructions in
   <a class="xref" href="#cloud-verification" title="Chapter 38. Post Installation Tasks">Chapter 38, <em>Post Installation Tasks</em></a>.
  </p><p>
   There are also a list of other common post-installation administrative tasks
   listed in the <a class="xref" href="#postinstall-checklist" title="Chapter 44. Other Common Post-Installation Tasks">Chapter 44, <em>Other Common Post-Installation Tasks</em></a> list.
  </p></div></div><div class="chapter " id="DesignateInstallOverview"><div class="titlepage"><div><div><h2 class="title"><span class="number">25 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DNS Service Installation Overview</span> <a title="Permalink" class="permalink" href="#DesignateInstallOverview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-designate-designate_install_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-designate-designate_install_overview.xml</li><li><span class="ds-label">ID: </span>DesignateInstallOverview</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#DesignateBIND"><span class="number">25.1 </span><span class="name">Installing the DNS Service with BIND</span></a></span></dt><dt><span class="section"><a href="#DesignatePowerDNS"><span class="number">25.2 </span><span class="name">Install the DNS Service with PowerDNS</span></a></span></dt><dt><span class="section"><a href="#DNS-NS"><span class="number">25.3 </span><span class="name">Configure DNS Domain and NS Records</span></a></span></dt><dt><span class="section"><a href="#DNS-MIGRATE"><span class="number">25.4 </span><span class="name">Migrate Zone/Pool to Worker/Producer after Upgrade</span></a></span></dt></dl></div></div><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> DNS Service supports several different backends for domain name
  service. The choice of backend must be included in the deployment model
  before the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> install is completed.
 </p><div id="id-1.3.6.8.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
   By default any user in the project is allowed to manage a DNS domain. This
   can be changed by updating the Policy.json file for designate.
  </p></div><p>
  The backends that are available within the DNS Service are separated into two
  categories, self-contained and external.
 </p><div class="table" id="DNSBackendTable"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 25.1: </span><span class="name">DNS Backends </span><a title="Permalink" class="permalink" href="#DNSBackendTable">#</a></h6></div><div class="table-contents"><table class="table" summary="DNS Backends" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /></colgroup><thead><tr><th>Category</th><th>Backend</th><th>Description</th><th>Recommended For</th></tr></thead><tbody><tr><td>Self-contained</td><td>BIND 9.x</td><td>
      All components necessary will be installed and configured as part of
      the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> install.
     </td><td>
      POCs and customers who wish to keep cloud and traditional DNS separated.
     </td></tr><tr><td>External</td><td>InfoBlox</td><td>
      The authoritative DNS server itself is external to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. Management
      and configuration is out of scope for the Cloud Lifecycle Manager but remains
      the responsibility of the customer.
     </td><td>
      Customers who wish to integrate with their existing DNS infrastructure.
     </td></tr></tbody></table></div></div><div class="sect1" id="DesignateBIND"><div class="titlepage"><div><div><h2 class="title"><span class="number">25.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing the DNS Service with BIND</span> <a title="Permalink" class="permalink" href="#DesignateBIND">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-designate-install_designate_BIND.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-designate-install_designate_BIND.xml</li><li><span class="ds-label">ID: </span>DesignateBIND</li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> DNS Service defaults to the BIND back-end if another back-end is
   not configured for domain name service. BIND will be deployed to one or
   more control planes clusters. The following configuration example shows how
   the BIND service is installed.
 </p><div class="sect2" id="sec-bind-configure-back-end"><div class="titlepage"><div><div><h3 class="title"><span class="number">25.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Back-end</span> <a title="Permalink" class="permalink" href="#sec-bind-configure-back-end">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-designate-install_designate_BIND.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-designate-install_designate_BIND.xml</li><li><span class="ds-label">ID: </span>sec-bind-configure-back-end</li></ul></div></div></div></div><p>
   Ensure the DNS Service components and the BIND component have been placed
   on a cluster. BIND can be placed on a cluster separate from the other DNS
   service components.
  </p><div class="verbatim-wrap"><pre class="screen">control-planes:
          - name: control-plane-1
          region-name: region1

          clusters:
          - name: cluster1
          service-components:
          - lifecycle-manager
          - mariadb
          - ip-cluster
          - apache2
          - ...
          - designate-api
          - designate-central
          - designate-pool-manager
          - designate-zone-manager
          - designate-mdns
          - designate-client
          - bind</pre></div><p>
   <span class="bold"><strong>Updating the Input Model</strong></span>
  </p><p>
   When the back-end is configured, add <code class="literal">bind-ext</code> to the file
   <code class="filename">network_groups.yml</code>.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Edit
     <code class="filename">~/openstack/my_cloud/definition/data/network_groups.yml</code>
     to add <code class="literal">bind-ext</code> to component-endpoints.
    </p><div class="verbatim-wrap"><pre class="screen">name: EXTERNAL-API
hostname-suffix: extapi
component-endpoints:
- bind-ext</pre></div></li><li class="step "><p>
     Save the file.
    </p></li></ol></div></div></div></div><div class="sect1" id="DesignatePowerDNS"><div class="titlepage"><div><div><h2 class="title"><span class="number">25.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Install the DNS Service with PowerDNS</span> <a title="Permalink" class="permalink" href="#DesignatePowerDNS">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-designate-install_designate_PowerDNS.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-designate-install_designate_PowerDNS.xml</li><li><span class="ds-label">ID: </span>DesignatePowerDNS</li></ul></div></div></div></div><div class="sect2" id="id-1.3.6.8.7.2"><div class="titlepage"><div><div><h3 class="title"><span class="number">25.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing DNS Service with PowerDNS</span> <a title="Permalink" class="permalink" href="#id-1.3.6.8.7.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-designate-install_designate_PowerDNS.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-designate-install_designate_PowerDNS.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> DNS Service and <span class="bold"><strong>PowerDNS</strong></span> can be
   installed together instead of the default
   <span class="bold"><strong>BIND</strong></span> backend. PowerDNS will be deployed to
   one or more control planes clusters. The following configuration example
   shows how the PowerDNS service is installed.
  </p></div><div class="sect2" id="id-1.3.6.8.7.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">25.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure the Backend</span> <a title="Permalink" class="permalink" href="#id-1.3.6.8.7.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-designate-install_designate_PowerDNS.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-designate-install_designate_PowerDNS.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To configure the backend for PowerDNS, follow these steps.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Ensure the DNS Service components and the PowerDNS component have been
     placed on a cluster. PowerDNS may be placed on a separate cluster to the
     other DNS Service components. Ensure the default
     <span class="bold"><strong>bind</strong></span> component has been removed.
    </p><div class="verbatim-wrap"><pre class="screen">control-planes:
          - name: control-plane-1
          region-name: region1

          clusters:
          - name: cluster1
          service-components:
          - lifecycle-manager
          - mariadb
          - ip-cluster
          - apache2
          - ...
          - designate-api
          - designate-central
          - designate-pool-manager
          - designate-zone-manager
          - designate-mdns
          - designate-client
          - powerdns</pre></div></li><li class="step "><p>
     Edit the
     <code class="filename">~/openstack/my_cloud/definitions/data/network_groups.yml</code>
     file to include the powerdns-ext.
    </p><div class="verbatim-wrap"><pre class="screen">- name: EXTERNAL-API
hostname-suffix: extapi
component-endpoints:
 - powerdns-ext
load-balancers:
 - provider: ip-cluster</pre></div></li><li class="step "><p>
     Edit the
     <code class="filename">~/openstack/my_cloud/definitions/data/firewall_rules.yml</code>
     to allow UDP/TCP access.
    </p><div class="verbatim-wrap"><pre class="screen">	    - name: DNSudp
      # network-groups lists the network group names that the rules apply to
      network-groups:
      - EXTERNAL-API
      rules:
      - type: allow
        # range of remote addresses in CIDR format that this rule applies to
        remote-ip-prefix:  0.0.0.0/0
        port-range-min: 53
        port-range-max: 53
        protocol: udp

    - name: DNStcp
      # network-groups lists the network group names that the rules apply to
      network-groups:
      - EXTERNAL-API
      rules:
      - type: allow
        # range of remote addresses in CIDR format that this rule applies to
        remote-ip-prefix:  0.0.0.0/0
        port-range-min: 53
        port-range-max: 53
        protocol: tcp</pre></div></li></ol></div></div><p>
   Please see <span class="intraxref">Book “Operations Guide CLM”, Chapter 10 “Managing Networking”, Section 10.3 “DNS Service Overview”, Section 10.3.2 “designate Initial Configuration”</span> for post-installation
   DNS Service configuration.
  </p></div></div><div class="sect1" id="DNS-NS"><div class="titlepage"><div><div><h2 class="title"><span class="number">25.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure DNS Domain and NS Records</span> <a title="Permalink" class="permalink" href="#DNS-NS">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-designate-designate_cfg_dns_ns.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-designate-designate_cfg_dns_ns.xml</li><li><span class="ds-label">ID: </span>DNS-NS</li></ul></div></div></div></div><p>
  To configure the default DNS domain and Name Server records for the default
  pool, follow these steps.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Ensure that <code class="literal">designate_config.yml</code> file is present in the
    <code class="literal">~/openstack/my_cloud/definition/data/designate</code> folder. If
    the file or folder is not present, create the folder and copy
    <code class="literal">designate_config.yml</code> file from one of the example input
    models (for example,
    <code class="filename">~/openstack/examples/entry-scale-kvm/data/designate/designate_config.yml</code>).
   </p></li><li class="step "><p>
    Modify the <span class="bold"><strong>dns_domain</strong></span> and/or
    <span class="bold"><strong>ns_records</strong></span> entries in the
    <code class="filename">designate_config.yml</code> file.
   </p><div class="verbatim-wrap"><pre class="screen">data:
dns_domain: example.org.
ns_records:
    hostname: ns1.example.org.
    priority: 1
    hostname: ns2.example.org.
    priority: 2</pre></div></li><li class="step "><p>
    Edit your input model's <code class="filename">control_plane.yml</code> file to
    include <span class="bold"><strong>DESIGNATE-CONFIG-CP1</strong></span> in
    <span class="bold"><strong>configuration-data</strong></span> section.
   </p><div class="verbatim-wrap"><pre class="screen">control-planes:
   - name: control-plane-1
     region-name: region1
     lifecycle-manager-target
     configuration-data:
        - DESIGNATE-CONFIG-CP1
        - NEUTRON-CONFIG-CP1</pre></div></li><li class="step "><p>
    Continue your cloud deployment by reviewing and committing your changes.
   </p><div class="verbatim-wrap"><pre class="screen">$ git add ~/openstack/my_cloud/definition/data/designate/designate_config.yml
$ git commit -m "Adding DNS Domain and NS Records"</pre></div></li></ol></div></div><div id="id-1.3.6.8.8.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   In an entry-scale model (<a class="xref" href="#entry-scale-kvm" title="9.3.1. Entry-Scale Cloud">Section 9.3.1, “Entry-Scale Cloud”</a>),
   you will have 3 ns_records since the DNS service runs on all three
   control planes.
  </p><p>
   In a mid-scale model (<a class="xref" href="#mid-scale-kvm" title="9.3.3. Single-Region Mid-Size Model">Section 9.3.3, “Single-Region Mid-Size Model”</a>) or
   dedicated metering, monitoring and logging model
   (<a class="xref" href="#entry-scale-kvm-mml" title="9.3.2. Entry Scale Cloud with Metering and Monitoring Services">Section 9.3.2, “Entry Scale Cloud with Metering and Monitoring Services”</a>), the above example would be
   correct since there are only two controller nodes.
  </p></div></div><div class="sect1" id="DNS-MIGRATE"><div class="titlepage"><div><div><h2 class="title"><span class="number">25.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Migrate Zone/Pool to Worker/Producer after Upgrade</span> <a title="Permalink" class="permalink" href="#DNS-MIGRATE">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-designate-designate_migrate_zone_pool_to_worker_producer.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-designate-designate_migrate_zone_pool_to_worker_producer.xml</li><li><span class="ds-label">ID: </span>DNS-MIGRATE</li></ul></div></div></div></div><p>
  After upgrade, the following steps may be used to migrate the zone/pool 
  scheme to the worker/producer scheme:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Edit the file
     <code class="filename">~/openstack/my_cloud/definitions/data/control_plane.yml</code> 
     to replace <code class="literal">designate-pool-manager</code> to <code class="literal">designate-worker</code> and <code class="literal">designate-zone-manager</code> to <code class="literal">designate-producer</code>
    </p><div class="verbatim-wrap"><pre class="screen">control-planes:
          - name: control-plane-1
          region-name: region1

          clusters:
          - name: cluster1
          service-components:
          - lifecycle-manager
          - mariadb
          - ip-cluster
          - apache2
          - ...
          - designate-api
          - designate-central
          - designate-worker
          - designate-producer
          - designate-mdns
          - designate-client
          - powerdns</pre></div></li><li class="step "><p>
     Commit your changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git commit -m "<em class="replaceable ">COMMIT_MESSAGE</em>"</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the <code class="literal">designate-migrate.yml</code> playbook to migrate 
     the designate services:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts designate-migrate.yml</pre></div></li></ol></div></div></div></div><div class="chapter " id="MagnumOverview"><div class="titlepage"><div><div><h2 class="title"><span class="number">26 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Magnum Overview</span> <a title="Permalink" class="permalink" href="#MagnumOverview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-magnum-magnum_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-magnum-magnum_overview.xml</li><li><span class="ds-label">ID: </span>MagnumOverview</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#MagnumArchitecture"><span class="number">26.1 </span><span class="name">Magnum Architecture</span></a></span></dt><dt><span class="section"><a href="#MagnumInstall"><span class="number">26.2 </span><span class="name">Install the Magnum Service</span></a></span></dt><dt><span class="section"><a href="#MagnumIntegrateDNS"><span class="number">26.3 </span><span class="name">Integrate Magnum with the DNS Service</span></a></span></dt></dl></div></div><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Magnum Service provides container orchestration engines such as
  Docker Swarm, Kubernetes, and Apache Mesos available as first class
  resources. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Magnum uses heat to orchestrate an OS image which
  contains Docker and Kubernetes and runs that image in either virtual machines
  or bare metal in a cluster configuration.
 </p><div class="sect1" id="MagnumArchitecture"><div class="titlepage"><div><div><h2 class="title"><span class="number">26.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Magnum Architecture</span> <a title="Permalink" class="permalink" href="#MagnumArchitecture">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-magnum-magnum_architecture.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-magnum-magnum_architecture.xml</li><li><span class="ds-label">ID: </span>MagnumArchitecture</li></ul></div></div></div></div><p>
  As an OpenStack API service, Magnum provides Container as a Service (CaaS)
  functionality. Magnum is capable of working with container orchestration
  engines (COE) such as Kubernetes, Docker Swarm, and Apache Mesos. Some
  operations work with a User CRUD (Create, Read, Update, Delete) filter.
 </p><p>
  <span class="bold"><strong>Components</strong></span>
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <span class="bold"><strong>Magnum API</strong></span>: RESTful API for cluster and
    cluster template operations.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Magnum Conductor</strong></span>: Performs operations on
    clusters requested by Magnum API in an asynchronous manner.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Magnum CLI</strong></span>: Command-line interface to the
    Magnum API.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Etcd (planned, currently using public
    service)</strong></span>: Remote key/value storage for distributed cluster
    bootstrap and discovery.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Kubemaster (in case of Kubernetes COE)</strong></span>:
    One or more VM(s) or baremetal server(s), representing a control plane for
    Kubernetes cluster.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Kubeminion (in case of Kubernetes COE)</strong></span>:
    One or more VM(s) or baremetal server(s), representing a workload node for
    Kubernetes cluster.
   </p></li></ul></div><div class="table" id="table-ebc-x5v-jz"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 26.1: </span><span class="name">Data </span><a title="Permalink" class="permalink" href="#table-ebc-x5v-jz">#</a></h6></div><div class="table-contents"><table class="table" summary="Data" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /><col class="c6" /></colgroup><thead><tr><th>Data Name</th><th>Confidentiality</th><th>Integrity</th><th>Availability</th><th>Backup?</th><th>Description</th></tr></thead><tbody><tr><td>Session Tokens</td><td>Confidential</td><td>High</td><td>Medium</td><td>No</td><td>Session tokens not stored.</td></tr><tr><td>System Request</td><td>Confidential</td><td>High</td><td>Medium</td><td>No</td><td>Data in motion or in MQ not stored.</td></tr><tr><td>MariaDB Database "Magnum"</td><td>Confidential</td><td>High</td><td>High</td><td>Yes</td><td>Contains user preferences. Backed up to swift daily.</td></tr><tr><td>etcd data</td><td>Confidential</td><td>High</td><td>Low</td><td>No</td><td>Kubemaster IPs and cluster info. Only used during cluster bootstrap.</td></tr></tbody></table></div></div><div class="figure" id="magnum-service-arch-diagram"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/media-magnum-magnum_service_arch_diagram.png" target="_blank"><img src="images/media-magnum-magnum_service_arch_diagram.png" width="" alt="Service Architecture Diagram for Kubernetes" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 26.1: </span><span class="name">Service Architecture Diagram for Kubernetes </span><a title="Permalink" class="permalink" href="#magnum-service-arch-diagram">#</a></h6></div></div><div class="table" id="table-fst-gxv-jz"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 26.2: </span><span class="name">Interfaces </span><a title="Permalink" class="permalink" href="#table-fst-gxv-jz">#</a></h6></div><div class="table-contents"><table class="table" summary="Interfaces" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /></colgroup><thead><tr><th>Interface</th><th>Network</th><th>Request</th><th>Response</th><th>Operation Description</th></tr></thead><tbody><tr><td>1</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> External-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Manage clusters
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> User
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> keystone token
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Manage objects that
       belong to current project
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Magnum API
      </p>
     </td><td>
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       CRUD operations on cluster templates and clusters
      </p>
     </td></tr><tr><td>2a</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> Internal-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> AMQP over HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Enqueue messages
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Magnum API
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> RabbitMQ username,
       password
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> RabbitMQ queue
       read/write operations
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> RabbitMQ
      </p>
     </td><td>
      <p>
       Operation status
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       Notifications issued when cluster CRUD operations requested
      </p>
     </td></tr><tr><td>2b</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> Internal-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> AMQP over HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Read queued messages
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Magnum Conductor
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> RabbitMQ username,
       password
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> RabbitMQ queue
       read/write operations
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> RabbitMQ
      </p>
     </td><td>
      <p>
       Operation status
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       Notifications issued when cluster CRUD operations requested
      </p>
     </td></tr><tr><td>3</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> Internal-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> MariaDB over HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Persist data in MariaDB
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Magnum Conductor
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> MariaDB username,
       password
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Magnum database
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> MariaDB
      </p>
     </td><td>
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       Persist cluster/cluster template data, read persisted data
      </p>
     </td></tr><tr><td>4</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> Internal-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Create per-cluster user in
       dedicated domain, no role assignments initially
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Magnum Conductor
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Trustee domain admin
       username, password
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Manage users in
       dedicated Magnum domain
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> keystone
      </p>
     </td><td>
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       Magnum generates user record in a dedicated keystone domain for each
       cluster
      </p>
     </td></tr><tr><td>5</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> Internal-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Create per-cluster user stack
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Magnum Conductor
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> keystone token
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Limited to scope of
       authorized user
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> heat
      </p>
     </td><td>
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       Magnum creates heat stack for each cluster
      </p>
     </td></tr><tr><td>6</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> External Network
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Bootstrap a cluster in public
       discovery <a class="link" href="https://discovery.etcd.io/" target="_blank">https://discovery.etcd.io/</a>
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Magnum Conductor
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Unguessable URL over
       HTTPS. URL is only available to software processes needing it.
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Read and update
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Public discovery service
      </p>
     </td><td>
      <p>
       Cluster discovery URL
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       Create key/value registry of specified size in public storage. This is
       used to stand up a cluster of kubernetes master nodes (refer to
       interface call #12).
      </p>
     </td></tr><tr><td>7</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> Internal-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Create cinder volumes
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> heat Engine
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> keystone token
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Limited to scope of
       authorized user
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> cinder API
      </p>
     </td><td>
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       heat creates cinder volumes as part of stack.
      </p>
     </td></tr><tr><td>8</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> Internal-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Create networks, routers, load
       balancers
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> heat Engine
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> keystone token
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Limited to scope of
       authorized user
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> neutron API
      </p>
     </td><td>
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       heat creates networks, routers, load balancers as part of the stack.
      </p>
     </td></tr><tr><td>9</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> Internal-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Create nova VMs, attach
       volumes
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> heat Engine
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> keystone token
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Limited to scope of
       authorized user
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> nova API
      </p>
     </td><td>
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       heat creates nova VMs as part of the stack.
      </p>
     </td></tr><tr><td>10</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> Internal-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Read pre-configured glance
       image
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> nova
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> keystone token
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Limited to scope of
       authorized user
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> glance API
      </p>
     </td><td>
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       nova uses pre-configured image in glance to bootstrap VMs.
      </p>
     </td></tr><tr><td>11a</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> External-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> heat notification
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Cluster member (VM or ironic
       node)
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> keystone token
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Limited to scope of
       authorized user
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> heat API
      </p>
     </td><td>
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       heat uses OS::heat::WaitCondition resource. VM is expected to call heat
       notification URL upon completion of certain bootstrap operation.
      </p>
     </td></tr><tr><td>11b</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> External-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> heat notification
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Cluster member (VM or ironic
       node)
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> keystone token
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Limited to scope of
       authorized user
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> heat API
      </p>
     </td><td>
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       heat uses OS::heat::WaitCondition resource. VM is expected to call heat
       notification URL upon completion of certain bootstrap operation.
      </p>
     </td></tr><tr><td>12</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> External-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Update cluster member state in
       a public registry at https://discovery.etcd.io
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Cluster member (VM or ironic
       node)
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Unguessable URL over HTTPS
       only available to software processes needing it.
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Read and update
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Public discovery service
      </p>
     </td><td>
      <p>
       Operation status
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       Update key/value pair in a registry created by interface call #6.
      </p>
     </td></tr><tr><td>13a</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> VxLAN encapsulated private
       network on the Guest network
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Various communications inside
       Kubernetes cluster
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Cluster member (VM or ironic
       node)
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Cluster member (VM or ironic
       node)
      </p>
     </td><td>
      <p>
       Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       Various calls performed to build Kubernetes clusters, deploy
       applications and put workload
      </p>
     </td></tr><tr><td>13b</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> VxLAN encapsulated private
       network on the Guest network
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Various communications inside
       Kubernetes cluster
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Cluster member (VM or ironic
       node)
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Cluster member (VM or ironic
       node)
      </p>
     </td><td>
      <p>
       Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       Various calls performed to build Kubernetes clusters, deploy
       applications and put workload
      </p>
     </td></tr><tr><td>14</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> Guest/External
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Download container images
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Cluster member (VM or ironic
       node)
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> None
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> None
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> External
      </p>
     </td><td>
      <p>
       Container image data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       Kubernetes makes calls to external repositories to download pre-packed
       container images
      </p>
     </td></tr><tr><td>15a</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> External/EXT_VM (Floating IP)
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Octavia load balancer
      </p>
     </td><td>
      <p>
       Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Tenant specific
      </p>
     </td><td>
      <p>
       External workload handled by container applications
      </p>
     </td></tr><tr><td>15b</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> Guest
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Cluster member (VM or ironic
       node)
      </p>
     </td><td>
      <p>
       Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Tenant specific
      </p>
     </td><td>
      <p>
       External workload handled by container applications
      </p>
     </td></tr><tr><td>15c</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> External/EXT_VM (Floating IP)
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Cluster member (VM or ironic
       node)
      </p>
     </td><td>
      <p>
       Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Tenant specific
      </p>
     </td><td>
      <p>
       External workload handled by container applications
      </p>
     </td></tr></tbody></table></div></div><p>
  <span class="bold"><strong>Dependencies</strong></span>
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    keystone
   </p></li><li class="listitem "><p>
    RabbitMQ
   </p></li><li class="listitem "><p>
    MariaDB
   </p></li><li class="listitem "><p>
    heat
   </p></li><li class="listitem "><p>
    glance
   </p></li><li class="listitem "><p>
    nova
   </p></li><li class="listitem "><p>
    cinder
   </p></li><li class="listitem "><p>
    neutron
   </p></li><li class="listitem "><p>
    barbican
   </p></li><li class="listitem "><p>
    swift
   </p></li></ul></div><p>
  <span class="bold"><strong>Implementation</strong></span>
 </p><p>
  Magnum API and Magnum Conductor are run on the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> controllers (or core
  nodes in case of mid-scale deployments).
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-networkImages-Mid-Scale-AllNetworks.png" target="_blank"><img src="images/media-networkImages-Mid-Scale-AllNetworks.png" width="" /></a></div></div><div class="table" id="security-groups-table"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 26.3: </span><span class="name">Security Groups </span><a title="Permalink" class="permalink" href="#security-groups-table">#</a></h6></div><div class="table-contents"><table class="table" summary="Security Groups" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /></colgroup><thead><tr><th>Source CIDR/Security Group</th><th>Port/Range</th><th>Protocol</th><th>Notes</th></tr></thead><tbody><tr><td>Any IP</td><td>22</td><td>SSH</td><td>Tenant Admin access</td></tr><tr><td>Any IP/Kubernetes Security Group</td><td>2379-2380</td><td>HTTPS</td><td>Etcd Traffic</td></tr><tr><td>Any IP/Kubernetes Security Group</td><td>6443</td><td>HTTPS</td><td>kube-apiserver</td></tr><tr><td>Any IP/Kubernetes Security Group</td><td>7080</td><td>HTTPS</td><td>kube-apiserver</td></tr><tr><td>Any IP/Kubernetes Security Group</td><td>8080</td><td>HTTPS</td><td>kube-apiserver</td></tr><tr><td>Any IP/Kubernetes Security Group</td><td>30000-32767</td><td>HTTPS</td><td>kube-apiserver</td></tr><tr><td>Any IP/Kubernetes Security Group</td><td>any</td><td>tenant app specific</td><td>tenant app specific</td></tr></tbody></table></div></div><div class="table" id="network-ports-table"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 26.4: </span><span class="name">Network Ports </span><a title="Permalink" class="permalink" href="#network-ports-table">#</a></h6></div><div class="table-contents"><table class="table" summary="Network Ports" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Port/Range</th><th>Protocol</th><th>Notes</th></tr></thead><tbody><tr><td>22</td><td>SSH</td><td>Admin Access</td></tr><tr><td>9511</td><td>HTTPS</td><td>Magnum API Access</td></tr><tr><td>2379-2380</td><td>HTTPS</td><td>Etcd (planned)</td></tr><tr><td> </td><td> </td><td> </td></tr></tbody></table></div></div><p>
  Summary of controls spanning multiple components and interfaces:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <span class="bold"><strong>Audit</strong></span>: Magnum performs logging. Logs are
    collected by the centralized logging service.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Authentication</strong></span>: Authentication via
    keystone tokens at APIs. Password authentication to MQ and DB using
    specific users with randomly-generated passwords.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Authorization</strong></span>: OpenStack provides admin
    and non-admin roles that are indicated in session tokens. Processes run at
    minimum privilege. Processes run as unique user/group definitions
    (magnum/magnum). Appropriate filesystem controls prevent other processes
    from accessing service’s files. Magnum config file is mode 600. Logs
    written using group adm, user magnum, mode 640. IPtables ensure that no
    unneeded ports are open. Security Groups provide authorization controls
    between in-cloud components.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Availability</strong></span>: Redundant hosts, clustered
    DB, and fail-over provide high availability.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Confidentiality</strong></span>: Network connections over
    TLS. Network separation via VLANs. Data and config files protected via
    filesystem controls. Unencrypted local traffic is bound to localhost.
    Separation of customer traffic on the TUL network via Open Flow (VxLANs).
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Integrity</strong></span>: Network connections over TLS.
    Network separation via VLANs. DB API integrity protected by SQL Alchemy.
    Data and config files are protected by filesystem controls. Unencrypted
    traffic is bound to localhost.
   </p></li></ul></div></div><div class="sect1" id="MagnumInstall"><div class="titlepage"><div><div><h2 class="title"><span class="number">26.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Install the Magnum Service</span> <a title="Permalink" class="permalink" href="#MagnumInstall">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-magnum-magnum_install.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-magnum-magnum_install.xml</li><li><span class="ds-label">ID: </span>MagnumInstall</li></ul></div></div></div></div><p>
  Installing the Magnum Service can be performed as part of a new
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 environment or can be added to an existing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9
  environment. Both installations require container management services,
  running in Magnum cluster VMs with access to specific Openstack API
  endpoints. The following TCP ports need to be open in your firewall to allow
  access from VMs to external (public) <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> endpoints.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>TCP Port</th><th>Service</th></tr></thead><tbody><tr><td>5000</td><td>Identity</td></tr><tr><td>8004</td><td>heat</td></tr><tr><td>9511</td><td>Magnum</td></tr></tbody></table></div><p>
  Magnum is dependent on the following OpenStack services.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    keystone
   </p></li><li class="listitem "><p>
    heat
   </p></li><li class="listitem "><p>
    nova KVM
   </p></li><li class="listitem "><p>
    neutron
   </p></li><li class="listitem "><p>
    glance
   </p></li><li class="listitem "><p>
    cinder
   </p></li><li class="listitem "><p>
    swift
   </p></li><li class="listitem "><p>
    barbican
   </p></li></ul></div><div id="id-1.3.6.9.4.6" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
   Magnum relies on the public discovery service
   <span class="emphasis"><em>https://discovery.etcd.io</em></span> during cluster bootstrapping
   and update. This service does not perform authentication checks. Although
   running a cluster cannot be harmed by unauthorized changes in the public
   discovery registry, it can be compromised during a cluster update operation.
   To avoid this, it is recommended that you keep your cluster discovery URL
   (that is,
   <code class="literal">https://discovery.etc.io/<em class="replaceable ">SOME_RANDOM_ID</em></code>)
   secret.
  </p></div><div class="sect2" id="id-1.3.6.9.4.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">26.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing Magnum as part of new <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 environment</span> <a title="Permalink" class="permalink" href="#id-1.3.6.9.4.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-magnum-magnum_install.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-magnum-magnum_install.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Magnum components are already included in example <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> models based on
   nova KVM, such as <span class="bold"><strong>entry-scale-kvm</strong></span>,
   <span class="bold"><strong>entry-scale-kvm-mml</strong></span> and
   <span class="bold"><strong>mid-scale</strong></span>. These models contain the Magnum
   dependencies (see above). You can follow generic installation instruction
   for Mid-Scale and Entry-Scale KM model by using this guide:
   <a class="xref" href="#install-kvm" title="Chapter 24. Installing Mid-scale and Entry-scale KVM">Chapter 24, <em>Installing Mid-scale and Entry-scale KVM</em></a>.
  </p><div id="id-1.3.6.9.4.7.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
      If you modify the cloud model to utilize a dedicated Cloud Lifecycle Manager, add
      <code class="literal">magnum-client</code> item to the list of service components
      for the Cloud Lifecycle Manager cluster.
     </p></li><li class="listitem "><p>
      Magnum needs a properly configured external endpoint. While preparing the
      cloud model, ensure that <code class="literal">external-name</code> setting in
      <code class="literal">data/network_groups.yml</code> is set to valid hostname,
      which can be resolved on DNS server, and a valid TLS certificate is
      installed for your external endpoint. For non-production test
      installations, you can omit <code class="literal">external-name</code>. In test
      installations, the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installer will use an IP address as a public
      endpoint hostname, and automatically generate a new certificate, signed
      by the internal CA. Please refer to <a class="xref" href="#tls30" title="Chapter 41. Configuring Transport Layer Security (TLS)">Chapter 41, <em>Configuring Transport Layer Security (TLS)</em></a> for more
      details.
     </p></li></ol></div></div></div><div class="sect2" id="sec-magnum-exist"><div class="titlepage"><div><div><h3 class="title"><span class="number">26.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding Magnum to an Existing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Environment</span> <a title="Permalink" class="permalink" href="#sec-magnum-exist">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-magnum-magnum_install.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-magnum-magnum_install.xml</li><li><span class="ds-label">ID: </span>sec-magnum-exist</li></ul></div></div></div></div><p>
   Adding Magnum to an already deployed <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 installation or during
   an upgrade can be achieved by performing the following steps.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Add items listed below to the list of service components in
     <code class="literal">~/openstack/my_cloud/definition/data/control_plane.yml</code>.
     Add them to clusters which have <code class="literal">server-role</code> set to
     <code class="literal">CONTROLLER-ROLE</code> (entry-scale models) or
     <code class="literal">CORE_ROLE</code> (mid-scale model).
    </p><div class="verbatim-wrap"><pre class="screen">- magnum-api
- magnum-conductor</pre></div></li><li class="step "><p>
     If your environment utilizes a dedicated Cloud Lifecycle Manager, add
     <code class="literal">magnum-client</code> to the list of service components for the
     Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Commit your changes to the local git repository. Run the following
     playbooks as described in <a class="xref" href="#using-git" title="Chapter 22. Using Git for Configuration Management">Chapter 22, <em>Using Git for Configuration Management</em></a> for your
     installation.
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <code class="literal">config-processor-run.yml</code>
      </p></li><li class="listitem "><p>
       <code class="literal">ready-deployment.yml</code>
      </p></li><li class="listitem "><p>
       <code class="literal">site.yml</code>
      </p></li></ul></div></li><li class="step "><p>
     Ensure that your external endpoint is configured correctly. The current
     public endpoint configuration can be verified by running the following
     commands on the Cloud Lifecycle Manager.
    </p><div class="verbatim-wrap"><pre class="screen">$ source service.osrc
$ openstack endpoint list --interface=public --service=identity
+-----------+---------+--------------+----------+---------+-----------+------------------------+
| ID        | Region  | Service Name | Service  | Enabled | Interface | URL                    |
|           |         |              | Type     |         |           |                        |
+-----------+---------+--------------+----------+---------+-----------+------------------------+
| d83...aa3 | region0 | keystone     | identity | True    | public    | https://10.245.41.168: |
|           |         |              |          |         |           |             5000/v2.0  |
+-----------+---------+--------------+----------+---------+-----------+------------------------+</pre></div><p>
     Ensure that the endpoint URL is using either an IP address, or a valid
     hostname, which can be resolved on the DNS server. If the URL is using an
     invalid hostname (for example, <code class="literal">myardana.test</code>), follow
     the steps in <a class="xref" href="#tls30" title="Chapter 41. Configuring Transport Layer Security (TLS)">Chapter 41, <em>Configuring Transport Layer Security (TLS)</em></a> to configure a valid external
     endpoint. You will need to update the <code class="literal">external-name</code>
     setting in the <code class="literal">data/network_groups.yml</code> to a valid
     hostname, which can be resolved on DNS server, and provide a valid TLS
     certificate for the external endpoint. For non-production test
     installations, you can omit the <code class="literal">external-name</code>. The
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installer will use an IP address as public endpoint hostname, and
     automatically generate a new certificate, signed by the internal CA.
     For more information, see <a class="xref" href="#tls30" title="Chapter 41. Configuring Transport Layer Security (TLS)">Chapter 41, <em>Configuring Transport Layer Security (TLS)</em></a>.
    </p></li></ol></div></div><div id="id-1.3.6.9.4.8.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
    By default <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> stores the private key used by Magnum and its
    passphrase in barbican which provides a secure place to store such
    information. You can change this such that this sensitive information is
    stored on the file system or in the database without encryption. Making
    such a change exposes you to the risk of this information being exposed
    to others. If stored in the database then any database backups, or a
    database breach, could lead to the disclosure of the sensitive
    information. Similarly, if stored unencrypted on the file system this
    information is exposed more broadly than if stored in barbican.
   </p></div></div></div><div class="sect1" id="MagnumIntegrateDNS"><div class="titlepage"><div><div><h2 class="title"><span class="number">26.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Integrate Magnum with the DNS Service</span> <a title="Permalink" class="permalink" href="#MagnumIntegrateDNS">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-magnum-magnum_integrate_dns.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-magnum-magnum_integrate_dns.xml</li><li><span class="ds-label">ID: </span>MagnumIntegrateDNS</li></ul></div></div></div></div><p>
  Integration with DNSaaS may be needed if:
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    The external endpoint is configured to use <code class="literal">myardana.test</code>
    as host name and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> front-end certificate is issued for this host name.
   </p></li><li class="listitem "><p>
    Minions are registered using nova VM names as hostnames Kubernetes API
    server. Most kubectl commands will not work if the VM name (for example,
    <code class="literal">cl-mu3eevqizh-1-b3vifun6qtuh-kube-minion-ff4cqjgsuzhy</code>)
    is not getting resolved at the provided DNS server.
   </p></li></ol></div><p>
  Follow these steps to integrate the Magnum Service with the DNS Service.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Allow connections from VMs to EXT-API
   </p><div class="verbatim-wrap"><pre class="screen">sudo modprobe 8021q
sudo ip link add link virbr5 name vlan108 type vlan id 108
sudo ip link set dev vlan108 up
sudo ip addr add 192.168.14.200/24 dev vlan108
sudo iptables -t nat -A POSTROUTING -o vlan108 -j MASQUERADE</pre></div></li><li class="step "><p>
    Run the designate reconfigure playbook.
   </p><div class="verbatim-wrap"><pre class="screen">$ cd ~/scratch/ansible/next/ardana/ansible/
$ ansible-playbook -i hosts/verb_hosts designate-reconfigure.yml</pre></div></li><li class="step "><p>
    Set up designate to resolve myardana.test correctly.
   </p><div class="verbatim-wrap"><pre class="screen">$ openstack zone create --email hostmaster@myardana.test myardana.test.
# wait for status to become active
$ EXTERNAL_VIP=$(grep HZN-WEB-extapi /etc/hosts | awk '{ print $1 }')
$ openstack recordset create --records $EXTERNAL_VIP --type A myardana.test. myardana.test.
# wait for status to become active
$ LOCAL_MGMT_IP=$(grep `hostname` /etc/hosts | awk '{ print $1 }')
$ nslookup myardana.test $LOCAL_MGMT_IP
Server:        192.168.14.2
Address:       192.168.14.2#53
Name:          myardana.test
Address:       192.168.14.5</pre></div></li><li class="step "><p>
    If you need to add/override a top level domain record, the following
    example should be used, substituting proxy.example.org with your own real
    address:
   </p><div class="verbatim-wrap"><pre class="screen">$ openstack tld create --name net
$ openstack zone create --email hostmaster@proxy.example.org proxy.example.org.
$ openstack recordset create --records 16.85.88.10 --type A proxy.example.org. proxy.example.org.
$ nslookup proxy.example.org. 192.168.14.2
Server:        192.168.14.2
Address:       192.168.14.2#53
Name:          proxy.example.org
Address:       16.85.88.10</pre></div></li><li class="step "><p>
    Enable propagation of dns_assignment and dns_name attributes to neutron
    ports, as per <a class="link" href="https://docs.openstack.org/neutron/rocky/admin/config-dns-int.html" target="_blank">https://docs.openstack.org/neutron/rocky/admin/config-dns-int.html</a>
   </p><div class="verbatim-wrap"><pre class="screen"># optionally add 'dns_domain = &lt;some domain name&gt;.' to [DEFAULT] section
# of ardana/ansible/roles/neutron-common/templates/neutron.conf.j2
stack@ksperf2-cp1-c1-m1-mgmt:~/openstack$ cat &lt;&lt;-EOF &gt;&gt;ardana/services/designate/api.yml

   provides-data:
   -   to:
       -   name: neutron-ml2-plugin
       data:
       -   option: extension_drivers
           values:
           -   dns
EOF
$ git commit -a -m "Enable DNS support for neutron ports"
$ cd ardana/ansible
$ ansible-playbook -i hosts/localhost config-processor-run.yml
$ ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
    Enable DNSaaS registration of created VMs by editing the
    <code class="filename">~/openstack/ardana/ansible/roles/neutron-common/templates/neutron.conf.j2</code>
    file. You will need to add <code class="literal">external_dns_driver =
    designate</code> to the <span class="bold"><strong>[DEFAULT]</strong></span>
    section and create a new <span class="bold"><strong>[designate]</strong></span>
    section for the designate specific configurations.
   </p><div class="verbatim-wrap"><pre class="screen">...
advertise_mtu = False
dns_domain = ksperf.
external_dns_driver = designate
{{ neutron_api_extensions_path|trim }}
{{ neutron_vlan_transparent|trim }}

# Add additional options here

[designate]
url = https://10.240.48.45:9001
admin_auth_url = https://10.240.48.45:35357/v3
admin_username = designate
admin_password = P8lZ9FdHuoW
admin_tenant_name = services
allow_reverse_dns_lookup = True
ipv4_ptr_zone_prefix_size = 24
ipv6_ptr_zone_prefix_size = 116
ca_cert = /etc/ssl/certs/ca-certificates.crt</pre></div></li><li class="step "><p>
    Commit your changes.
   </p><div class="verbatim-wrap"><pre class="screen">$ git commit -a -m "Enable DNSaaS registration of nova VMs"
[site f4755c0] Enable DNSaaS registration of nova VMs
1 file changed, 11 insertions(+)</pre></div></li></ol></div></div></div></div><div class="chapter " id="install-esx-ovsvapp"><div class="titlepage"><div><div><h2 class="title"><span class="number">27 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing ESX Computes and OVSvAPP</span> <a title="Permalink" class="permalink" href="#install-esx-ovsvapp">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/install-esx_computes-ovsvapp.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>install-esx_computes-ovsvapp.xml</li><li><span class="ds-label">ID: </span>install-esx-ovsvapp</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#sec-ironic-prereqs"><span class="number">27.1 </span><span class="name">Prepare for Cloud Installation</span></a></span></dt><dt><span class="section"><a href="#sec-ironic-setup-deployer"><span class="number">27.2 </span><span class="name">Setting Up the Cloud Lifecycle Manager</span></a></span></dt><dt><span class="section"><a href="#esxi-overview"><span class="number">27.3 </span><span class="name">Overview of ESXi and OVSvApp</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.10.6"><span class="number">27.4 </span><span class="name">VM Appliances Used in OVSvApp Implementation</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.10.7"><span class="number">27.5 </span><span class="name">Prerequisites for Installing ESXi and Managing with vCenter</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.10.8"><span class="number">27.6 </span><span class="name">ESXi/vCenter System Requirements</span></a></span></dt><dt><span class="section"><a href="#create-esx-cluster"><span class="number">27.7 </span><span class="name">Creating an ESX Cluster</span></a></span></dt><dt><span class="section"><a href="#config-dvs-pg"><span class="number">27.8 </span><span class="name">Configuring the Required Distributed vSwitches and Port Groups</span></a></span></dt><dt><span class="section"><a href="#create-vapp-template"><span class="number">27.9 </span><span class="name">Create a SUSE-based Virtual Appliance Template in vCenter</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.10.12"><span class="number">27.10 </span><span class="name">ESX Network Model Requirements</span></a></span></dt><dt><span class="section"><a href="#create-vms-vapp-template"><span class="number">27.11 </span><span class="name">Creating and Configuring Virtual Machines Based on Virtual Appliance
 Template</span></a></span></dt><dt><span class="section"><a href="#collect-vcenter-credentials"><span class="number">27.12 </span><span class="name">Collect vCenter Credentials and UUID</span></a></span></dt><dt><span class="section"><a href="#edit-input-models"><span class="number">27.13 </span><span class="name">Edit Input Models to Add and Configure Virtual Appliances</span></a></span></dt><dt><span class="section"><a href="#run-config-processor"><span class="number">27.14 </span><span class="name">Running the Configuration Processor With Applied Changes</span></a></span></dt><dt><span class="section"><a href="#test-esx-environment"><span class="number">27.15 </span><span class="name">Test the ESX-OVSvApp Environment</span></a></span></dt></dl></div></div><p>
  This section describes the installation step requirements for ESX
  Computes (nova-proxy) and OVSvAPP.
 </p><div class="sect1" id="sec-ironic-prereqs"><div class="titlepage"><div><div><h2 class="title"><span class="number">27.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prepare for Cloud Installation</span> <a title="Permalink" class="permalink" href="#sec-ironic-prereqs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/install-esx_computes-ovsvapp.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>install-esx_computes-ovsvapp.xml</li><li><span class="ds-label">ID: </span>sec-ironic-prereqs</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Review the <a class="xref" href="#preinstall-checklist" title="Chapter 14. Pre-Installation Checklist">Chapter 14, <em>Pre-Installation Checklist</em></a> about recommended
     pre-installation tasks.
    </p></li><li class="step "><p>
     Prepare the Cloud Lifecycle Manager node. The Cloud Lifecycle Manager must be accessible either directly or via
     <code class="literal">ssh</code>, and have SUSE Linux Enterprise Server 12 SP4 installed. All nodes must
     be accessible to the Cloud Lifecycle Manager. If the nodes do not have direct access to
     online Cloud subscription channels, the Cloud Lifecycle Manager node will need to host the
     Cloud repositories.
    </p><ol type="a" class="substeps "><li class="step "><p>
       If you followed the installation instructions for Cloud Lifecycle Manager server (see <a class="xref" href="#cha-depl-dep-inst" title="Chapter 15. Installing the Cloud Lifecycle Manager server">Chapter 15, <em>Installing the Cloud Lifecycle Manager server</em></a>), <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> software should already be
       installed. Double-check whether SUSE Linux Enterprise and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are properly
       registered at the SUSE Customer Center by starting YaST and running <span class="guimenu ">Software</span> › <span class="guimenu ">Product
       Registration</span>.
      </p><p>
       If you have not yet installed <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, do so by starting YaST and
       running <span class="guimenu ">Software</span> › <span class="guimenu ">Product
       Registration</span> › <span class="guimenu ">Select
       Extensions</span>. Choose <span class="guimenu "><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span>
       and follow the on-screen instructions. Make sure to register
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> during the installation process and to install the software
       pattern <code class="literal">patterns-cloud-ardana</code>.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper -n in patterns-cloud-ardana</pre></div></li><li class="step "><p>
       Ensure the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> media repositories and updates repositories are
       made available to all nodes in your deployment. This can be accomplished
       either by configuring the Cloud Lifecycle Manager server as an SMT mirror as described in
       <a class="xref" href="#app-deploy-smt-lcm" title="Chapter 16. Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)">Chapter 16, <em>Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</em></a> or by syncing or mounting the
       Cloud and updates repositories to the Cloud Lifecycle Manager server as described in <a class="xref" href="#cha-depl-repo-conf-lcm" title="Chapter 17. Software Repository Setup">Chapter 17, <em>Software Repository Setup</em></a>.
      </p></li><li class="step "><p>
       Configure passwordless <code class="command">sudo</code> for the user created when
       setting up the node (as described in <a class="xref" href="#sec-depl-adm-inst-user" title="15.4. Creating a User">Section 15.4, “Creating a User”</a>). Note that this is
       <span class="emphasis"><em>not</em></span> the user <code class="systemitem">ardana</code> that will be used later in this
       procedure. In the following we assume you named the user <code class="systemitem">cloud</code>. Run the command
       <code class="command">visudo</code> as user <code class="systemitem">root</code> and add the following line
       to the end of the file:
      </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable ">CLOUD</em> ALL = (root) NOPASSWD:ALL</pre></div><p>
       Make sure to replace <em class="replaceable ">CLOUD</em> with your user name
       choice.
      </p></li><li class="step "><p>
       Set the password for the user
       <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo passwd ardana</pre></div></li><li class="step "><p>
       Become the user <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>su - ardana</pre></div></li><li class="step "><p>
       Place a copy of the SUSE Linux Enterprise Server 12 SP4 <code class="filename">.iso</code> in the
       <code class="systemitem">ardana</code> home directory,
       <code class="filename">var/lib/ardana</code>, and rename it to
       <code class="filename">sles12sp4.iso</code>.
      </p></li><li class="step "><p>
       Install the templates, examples, and working model directories:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>/usr/bin/ardana-init</pre></div></li></ol></li></ol></div></div></div><div class="sect1" id="sec-ironic-setup-deployer"><div class="titlepage"><div><div><h2 class="title"><span class="number">27.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting Up the Cloud Lifecycle Manager</span> <a title="Permalink" class="permalink" href="#sec-ironic-setup-deployer">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/install-esx_computes-ovsvapp.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>install-esx_computes-ovsvapp.xml</li><li><span class="ds-label">ID: </span>sec-ironic-setup-deployer</li></ul></div></div></div></div><div class="sect2" id="id-1.3.6.10.4.2"><div class="titlepage"><div><div><h3 class="title"><span class="number">27.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing the Cloud Lifecycle Manager</span> <a title="Permalink" class="permalink" href="#id-1.3.6.10.4.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/install-esx_computes-ovsvapp.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>install-esx_computes-ovsvapp.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    Running the <code class="command">ARDANA_INIT_AUTO=1</code> command is optional to
    avoid stopping for authentication at any step. You can also run
    <code class="command">ardana-init</code>to launch the Cloud Lifecycle Manager.  You will be prompted to
    enter an optional SSH passphrase, which is used to protect the key used by
    Ansible when connecting to its client nodes.  If you do not want to use a
    passphrase, press <span class="keycap">Enter</span> at the prompt.
   </p><p>
    If you have protected the SSH key with a passphrase, you can avoid having
    to enter the passphrase on every attempt by Ansible to connect to its
    client nodes with the following commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>eval $(ssh-agent)
<code class="prompt user">ardana &gt; </code>ssh-add ~/.ssh/id_rsa</pre></div><p>
    The Cloud Lifecycle Manager will contain the installation scripts and configuration files to
    deploy your cloud. You can set up the Cloud Lifecycle Manager on a dedicated node or you do
    so on your first controller node. The default choice is to use the first
    controller node as the Cloud Lifecycle Manager.
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Download the product from:
     </p><ol type="a" class="substeps "><li class="step "><p>
        <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a>
       </p></li></ol></li><li class="step "><p>
      Boot your Cloud Lifecycle Manager from the SLES ISO contained in the download.
     </p></li><li class="step "><p>
      Enter <code class="literal">install</code> (all lower-case, exactly as spelled out
      here) to start installation.
     </p></li><li class="step "><p>
      Select the language. Note that only the English language selection is
      currently supported.
     </p></li><li class="step "><p>
      Select the location.
     </p></li><li class="step "><p>
      Select the keyboard layout.
     </p></li><li class="step "><p>
      Select the primary network interface, if prompted:
     </p><ol type="a" class="substeps "><li class="step "><p>
        Assign IP address, subnet mask, and default gateway
       </p></li></ol></li><li class="step "><p>
      Create new account:
     </p><ol type="a" class="substeps "><li class="step "><p>
        Enter a username.
       </p></li><li class="step "><p>
        Enter a password.
       </p></li><li class="step "><p>
        Enter time zone.
       </p></li></ol></li></ol></div></div><p>
    Once the initial installation is finished, complete the Cloud Lifecycle Manager setup with
    these steps:
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Ensure your Cloud Lifecycle Manager has a valid DNS nameserver specified in
      <code class="literal">/etc/resolv.conf</code>.
     </p></li><li class="step "><p>
      Set the environment variable LC_ALL:
     </p><div class="verbatim-wrap"><pre class="screen">export LC_ALL=C</pre></div><div id="id-1.3.6.10.4.2.8.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
       This can be added to <code class="filename">~/.bashrc</code> or
       <code class="filename">/etc/bash.bashrc</code>.
      </p></div></li></ol></div></div><p>
    The node should now have a working SLES setup.
   </p></div></div><div class="sect1" id="esxi-overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">27.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Overview of ESXi and OVSvApp</span> <a title="Permalink" class="permalink" href="#esxi-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/install-esx_computes-ovsvapp.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>install-esx_computes-ovsvapp.xml</li><li><span class="ds-label">ID: </span>esxi-overview</li></ul></div></div></div></div><p>
   ESXi is a hypervisor developed by VMware for deploying and serving virtual
   computers. OVSvApp is a service VM that allows for leveraging advanced
   networking capabilities that OpenStack neutron provides. As a result,
   OpenStack features can be added quickly with minimum effort where ESXi is
   used. OVSvApp allows for hosting VMs on ESXi hypervisors together with the
   flexibility of creating port groups dynamically on Distributed Virtual
   Switches (DVS). Network traffic can then be steered through the OVSvApp VM
   which provides VLAN and VXLAN underlying infrastructure for VM communication
   and security features based on OpenStack. More information is available at
   the <a class="link" href="https://wiki.openstack.org/wiki/Neutron/Networking-vSphere" target="_blank">OpenStack
   wiki</a>.
  </p><p>
   The diagram below illustrates the OVSvApp architecture.
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/OVSvApp-Architecture.png" target="_blank"><img src="images/OVSvApp-Architecture.png" width="" /></a></div></div></div><div class="sect1" id="id-1.3.6.10.6"><div class="titlepage"><div><div><h2 class="title"><span class="number">27.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">VM Appliances Used in OVSvApp Implementation</span> <a title="Permalink" class="permalink" href="#id-1.3.6.10.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/install-esx_computes-ovsvapp.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>install-esx_computes-ovsvapp.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The default configuration deployed with the Cloud Lifecycle Manager for VMware ESX hosts uses
   service appliances that run as VMs on the VMware hypervisor. There is one
   OVSvApp VM per VMware ESX host and one nova Compute Proxy per VMware cluster
   or VMware vCenter Server. Instructions for how to create a template for the
   Nova Compute Proxy or ovsvapp can be found at 
   <a class="xref" href="#create-vapp-template" title="27.9. Create a SUSE-based Virtual Appliance Template in vCenter">Section 27.9, “Create a SUSE-based Virtual Appliance Template in vCenter”</a>.
  </p><div class="sect2" id="id-1.3.6.10.6.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">27.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">OVSvApp VM</span> <a title="Permalink" class="permalink" href="#id-1.3.6.10.6.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/install-esx_computes-ovsvapp.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>install-esx_computes-ovsvapp.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   OVSvApp implementation is comprised of:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     a service VM called OVSvApp VM hosted on each ESXi hypervisor within a
     cluster, and
    </p></li><li class="listitem "><p>
     two vSphere Distributed vSwitches (DVS).
    </p></li></ul></div><p>
   OVSvApp VMs run SUSE Linux Enterprise and have Open vSwitch installed with an agent called
   <code class="literal">OVSvApp agent</code>. The OVSvApp VM routes network traffic to
   the various VMware tenants and cooperates with the <span class="productname">OpenStack</span> deployment to
   configure the appropriate port and network settings for VMware tenants.
  </p></div><div class="sect2" id="id-1.3.6.10.6.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">27.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">nova Compute Proxy VM</span> <a title="Permalink" class="permalink" href="#id-1.3.6.10.6.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/install-esx_computes-ovsvapp.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>install-esx_computes-ovsvapp.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The nova compute proxy is the <code class="literal">nova-compute</code> service
   for VMware ESX. Only one instance of this service is required for each ESX
   cluster that is deployed and is communicating with a single VMware vCenter
   server. (This is not like KVM where the <code class="literal">nova-compute</code>
   service must run on every KVM Host.) The single instance of
   <code class="literal">nova-compute</code> service can run in the <span class="productname">OpenStack</span> controller
   node or any other service node in your cloud. The main component of the
   <code class="literal">nova-compute</code> VM is the OVSvApp nova VCDriver that talks
   to the VMware vCenter server to perform VM operations such as VM creation
   and deletion.
  </p></div></div><div class="sect1" id="id-1.3.6.10.7"><div class="titlepage"><div><div><h2 class="title"><span class="number">27.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites for Installing ESXi and Managing with vCenter</span> <a title="Permalink" class="permalink" href="#id-1.3.6.10.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/install-esx_computes-ovsvapp.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>install-esx_computes-ovsvapp.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   ESX/vCenter integration is not fully automatic. vCenter administrators are
   responsible for taking steps to ensure secure operation.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The VMware administrator is responsible for administration of the vCenter
     servers and the ESX nodes using the VMware administration tools. These
     responsibilities include:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Installing and configuring vCenter server
      </p></li><li class="listitem "><p>
       Installing and configuring ESX server and ESX cluster
      </p></li><li class="listitem "><p>
       Installing and configuring shared datastores
      </p></li><li class="listitem "><p>
       Establishing network connectivity between the ESX network and the Cloud Lifecycle Manager
       <span class="productname">OpenStack</span> management network
      </p></li></ul></div></li><li class="listitem "><p>
     The VMware administration staff is responsible for the review of vCenter
     logs. These logs are not automatically included in Cloud Lifecycle Manager <span class="productname">OpenStack</span> centralized logging.
    </p></li><li class="listitem "><p>
     The VMware administrator is responsible for administration of the vCenter
     servers and the ESX nodes using the VMware administration tools.
    </p></li><li class="listitem "><p>
     Logging levels for vCenter should be set appropriately to prevent logging
     of the password for the Cloud Lifecycle Manager <span class="productname">OpenStack</span> message queue.
    </p></li><li class="listitem "><p>
     The vCenter cluster and ESX Compute nodes must be appropriately backed up.
    </p></li><li class="listitem "><p>
     Backup procedures for vCenter should ensure that the file containing the
     Cloud Lifecycle Manager <span class="productname">OpenStack</span> configuration as part of nova and cinder volume
     services is backed up and the backups are protected appropriately.
    </p></li><li class="listitem "><p>
     Since the file containing the Cloud Lifecycle Manager <span class="productname">OpenStack</span> message queue password could
     appear in the swap area of a vCenter server, appropriate controls should
     be applied to the vCenter cluster to prevent discovery of the password via
     snooping of the swap area or memory dumps.
    </p></li><li class="listitem "><p>
     It is recommended to have a common shared storage for all the ESXi hosts
     in a particular cluster.
    </p></li><li class="listitem "><p>
     Ensure that you have enabled HA (High Availability) and DRS (Distributed
     Resource Scheduler) settings in a cluster configuration before running the
     installation. DRS and HA are disabled only for OVSvApp. This is done so that it
     does not move to a different host. If you do not enable DRS and HA prior to
     installation then you will not be able to disable it only for OVSvApp. As
     a result DRS or HA could migrate OVSvApp to a different host, which would create a
     network loop.
    </p></li></ul></div><div id="id-1.3.6.10.7.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   No two clusters should have the same name across datacenters in a given
   vCenter.
  </p></div></div><div class="sect1" id="id-1.3.6.10.8"><div class="titlepage"><div><div><h2 class="title"><span class="number">27.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">ESXi/vCenter System Requirements</span> <a title="Permalink" class="permalink" href="#id-1.3.6.10.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/install-esx_computes-ovsvapp.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>install-esx_computes-ovsvapp.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   For information about recommended hardware minimums, consult
   <a class="xref" href="#rec-min-entryscale-esx-kvm" title="3.2. Recommended Hardware Minimums for an Entry-scale ESX KVM Model">Section 3.2, “Recommended Hardware Minimums for an Entry-scale ESX KVM Model”</a>.
  </p></div><div class="sect1" id="create-esx-cluster"><div class="titlepage"><div><div><h2 class="title"><span class="number">27.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating an ESX Cluster</span> <a title="Permalink" class="permalink" href="#create-esx-cluster">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/create-esx-cluster.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>create-esx-cluster.xml</li><li><span class="ds-label">ID: </span>create-esx-cluster</li></ul></div></div></div></div><p>
   Steps to create an ESX Cluster:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Download the ESXi Hypervisor and vCenter Appliance from the VMware
     website.
    </p></li><li class="step "><p>
     Install the ESXi Hypervisor.
    </p></li><li class="step "><p>
     Configure the Management Interface.
    </p></li><li class="step "><p>
     Enable the CLI and Shell access.
    </p></li><li class="step "><p>
     Set the password and login credentials.
    </p></li><li class="step "><p>
     Extract the vCenter Appliance files.
    </p></li><li class="step "><p>
     The vCenter Appliance offers two ways to install the vCenter. The
     directory <code class="filename">vcsa-ui-installer</code> contains the graphical
     installer. The <code class="filename">vcsa-cli-installer</code> directory contains
     the command line installer. The remaining steps demonstrate using the
     <code class="filename">vcsa-ui-installer</code> installer.
    </p></li><li class="step "><p>
     In the <code class="filename">vcsa-ui-installer</code>, click the
     <span class="guimenu ">installer</span> to start installing the vCenter Appliance in
     the ESXi Hypervisor.
    </p></li><li class="step "><p>
     Note the <em class="replaceable ">MANAGEMENT IP</em>, <em class="replaceable ">USER
     ID</em>, and <em class="replaceable ">PASSWORD</em> of the ESXi
     Hypervisor.
    </p></li><li class="step "><p>
     Assign an <em class="replaceable ">IP ADDRESS</em>, <em class="replaceable ">USER
     ID</em>, and <em class="replaceable ">PASSWORD</em> to the vCenter
     server.
    </p></li><li class="step "><p>
     Complete the installation.
    </p></li><li class="step "><p>
     When the installation is finished, point your Web browser to the
     <em class="replaceable ">IP ADDRESS</em> of the vCenter. Connect to the
     vCenter by clicking on link in the browser.
    </p></li><li class="step "><p>
     Enter the information for the vCenter you just created: <em class="replaceable ">IP
     ADDRESS</em>, <em class="replaceable ">USER ID</em>, and
     <em class="replaceable ">PASSWORD</em>.
    </p></li><li class="step "><p>
     When connected, configure the following:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <code class="literal">Datacenter</code>
      </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Go to <code class="literal">Home</code> &gt; <code class="literal">Inventory</code> &gt;
         <code class="literal">Hosts and Clusters</code>
        </p></li><li class="step "><p>
         Select File &gt; New &gt; Datacenter
        </p></li><li class="step "><p>
         Rename the datacenter
        </p></li></ol></div></div></li><li class="listitem "><p>
       Cluster
      </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Right-click a datacenter or directory in the vSphere Client and select
         <span class="guimenu ">New Cluster</span>.
        </p></li><li class="step "><p>
         Enter a name for the cluster.
        </p></li><li class="step "><p>
         Choose cluster features.
        </p></li></ol></div></div></li><li class="listitem "><p>
       Add a Host to Cluster
      </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         In the vSphere Web Client, navigate to a datacenter, cluster, or
         directory within a datacenter.
        </p></li><li class="step "><p>
         Right-click the datacenter, cluster, or directory and select
         <span class="guimenu ">Add Host</span>.
        </p></li><li class="step "><p>
          Type the IP address or the name of the host and click
          <span class="guimenu ">Next</span>.
         </p></li><li class="step "><p>
          Enter the administrator credentials and click <span class="guimenu ">Next</span>.
         </p></li><li class="step "><p>
          Review the host summary and click <span class="guimenu ">Next</span>.
         </p></li><li class="step "><p>
          Assign a license key to the host.
         </p></li></ol></div></div></li></ul></div></li></ol></div></div></div><div class="sect1" id="config-dvs-pg"><div class="titlepage"><div><div><h2 class="title"><span class="number">27.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Required Distributed vSwitches and Port Groups</span> <a title="Permalink" class="permalink" href="#config-dvs-pg">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/config-dvs-pg.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>config-dvs-pg.xml</li><li><span class="ds-label">ID: </span>config-dvs-pg</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="#create-esxi-trunk-dvs" title="27.8.1. Creating ESXi TRUNK DVS and Required Portgroup">Section 27.8.1, “Creating ESXi TRUNK DVS and Required Portgroup”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#create-esxi-mgmt-dvs" title="27.8.2. Creating ESXi MGMT DVS and Required Portgroup">Section 27.8.2, “Creating ESXi MGMT DVS and Required Portgroup”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#config-ansible-playbook" title="27.8.3. Configuring OVSvApp Network Resources Using Ansible-Playbook">Section 27.8.3, “Configuring OVSvApp Network Resources Using Ansible-Playbook”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#config-ovsvapp-python-vsphere" title="27.8.4. Configuring OVSVAPP Using Python-Networking-vSphere">Section 27.8.4, “Configuring OVSVAPP Using Python-Networking-vSphere”</a>
   </p></li></ul></div><p>
  The required Distributed vSwitches (DVS) and port groups can be created by
  using the vCenter graphical user interface (GUI) or by using the command line
  tool provided by <code class="literal">python-networking-vsphere</code>. The vCenter
  GUI is recommended.
 </p><p>
  OVSvApp virtual machines (VMs) give ESX installations the ability to leverage
  some of the advanced networking capabilities and other benefits <span class="productname">OpenStack</span>
  provides. In particular, OVSvApp allows for hosting VMs on ESX/ESXi
  hypervisors together with the flexibility of creating port groups dynamically
  on Distributed Virtual Switch.
 </p><p>
  A port group is a management object for aggregation of multiple ports (on a
  virtual switch) under a common configuration. A VMware port group is used to
  group together a list of ports in a virtual switch (DVS in this section) so
  that they can be configured all at once. The member ports of a port group
  inherit their configuration from the port group, allowing for configuration
  of a port by simply dropping it into a predefined port group.
 </p><p>
  The following sections cover configuring OVSvApp switches on ESX. More
  information about OVSvApp is available at
     <a class="link" href="https://wiki.openstack.org/wiki/Neutron/Networking-vSphere" target="_blank">https://wiki.openstack.org/wiki/Neutron/Networking-vSphere</a>
 </p><p>
  The diagram below illustrates a typical configuration that uses OVSvApp and
  Distributed vSwitches.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/ovsapp-dvs-esx.png" target="_blank"><img src="images/ovsapp-dvs-esx.png" width="" /></a></div></div><p>
  Detailed instructions are shown in the following sections for four example
  installations and two command line procedures.
 </p><div class="sect2" id="create-esxi-trunk-dvs"><div class="titlepage"><div><div><h3 class="title"><span class="number">27.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating ESXi TRUNK DVS and Required Portgroup</span> <a title="Permalink" class="permalink" href="#create-esxi-trunk-dvs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/create-esxi-trunk-dvs.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>create-esxi-trunk-dvs.xml</li><li><span class="ds-label">ID: </span>create-esxi-trunk-dvs</li></ul></div></div></div></div><p>
    The process of creating an ESXi Trunk Distributed vSwitch (DVS) consists of three
    steps: create a switch, add host and physical adapters, and add a port
    group. Use the following detailed instructions to create a trunk DVS and a
    required portgroup. These instructions use a graphical user interface
    (GUI). The GUI menu options may vary slightly depending on the specific version
    of vSphere installed. Command line interface (CLI) instructions are below the GUI
    instructions.
   </p><div class="sect3" id="id-1.3.6.10.10.10.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">27.8.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating ESXi Trunk DVS with vSphere Web Client</span> <a title="Permalink" class="permalink" href="#id-1.3.6.10.10.10.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/create-esxi-trunk-dvs.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>create-esxi-trunk-dvs.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Create the switch.
     </p><ol type="a" class="substeps "><li class="step "><p>
        Using vSphere webclient, connect to the vCenter server.
       </p></li><li class="step "><p>
        Under <span class="guimenu ">Hosts and cluster</span>, right-click on the
        appropriate datacenter. Select <span class="guimenu ">Distributed Switch</span>
        &gt; <span class="guimenu ">New Distributed Switch</span>.
       </p></li><li class="step "><p>
        Name the switch <code class="literal">TRUNK</code>. Click <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Select version 6.0.0 or larger. Click <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Under <span class="guimenu ">Edit settings</span>, lower the number of uplink
        ports to the lowest possible number (0 or 1).  Uncheck <span class="guimenu ">Create a
        default port group</span>. Click <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Under <span class="guimenu ">Ready to complete</span>, verify the settings are
        correct and click <span class="guimenu ">Finish</span>.
       </p></li></ol></li><li class="step "><p>
      Add host and physical adapters.
     </p><ol type="a" class="substeps "><li class="step "><p>
       Under <span class="guimenu ">Networking</span> find the DVS named
       <code class="literal">TRUNK</code> you just created. Right-click on it and select
       <span class="guimenu ">Manage hosts</span>.
      </p></li><li class="step "><p>
        Under <span class="guimenu ">Select task</span>, select <span class="guimenu ">Add
        hosts</span>. Click <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Click <span class="guimenu ">New hosts</span>.
       </p></li><li class="step "><p>
        Select the <code class="literal">CURRENT ESXI HOST</code> and select
        <span class="guimenu ">OK</span>. Click <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Under <span class="guimenu ">Select network adapter tasks</span>, select
        <span class="guimenu ">Manage advanced host settings</span> and
        <span class="bold"><strong>UNCHECK</strong></span> all other boxes. Click
        <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Under <span class="guimenu ">Advanced host settings</span>, check that the <code class="literal">Maximum
        Number of Ports</code> reads <code class="literal">(auto)</code>. There
        is nothing else to do. Click <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Under <span class="guimenu ">Ready to complete</span>, verify that one and only
        one host is being added and click <span class="guimenu ">Finish</span>.
       </p></li></ol></li><li class="step "><p>
      Add port group.
     </p><ol type="a" class="substeps "><li class="step "><p>
        Right-click on the TRUNK DVS that was just created (or modified) and
        select <span class="guimenu ">Distributed Port Group</span> &gt; <span class="guimenu ">New
        Distributed Port Group</span>.
       </p></li><li class="step "><p>
        Name the port group <code class="literal">TRUNK-PG</code>. Click <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Under <span class="guimenu ">Configure settings</span> select:
       </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          <code class="literal">port binding</code> &gt; <code class="literal">Static
          binding</code>
         </p></li><li class="listitem "><p>
          <code class="literal">port allocation</code> &gt; <code class="literal">Elastic</code>
         </p></li><li class="listitem "><p>
          <code class="literal">vlan type</code> &gt; <code class="literal">VLAN trunking</code> with
          range of 1–4094.
         </p></li></ul></div></li><li class="step "><p>
        Check <code class="literal">Customized default policies
        configuration</code>. Click <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Under <span class="guimenu ">Security</span> use the following values:
       </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th><p>Setting</p></th><th><p>Value</p></th></tr></thead><tbody><tr><td><p>promiscuous mode</p></td><td><p>accept</p></td></tr><tr><td><p>MAC address changes</p></td><td><p>reject</p></td></tr><tr><td><p>Forged transmits</p></td><td><p>accept</p></td></tr></tbody></table></div></li><li class="step "><p>
        Set <span class="guimenu ">Autoexpand</span> to <code class="literal">true</code> (port
        count growing).
       </p></li><li class="step "><p>
        Skip <span class="guimenu ">Traffic shaping</span> and click
        <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Skip <span class="guimenu ">Teaming and fail over</span> and click <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Skip <span class="guimenu ">Monitoring</span> and click
        <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Under <span class="guimenu ">Miscellaneous</span> there is nothing to be
        done. Click <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Under <span class="guimenu ">Edit additional settings</span> add a description if
        desired. Click <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Under <span class="guimenu ">Ready to complete</span> verify everything is as
        expected and click <span class="guimenu ">Finish</span>.
       </p></li></ol></li></ol></div></div></div></div><div class="sect2" id="create-esxi-mgmt-dvs"><div class="titlepage"><div><div><h3 class="title"><span class="number">27.8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating ESXi MGMT DVS and Required Portgroup</span> <a title="Permalink" class="permalink" href="#create-esxi-mgmt-dvs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/create-esxi-mgmt-dvs.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>create-esxi-mgmt-dvs.xml</li><li><span class="ds-label">ID: </span>create-esxi-mgmt-dvs</li></ul></div></div></div></div><p>
    The process of creating an ESXi Mgmt Distributed vSwitch (DVS) consists of three
    steps: create a switch, add host and physical adapters, and add a port
    group. Use the following detailed instructions to create a mgmt DVS and a
    required portgroup.
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Create the switch.
     </p><ol type="a" class="substeps "><li class="step "><p>
        Using the vSphere webclient, connect to the vCenter server.
       </p></li><li class="step "><p>
        Under <span class="guimenu ">Hosts and Cluster</span>, right-click on the
        appropriate datacenter, and select <code class="literal">Distributed
        Switch</code> &gt; <code class="literal">New Distributed Switch</code>
       </p></li><li class="step "><p>
        Name the switch <code class="literal">MGMT</code>. Click 
        <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Select version 6.0.0 or higher. Click <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Under <span class="guimenu ">Edit settings</span>, select the appropriate number
        of uplinks. The <code class="literal">MGMT</code> DVS is what connects the ESXi
        host to the <span class="productname">OpenStack</span> management network. Uncheck <code class="literal">Create a default
        port group</code>. Click <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Under <span class="guimenu ">Ready to complete</span>, verify the settings are
        correct. Click <span class="guimenu ">Finish</span>.
       </p></li></ol></li><li class="step "><p>
      Add host and physical adapters to Distributed Virtual Switch.
     </p><ol type="a" class="substeps "><li class="step "><p>
        Under <code class="literal">Networking</code>, find the <code class="literal">MGMT</code>
        DVS you just created. Right-click on it and select <span class="guimenu ">Manage
        hosts</span>.
       </p></li><li class="step "><p>
        Under <span class="guimenu ">Select task</span>, select <span class="guimenu ">Add
        hosts</span>. Click <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Click <span class="guimenu ">New hosts</span>.
       </p></li><li class="step "><p>
        Select the current ESXi host and select <span class="guimenu ">OK</span>. Click 
        <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Under <span class="guimenu ">Select network adapter tasks</span>, select
        <span class="guimenu ">Manage physical adapters</span> and <span class="bold"><strong>UNCHECK</strong></span> all other boxes. Click 
        <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Under <span class="guimenu ">Manage physical network adapters</span>, click on the
        interface you are using to connect the ESXi to the <span class="productname">OpenStack</span> management
        network. The name is of the form <code class="literal">vmnic#</code> (for
        example, <code class="literal">vmnic0</code>, <code class="literal">vmnic1</code>, etc.). When the
        interface is highlighted, select <span class="guimenu ">Assign uplink</span> then
        select the uplink name to assign or auto assign. Repeat the process for
        each uplink physical NIC you will be using to connect to the <span class="productname">OpenStack</span>
        data network.  Click <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Verify that you understand and accept the impact shown by <span class="guimenu ">Analyze
        impact</span>. Click <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Verify that everything is correct and click on
        <span class="guimenu ">Finish</span>.
       </p></li></ol></li><li class="step "><p>
      Add MGMT port group to switch.
     </p><ol type="a" class="substeps "><li class="step "><p>
        Right-click on the <code class="literal">MGMT</code> DVS and select
        <span class="guimenu ">Distributed Port Group</span> &gt; <span class="guimenu ">New Distributed
        Port Group</span>.
       </p></li><li class="step "><p>
        Name the port group <code class="literal">MGMT-PG</code>. Click 
        <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Under <span class="guimenu ">Configure settings</span>, select:
       </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          <code class="literal">port binding</code> &gt; <code class="literal">Static
          binding</code>
         </p></li><li class="listitem "><p>
          <code class="literal">port allocation</code> &gt; <code class="literal">Elastic</code>
         </p></li><li class="listitem "><p>
          <code class="literal">vlan type</code> &gt; <code class="literal">None</code>
         </p></li></ul></div><p>
        Click <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Under <span class="guimenu ">Ready to complete</span>, verify that everything is
        as expected and click <span class="guimenu ">Finish</span>.
       </p></li></ol></li><li class="step "><p>
      Add GUEST port group to the switch.
     </p><ol type="a" class="substeps "><li class="step "><p>
        Right-click on the DVS (MGMT) that was just created (or modified).
        Select <span class="guimenu ">Distributed Port Group</span> &gt; <span class="guimenu ">New
        Distributed Port Group</span>.
       </p></li><li class="step "><p>
        Name the port group <code class="literal">GUEST-PG</code>. Click
        <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Under <span class="guimenu ">Configure settings</span>, select:
       </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          <code class="literal">port binding</code> &gt; <code class="literal">Static binding</code>
         </p></li><li class="listitem "><p>
          <code class="literal">port allocation</code> &gt; <code class="literal">Elastic</code>
         </p></li><li class="listitem "><p>
          <code class="literal">vlan type</code> &gt; <code class="literal">VLAN trunking</code> The
          VLAN range corresponds to the VLAN ids being used by the <span class="productname">OpenStack</span>
          underlay. This is the same VLAN range as configured in the
          <code class="filename">neutron.conf</code> configuration file for the neutron
          server.
         </p></li></ul></div></li><li class="step "><p>
        Select <span class="guimenu ">Customize default policies configuration</span>.
        Click <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Under <span class="guimenu ">Security</span>, use the following settings:
       </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th>
            <p>
             setting
            </p>
           </th><th>
            <p>
             value
            </p>
           </th></tr></thead><tbody><tr><td>
            <p>
             promiscuous mode
            </p>
           </td><td>
            <p>
             accept
            </p>
           </td></tr><tr><td>
            <p>
             MAC address changes
            </p>
           </td><td>
            <p>
             reject
            </p>
           </td></tr><tr><td>
            <p>
             Forged transmits
            </p>
           </td><td>
            <p>
             accept
            </p>
           </td></tr></tbody></table></div></li><li class="step "><p>
        Skip <span class="guimenu ">Traffic shaping</span> and click
        <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Under <span class="guimenu ">Teaming and fail over</span>, make changes appropriate
        for your network and deployment.
       </p></li><li class="step "><p>
        Skip <span class="guimenu ">Monitoring</span> and click
        <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Skip <span class="guimenu ">Miscellaneous</span> and click <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Under <span class="guimenu ">Edit addition settings</span>, add a description if
        desired. Click <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Under <span class="guimenu ">Ready to complete</span>, verify everything is as
       expected. Click <span class="guimenu ">Finish</span>.
       </p></li></ol></li><li class="step "><p>
      Add ESX-CONF port group.
     </p><ol type="a" class="substeps "><li class="step "><p>
        Right-click on the DVS (MGMT) that was just created (or
        modified). Select <span class="guimenu ">Distributed Port Group</span> &gt;
        <span class="guimenu ">New Distributed Port Group</span>.
       </p></li><li class="step "><p>
        Name the port group <code class="literal">ESX-CONF-PG</code>. Click
        <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Under <span class="guimenu ">Configure settings</span>, select:
       </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          <code class="literal">port binding</code> &gt; <code class="literal">Static
          binding</code>
         </p></li><li class="listitem "><p>
          <code class="literal">port allocation</code> &gt; <code class="literal">Elastic</code>
         </p></li><li class="listitem "><p>
          <code class="literal">vlan type</code> &gt; <code class="literal">None</code>
         </p></li></ul></div><p>
        Click <span class="guimenu ">Next</span>.
       </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          <code class="literal">port binding</code> &gt; <code class="literal">Static
          binding</code>
         </p></li><li class="listitem "><p>
          <code class="literal">port allocation</code> &gt; <code class="literal">Elastic</code>
         </p></li><li class="listitem "><p>
          <code class="literal">vlan type</code> &gt; <code class="literal">None</code>
         </p></li></ul></div><p>
        Click <span class="guimenu ">Next</span>.
       </p></li><li class="step "><p>
        Under <span class="guimenu ">Ready to complete</span>, verify that everything is
        as expected and click <span class="guimenu ">Finish</span>.
       </p></li></ol></li></ol></div></div></div><div class="sect2" id="config-ansible-playbook"><div class="titlepage"><div><div><h3 class="title"><span class="number">27.8.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring OVSvApp Network Resources Using Ansible-Playbook</span> <a title="Permalink" class="permalink" href="#config-ansible-playbook">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/config-ansible_playbook.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>config-ansible_playbook.xml</li><li><span class="ds-label">ID: </span>config-ansible-playbook</li></ul></div></div></div></div><p>
  The Ardana ansible playbook
  <code class="filename">neutron-create-ovsvapp-resources.yml</code> can be used to
  create Distributed Virtual Switches and Port Groups on a vCenter cluster.
 </p><p>
  The playbook requires the following inputs:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <code class="literal">vcenter_username</code>
   </p></li><li class="listitem "><p>
    <code class="literal">vcenter_encrypted_password</code>
   </p></li><li class="listitem "><p>
    <code class="literal">vcenter_ip</code>
   </p></li><li class="listitem "><p>
    <code class="literal">vcenter_port</code> (default 443)
   </p></li><li class="listitem "><p>
    <code class="literal">vc_net_resources_location</code> This is the path to a file which
    contains the definition of the resources to be created. The definition is
    in JSON format.
   </p></li></ul></div><p>
  The <code class="filename">neutron-create-ovsvapp-resources.yml</code> playbook is
  not set up by default. In order to execute the playbook from the Cloud Lifecycle Manager, the
  <code class="literal">python-networking-vsphere</code> package must be installed.</p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Install the <code class="literal">python-networking-vsphere</code> package:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper install python-networking-vsphere</pre></div></li><li class="step "><p>Run the playbook:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook neutron-create-ovsvapp-resources.yml \
-i hosts/verb_hosts -vvv -e 'variable_host=localhost
vcenter_username=<em class="replaceable ">USERNAME</em>
vcenter_encrypted_password=<em class="replaceable ">ENCRYPTED_PASSWORD</em>
vcenter_ip=<em class="replaceable ">IP_ADDRESS</em>
vcenter_port=443
vc_net_resources_location=<em class="replaceable ">LOCATION_TO_RESOURCE_DEFINITION_FILE</em></pre></div></li></ol></div></div><p>
  The <code class="literal">RESOURCE_DEFINITION_FILE</code> is in JSON format and
  contains the resources to be created.
 </p><p>
  Sample file contents:
 </p><div class="verbatim-wrap"><pre class="screen">{
  "datacenter_name": "DC1",
  "host_names": [
    "192.168.100.21",
    "192.168.100.222"
  ],
  "network_properties": {
    "switches": [
      {
        "type": "dvSwitch",
        "name": "TRUNK",
        "pnic_devices": [],
        "max_mtu": "1500",
        "description": "TRUNK DVS for ovsvapp.",
        "max_ports": 30000
      },
      {
        "type": "dvSwitch",
        "name": "MGMT",
        "pnic_devices": [
          "vmnic1"
        ],
        "max_mtu": "1500",
        "description": "MGMT DVS for ovsvapp. Uses 'vmnic0' to connect to OpenStack Management network",
        "max_ports": 30000
      }
    ],
    "portGroups": [
      {
        "name": "TRUNK-PG",
        "vlan_type": "trunk",
        "vlan_range_start": "1",
        "vlan_range_end": "4094",
        "dvs_name": "TRUNK",
        "nic_teaming": null,
        "allow_promiscuous": true,
        "forged_transmits": true,
        "auto_expand": true,
        "description": "TRUNK port group. Configure as trunk for vlans 1-4094. Default nic_teaming selected."
      },
      {
        "name": "MGMT-PG",
        "dvs_name": "MGMT",
        "nic_teaming": null,
        "description": "MGMT port group. Configured as type 'access' (vlan with vlan_id = 0, default). Default nic_teaming. Promiscuous false, forged_transmits default"
      },
      {
        "name": "GUEST-PG",
        "dvs_name": "GUEST",
        "vlan_type": "MGMT",
        "vlan_range_start": 100,
        "vlan_range_end": 200,
        "nic_teaming": null,
        "allow_promiscuous": true,
        "forged_transmits": true,
        "auto_expand": true,
        "description": "GUEST port group. Configure for vlans 100 through 200."
      },
      {
        "name": "ESX-CONF-PG",
        "dvs_name": "MGMT",
        "nic_teaming": null,
        "description": "ESX-CONF port group. Configured as type 'access' (vlan with vlan_id = 0, default)."
      }
    ]
  }
}</pre></div></div><div class="sect2" id="config-ovsvapp-python-vsphere"><div class="titlepage"><div><div><h3 class="title"><span class="number">27.8.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring OVSVAPP Using Python-Networking-vSphere</span> <a title="Permalink" class="permalink" href="#config-ovsvapp-python-vsphere">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/config-ovsvapp-python-vsphere.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>config-ovsvapp-python-vsphere.xml</li><li><span class="ds-label">ID: </span>config-ovsvapp-python-vsphere</li></ul></div></div></div></div><p>
  Scripts can be used with the <a class="link" href="https://wiki.openstack.org/wiki/Neutron/Networking-vSphere" target="_blank">Networking-vSphere
  Project</a>. The scripts automate some of the process of configuring OVSvAPP
  from the command line. The following are help entries for two
  of the scripts:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>cd /opt/repos/networking-vsphere
 <code class="prompt user">tux &gt; </code>ovsvapp-manage-dvs -h
usage: ovsvapp-manage-dvs [-h] [--tcp tcp_port]
                          [--pnic_devices pnic_devices [pnic_devices ...]]
                          [--max_mtu max_mtu]
                          [--host_names host_names [host_names ...]]
                          [--description description] [--max_ports max_ports]
                          [--cluster_name cluster_name] [--create]
                          [--display_spec] [-v]
                          dvs_name vcenter_user vcenter_password vcenter_ip
                          datacenter_name
positional arguments:
  dvs_name              Name to use for creating the DVS
  vcenter_user          Username to be used for connecting to vCenter
  vcenter_password      Password to be used for connecting to vCenter
  vcenter_ip            IP address to be used for connecting to vCenter
  datacenter_name       Name of data center where the DVS will be created
optional arguments:
  -h, --help            show this help message and exit
  --tcp tcp_port        TCP port to be used for connecting to vCenter
  --pnic_devices pnic_devices [pnic_devices ...]
                        Space separated list of PNIC devices for DVS
  --max_mtu max_mtu     MTU to be used by the DVS
  --host_names host_names [host_names ...]
                        Space separated list of ESX hosts to add to DVS
  --description description
                        DVS description
  --max_ports max_ports
                        Maximum number of ports allowed on DVS
  --cluster_name cluster_name
                        Cluster name to use for DVS
  --create              Create DVS on vCenter
  --display_spec        Print create spec of DVS
 -v                    Verbose output</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>cd /opt/repos/networking-vsphere
 <code class="prompt user">tux &gt; </code>ovsvapp-manage-dvpg -h
usage: ovsvapp-manage-dvpg [-h] [--tcp tcp_port] [--vlan_type vlan_type]
                           [--vlan_id vlan_id]
                           [--vlan_range_start vlan_range_start]
                           [--vlan_range_stop vlan_range_stop]
                           [--description description] [--allow_promiscuous]
                           [--allow_forged_transmits] [--notify_switches]
                           [--network_failover_detection]
                           [--load_balancing {loadbalance_srcid,loadbalance_ip,loadbalance_srcmac,loadbalance_loadbased,failover_explicit}]
                           [--create] [--display_spec]
                           [--active_nics ACTIVE_NICS [ACTIVE_NICS ...]] [-v]
                           dvpg_name vcenter_user vcenter_password vcenter_ip
                           dvs_name
positional arguments:
  dvpg_name             Name to use for creating theDistributed Virtual Port
                        Group (DVPG)
  vcenter_user          Username to be used for connecting to vCenter
  vcenter_password      Password to be used for connecting to vCenter
  vcenter_ip            IP address to be used for connecting to vCenter
  dvs_name              Name of the Distributed Virtual Switch (DVS) to create
                        the DVPG in
optional arguments:
  -h, --help            show this help message and exit
  --tcp tcp_port        TCP port to be used for connecting to vCenter
  --vlan_type vlan_type
                        Vlan type to use for the DVPG
  --vlan_id vlan_id     Vlan id to use for vlan_type='vlan'
  --vlan_range_start vlan_range_start
                        Start of vlan id range for vlan_type='trunk'
  --vlan_range_stop vlan_range_stop
                        End of vlan id range for vlan_type='trunk'
  --description description
                        DVPG description
  --allow_promiscuous   Sets promiscuous mode of DVPG
  --allow_forged_transmits
                        Sets forge transmit mode of DVPG
  --notify_switches     Set nic teaming 'notify switches' to True.
  --network_failover_detection
                        Set nic teaming 'network failover detection' to True
  --load_balancing {loadbalance_srcid,loadbalance_ip,loadbalance_srcmac,loadbalance_loadbased,failover_explicit}
                        Set nic teaming load balancing algorithm.
                        Default=loadbalance_srcid
  --create              Create DVPG on vCenter
  --display_spec        Send DVPG's create spec to OUTPUT
  --active_nics ACTIVE_NICS [ACTIVE_NICS ...]
                        Space separated list of active nics to use in DVPG nic
                        teaming
 -v                    Verbose output</pre></div></div></div><div class="sect1" id="create-vapp-template"><div class="titlepage"><div><div><h2 class="title"><span class="number">27.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create a SUSE-based Virtual Appliance Template in vCenter</span> <a title="Permalink" class="permalink" href="#create-vapp-template">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/create-vapp_template-vcenter.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>create-vapp_template-vcenter.xml</li><li><span class="ds-label">ID: </span>create-vapp-template</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Download the SLES12-SP4 ISO image
    (<code class="filename">SLE-12-SP4-Server-DVD-x86_64-GM-DVD1.iso</code>) from <a class="link" href="https://www.suse.com/products/server/download/" target="_blank">https://www.suse.com/products/server/download/</a>. You need to
    sign in or create a SUSE customer service account before downloading.
   </p></li><li class="step "><p>
    Create a new Virtual Machine in vCenter Resource Pool.
   </p></li><li class="step "><p>
    Configure the Storage selection.
   </p></li><li class="step "><p>
    Configure the Guest Operating System.
   </p></li><li class="step "><p>
    Create a Disk.
   </p></li><li class="step "><p>
    Ready to Complete.
   </p></li><li class="step "><p>
    Edit Settings before booting the VM with additional Memory, typically
    16GB or 32GB, though large scale environments may require larger memory
    allocations.
   </p></li><li class="step "><p>
    Edit Settings before booting the VM with additional Network Settings.
    Ensure there are four network adapters, one each for TRUNK, MGMT, 
    ESX-CONF, and GUEST.
   </p></li><li class="step "><p>
    Attach the ISO image to the DataStore.
   </p></li><li class="step "><p>
    Configure the 'disk.enableUUID=TRUE' flag in the General - Advanced
    Settings.
   </p></li><li class="step "><p>
    After attaching the CD/DVD drive with the ISO image and completing the
    initial VM configuration, power on the VM by clicking the Play button on
    the VM's summary page.
   </p></li><li class="step "><p>
    Click <span class="guimenu ">Installation</span> when the VM boots from the console
    window.
   </p></li><li class="step "><p>
    Accept the License agreement, language and Keyboard selection.
   </p></li><li class="step "><p>
    Select the System Role to Xen Virtualization Host.
   </p></li><li class="step "><p>
    Select the 'Proposed Partitions' in the Suggested Partition screen.
   </p></li><li class="step "><p>
    Edit the Partitions to select the 'LVM' Mode and then select the 'ext4'
    filesystem type.
   </p></li><li class="step "><p>
    Increase the size of the root partition from 10GB to 60GB.
   </p></li><li class="step "><p>
    Create an additional logical volume to accommodate the LV_CRASH volume
    (15GB). Do not mount the volume at this time, it will be used later.
   </p></li><li class="step "><p>
    Configure the Admin User/Password and User name.
   </p></li><li class="step "><p>
    Installation Settings (Disable Firewall and enable SSH).
   </p></li><li class="step "><p>
    The operating system successfully installs and the VM reboots.
   </p></li><li class="step "><p>
    Check that the contents of the ISO files are copied to the locations shown
    below on your Cloud Lifecycle Manager. This may already be completed on the Cloud Lifecycle Manager.
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Mount or copy the contents of
      <code class="filename">SLE-12-SP4-Server-DVD-x86_64-GM-DVD1.iso</code> to
      <code class="filename">/opt/ardana_packager/ardana/sles12/zypper/OS/</code>
      (create the directory if it is missing).
     </p></li></ul></div></li><li class="step "><p>
    Log in to the VM with the configured user credentials.
   </p></li><li class="step "><p>
    The VM must be set up before a template can be created with it. The
    IP addresses configured here are temporary and will need to be
    reconfigured as VMs are created using this template. The temporary
    IP address should not overlap with the network range for
    the MGMT network.
   </p><ol type="a" class="substeps "><li class="step "><p>
      The VM should now have four network interfaces. Configure them as
      follows:
     </p><ol type="i" class="substeps "><li class="step "><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd /etc/sysconfig/network
       <code class="prompt user">tux &gt; </code>sudo ls</pre></div><p>
        The directory will contain the files: <code class="filename">ifcfg-br0</code>,
        <code class="filename">ifcfg-br1</code>, <code class="filename">ifcfg-br2</code>,
        <code class="filename">ifcfg-br3</code>, <code class="filename">ifcfg-eth0</code>,
        <code class="filename">ifcfg-eth1</code>, <code class="filename">ifcfg-eth2</code>, and
        <code class="filename">ifcfg-eth3</code>.
       </p></li><li class="step "><p>
        If you have configured a default route while installing the VM, then
        there will be a <code class="filename">routes</code> file.
       </p></li><li class="step "><p>
        Note the IP addresses configured for MGMT.
       </p></li><li class="step "><p>
	Configure the temporary IP for the MGMT network.
	Edit the <code class="filename">ifcfg-eth1</code> file.
       </p><ol type="A" class="substeps "><li class="step "><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo vi /etc/sysconfig/network/ifcfg-eth1
BOOTPROTO='static'
BROADCAST=''
ETHTOOL_OPTIONS=''
IPADDR='192.168.24.132/24' (Configure the IP address of the MGMT Interface)
MTU=''
NETWORK=''
REMOTE_IPADDR=''
STARTMODE='auto'</pre></div></li></ol></li><li class="step "><p>
        Edit the <code class="filename">ifcfg-eth0</code>,
        <code class="filename">ifcfg-eth2</code>, and
        <code class="filename">ifcfg-eth3</code> files.
       </p><ol type="A" class="substeps "><li class="step "><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo vi /etc/sysconfig/network/ifcfg-eth0
BOOTPROTO='static'
BROADCAST=''
ETHTOOL_OPTIONS=''
IPADDR=''
MTU=''
NETWORK=''
REMOTE_IPADDR=''
STARTMODE='auto'</pre></div></li><li class="step "><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo vi /etc/sysconfig/network/ifcfg-eth2
BOOTPROTO=''
BROADCAST=''
ETHTOOL_OPTIONS=''
IPADDR=''
MTU=''
NAME='VMXNET3 Ethernet Controller'
NETWORK=''
REMOTE_IPADDR=''
STARTMODE='auto'</pre></div></li><li class="step "><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo vi /etc/sysconfig/network/ifcfg-eth3
BOOTPROTO=''
BROADCAST=''
ETHTOOL_OPTIONS=''
IPADDR=''
MTU=''
NAME='VMXNET3 Ethernet Controller'
NETWORK=''
REMOTE_IPADDR=''
STARTMODE='auto'</pre></div></li><li class="step "><p>
          If the default route is not configured, add a default
          route file manually.
         </p><ol type="I" class="substeps "><li class="step "><p>
            Create a file <code class="filename">routes</code> in
            <code class="filename">/etc/sysconfig/network</code>.
           </p></li><li class="step "><p>
            Edit the file to add your default route.
           </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo sudo vi routes
           default 192.168.24.140 - -</pre></div></li></ol></li></ol></li><li class="step "><p>
        Delete all the bridge configuration files, which are not required:
        <code class="filename">ifcfg-br0</code>, <code class="filename">ifcfg-br1</code>,
        <code class="filename">ifcfg-br2</code>, and <code class="filename">ifcfg-br3</code>.
       </p></li></ol></li><li class="step "><p>
      Add <code class="literal">ardana</code> user and home directory if that is not your
      default <code class="literal">user</code>. The username and password should be
      <code class="literal">ardana/ardana</code>.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo useradd -m ardana
      <code class="prompt user">tux &gt; </code>sudo passwd ardana</pre></div></li><li class="step "><p>
      Create a <code class="literal">ardana</code> usergroup in the VM if it does not
      exist.
     </p><ol type="i" class="substeps "><li class="step "><p>
        Check for an existing <code class="literal">ardana</code> group.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo groups ardana</pre></div></li><li class="step "><p>
        Add <code class="literal">ardana</code> group if necessary.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo groupadd ardana</pre></div></li><li class="step "><p>
        Add <code class="literal">ardana</code> user to the <code class="literal">ardana</code>
        group.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo gpasswd -a ardana ardana</pre></div></li></ol></li><li class="step "><p>
      Allow the <code class="literal">ardana</code> user to <code class="literal">sudo</code>
      without password. Setting up <code class="literal">sudo</code> on SLES is covered
      in the SUSE documentation at <a class="link" href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-admin/#sec-sudo-conf" target="_blank">https://documentation.suse.com/sles/15-SP1/single-html/SLES-admin/#sec-sudo-conf</a>.
      We recommend creating user specific sudo config files in the
      <code class="filename">/etc/sudoers.d</code> directory. Create an
      <code class="filename">/etc/sudoers.d/ardana</code> config file with the following
      content to allow sudo commands without the requirement of a password.
     </p><div class="verbatim-wrap"><pre class="screen">ardana ALL=(ALL) NOPASSWD:ALL</pre></div></li><li class="step "><p>
      Add the Zypper repositories using the ISO-based repositories created
      previously. Change the value of <code class="literal">deployer_ip</code> if necessary.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo <em class="replaceable ">DEPLOYER_IP</em>=192.168.24.140
     <code class="prompt user">tux &gt; </code>sudo zypper addrepo --no-gpgcheck --refresh \
     http://$deployer_ip:79/ardana/sles12/zypper/OS SLES-OS
     <code class="prompt user">tux &gt; </code>sudo zypper addrepo --no-gpgcheck --refresh \
     http://$deployer_ip:79/ardana/sles12/zypper/SDK SLES-SDK</pre></div><p>
      Verify that the repositories have been added.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>zypper repos --detail</pre></div></li><li class="step "><p>
      Set up SSH access that does not require a password to the temporary
      IP address that was configured for <code class="literal">eth1</code> .
     </p><p>
      When you have started the installation using the Cloud Lifecycle Manager or if you are
      adding a SLES node to an existing cloud, the Cloud Lifecycle Manager public key needs to
      be copied to the SLES node. You can do this by copying
      <code class="filename">~/.ssh/authorized_keys</code> from another node
      in the cloud to the same location on the SLES node. If you are
      installing a new cloud, this file will be available on the nodes after
      running the <code class="filename">bm-reimage.yml</code> playbook.
     </p><div id="id-1.3.6.10.11.2.24.2.6.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
       Ensure that there is global read access to the
       file <code class="filename">~/.ssh/authorized_keys</code>.
      </p></div><p>
      Test passwordless ssh from the Cloud Lifecycle Manager and check your ability to remotely
      execute sudo commands.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>ssh ardana@<em class="replaceable ">IP_OF_SLES_NODE_eth1</em>
     "sudo tail -5 /var/log/messages"</pre></div></li></ol></li><li class="step "><p>
    Shutdown the VM and create a template out of the VM appliance for future
    use.
   </p></li><li class="step "><p>
    The VM Template will be saved in your vCenter Datacenter and you can view
    it from <span class="guimenu ">VMS and Templates</span> menu. Note that menu options
    will vary slightly depending on the version of vSphere that is deployed.
   </p></li></ol></div></div></div><div class="sect1" id="id-1.3.6.10.12"><div class="titlepage"><div><div><h2 class="title"><span class="number">27.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">ESX Network Model Requirements</span> <a title="Permalink" class="permalink" href="#id-1.3.6.10.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/install-esx_computes-ovsvapp.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>install-esx_computes-ovsvapp.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
     For this model the following networks are needed:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><code class="literal">MANAGEMENT-NET</code> : This is an untagged network this is used for the control plane as well as the esx-compute proxy and ovsvapp VMware instance.  It is tied to the MGMT DVS/PG in vSphere. 
     </p></li><li class="listitem "><p><code class="literal">EXTERNAL-API_NET</code> : This is a tagged network for the external/public API. There is no difference in this model from those without ESX and there is no additional setup needed in vSphere for this network.
      </p></li><li class="listitem "><p><code class="literal">EXTERNAL-VM-NET</code> : This is a tagged network used for Floating IP (FIP) assignment to running instances. There is no difference in this model from those without ESX and there is no additional setup needed in vSphere for this network.
      </p></li><li class="listitem "><p><code class="literal">GUEST-NET</code> : This is a tagged network used internally for neutron.  It is tied to the GUEST PG in vSphere.
      </p></li><li class="listitem "><p><code class="literal">ESX-CONF-NET</code> : This is a separate configuration network for ESX that must be reachable via the MANAGEMENT-NET.  It is tied to the ESX-CONF PG in vSphere.
      </p></li><li class="listitem "><p><code class="literal">TRUNK-NET</code> : This is an untagged network used internally for ESX. It is tied to the TRUNC DVS/PG in vSphere.
      </p></li></ul></div></div><div class="sect1" id="create-vms-vapp-template"><div class="titlepage"><div><div><h2 class="title"><span class="number">27.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating and Configuring Virtual Machines Based on Virtual Appliance
 Template</span> <a title="Permalink" class="permalink" href="#create-vms-vapp-template">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/create-vms-vapp_template.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>create-vms-vapp_template.xml</li><li><span class="ds-label">ID: </span>create-vms-vapp-template</li></ul></div></div></div></div><p>
  The following process for creating and configuring VMs from the vApp template
  should be repeated for every cluster in the DataCenter. Each cluster should
  host a nova Proxy VM, and each host in a cluster should have an OVSvApp
  VM running. The following method uses the <code class="literal">vSphere Client
  Management Tool</code> to deploy saved templates from the vCenter.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Identify the cluster that you want nova Proxy to manage.
   </p></li><li class="step "><p>
    Create a VM from the template on a chosen cluster.
   </p></li><li class="step "><p>
    The first VM deployed is the <code class="literal">nova-compute-proxy</code> VM.
    This VM can reside on any <code class="literal">HOST</code> inside a
    cluster. There should be only one instance of this VM in a cluster.
   </p></li><li class="step "><p>
    The <code class="literal">nova-compute-proxy</code> only uses two of the
    four interfaces configured previously (<code class="literal">ESX_CONF</code> and
    <code class="literal">MANAGEMENT</code>).
   </p><div id="id-1.3.6.10.13.3.4.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     Do not swap the interfaces. They must be in the specified order
     (<code class="literal">ESX_CONF</code> is <code class="literal">eth0</code>,
     <code class="literal">MGMT</code> is <code class="literal">eth1</code>).
    </p></div></li><li class="step "><p>
    After the VM has been deployed, log in to it with
    <code class="literal">ardana/ardana</code> credentials. Log in to the VM with SSH using
    the <code class="literal">MGMT</code> IP address. Make sure that all root level
    commands work with <code class="literal">sudo</code>. This is required for the Cloud Lifecycle Manager
    to configure the appliance for services and networking.
   </p></li><li class="step "><p>
    Install another VM from the template and name it
    <code class="literal">OVSvApp-VM1-HOST1</code>. (You can add a suffix with the
    host name to identify the host it is associated with).
   </p><div id="id-1.3.6.10.13.3.6.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    The VM must have four interfaces configured in the right order. The VM must
    be accessible from the Management Network through SSH from the Cloud Lifecycle Manager.
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <code class="filename">/etc/sysconfig/network/ifcfg-eth0</code> is
       <code class="literal">ESX_CONF</code>.
      </p></li><li class="listitem "><p>
       <code class="filename">/etc/sysconfig/network/ifcfg-eth1</code> is
       <code class="literal">MGMT</code>.
      </p></li><li class="listitem "><p>
       <code class="filename">/etc/sysconfig/network/ifcfg-eth2</code> is
       <code class="literal">TRUNK</code>.
      </p></li><li class="listitem "><p>
       <code class="filename">/etc/sysconfig/network/ifcfg-eth3</code> is
       <code class="literal">GUEST</code>.
      </p></li></ul></div></div></li><li class="step "><p>
    If there is more than one <code class="literal">HOST</code> in the cluster, deploy
    another VM from the Template and name it
    <code class="literal">OVSvApp-VM2-HOST2</code>.
   </p></li><li class="step "><p>
    If the OVSvApp VMs end up on the same <code class="literal">HOST</code>, then
    manually separate the VMs and follow the instructions below to add rules
    for High Availability (HA) and Distributed Resource Scheduler (DRS).
   </p><div id="id-1.3.6.10.13.3.8.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     HA seeks to minimize system downtime and data loss. See also <a class="xref" href="#HP3-0HA" title="Chapter 4. High Availability">Chapter 4, <em>High Availability</em></a>. DRS is a utility that balances computing workloads
     with available resources in a virtualized environment.
    </p></div></li><li class="step "><p>
    When installed from a template to a cluster, the VM will not be bound to a
    particular host if you have more than one Hypervisor. The requirement for
    the OVSvApp is that there be only one OVSvApp Appliance per host and that
    it should be constantly bound to the same host. DRS or VMotion should
    not be allowed to migrate the VMs from the existing HOST. This would cause
    major network interruption. In order to achieve this we need to configure
    rules in the cluster HA and DRS settings.
   </p><div id="id-1.3.6.10.13.3.9.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     VMotion enables the live migration of running virtual machines from one
     physical server to another with zero downtime, continuous service
     availability, and complete transaction integrity.
    </p></div></li><li class="step "><p>
    Configure rules for OVSvApp VMs.
   </p><ol type="a" class="substeps "><li class="step "><p>
      Configure <span class="guimenu ">vSphere HA - Virtual Machine Options</span>.
     </p></li><li class="step "><p>
      <span class="guimenu ">Use Cluster Setting</span> must be disabled.
     </p></li><li class="step "><p>
      VM should be <code class="literal">Power-On</code>.
     </p></li></ol></li><li class="step "><p>
    Configure <span class="guimenu ">Cluster DRS Groups/Rules</span>.
   </p><ol type="a" class="substeps "><li class="step "><p>
      Configure <span class="guimenu ">vSphere DRS - DRS Group Manager</span>.
     </p></li><li class="step "><p>
      Create a DRS Group for the OVSvApp VMs.
     </p></li><li class="step "><p>
      Add VMs to the DRS Group.
     </p></li><li class="step "><p>
      Add appropriate <span class="guimenu ">Rules</span> to the DRS Groups.
     </p></li></ol></li><li class="step "><p>
    All three VMs are up and running. Following the preceding process, there is
    one nova Compute Proxy VM per cluster, and
    <code class="literal">OVSvAppVM1</code> and <code class="literal">OVSvAppVM2</code> on each
    HOST in the cluster.
   </p></li><li class="step "><p>
    Record the configuration attributes of the VMs.
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      nova Compute Proxy VM:
     </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
        <code class="literal">Cluster Name</code> where this VM is located
       </p></li><li class="listitem "><p>
        <code class="literal">Management IP Address</code>
       </p></li><li class="listitem "><p>
        <code class="literal">VM Name</code> The actual name given to the VM to identify
        it.
       </p></li></ul></div></li><li class="listitem "><p>
      OVSvAppVM1
     </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
        <code class="literal">Cluster Name</code> where this VM is located
       </p></li><li class="listitem "><p>
        <code class="literal">Management IP Address</code>
       </p></li><li class="listitem "><p>
        <code class="literal">esx_hostname</code> that this OVSvApp is bound to
       </p></li><li class="listitem "><p>
        <code class="literal">cluster_dvs_mapping</code> The Distributed vSwitch name
        created in the datacenter for this particular cluster.
       </p><p>
        Example format:
       </p><p>
       <em class="replaceable ">DATA_CENTER</em>/host/<em class="replaceable ">CLUSTERNAME</em>:
       <em class="replaceable ">DVS-NAME</em> Do not substitute for
       <code class="literal">host</code>'. It is a constant.
       </p></li></ul></div></li><li class="listitem "><p>
      OVSvAppVM2:
     </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
        <code class="literal">Cluster Name</code> where this VM is located
       </p></li><li class="listitem "><p>
        <code class="literal">Management IP Address</code>
       </p></li><li class="listitem "><p>
        <code class="literal">esx_hostname</code> that this OVSvApp is bound to
       </p></li><li class="listitem "><p>
        <code class="literal">cluster_dvs_mapping</code> The Distributed vSwitch name
        created in the datacenter for this particular cluster.
       </p><p>
        Example format:
       </p><p>
       <em class="replaceable ">DATA_CENTER</em>/host/<em class="replaceable ">CLUSTERNAME</em>:
       <em class="replaceable ">DVS-NAME</em> Do not substitute for
       <code class="literal">host</code>'. It is a constant.
       </p></li></ul></div></li></ul></div></li></ol></div></div></div><div class="sect1" id="collect-vcenter-credentials"><div class="titlepage"><div><div><h2 class="title"><span class="number">27.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Collect vCenter Credentials and UUID</span> <a title="Permalink" class="permalink" href="#collect-vcenter-credentials">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/collect-vcenter-credentials.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>collect-vcenter-credentials.xml</li><li><span class="ds-label">ID: </span>collect-vcenter-credentials</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Obtain the vCenter UUID from vSphere with the URL shown below:
   </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable ">VCENTER-IP</em>/mob/?moid=ServiceInstance&amp;doPath=content.about</pre></div><p>
    Select the field <code class="literal">instanceUUID</code>. Copy and paste the
    <span class="bold"><strong>value</strong></span> of <code class="literal"># field
    instanceUUID</code>.
   </p></li><li class="listitem "><p>
    Record the <code class="literal">UUID</code>
   </p></li><li class="listitem "><p>
    Record the <code class="literal">vCenter Password</code>
   </p></li><li class="listitem "><p>
    Record the <code class="literal">vCenter Management IP</code>
   </p></li><li class="listitem "><p>
    Record the <code class="literal">DataCenter Name</code>
   </p></li><li class="listitem "><p>
    Record the <code class="literal">Cluster Name</code>
   </p></li><li class="listitem "><p>
    Record the <code class="literal">DVS (Distributed vSwitch) Name</code>
   </p></li></ul></div></div><div class="sect1" id="edit-input-models"><div class="titlepage"><div><div><h2 class="title"><span class="number">27.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Edit Input Models to Add and Configure Virtual Appliances</span> <a title="Permalink" class="permalink" href="#edit-input-models">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/edit-input_models.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>edit-input_models.xml</li><li><span class="ds-label">ID: </span>edit-input-models</li></ul></div></div></div></div><p>
  The following steps should be used to edit the Ardana input model data to add
  and configure the Virtual Appliances that were just created. The process
  assumes that the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is deployed and a valid Cloud Lifecycle Manager is in place.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Edit the following files in
    <code class="filename">~/openstack/my_cloud/definition/data/</code>:
    <code class="filename">servers.yml</code>, <code class="filename">disks_app_vm.yml</code>,
    and <code class="filename">pass_through.yml</code>. Fill in attribute values
    recorded in the previous step.
   </p></li><li class="step "><p>
    Follow the instructions in <code class="filename">pass_through.yml</code> to encrypt
    your vCenter password using an encryption key.
   </p></li><li class="step "><p>
    Export an environment variable for the encryption key.
   </p><div class="verbatim-wrap"><pre class="screen">ARDANA_USER_PASSWORD_ENCRYPT_KEY=ENCRYPTION_KEY</pre></div></li><li class="step "><p>
    Run <code class="filename">~ardana/openstack/ardana/ansible/ardanaencrypt.py</code> script. 
    It will prompt for <code class="literal">unencrypted value?</code>. Enter the unencrypted vCenter
    password and it will return an encrypted string.
   </p></li><li class="step "><p>
    Copy and paste the encrypted password string in the
    <code class="filename">pass_through.yml</code> file as a value for the
    <code class="literal">password</code> field <span class="bold"><strong>enclosed in double
    quotes</strong></span>.
   </p></li><li class="step "><p>
    Enter the <code class="literal">username</code>, <code class="literal">ip</code>, and
    <code class="literal">id</code> of the vCenter server in the Global section of the
    <code class="filename">pass_through.yml</code> file. Use the values recorded in the
    previous step.
   </p></li><li class="step "><p>
     In the <code class="literal">servers</code> section of the
     <code class="filename">pass_through.yml</code> file, add the details about the
     nova Compute Proxy and OVSvApp VMs that was recorded in the previous
     step.
    </p><div class="verbatim-wrap"><pre class="screen"># Here the 'id' refers to the name of the node running the
      # esx-compute-proxy. This is identical to the 'servers.id' in
      # servers.yml.
      # NOTE: There should be one esx-compute-proxy node per ESX
      # resource pool or cluster.
      # cluster_dvs_mapping in the format
      # 'Datacenter-name/host/Cluster-Name:Trunk-DVS-Name'
      # Here 'host' is a string and should not be changed or
      # substituted.
      # vcenter_id is same as the 'vcenter-uuid' obtained in the global
      # section.
      # 'id': is the name of the appliance manually installed
      # 'vcenter_cluster': Name of the vcenter target cluster
      # esx_hostname: Name of the esx host hosting the ovsvapp
      # NOTE: For every esx host in a cluster there should be an ovsvapp
      # instance running.
      id: esx-compute1
      data:
        vmware:
          vcenter_cluster: &lt;vmware cluster1 name&gt;
          vcenter_id: &lt;vcenter-uuid&gt;
    -
      id: ovsvapp1
      data:
        vmware:
          vcenter_cluster: &lt;vmware cluster1 name&gt;
          cluster_dvs_mapping: &lt;cluster dvs mapping&gt;
          esx_hostname: &lt;esx hostname hosting the ovsvapp&gt;
          vcenter_id: &lt;vcenter-uuid&gt;
    -
      id: ovsvapp2
      data:
        vmware:
          vcenter_cluster: &lt;vmware cluster1 name&gt;
          cluster_dvs_mapping: &lt;cluster dvs mapping&gt;
          esx_hostname: &lt;esx hostname hosting the ovsvapp&gt;
          vcenter_id: &lt;vcenter-uuid&gt;</pre></div><p>
     The VM <code class="literal">id</code> string should match exactly with the data
     written in the <code class="filename">servers.yml</code> file.
    </p></li><li class="step "><p>
     Edit the <code class="filename">servers.yml</code> file, adding the nova Proxy
     VM and OVSvApp information recorded in the previous step.
    </p><div class="verbatim-wrap"><pre class="screen"># Below entries shall be added by the user
    # for entry-scale-kvm-esx after following
    # the doc instructions in creating the
    # esx-compute-proxy VM Appliance and the
    # esx-ovsvapp VM Appliance.
    # Added just for the reference
    # NOTE: There should be one esx-compute per
    # Cluster and one ovsvapp per Hypervisor in
    # the Cluster.
    # id - is the name of the virtual appliance
    # ip-addr - is the Mgmt ip address of the appliance
    # The values shown below are examples and has to be
    # substituted based on your setup.
    # Nova Compute proxy node
    - id: esx-compute1
      server-group: RACK1
      ip-addr: 192.168.24.129
      role: ESX-COMPUTE-ROLE
    # OVSVAPP node
    - id: ovsvapp1
      server-group: RACK1
      ip-addr: 192.168.24.130
      role: OVSVAPP-ROLE
    - id: ovsvapp2
      server-group: RACK1
      ip-addr: 192.168.24.131
      role: OVSVAPP-ROLE</pre></div><p>
     Examples of <code class="filename">pass_through.yml</code> and
     <code class="filename">servers.yml</code> files:
    </p><div class="verbatim-wrap"><pre class="screen">pass_through.yml
product:
  version: 2
pass-through:
  global:
    vmware:
      - username: administrator@vsphere.local
        ip: 10.84.79.3
        port: '443'
        cert_check: false
        password: @hos@U2FsdGVkX19aqGOUYGgcAIMQSN2lZ1X+gyNoytAGCTI=
        id: a0742a39-860f-4177-9f38-e8db82ad59c6
  servers:
    - data:
        vmware:
          vcenter_cluster: QE
          vcenter_id: a0742a39-860f-4177-9f38-e8db82ad59c6
      id: lvm-nova-compute1-esx01-qe
    - data:
        vmware:
          vcenter_cluster: QE
          cluster_dvs_mapping: 'PROVO/host/QE:TRUNK-DVS-QE'
          esx_hostname: esx01.qe.provo
          vcenter_id: a0742a39-860f-4177-9f38-e8db82ad59c6
      id: lvm-ovsvapp1-esx01-qe
    - data:
        vmware:
          vcenter_cluster: QE
          cluster_dvs_mapping: 'PROVO/host/QE:TRUNK-DVS-QE'
          esx_hostname: esx02.qe.provo
          vcenter_id: a0742a39-860f-4177-9f38-e8db82ad59c6
          id: lvm-ovsvapp2-esx02-qe</pre></div><div class="verbatim-wrap"><pre class="screen">servers.yml
product:
  version: 2
servers:
  - id: deployer
    ilo-ip: 192.168.10.129
    ilo-password: 8hAcPMne
    ilo-user: CLM004
    ip-addr: 192.168.24.125
    is-deployer: true
    mac-addr: '8c:dc:d4:b4:c5:4c'
    nic-mapping: MY-2PORT-SERVER
    role: DEPLOYER-ROLE
    server-group: RACK1
  - id: controller3
    ilo-ip: 192.168.11.52
    ilo-password: 8hAcPMne
    ilo-user: HLM004
    ip-addr: 192.168.24.128
    mac-addr: '8c:dc:d4:b5:ed:b8'
    nic-mapping: MY-2PORT-SERVER
    role: CONTROLLER-ROLE
    server-group: RACK1
  - id: controller2
    ilo-ip: 192.168.10.204
    ilo-password: 8hAcPMne
    ilo-user: HLM004
    ip-addr: 192.168.24.127
    mac-addr: '8c:dc:d4:b5:ca:c8'
    nic-mapping: MY-2PORT-SERVER
    role: CONTROLLER-ROLE
    server-group: RACK2
  - id: controller1
    ilo-ip: 192.168.11.57
    ilo-password: 8hAcPMne
    ilo-user: CLM004
    ip-addr: 192.168.24.126
    mac-addr: '5c:b9:01:89:c6:d8'
    nic-mapping: MY-2PORT-SERVER
    role: CONTROLLER-ROLE
    server-group: RACK3
  # Nova compute proxy for QE cluster added manually
  - id: lvm-nova-compute1-esx01-qe
    server-group: RACK1
    ip-addr: 192.168.24.129
    role: ESX-COMPUTE-ROLE
  # OVSvApp VM for QE cluster added manually
  # First ovsvapp vm in esx01 node
  - id: lvm-ovsvapp1-esx01-qe
    server-group: RACK1
    ip-addr: 192.168.24.132
    role: OVSVAPP-ROLE
  # Second ovsvapp vm in esx02 node
  - id: lvm-ovsvapp2-esx02-qe
    server-group: RACK1
    ip-addr: 192.168.24.131
    role: OVSVAPP-ROLE
baremetal:
  subnet: 192.168.24.0
  netmask: 255.255.255.0</pre></div></li><li class="step "><p>
     Edit the <code class="filename">disks_app_vm.yml</code> file based on your
     <code class="literal">lvm</code> configuration. The attributes of <code class="literal">Volume
     Group</code>, <code class="literal">Physical Volume</code>, and <code class="literal">Logical
     Volumes</code> must be edited based on the <code class="literal">LVM</code>
     configuration of the VM.
    </p><p>
     When you partitioned <code class="literal">LVM</code> during installation, you
     received <code class="literal">Volume Group</code> name, <code class="literal">Physical
     Volume</code> name and <code class="literal">Logical Volumes</code> with their
     partition sizes.
    </p><p>
     This information can be retrieved from any of the VMs (nova Proxy VM
     or the OVSvApp VM):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo pvdisplay</pre></div><div class="verbatim-wrap"><pre class="screen"># — Physical volume —
    # PV Name /dev/sda1
    # VG Name system
    # PV Size 80.00 GiB / not usable 3.00 MiB
    # Allocatable yes
    # PE Size 4.00 MiB
    # Total PE 20479
    # Free PE 511
    # Allocated PE 19968
    # PV UUID 7Xn7sm-FdB4-REev-63Z3-uNdM-TF3H-S3ZrIZ</pre></div><p>
     The Physical Volume Name is <code class="literal">/dev/sda1</code>. And the Volume
     Group Name is <code class="literal">system</code>.
    </p><p>
     To find <code class="literal">Logical Volumes</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo fdisk -l</pre></div><div class="verbatim-wrap"><pre class="screen"># Disk /dev/sda: 80 GiB, 85899345920 bytes, 167772160 sectors
    # Units: sectors of 1 * 512 = 512 bytes
    # Sector size (logical/physical): 512 bytes / 512 bytes
    # I/O size (minimum/optimal): 512 bytes / 512 bytes
    # Disklabel type: dos
    # Disk identifier: 0x0002dc70
    # Device Boot Start End Sectors Size Id Type
    # /dev/sda1 * 2048 167772159 167770112 80G 8e Linux LVM
    # Disk /dev/mapper/system-root: 60 GiB, 64424509440 bytes,
    # 125829120 sectors
    # Units: sectors of 1 * 512 = 512 bytes
    # Sector size (logical/physical): 512 bytes / 512 bytes
    # I/O size (minimum/optimal): 512 bytes / 512 bytes
    # Disk /dev/mapper/system-swap: 2 GiB, 2147483648 bytes, 4194304 sectors
    # Units: sectors of 1 * 512 = 512 bytes
    # Sector size (logical/physical): 512 bytes / 512 bytes
    # I/O size (minimum/optimal): 512 bytes / 512 bytes
    # Disk /dev/mapper/system-LV_CRASH: 16 GiB, 17179869184 bytes,
    # 33554432 sectors
    # Units: sectors of 1 * 512 = 512 bytes
    # Sector size (logical/physical): 512 bytes / 512 bytes
    # I/O size (minimum/optimal): 512 bytes / 512 bytes
    # NOTE: Even though we have configured the SWAP partition, it is
    # not required to be configured in here. Just configure the root
    # and the LV_CRASH partition</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       The line with <code class="literal">/dev/mapper/system-root: 60 GiB, 64424509440
       bytes</code> indicates that the first logical partition is
       <code class="literal">root</code>.
      </p></li><li class="listitem "><p>
       The line with <code class="literal">/dev/mapper/system-LV_CRASH: 16 GiB, 17179869184
       bytes</code> indicates that the second logical partition is
       <code class="literal">LV_CRASH</code>.
      </p></li><li class="listitem "><p>
       The line with <code class="literal">/dev/mapper/system-swap: 2 GiB, 2147483648 bytes,
       4194304 sectors</code> indicates that the third logical partition is
       <code class="literal">swap</code>.
      </p></li></ul></div></li><li class="step "><p>
     Edit the <code class="filename">disks_app_vm.yml</code> file. It is not necessary
     to configure the <code class="literal">swap</code> partition.
    </p><div class="verbatim-wrap"><pre class="screen">volume-groups:
    - name: system (Volume Group Name)
      physical-volumes:
       - /dev/sda1 (Physical Volume Name)
      logical-volumes:
        - name: root   ( Logical Volume 1)
          size: 75%    (Size in percentage)
          fstype: ext4 ( filesystem type)
          mount: /     ( Mount point)
        - name: LV_CRASH   (Logical Volume 2)
          size: 20%        (Size in percentage)
          mount: /var/crash (Mount point)
          fstype: ext4      (filesystem type)
          mkfs-opts: -O large_file</pre></div><p>
     An example <code class="filename">disks_app_vm.yml</code> file:
    </p><div class="verbatim-wrap"><pre class="screen">disks_app_vm.yml
---
  product:
    version: 2
  disk-models:
  - name: APP-VM-DISKS
    # Disk model to be used for application vms such as nova-proxy and ovsvapp
    # /dev/sda1 is used as a volume group for /, /var/log and /var/crash
    # Additional disks can be added to either volume group
    #
    # NOTE: This is just an example file and has to filled in by the user
    # based on the lvm partition map for their virtual appliance
    # While installing the operating system opt for the LVM partition and
    # create three partitions as shown below
    # Here is an example partition map
    # In this example we have three logical partitions
    # root partition (75%)
    # swap (5%) and
    # LV_CRASH (20%)
    # Run this command 'sudo pvdisplay' on the virtual appliance to see the
    # output as shown below
    #
    # — Physical volume —
    # PV Name /dev/sda1
    # VG Name system
    # PV Size 80.00 GiB / not usable 3.00 MiB
    # Allocatable yes
    # PE Size 4.00 MiB
    # Total PE 20479
    # Free PE 511
    # Allocated PE 19968
    # PV UUID 7Xn7sm-FdB4-REev-63Z3-uNdM-TF3H-S3ZrIZ
    #
    # Next run the following command on the virtual appliance
    #
    # sudo fdisk -l
    # The output will be as shown below
    #
    # Disk /dev/sda: 80 GiB, 85899345920 bytes, 167772160 sectors
    # Units: sectors of 1 * 512 = 512 bytes
    # Sector size (logical/physical): 512 bytes / 512 bytes
    # I/O size (minimum/optimal): 512 bytes / 512 bytes
    # Disklabel type: dos
    # Disk identifier: 0x0002dc70
    # Device Boot Start End Sectors Size Id Type
    # /dev/sda1 * 2048 167772159 167770112 80G 8e Linux LVM
    # Disk /dev/mapper/system-root: 60 GiB, 64424509440 bytes,
    # 125829120 sectors
    # Units: sectors of 1 * 512 = 512 bytes
    # Sector size (logical/physical): 512 bytes / 512 bytes
    # I/O size (minimum/optimal): 512 bytes / 512 bytes
    # Disk /dev/mapper/system-swap: 2 GiB, 2147483648 bytes, 4194304 sectors
    # Units: sectors of 1 * 512 = 512 bytes
    # Sector size (logical/physical): 512 bytes / 512 bytes
    # I/O size (minimum/optimal): 512 bytes / 512 bytes
    # Disk /dev/mapper/system-LV_CRASH: 16 GiB, 17179869184 bytes,
    # 33554432 sectors
    # Units: sectors of 1 * 512 = 512 bytes
    # Sector size (logical/physical): 512 bytes / 512 bytes
    # I/O size (minimum/optimal): 512 bytes / 512 bytes
    # NOTE: Even though we have configured the SWAP partition, it is
    # not required to be configured in here. Just configure the root
    # and the LV_CRASH partition
    volume-groups:
      - name: system
        physical-volumes:
         - /dev/sda1
        logical-volumes:
          - name: root
            size: 75%
            fstype: ext4
            mount: /
          - name: LV_CRASH
            size: 20%
            mount: /var/crash
            fstype: ext4
            mkfs-opts: -O large_file</pre></div></li></ol></div></div></div><div class="sect1" id="run-config-processor"><div class="titlepage"><div><div><h2 class="title"><span class="number">27.14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Running the Configuration Processor With Applied Changes</span> <a title="Permalink" class="permalink" href="#run-config-processor">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/run-config-processor.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>run-config-processor.xml</li><li><span class="ds-label">ID: </span>run-config-processor</li></ul></div></div></div></div><p>
  If the changes are being applied to a previously deployed cloud, then after
  the previous section is completed, the Configuration Processor should be
  run with the changes that were applied.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    If nodes have been added after deployment, run the <code class="filename">site.yml</code> playbook with
    <code class="literal">generate_hosts_file</code>.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --tag "generate_hosts_file"</pre></div></li><li class="step "><p>
    Run the Configuration Processor
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml \
-e remove_deleted_servers="y" -e free_unused_addresses="y"</pre></div></li><li class="step "><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
    Run the <code class="filename">site.yml</code> playbook against only the VMs that
    were added.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --limit hlm004-cp1-esx-comp0001-mgmt, \
hlm004-cp1-esx-ovsvapp0001-mgmt,hlm004-cp1-esx-ovsvapp0002-mgmt</pre></div></li><li class="step "><p>
    Add nodes to monitoring by running the following playbook on those nodes:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-deploy.yml --tags "active_ping_checks"</pre></div></li></ol></div></div><p>
  If the changes are being applied ahead of deploying a new (greenfield) cloud, then after
  the previous section is completed, the following steps should be run.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Run the Configuration Processor
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/ardana/ansible
	<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
    Run the <code class="filename">site.yml</code> playbook against only the VMs that
    were added.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
	<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml</pre></div></li></ol></div></div></div><div class="sect1" id="test-esx-environment"><div class="titlepage"><div><div><h2 class="title"><span class="number">27.15 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Test the ESX-OVSvApp Environment</span> <a title="Permalink" class="permalink" href="#test-esx-environment">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/test-esx-environment.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>test-esx-environment.xml</li><li><span class="ds-label">ID: </span>test-esx-environment</li></ul></div></div></div></div><p>
  When all of the preceding installation steps have been completed, test the
  ESX-OVSvApp environment with the following steps:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    SSH to the Controller
   </p></li><li class="step "><p>
    Source the <code class="filename">service.osrc</code> file
   </p></li><li class="step "><p>
    Create a Network
   </p></li><li class="step "><p>
    Create a Subnet
   </p></li><li class="step "><p>
    Create a VMware-based glance image if there is not one available in the
    glance repo. The following instructions can be used to create an image
    that can be used by nova to to create a VM in vCenter.
   </p><ol type="a" class="substeps "><li class="step "><p>
      Download a <code class="literal">vmdk</code> image file for the corresponding
      distro that you want for a VM.
     </p></li><li class="step "><p>
      Create a nova image for VMware Hypervisor
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack image create --name <em class="replaceable ">DISTRO</em> \
--container-format bare --disk-format vmdk --property vmware_disktype="sparse" \
--property vmware_adaptertype="ide" --property hypervisor_type=vmware &lt;
<em class="replaceable ">SERVER_CLOUDIMG.VMDK</em></pre></div><div class="verbatim-wrap"><pre class="screen">+--------------------+--------------------------------------+
| Property           | Value                                |
+--------------------+--------------------------------------+
| checksum           | 45a4a06997e64f7120795c68beeb0e3c     |
| container_format   | bare                                 |
| created_at         | 2018-02-17T10:42:14Z                 |
| disk_format        | vmdk                                 |
| hypervisor_type    | vmware                               |
| id                 | 17e4915a-ada0-4b95-bacf-ba67133f39a7 |
| min_disk           | 0                                    |
| min_ram            | 0                                    |
| name               | leap                                 |
| owner              | 821b7bb8148f439191d108764301af64     |
| protected          | False                                |
| size               | 372047872                            |
| status             | active                               |
| tags               | []                                   |
| updated_at         | 2018-02-17T10:42:23Z                 |
| virtual_size       | None                                 |
| visibility         | shared                               |
| vmware_adaptertype | ide                                  |
| vmware_disktype    | sparse                               |
+--------------------+--------------------------------------+</pre></div><p>
      The image you created needs to be uploaded or saved. Otherwise the size
      will still be <code class="literal">0</code>.
     </p></li><li class="step "><p>
      Upload/save the image
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack image save --file \
./<em class="replaceable ">SERVER_CLOUDIMG.VMDK</em> 17e4915...133f39a7</pre></div></li><li class="step "><p>
      After saving the image, check that it is active and has a valid size.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack image list</pre></div><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+------------------------+--------+
| ID                                   | Name                   | Status |
+--------------------------------------+------------------------+--------+
| c48a9349-8e5c-4ca7-81ac-9ed8e2cab3aa | cirros-0.3.2-i386-disk | active |
| 17e4915a-ada0-4b95-bacf-ba67133f39a7 | leap                   | active |
+--------------------------------------+------------------------+--------+</pre></div></li><li class="step "><p>
      Check the details of the image
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack image show 17e4915...133f39a7</pre></div><div class="verbatim-wrap"><pre class="screen">+------------------+------------------------------------------------------------------------------+
| Field            | Value                                                                       |
+------------------+------------------------------------------------------------------------------+
| checksum         | 45a4a06997e64f7120795c68beeb0e3c                                            |
| container_format | bare                                                                        |
| created_at       | 2018-02-17T10:42:14Z                                                        |
| disk_format      | vmdk                                                                        |
| file             | /v2/images/40aa877c-2b7a-44d6-9b6d-f635dcbafc77/file                        |
| id               | 17e4915a-ada0-4b95-bacf-ba67133f39a7                                        |
| min_disk         | 0                                                                           |
| min_ram          | 0                                                                           |
| name             | leap                                                                        |
| owner            | 821b7bb8148f439191d108764301af64                                            |
| properties       | hypervisor_type='vmware', vmware_adaptertype='ide', vmware_disktype='sparse' |
| protected        | False                                                                       |
| schema           | /v2/schemas/image                                                           |
| size             | 372047872                                                                   |
| status           | active                                                                      |
| tags             |                                                                             |
| updated_at       | 2018-02-17T10:42:23Z                                                        |
| virtual_size     | None                                                                        |
| visibility       | shared                                                                      |
+------------------+------------------------------------------------------------------------------+</pre></div></li></ol></li><li class="step "><p>
      Create security rules. All security rules must be created before any VMs
      are created. Otherwise the security rules will have no impact. Rebooting
      or restarting <code class="literal">neutron-ovsvapp-agent</code> will have no
      effect. The following example shows creating a security rule for ICMP:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack security group rule create default --protocol icmp</pre></div></li><li class="step "><p>
      Create a nova instance with the VMware VMDK-based image and target it
      to the new cluster in the vCenter.
     </p></li><li class="step "><p>
      The new VM will appear in the vCenter.
     </p></li><li class="step "><p>
      The respective PortGroups for the OVSvApp on the Trunk-DVS will be
      created and connected.
     </p></li><li class="step "><p>
      Test the VM for connectivity and service.
     </p></li></ol></div></div></div></div><div class="chapter " id="integrate-nsx-vsphere"><div class="titlepage"><div><div><h2 class="title"><span class="number">28 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Integrating NSX for vSphere</span> <a title="Permalink" class="permalink" href="#integrate-nsx-vsphere">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/integrate-nsx-vsphere.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>integrate-nsx-vsphere.xml</li><li><span class="ds-label">ID: </span>integrate-nsx-vsphere</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#nsx-vsphere-vm"><span class="number">28.1 </span><span class="name">Integrating with NSX for vSphere</span></a></span></dt><dt><span class="section"><a href="#nsx-vsphere-baremetal"><span class="number">28.2 </span><span class="name">Integrating with NSX for vSphere on Baremetal</span></a></span></dt><dt><span class="section"><a href="#nsx-verification"><span class="number">28.3 </span><span class="name">Verifying the NSX-v Functionality After Integration</span></a></span></dt></dl></div></div><p>
  This section describes the installation and integration of NSX-v, a Software
  Defined Networking (SDN) network virtualization and security platform for
  VMware's vSphere.
 </p><p>
  VMware's NSX embeds networking and security functionality, normally handled
  by hardware, directly into the hypervisor. NSX can reproduce, in software, an
  entire networking environment, and provides a complete set of logical
  networking elements and services including logical switching, routing,
  firewalling, load balancing, VPN, QoS, and monitoring. Virtual networks are
  programmatically provisioned and managed independent of the underlying
  hardware.
 </p><p>
  VMware's neutron plugin called NSX for vSphere (NSX-v) has been tested under
  the following scenarios:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Virtual <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> deployment
   </p></li><li class="listitem "><p>
    Baremetal <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> deployment
   </p></li></ul></div><p>
  Installation instructions are provided for both scenarios. This documentation
  is meant as an example of how to integrate VMware's NSX-v neutron plugin
  with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. The examples in this documentation are not suitable for
  all environments. To configure this for your specific environment, use the
  design guide <a class="link" href="https://communities.vmware.com/servlet/JiveServlet/downloadBody/27683-102-8-41631/NSX" target="_blank">Reference
  Design: VMware® NSX for vSphere (NSX) Network Virtualization Design
  Guide</a>.
 </p><p>
  This section includes instructions for:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Integrating with NSX for vSphere on Baremetal
   </p></li><li class="listitem "><p>
    Integrating with NSX for vSphere on virtual machines with changes necessary
    for Baremetal integration
   </p></li><li class="listitem "><p>
    Verifying NSX-v functionality
   </p></li></ul></div><div class="sect1" id="nsx-vsphere-vm"><div class="titlepage"><div><div><h2 class="title"><span class="number">28.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Integrating with NSX for vSphere</span> <a title="Permalink" class="permalink" href="#nsx-vsphere-vm">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-vm.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-vm.xml</li><li><span class="ds-label">ID: </span>nsx-vsphere-vm</li></ul></div></div></div></div><p>
  This section describes the installation steps and requirements for
  integrating with NSX for vSphere on virtual machines and baremetal hardware.
 </p><div class="sect2" id="nsx-pre-integration"><div class="titlepage"><div><div><h3 class="title"><span class="number">28.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Pre-Integration Checklist</span> <a title="Permalink" class="permalink" href="#nsx-pre-integration">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-pre_integration.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-pre_integration.xml</li><li><span class="ds-label">ID: </span>nsx-pre-integration</li></ul></div></div></div></div><p>
    The following installation and integration instructions assumes an
    understanding of VMware's ESXI and vSphere products for setting up virtual
    environments.
   </p><p>
    Please review the following requirements for the VMware vSphere
    environment.
   </p><p>
    <span class="bold"><strong>Software Requirements</strong></span>
   </p><p>
    Before you install or upgrade NSX, verify your software versions. The
    following are the required versions.
   </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th><p>Software</p></th><th><p>Version</p></th></tr></thead><tbody><tr><td><p><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></p></td><td><p>8</p></td></tr><tr><td><p>VMware NSX-v Manager</p></td><td><p>6.3.4 or higher</p></td></tr><tr><td><p>VMWare NSX-v neutron Plugin</p></td><td><p>Pike Release (TAG=11.0.0)</p></td></tr><tr><td><p>VMWare ESXi and vSphere Appliance (vSphere web Client)</p></td><td><p>6.0 or higher</p></td></tr></tbody></table></div><p>
    A vCenter server (appliance) is required to manage the vSphere
    environment. It is recommended that you install a vCenter appliance as an
    ESX virtual machine.
   </p><div id="id-1.3.6.11.9.3.8" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
     Each ESXi compute cluster is required to have shared storage between the
     hosts in the cluster, otherwise attempts to create instances through
     nova-compute will fail.
    </p></div></div><div class="sect2" id="id-1.3.6.11.9.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">28.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing <span class="productname">OpenStack</span></span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.9.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-vm.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-vm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="productname">OpenStack</span> can be deployed in two ways: on baremetal (physical hardware) or in
   an ESXi virtual environment on virtual machines. The following instructions
   describe how to install <span class="productname">OpenStack</span>.
 </p><div id="id-1.3.6.11.9.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   <span class="bold"><strong>Changes for installation on baremetal hardware are noted in each
   section.</strong></span>  
  </p></div><p>
   This deployment example will consist of two ESXi clusters at minimum: a
   <code class="literal">control-plane</code> cluster and a <code class="literal">compute</code>
   cluster. The control-plane cluster must have 3 ESXi hosts minimum (due to
   VMware's recommendation that each NSX controller virtual machine is on a
   separate host). The compute cluster must have 2 ESXi hosts minimum.  There
   can be multiple compute clusters. The following table outlines the virtual
   machine specifications to be built in the control-plane cluster:
  </p><div class="table" id="nsx-hw-reqs-vm"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 28.1: </span><span class="name">NSX Hardware Requirements for Virtual Machine Integration </span><a title="Permalink" class="permalink" href="#nsx-hw-reqs-vm">#</a></h6></div><div class="table-contents"><table class="table" summary="NSX Hardware Requirements for Virtual Machine Integration" border="1"><colgroup><col class="1" /><col class="2" /><col class="3" /><col class="4" /><col class="5" /><col class="6" /></colgroup><thead><tr><th><p>Virtual Machine Role</p></th><th><p>Required Number</p></th><th><p>Disk</p></th><th><p>Memory</p></th><th><p>Network</p></th><th><p>CPU</p></th></tr></thead><tbody><tr><td>
       <p>
        Dedicated lifecycle manager
       </p>
       <p>
        <span class="bold"><strong>Baremetal - not needed</strong></span>
       </p>
      </td><td><p>1</p></td><td><p>100GB</p></td><td><p>8GB</p></td><td><p>3 VMXNET Virtual Network Adapters</p></td><td><p>4 vCPU</p></td></tr><tr><td>
       <p>
        Controller virtual machines
       </p>
       <p>
        <span class="bold"><strong>Baremetal - not needed</strong></span>
       </p>
      </td><td><p>3</p></td><td><p>3 x 300GB</p></td><td><p>32GB</p></td><td><p>3 VMXNET Virtual Network Adapters</p></td><td><p>8 vCPU</p></td></tr><tr><td><p>Compute virtual machines</p></td><td><p>1 per compute cluster</p></td><td><p>80GB</p></td><td><p>4GB</p></td><td><p>3 VMXNET Virtual Network Adapters</p></td><td><p>2 vCPU</p></td></tr><tr><td><p>NSX Edge Gateway/DLR/Metadata-proxy appliances</p></td><td></td><td><p>Autogenerated by NSXv</p></td><td><p>Autogenerated by NSXv</p></td><td><p>Autogenerated by NSXv</p></td><td><p>Autogenerated by NSXv</p></td></tr></tbody></table></div></div><p>
   <span class="bold"><strong>Baremetal: In addition to the ESXi hosts, it is
   recommended to have one physical host for the Cloud Lifecycle Manager node and three physical
   hosts for the controller nodes.</strong></span>
  </p><div class="sect3" id="nsx-ntwk-requirements"><div class="titlepage"><div><div><h4 class="title"><span class="number">28.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Requirements</span> <a title="Permalink" class="permalink" href="#nsx-ntwk-requirements">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-ntwk-requirements.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-ntwk-requirements.xml</li><li><span class="ds-label">ID: </span>nsx-ntwk-requirements</li></ul></div></div></div></div><p>
  NSX-v requires the following for networking:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    The ESXi hosts, vCenter, and the NSX Manager appliance must resolve DNS lookup.
   </p></li><li class="listitem "><p>
    The ESXi host must have the NTP service configured and enabled.
   </p></li><li class="listitem "><p>
    Jumbo frames must be enabled on the switch ports that the ESXi hosts are connected to.
   </p></li><li class="listitem "><p>
    The ESXi hosts must have at least 2 physical network cards each.
   </p></li></ul></div></div><div class="sect3" id="nsx-ntwk-model"><div class="titlepage"><div><div><h4 class="title"><span class="number">28.1.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Model</span> <a title="Permalink" class="permalink" href="#nsx-ntwk-model">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-ntwk-model.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-ntwk-model.xml</li><li><span class="ds-label">ID: </span>nsx-ntwk-model</li></ul></div></div></div></div><p>
  The model in these instructions requires the following networks:
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.6.11.9.4.8.3.1"><span class="term ">ESXi Hosts and vCenter</span></dt><dd><p>
     This is the network that the ESXi hosts and vCenter use to route traffic with.
    </p></dd><dt id="id-1.3.6.11.9.4.8.3.2"><span class="term ">NSX Management</span></dt><dd><p>
      The network which the NSX controllers and NSX Manager will use.
    </p></dd><dt id="id-1.3.6.11.9.4.8.3.3"><span class="term ">NSX VTEP Pool</span></dt><dd><p>
     The network that NSX uses to create endpoints for VxLAN tunnels.
    </p></dd><dt id="id-1.3.6.11.9.4.8.3.4"><span class="term ">Management</span></dt><dd><p>
     The network that <span class="productname">OpenStack</span> uses for deployment and maintenance of the cloud.
    </p></dd><dt id="id-1.3.6.11.9.4.8.3.5"><span class="term ">Internal API (optional)</span></dt><dd><p>
     The network group that will be used for management (private API) traffic within the cloud.
    </p></dd><dt id="id-1.3.6.11.9.4.8.3.6"><span class="term ">External API</span></dt><dd><p>
     This is the network that users will use to make requests to the cloud.
    </p></dd><dt id="id-1.3.6.11.9.4.8.3.7"><span class="term ">External VM</span></dt><dd><p>
     VLAN-backed provider network for external access to guest VMs (floating IPs).
    </p></dd></dl></div></div><div class="sect3" id="id-1.3.6.11.9.4.9"><div class="titlepage"><div><div><h4 class="title"><span class="number">28.1.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">vSphere port security settings</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.9.4.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-vm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-vm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="bold"><strong>Baremetal: Even though the <span class="productname">OpenStack</span>
   deployment is on baremetal, it is still necessary to define each VLAN within
   a vSphere Distributed Switch for the nova compute proxy virtual
   machine.</strong></span>
  </p><p>
   The vSphere port security settings for both VMs and baremetal are shown in the
   table below.
   </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="1" /><col class="2" /><col class="3" /><col class="4" /></colgroup><thead><tr><th><p>Network Group</p></th><th><p>VLAN Type</p></th><th><p>Interface</p></th><th><p>vSphere Port Group Security Settings</p></th></tr></thead><tbody><tr><td><p>ESXi Hosts and vCenter</p></td><td><p>Tagged</p></td><td><p>N/A</p></td><td><p>Defaults</p></td></tr><tr><td><p>NSX Manager</p>
      <p>Must be able to reach ESXi Hosts and vCenter</p>
      </td><td><p>Tagged</p></td><td><p>N/A</p></td><td><p>Defaults</p></td></tr><tr><td><p>NSX VTEP Pool</p></td><td><p>Tagged</p></td><td><p>N/A</p></td><td><p>Defaults</p></td></tr><tr><td><p>Management</p></td><td><p>Tagged or Untagged</p></td><td><p>eth0</p></td><td>
       <p>
        <span class="bold"><strong>Promiscuous Mode</strong></span>: Accept
       </p>
       <p>
        <span class="bold"><strong>MAC Address Changes</strong></span>: Reject
       </p>
       <p>
        <span class="bold"><strong>Forged Transmits</strong></span>:Reject
       </p>
      </td></tr><tr><td>
       <p>
        Internal API (Optional, may be combined with the Management Network. If
        network segregation is required for security reasons, you can keep this
        as a separate network.)
       </p>
      </td><td><p>Tagged</p></td><td><p>eth2</p></td><td>
       <p>
        <span class="bold"><strong>Promiscuous Mode</strong></span>: Accept
       </p>
       <p>
        <span class="bold"><strong>MAC Address Changes</strong></span>: Reject
       </p>
       <p>
        <span class="bold"><strong>Forged Transmits</strong></span>: Accept
       </p>
      </td></tr><tr><td><p>External API (Public)</p></td><td><p>Tagged</p></td><td><p>eth1</p></td><td>
       <p>
        <span class="bold"><strong>Promiscuous Mode</strong></span>: Accept
       </p>
       <p>
        <span class="bold"><strong>MAC Address Changes</strong></span>: Reject
       </p>
       <p>
        <span class="bold"><strong>Forged Transmits</strong></span>: Accept
       </p>
      </td></tr><tr><td><p>External VM</p></td><td><p>Tagged</p></td><td><p>N/A</p></td><td>
       <p>
        <span class="bold"><strong>Promiscuous Mode</strong></span>: Accept
       </p>
       <p>
        <span class="bold"><strong>MAC Address Changes</strong></span>: Reject
       </p>
       <p>
        <span class="bold"><strong>Forged Transmits</strong></span>: Accept
       </p>
      </td></tr><tr><td><p><span class="bold"><strong>Baremetal Only: IPMI</strong></span></p></td><td><p>Untagged</p></td><td><p>N/A</p></td><td><p>N/A</p></td></tr></tbody></table></div></div><div class="sect3" id="nsx-configure-vsphere-env"><div class="titlepage"><div><div><h4 class="title"><span class="number">28.1.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the vSphere Environment</span> <a title="Permalink" class="permalink" href="#nsx-configure-vsphere-env">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-configure-vsphere_env.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-configure-vsphere_env.xml</li><li><span class="ds-label">ID: </span>nsx-configure-vsphere-env</li></ul></div></div></div></div><p>
   Before deploying <span class="productname">OpenStack</span> with NSX-v, the VMware vSphere environment must be
   properly configured, including setting up vSphere distributed switches and
   port groups. For detailed instructions, see <a class="xref" href="#install-esx-ovsvapp" title="Chapter 27. Installing ESX Computes and OVSvAPP">Chapter 27, <em>Installing ESX Computes and OVSvAPP</em></a>.
  </p><p>
   Installing and configuring the VMware NSX Manager and creating the NSX
   network within the vSphere environment is covered below.
  </p><p>
   Before proceeding with the installation, ensure that the following are
   configured in the vSphere environment.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The vSphere datacenter is configured with at least two clusters, one
     <span class="bold"><strong>control-plane</strong></span> cluster and one <span class="bold"><strong>compute</strong></span> cluster.
    </p></li><li class="listitem "><p>
     Verify that all software, hardware, and networking requirements have been
     met.
    </p></li><li class="listitem "><p>
     Ensure the vSphere distributed virtual switches (DVS) are configured for each
     cluster.
    </p></li></ul></div><div id="id-1.3.6.11.9.4.10.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    The MTU setting for each DVS should be set to 1600. NSX should
    automatically apply this setting to each DVS during the setup
    process. Alternatively, the setting can be manually applied to each DVS
    before setup if desired.
   </p></div><p>
   Make sure there is a copy of the SUSE Linux Enterprise Server 12 SP4 <code class="filename">.iso</code> in the
   <code class="literal">ardana</code> home directory,
   <code class="filename">var/lib/ardana</code>, and that it is called
   <code class="filename">sles12sp4.iso</code>.
  </p><p>
   Install the <code class="literal">open-vm-tools</code> package.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper install open-vm-tools</pre></div><div class="sect4" id="nsx-install-manager"><div class="titlepage"><div><div><h5 class="title"><span class="number">28.1.2.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Install NSX Manager</span> <a title="Permalink" class="permalink" href="#nsx-install-manager">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-install-manager.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-install-manager.xml</li><li><span class="ds-label">ID: </span>nsx-install-manager</li></ul></div></div></div></div><p>
  The NSX Manager is the centralized network management component of NSX. It
  provides a single point of configuration and REST API entry-points.
 </p><p>
   The NSX Manager is installed as a virtual appliance on one of the ESXi hosts
   within the vSphere environment. This guide will cover installing the
   appliance on one of the ESXi hosts within the control-plane cluster. For
   more detailed information, refer to <a class="link" href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-D8578F6E-A40C-493A-9B43-877C2B75ED52.html" target="_blank">VMware's
   NSX Installation Guide.</a>
 </p><p>
  To install the NSX Manager, download the virtual appliance from <a class="link" href="https://www.vmware.com/go/download-nsx-vsphere" target="_blank">VMware</a> and
  deploy the appliance within vCenter onto one of the ESXi hosts. For
  information on deploying appliances within vCenter, refer to VMware's
  documentation for ESXi <a class="link" href="https://pubs.vmware.com/vsphere-55/index.jsp?topic=%2Fcom.vmware.vsphere.vm_admin.doc%2FGUID-AFEDC48B-C96F-4088-9C1F-4F0A30E965DE.html" target="_blank">5.5</a>
  or <a class="link" href="https://pubs.vmware.com/vsphere-60/index.jsp?topic=%2Fcom.vmware.vsphere.vm_admin.doc%2FGUID-AFEDC48B-C96F-4088-9C1F-4F0A30E965DE.html" target="_blank">6.0</a>.
 </p><p>
  During the deployment of the NSX Manager appliance, be aware of the
  following:
 </p><p>
  When prompted, select <span class="guimenu ">Accept extra configuration options</span>.
  This will present options for configuring IPv4 and IPv6 addresses, the
  default gateway, DNS, NTP, and SSH properties during the installation, rather
  than configuring these settings manually after the installation.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Choose an ESXi host that resides within the control-plane cluster.
   </p></li><li class="listitem "><p>
    Ensure that the network mapped port group is the DVS port group that
    represents the VLAN the NSX Manager will use for its networking (in this
    example it is labeled as the <code class="literal">NSX Management</code> network).
   </p></li></ul></div><div id="id-1.3.6.11.9.4.10.10.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   The IP address assigned to the NSX Manager must be able to resolve
   reverse DNS.
  </p></div><p>
  Power on the NSX Manager virtual machine after it finishes deploying and wait
  for the operating system to fully load. When ready, carry out the following
  steps to have the NSX Manager use single sign-on (SSO) and to
  register the NSX Manager with vCenter:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Open a web browser and enter the hostname or IP address that was assigned
    to the NSX Manager during setup.
   </p></li><li class="step "><p>
    Log in with the username <code class="literal">admin</code> and the
    password set during the deployment.
   </p></li><li class="step "><p>
    After logging in, click on <span class="guimenu ">Manage vCenter Registration</span>.
   </p></li><li class="step "><p>
    Configure the NSX Manager to connect to the vCenter server.
   </p></li><li class="step "><p>
    Configure NSX manager for single sign on (SSO) under the <span class="guimenu ">Lookup
    Server URL</span> section.
   </p></li></ol></div></div><div id="id-1.3.6.11.9.4.10.10.11" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   When configuring SSO, use <code class="literal">Lookup Service Port 443</code> for
   vCenter version 6.0. Use <code class="literal">Lookup Service Port 7444</code> for
   vCenter version 5.5.
  </p><p>
   SSO makes vSphere and NSX more secure by allowing the various components to
   communicate with each other through a secure token exchange mechanism,
   instead of requiring each component to authenticate a user separately. For
   more details, refer to VMware's documentation on <a class="link" href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-523B0D77-AAB9-4535-B326-1716967EC0D2.html" target="_blank">Configure
   Single Sign-On</a>.
  </p></div><p>
  Both the <code class="literal">Lookup Service URL</code> and the <code class="literal">vCenter
  Server</code> sections should have a status of
  <code class="literal">connected</code> when configured properly.
 </p><p>
  Log into the vSphere Web Client (log out and and back in if already logged
  in). The NSX Manager will appear under the <span class="guimenu ">Networking &amp;
  Security</span> section of the client.
 </p><div id="id-1.3.6.11.9.4.10.10.14" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   The <span class="guimenu ">Networking &amp; Security</span> section will not appear
   under the vSphere desktop client. Use of the web client is required for the
   rest of this process.
  </p></div></div><div class="sect4" id="nsx-add-controllers"><div class="titlepage"><div><div><h5 class="title"><span class="number">28.1.2.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Add NSX Controllers</span> <a title="Permalink" class="permalink" href="#nsx-add-controllers">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-add-controllers.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-add-controllers.xml</li><li><span class="ds-label">ID: </span>nsx-add-controllers</li></ul></div></div></div></div><p>
  The NSX controllers serve as the central control point for all logical
  switches within the vSphere environment's network, and they maintain
  information about all hosts, logical switches (VXLANs), and distributed
  logical routers.
 </p><p>
  NSX controllers will each be deployed as a virtual appliance on the ESXi
  hosts within the control-plane cluster to form the NSX Controller
  cluster. For details about NSX controllers and the NSX control plane in
  general, refer to <a class="link" href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-4E0FEE83-CF2C-45E0-B0E6-177161C3D67C.html" target="_blank">VMware's
  NSX documentation</a>.
 </p><div id="id-1.3.6.11.9.4.10.11.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   Whatever the size of the NSX deployment, the following conditions must be
   met:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Each NSX Controller cluster must contain three controller nodes. Having a
     different number of controller nodes is not supported.
    </p></li><li class="listitem "><p>
     Before deploying NSX Controllers, you must deploy an NSX Manager appliance
     and register vCenter with NSX Manager.
    </p></li><li class="listitem "><p>
     Determine the IP pool settings for your controller cluster, including the
     gateway and IP address range. DNS settings are optional.
    </p></li><li class="listitem "><p>
     The NSX Controller IP network must have connectivity to the NSX Manager
     and to the management interfaces on the ESXi hosts.
    </p></li></ul></div></div><p>
   Log in to the vSphere web client and do the following steps to add the NSX
   controllers:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     In vCenter, navigate to <span class="guimenu ">Home</span>, select
     <span class="guimenu ">Networking &amp;
     Security</span> › <span class="guimenu ">Installation</span>, and then
     select the <span class="guimenu ">Management</span> tab.
    </p></li><li class="step "><p>
     In the <span class="guimenu ">NSX Controller nodes</span> section, click the
     <span class="guimenu ">Add Node</span> icon represented by a green plus sign.
    </p></li><li class="step "><p>
     Enter the NSX Controller settings appropriate to your
     environment. If you are following this example, use the control-plane
     clustered ESXi hosts and control-plane DVS port group for the controller
     settings.
    </p></li><li class="step "><p>
     If it has not already been done, create an IP pool for the NSX Controller
     cluster with at least three IP addressess by clicking <span class="guimenu ">New IP
     Pool</span>. Individual controllers can be in separate IP subnets, if
     necessary.
    </p></li><li class="step "><p>
     Click <span class="guimenu ">OK</span> to deploy the controller. After the first controller is
     completely deployed, deploy two additional controllers.
    </p></li></ol></div></div><div id="id-1.3.6.11.9.4.10.11.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    Three NSX controllers is mandatory. VMware recommends configuring a DRS
    anti-affinity rule to prevent the controllers from residing on the same
    ESXi host. See more information about <a class="link" href="https://pubs.vmware.com/vsphere-51/index.jsp?topic=%2Fcom.vmware.vsphere.resmgmt.doc%2FGUID-FF28F29C-8B67-4EFF-A2EF-63B3537E6934.html" target="_blank">DRS
    Affinity Rules</a>.
   </p></div></div><div class="sect4" id="nsx-prepare-clusters"><div class="titlepage"><div><div><h5 class="title"><span class="number">28.1.2.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prepare Clusters for NSX Management</span> <a title="Permalink" class="permalink" href="#nsx-prepare-clusters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-prepare-clusters.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-prepare-clusters.xml</li><li><span class="ds-label">ID: </span>nsx-prepare-clusters</li></ul></div></div></div></div><p>
  During <span class="guimenu ">Host Preparation</span>, the NSX Manager:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Installs the NSX kernel modules on ESXi hosts that are members of vSphere
    clusters
   </p></li><li class="listitem "><p>
    Builds the NSX control-plane and management-plane infrastructure
   </p></li></ul></div><p>
  The NSX kernel modules are packaged in <code class="filename">VIB</code>
  (vSphere Installation Bundle) files. They run within the hypervisor kernel and
  provide services such as distributed routing, distributed firewall, and VXLAN
  bridging capabilities. These files are installed on a per-cluster level, and
  the setup process deploys the required software on all ESXi hosts in the
  target cluster. When a new ESXi host is added to the cluster, the required
  software is automatically installed on the newly added host.
 </p><p>
  Before beginning the NSX host preparation process, make sure of the following
  in your environment:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Register vCenter with NSX Manager and deploy the NSX controllers.
   </p></li><li class="listitem "><p>
    Verify that DNS reverse lookup returns a fully qualified domain name when
    queried with the IP address of NSX Manager.
   </p></li><li class="listitem "><p>
    Verify that the ESXi hosts can resolve the DNS name of vCenter server.
   </p></li><li class="listitem "><p>
    Verify that the ESXi hosts can connect to vCenter Server on port 80.
   </p></li><li class="listitem "><p>
    Verify that the network time on vCenter Server and the ESXi hosts is
    synchronized.
   </p></li><li class="listitem "><p>
    For each vSphere cluster that will participate in NSX, verify that the ESXi
    hosts within each respective cluster are attached to a common VDS.
   </p><p>
    For example, given a deployment with two clusters named Host1 and
    Host2. Host1 is attached to VDS1 and VDS2. Host2 is attached to VDS1 and
    VDS3. When you prepare a cluster for NSX, you can only associate NSX with
    VDS1 on the cluster. If you add another host (Host3) to the cluster and
    Host3 is not attached to VDS1, it is an invalid configuration, and Host3
    will not be ready for NSX functionality.
   </p></li><li class="listitem "><p>
    If you have vSphere Update Manager (VUM) in your environment, you must
    disable it before preparing clusters for network virtualization. For
    information on how to check if VUM is enabled and how to disable it if
    necessary, see the <a class="link" href="http://kb.vmware.com/kb/2053782" target="_blank">VMware knowledge base</a>.
   </p></li><li class="listitem "><p>
    In the vSphere web client, ensure that the cluster is in the resolved state
    (listed under the <span class="guimenu ">Host Preparation</span> tab). If the Resolve option does not
    appear in the cluster's Actions list, then it is in a resolved state.
   </p></li></ul></div><p>
  To prepare the vSphere clusters for NSX:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    In vCenter, select <span class="guimenu ">Home</span> › <span class="guimenu ">Networking
    &amp; Security</span> › <span class="guimenu ">Installation</span>, and
    then select the <span class="guimenu ">Host Preparation</span> tab.
   </p></li><li class="step "><p>
    Continuing with the example in these instructions, click on the
    <span class="guimenu ">Actions</span> button (gear icon) and select
    <span class="guimenu ">Install</span> for both the control-plane cluster and compute
    cluster (if you are using something other than this example, then only
    install on the clusters that require NSX logical switching, routing, and
    firewalls).
   </p></li><li class="step "><p>
    Monitor the installation until the <code class="literal">Installation Status</code>
    column displays a green check mark.
   </p><div id="id-1.3.6.11.9.4.10.12.8.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
     While installation is in
     progress, do not deploy, upgrade, or uninstall any service or component.
    </p></div><div id="id-1.3.6.11.9.4.10.12.8.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
     If the <code class="literal">Installation Status</code> column displays a red
     warning icon and says <code class="literal">Not Ready</code>, click
     <span class="guimenu ">Resolve</span>. Clicking <span class="guimenu ">Resolve</span> might
     result in a reboot of the host. If the installation is still not
     successful, click the warning icon. All errors will be displayed. Take the
     required action and click <span class="guimenu ">Resolve</span> again.
    </p></div></li><li class="step "><p>
    To verify the VIBs (<code class="filename">esx-vsip</code> and
    <code class="filename">esx-vxlan</code>) are installed and registered, SSH into an
    ESXi host within the prepared cluster. List the names and versions of the
    VIBs installed by running the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>esxcli software vib list | grep esx</pre></div><div class="verbatim-wrap"><pre class="screen">...
esx-vsip      6.0.0-0.0.2732470    VMware  VMwareCertified   2015-05-29
esx-vxlan     6.0.0-0.0.2732470    VMware  VMwareCertified   2015-05-29
...</pre></div></li></ol></div></div><div id="id-1.3.6.11.9.4.10.12.9" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   After host preparation:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     A host reboot is not required
    </p></li><li class="listitem "><p>
     If you add a host to a prepared cluster, the NSX VIBs are automatically
     installed on the host.
    </p></li><li class="listitem "><p>
     If you move a host to an unprepared cluster, the NSX VIBs are
     automatically uninstalled from the host. In this case, a host reboot
     is required to complete the uninstall process.
    </p></li></ul></div></div></div><div class="sect4" id="nsx-configure-vxlan-transport"><div class="titlepage"><div><div><h5 class="title"><span class="number">28.1.2.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure VXLAN Transport Parameters</span> <a title="Permalink" class="permalink" href="#nsx-configure-vxlan-transport">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-configure-vxlan-transport.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-configure-vxlan-transport.xml</li><li><span class="ds-label">ID: </span>nsx-configure-vxlan-transport</li></ul></div></div></div></div><p>
  VXLAN is configured on a per-cluster basis, where each vSphere cluster that
  is to participate in NSX is mapped to a vSphere Distributed Virtual Switch
  (DVS). When mapping a vSphere cluster to a DVS, each ESXi host in that
  cluster is enabled for logical switches. The settings chosen in this section
  will be used in creating the VMkernel interface.
 </p><p>
  Configuring transport parameters involves selecting a DVS, a VLAN ID, an MTU
  size, an IP addressing mechanism, and a NIC teaming policy. The MTU for each
  switch must be set to 1550 or higher. By default, it is set to 1600 by
  NSX. This is also the recommended setting for integration with <span class="productname">OpenStack</span>.
 </p><p>
  To configure the VXLAN transport parameters:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    In the vSphere web client, navigate to
    <span class="guimenu ">Home</span> › <span class="guimenu ">Networking &amp;
    Security</span> › <span class="guimenu ">Installation</span>.
   </p></li><li class="step "><p>
    Select the <span class="guimenu ">Host Preparation</span> tab.
   </p></li><li class="step "><p>
    Click the <span class="guimenu ">Configure</span> link in the VXLAN column.
   </p></li><li class="step "><p>
    Enter the required information.
   </p></li><li class="step "><p>
    If you have not already done so, create an IP pool for the VXLAN tunnel end
    points (VTEP) by clicking <span class="guimenu ">New IP Pool</span>:
   </p></li><li class="step "><p>
    Click <span class="guimenu ">OK</span> to create the VXLAN network.
   </p></li></ol></div></div><p>
  When configuring the VXLAN transport network, consider the following:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Use a NIC teaming policy that best suits the environment being
    built. <code class="literal">Load Balance - SRCID</code> as the VMKNic teaming policy
    is usually the most flexible out of all the available options. This allows
    each host to have a VTEP vmkernel interface for each dvuplink on the
    selected distributed switch (two dvuplinks gives two VTEP interfaces per
    ESXi host).
   </p></li><li class="listitem "><p>
    Do not mix different teaming policies for different portgroups on a VDS
    where some use Etherchannel or Link Aggregation Control Protocol (LACPv1 or
    LACPv2) and others use a different teaming policy. If uplinks are shared in
    these different teaming policies, traffic will be interrupted. If logical
    routers are present, there will be routing problems. Such a configuration
    is not supported and should be avoided.
   </p></li><li class="listitem "><p>
    For larger environments it may be better to use DHCP for the VMKNic IP
    Addressing.
   </p></li><li class="listitem "><p>
    For more information and further guidance, see the <a class="link" href="https://communities.vmware.com/docs/DOC-27683" target="_blank">VMware NSX for
    vSphere Network Virtualization Design Guide</a>.
   </p></li></ul></div></div><div class="sect4" id="nsx-assign-segment-id-pool"><div class="titlepage"><div><div><h5 class="title"><span class="number">28.1.2.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Assign Segment ID Pool</span> <a title="Permalink" class="permalink" href="#nsx-assign-segment-id-pool">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-assign-segment_id-pool.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-assign-segment_id-pool.xml</li><li><span class="ds-label">ID: </span>nsx-assign-segment-id-pool</li></ul></div></div></div></div><p>
  Each VXLAN tunnel will need a segment ID to isolate its network
  traffic. Therefore, it is necessary to configure a segment ID pool for the
  NSX VXLAN network to use. If an NSX controller is not deployed within the
  vSphere environment, a multicast address range must be added to spread
  traffic across the network and avoid overloading a single multicast address.
 </p><p>
  For the purposes of the example in these instructions, do the following steps
  to assign a segment ID pool. Otherwise, follow best practices as outlined in
  <a class="link" href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-7B33DE72-78A7-448C-A61C-9B41D1EB12AD.html" target="_blank">VMware's
  documentation</a>.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    In the vSphere web client, navigate to
    <span class="guimenu ">Home</span> › <span class="guimenu ">Networking &amp;
    Security</span> › <span class="guimenu ">Installation</span>.
   </p></li><li class="step "><p>
    Select the <span class="guimenu ">Logical Network Preparation</span> tab.
   </p></li><li class="step "><p>
    Click <span class="guimenu ">Segment ID</span>, and then <span class="guimenu ">Edit</span>.
   </p></li><li class="step "><p>
    Click <span class="guimenu ">OK</span> to save your changes.
   </p></li></ol></div></div></div><div class="sect4" id="nsx-create-transport-zone"><div class="titlepage"><div><div><h5 class="title"><span class="number">28.1.2.4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create a Transport Zone</span> <a title="Permalink" class="permalink" href="#nsx-create-transport-zone">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-create-transport-zone.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-create-transport-zone.xml</li><li><span class="ds-label">ID: </span>nsx-create-transport-zone</li></ul></div></div></div></div><p>
  A transport zone controls which hosts a logical switch can reach and has the
  following characteristics.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    It can span one or more vSphere clusters.
   </p></li><li class="listitem "><p>
    Transport zones dictate which clusters can participate in the use of a
    particular network. Therefore they dictate which VMs can participate in the
    use of a particular network.
   </p></li><li class="listitem "><p>
    A vSphere NSX environment can contain one or more transport zones based on the
    environment's requirements.
   </p></li><li class="listitem "><p>
    A host cluster can belong to multiple transport
    zones.
   </p></li><li class="listitem "><p>
    A logical switch can belong to only one transport zone.
   </p></li></ul></div><div id="id-1.3.6.11.9.4.10.15.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   <span class="productname">OpenStack</span> has only been verified to work with a single transport zone within
   a vSphere NSX-v environment. Other configurations are currently not
   supported.
  </p></div><p>
   For more information on transport zones, refer to <a class="link" href="https://pubs.vmware.com/NSX-62/topic/com.vmware.nsx.install.doc/GUID-0B3BD895-8037-48A8-831C-8A8986C3CA42.html" target="_blank">VMware's
   Add A Transport Zone</a>.
  </p><p>
   To create a transport zone:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     In the vSphere web client, navigate to
     <span class="guimenu ">Home</span> › <span class="guimenu ">Networking &amp;
     Security</span> › <span class="guimenu ">Installation</span>.
    </p></li><li class="step "><p>
     Select the <span class="guimenu ">Logical Network Preparation</span> tab.
    </p></li><li class="step "><p>
     Click <span class="guimenu ">Transport Zones</span>, and then click the <span class="guimenu ">New
     Transport Zone</span> (New Logical Switch) icon.
    </p></li><li class="step "><p>
     In the <span class="guimenu ">New Transport Zone</span> dialog box, type a name and
     an optional description for the transport zone.
    </p></li><li class="step "><p>
     For these example instructions, select the control plane mode as
     <code class="literal">Unicast</code>.
    </p><div id="id-1.3.6.11.9.4.10.15.7.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      Whether there is a controller in the environment or if the environment is
      going to use multicast addresses will determine the control plane mode to
      select:
     </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
        <code class="literal">Unicast</code> (what this set of instructions uses): The
        control plane is handled by an NSX controller. All unicast traffic
        leverages optimized headend replication. No multicast IP addresses or
        special network configuration is required.
       </p></li><li class="listitem "><p>
        <code class="literal">Multicast</code>: Multicast IP addresses in the physical
        network are used for the control plane. This mode is recommended only
        when upgrading from older VXLAN deployments. Requires PIM/IGMP in the
        physical network.
       </p></li><li class="listitem "><p>
        <code class="literal">Hybrid</code>: Offloads local traffic replication to the
        physical network (L2 multicast). This requires IGMP snooping on the
        first-hop switch and access to an IGMP querier in each VTEP subnet, but
        does not require PIM. The first-hop switch handles traffic replication
        for the subnet.
       </p></li></ul></div></div></li><li class="step "><p>
     Select the clusters to be added to the transport zone.
    </p></li><li class="step "><p>
     Click <span class="guimenu ">OK</span> to save your changes.
    </p></li></ol></div></div></div><div class="sect4" id="id-1.3.6.11.9.4.10.16"><div class="titlepage"><div><div><h5 class="title"><span class="number">28.1.2.4.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.9.4.10.16">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-configure-vsphere_env.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-configure-vsphere_env.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    With vSphere environment setup completed, the <span class="productname">OpenStack</span> can be deployed. The
    following sections will cover creating virtual machines within the vSphere
    environment, configuring the cloud model and integrating NSX-v neutron
    core plugin into the <span class="productname">OpenStack</span>:
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Create the virtual machines
     </p></li><li class="step "><p>
      Deploy the Cloud Lifecycle Manager
     </p></li><li class="step "><p>
      Configure the neutron environment with NSX-v
     </p></li><li class="step "><p>
      Modify the cloud input model
     </p></li><li class="step "><p>
      Set up the parameters
     </p></li><li class="step "><p>
      Deploy the Operating System with Cobbler
     </p></li><li class="step "><p>
      Deploy the cloud
     </p></li></ol></div></div></div></div><div class="sect3" id="id-1.3.6.11.9.4.11"><div class="titlepage"><div><div><h4 class="title"><span class="number">28.1.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.9.4.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-vm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-vm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    Within the vSphere environment, create the <span class="productname">OpenStack</span> virtual machines. At
    minimum, there must be the following:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      One Cloud Lifecycle Manager deployer
     </p></li><li class="listitem "><p>
      Three <span class="productname">OpenStack</span> controllers
     </p></li><li class="listitem "><p>
      One <span class="productname">OpenStack</span> neutron compute proxy
     </p></li></ul></div><p>
    For the minimum NSX hardware requirements, refer to <a class="xref" href="#nsx-hw-reqs-vm" title="NSX Hardware Requirements for Virtual Machine Integration">Table 28.1, “NSX Hardware Requirements for Virtual Machine Integration”</a>.
   </p><p>
    If ESX VMs are to be used as nova compute proxy nodes, set up three LAN
    interfaces in each virtual machine as shown in the networking model table below.  There must
    be at least one nova compute proxy node per cluster.
   </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th><p>Network Group</p></th><th><p>Interface</p></th></tr></thead><tbody><tr><td><p>Management</p></td><td><p><code class="literal">eth0</code></p></td></tr><tr><td><p>External API</p></td><td><p><code class="literal">eth1</code></p></td></tr><tr><td><p>Internal API</p></td><td><p><code class="literal">eth2</code></p></td></tr></tbody></table></div><div class="sect4" id="nsx-advanced-config"><div class="titlepage"><div><div><h5 class="title"><span class="number">28.1.2.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Advanced Configuration Option</span> <a title="Permalink" class="permalink" href="#nsx-advanced-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-advanced-config.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-advanced-config.xml</li><li><span class="ds-label">ID: </span>nsx-advanced-config</li></ul></div></div></div></div><div id="id-1.3.6.11.9.4.11.7.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   Within vSphere for each in the virtual machine:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     In the <span class="guimenu ">Options</span> section, under <span class="guimenu ">Advanced
     configuration parameters</span>, ensure that
     <code class="literal">disk.EnableUUIDoption</code> is set to
     <code class="literal">true</code>.
    </p></li><li class="listitem "><p>
     If the option does not exist, it must be added. This
     option is required for the <span class="productname">OpenStack</span> deployment.
    </p></li><li class="listitem "><p>
     If the option is not specified, then the deployment will fail when
     attempting to configure the disks of each virtual machine.
    </p></li></ul></div></div></div><div class="sect4" id="sec-nsx-setup-deployer"><div class="titlepage"><div><div><h5 class="title"><span class="number">28.1.2.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting Up the Cloud Lifecycle Manager</span> <a title="Permalink" class="permalink" href="#sec-nsx-setup-deployer">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-vm.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-vm.xml</li><li><span class="ds-label">ID: </span>sec-nsx-setup-deployer</li></ul></div></div></div></div><div class="sect5" id="id-1.3.6.11.9.4.11.8.2"><div class="titlepage"><div><div><h6 class="title"><span class="number">28.1.2.5.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing the Cloud Lifecycle Manager</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.9.4.11.8.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-vm.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-vm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    Running the <code class="command">ARDANA_INIT_AUTO=1</code> command is optional to
    avoid stopping for authentication at any step. You can also run
    <code class="command">ardana-init</code>to launch the Cloud Lifecycle Manager.  You will be prompted to
    enter an optional SSH passphrase, which is used to protect the key used by
    Ansible when connecting to its client nodes.  If you do not want to use a
    passphrase, press <span class="keycap">Enter</span> at the prompt.
   </p><p>
    If you have protected the SSH key with a passphrase, you can avoid having
    to enter the passphrase on every attempt by Ansible to connect to its
    client nodes with the following commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>eval $(ssh-agent)
<code class="prompt user">ardana &gt; </code>ssh-add ~/.ssh/id_rsa</pre></div><p>
    The Cloud Lifecycle Manager will contain the installation scripts and configuration files to
    deploy your cloud. You can set up the Cloud Lifecycle Manager on a dedicated node or you do
    so on your first controller node. The default choice is to use the first
    controller node as the Cloud Lifecycle Manager.
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Download the product from:
     </p><ol type="a" class="substeps "><li class="step "><p>
        <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a>
       </p></li></ol></li><li class="step "><p>
      Boot your Cloud Lifecycle Manager from the SLES ISO contained in the download.
     </p></li><li class="step "><p>
      Enter <code class="literal">install</code> (all lower-case, exactly as spelled out
      here) to start installation.
     </p></li><li class="step "><p>
      Select the language. Note that only the English language selection is
      currently supported.
     </p></li><li class="step "><p>
      Select the location.
     </p></li><li class="step "><p>
      Select the keyboard layout.
     </p></li><li class="step "><p>
      Select the primary network interface, if prompted:
     </p><ol type="a" class="substeps "><li class="step "><p>
        Assign IP address, subnet mask, and default gateway
       </p></li></ol></li><li class="step "><p>
      Create new account:
     </p><ol type="a" class="substeps "><li class="step "><p>
        Enter a username.
       </p></li><li class="step "><p>
        Enter a password.
       </p></li><li class="step "><p>
        Enter time zone.
       </p></li></ol></li></ol></div></div><p>
    Once the initial installation is finished, complete the Cloud Lifecycle Manager setup with
    these steps:
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Ensure your Cloud Lifecycle Manager has a valid DNS nameserver specified in
      <code class="literal">/etc/resolv.conf</code>.
     </p></li><li class="step "><p>
      Set the environment variable LC_ALL:
     </p><div class="verbatim-wrap"><pre class="screen">export LC_ALL=C</pre></div><div id="id-1.3.6.11.9.4.11.8.2.8.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
       This can be added to <code class="filename">~/.bashrc</code> or
       <code class="filename">/etc/bash.bashrc</code>.
      </p></div></li></ol></div></div><p>
    The node should now have a working SLES setup.
   </p></div></div><div class="sect4" id="nsx-configure-neutron-env-nsx"><div class="titlepage"><div><div><h5 class="title"><span class="number">28.1.2.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure the Neutron Environment with NSX-v</span> <a title="Permalink" class="permalink" href="#nsx-configure-neutron-env-nsx">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-configure-neutron-env-nsx.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-configure-neutron-env-nsx.xml</li><li><span class="ds-label">ID: </span>nsx-configure-neutron-env-nsx</li></ul></div></div></div></div><p>
  In summary, integrating NSX with vSphere has four major steps:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Modify the input model to define the server roles, servers, network roles
    and networks. <a class="xref" href="#nsx-modify-input-model" title="28.1.2.5.3.2. Modify the Input Model">Section 28.1.2.5.3.2, “Modify the Input Model”</a>
   </p></li><li class="step "><p>
    Set up the parameters needed for neutron and nova to communicate with the
    ESX and NSX Manager. <a class="xref" href="#nsx-deploy-os-cobbler" title="28.1.2.5.3.3. Deploying the Operating System with Cobbler">Section 28.1.2.5.3.3, “Deploying the Operating System with Cobbler”</a>
   </p></li><li class="step "><p>
    Do the steps to deploy the cloud. <a class="xref" href="#nsx-deploy-cloud" title="28.1.2.5.3.4. Deploying the Cloud">Section 28.1.2.5.3.4, “Deploying the Cloud”</a>
   </p></li></ol></div></div><div class="sect5" id="nsx-import-third-party"><div class="titlepage"><div><div><h6 class="title"><span class="number">28.1.2.5.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Third-Party Import of VMware NSX-v Into neutron and
 python-neutronclient</span> <a title="Permalink" class="permalink" href="#nsx-import-third-party">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-import-third_party.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-import-third_party.xml</li><li><span class="ds-label">ID: </span>nsx-import-third-party</li></ul></div></div></div></div><p>
  To import the NSX-v neutron core-plugin into Cloud Lifecycle Manager, run the third-party
  import playbook.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost third-party-import.yml</pre></div></div><div class="sect5" id="nsx-modify-input-model"><div class="titlepage"><div><div><h6 class="title"><span class="number">28.1.2.5.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Modify the Input Model</span> <a title="Permalink" class="permalink" href="#nsx-modify-input-model">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-modify-input-model.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-modify-input-model.xml</li><li><span class="ds-label">ID: </span>nsx-modify-input-model</li></ul></div></div></div></div><p>
  After the third-party import has completed successfully, modify the input
  model:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Prepare for input model changes
   </p></li><li class="step "><p>
    Define the servers and server roles needed for a NSX-v cloud.
   </p></li><li class="step "><p>
    Define the necessary networks and network groups
   </p></li><li class="step "><p>
    Specify the services needed to be deployed on the Cloud Lifecycle Manager controllers and the
    nova ESX compute proxy nodes.
   </p></li><li class="step "><p>
    Commit the changes and run the configuration processor.
   </p></li></ol></div></div><div class="sect6" id="nsx-prepare-input-model-changes"><div class="titlepage"><div><div><h6 class="title"><span class="number">28.1.2.5.3.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prepare for Input Model Changes</span> <a title="Permalink" class="permalink" href="#nsx-prepare-input-model-changes">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-prepare-input-model-changes.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-prepare-input-model-changes.xml</li><li><span class="ds-label">ID: </span>nsx-prepare-input-model-changes</li></ul></div></div></div></div><p>
  The previous steps created a modified <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> tarball with the NSX-v
  core plugin in the neutron and <code class="literal">neutronclient</code> venvs. The
  <code class="filename">tar</code> file can now be extracted and the
  <code class="filename">ardana-init.bash</code> script can be run to set up the
  deployment files and directories. If a modified <code class="filename">tar</code> file
  was not created, then extract the tar from the /media/cdrom/ardana location.
 </p><p>
  To run the <code class="filename">ardana-init.bash</code> script which is included in
  the build, use this commands:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>~/ardana/hos-init.bash</pre></div></div><div class="sect6" id="nsx-create-input-model"><div class="titlepage"><div><div><h6 class="title"><span class="number">28.1.2.5.3.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create the Input Model</span> <a title="Permalink" class="permalink" href="#nsx-create-input-model">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-create-input-model.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-create-input-model.xml</li><li><span class="ds-label">ID: </span>nsx-create-input-model</li></ul></div></div></div></div><p>
  Copy the example input model to
  <code class="filename">~/openstack/my_cloud/definition/</code> directory:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/ardana-extensions/ardana-extensions-nsx/vmware/examples/models/entry-scale-nsx
<code class="prompt user">ardana &gt; </code>cp -R entry-scale-nsx ~/openstack/my_cloud/definition</pre></div><p>
  Refer to the reference input model in
  <code class="filename">ardana-extensions/ardana-extensions-nsx/vmware/examples/models/entry-scale-nsx/</code>
  for details about how these definitions should be made.  The main differences
  between this model and the standard Cloud Lifecycle Manager input models are:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Only the neutron-server is deployed.  No other neutron agents are deployed.
   </p></li><li class="listitem "><p>
    Additional parameters need to be set in
    <code class="filename">pass_through.yml</code> and
    <code class="filename">nsx/nsx_config.yml</code>.
   </p></li><li class="listitem "><p>
    nova ESX compute proxy nodes may be ESX virtual machines.
   </p></li></ul></div><div class="sect6" id="id-1.3.6.11.9.4.11.9.5.5.6"><div class="titlepage"><div><div><h6 class="title"><span class="number">28.1.2.5.3.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Set up the Parameters</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.9.4.11.9.5.5.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-create-input-model.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-create-input-model.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The special parameters needed for the NSX-v integrations are set in the
   files <code class="filename">pass_through.yml</code> and
   <code class="filename">nsx/nsx_config.yml</code>. They are in the
   <code class="filename">~/openstack/my_cloud/definition/data</code> directory.
  </p><p>
   Parameters in <code class="filename">pass_through.yml</code> are in the sample input
   model in the
   <code class="filename">ardana-extensions/ardana-extensions-nsx/vmware/examples/models/entry-scale-nsx/</code>
   directory.  The comments in the sample input model file describe how to
   locate the values of the required parameters.
  </p><div class="verbatim-wrap"><pre class="screen">#
# (c) Copyright 2017 SUSE LLC
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---
product:
  version: 2
pass-through:
  global:
    vmware:
      - username: <em class="replaceable ">VCENTER_ADMIN_USERNAME</em>
        ip: <em class="replaceable ">VCENTER_IP</em>
        port: 443
        cert_check: false
        # The password needs to be encrypted using the script
        # openstack/ardana/ansible/ardanaencrypt.py on the deployer:
        #
        # $ cd ~/openstack/ardana/ansible
        # $ export ARDANA_USER_PASSWORD_ENCRYPT_KEY=<em class="replaceable ">ENCRYPTION_KEY</em>
        # $ ./ardanaencrypt.py
        #
        # The script will prompt for the vCenter password. The string
        # generated is the encrypted password. Enter the string
        # enclosed by double-quotes below.
        password: "<em class="replaceable ">ENCRYPTED_PASSWD_FROM_ARDANAENCRYPT</em>"

        # The id is is obtained by the URL
        # https://<em class="replaceable ">VCENTER_IP</em>/mob/?moid=ServiceInstance&amp;doPath=content%2eabout,
        # field instanceUUID.
        id: <em class="replaceable ">VCENTER_UUID</em>
  servers:
    -
      # Here the 'id' refers to the name of the node running the
      # esx-compute-proxy. This is identical to the 'servers.id' in
      # servers.yml. There should be one esx-compute-proxy node per ESX
      # resource pool.
      id: esx-compute1
      data:
        vmware:
          vcenter_cluster: <em class="replaceable ">VMWARE_CLUSTER1_NAME</em>
          vcenter_id: <em class="replaceable ">VCENTER_UUID</em>
    -
      id: esx-compute2
      data:
        vmware:
          vcenter_cluster: <em class="replaceable ">VMWARE_CLUSTER2_NAME</em>
          vcenter_id: <em class="replaceable ">VCENTER_UUID</em></pre></div><p>
   There are parameters in <code class="filename">nsx/nsx_config.yml</code>.  The
   comments describes how to retrieve the values.
  </p><div class="verbatim-wrap"><pre class="screen"># (c) Copyright 2017 SUSE LLC
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---
  product:
    version: 2
  configuration-data:
    - name: NSX-CONFIG-CP1
      services:
        - nsx
      data:
        # (Required) URL for NSXv manager (e.g - https://management_ip).
        manager_uri: 'https://<em class="replaceable ">NSX_MGR_IP</em>

        # (Required) NSXv username.
        user: 'admin'

        # (Required) Encrypted NSX Manager password.
        # Password encryption is done by the script
        # ~/openstack/ardana/ansible/ardanaencrypt.py on the deployer:
        #
        # $ cd ~/openstack/ardana/ansible
        # $ export ARDANA_USER_PASSWORD_ENCRYPT_KEY=<em class="replaceable ">ENCRYPTION_KEY</em>
        # $ ./ardanaencrypt.py
        #
        # NOTE: Make sure that the NSX Manager password is encrypted with the same key
        # used to encrypt the VCenter password.
        #
        # The script will prompt for the NSX Manager password. The string
        # generated is the encrypted password. Enter the string enclosed
        # by double-quotes below.
        password: "<em class="replaceable ">ENCRYPTED_NSX_MGR_PASSWD_FROM_ARDANAENCRYPT</em>"
        # (Required) datacenter id for edge deployment.
        # Retrieved using
        #    http://<em class="replaceable ">VCENTER_IP_ADDR</em>/mob/?moid=ServiceInstance&amp;doPath=content
        # click on the value from the rootFolder property. The datacenter_moid is
        # the value of the childEntity property.
        # The vCenter-ip-address comes from the file pass_through.yml in the
        # input model under "pass-through.global.vmware.ip".
        datacenter_moid: 'datacenter-21'
        # (Required) id of logic switch for physical network connectivity.
        # How to retrieve
        # 1. Get to the same page where the datacenter_moid is found.
        # 2. Click on the value of the rootFolder property.
        # 3. Click on the value of the childEntity property
        # 4. Look at the network property. The external network is
        #    network associated with EXTERNAL VM in VCenter.
        external_network: 'dvportgroup-74'
        # (Required) clusters ids containing OpenStack hosts.
        # Retrieved using http://<em class="replaceable ">VCENTER_IP_ADDR</em>/mob, click on the value
        # from the rootFolder property. Then click on the value of the
        # hostFolder property. Cluster_moids are the values under childEntity
        # property of the compute clusters.
        cluster_moid: 'domain-c33,domain-c35'
        # (Required) resource-pool id for edge deployment.
        resource_pool_id: 'resgroup-67'
        # (Optional) datastore id for edge deployment. If not needed,
        # do not declare it.
        # datastore_id: 'datastore-117'

        # (Required) network scope id of the transport zone.
        # To get the vdn_scope_id, in the vSphere web client from the Home
        # menu:
        #   1. click on Networking &amp; Security
        #   2. click on installation
        #   3. click on the Logical Netowrk Preparation tab.
        #   4. click on the Transport Zones button.
        #   5. Double click on the transport zone being configure.
        #   6. Select Manage tab.
        #   7. The vdn_scope_id will appear at the end of the URL.
        vdn_scope_id: 'vdnscope-1'

        # (Optional) Dvs id for VLAN based networks. If not needed,
        # do not declare it.
        # dvs_id: 'dvs-68'

        # (Required) backup_edge_pool: backup edge pools management range,
        # - edge_type&gt;[edge_size]:<em class="replaceable ">MINIMUM_POOLED_EDGES</em>:<em class="replaceable ">MAXIMUM_POOLED_EDGES</em>
        # - edge_type: service (service edge) or  vdr (distributed edge)
        # - edge_size:  compact ,  large (by default),  xlarge  or  quadlarge
        backup_edge_pool: 'service:compact:4:10,vdr:compact:4:10'

        # (Optional) mgt_net_proxy_ips: management network IP address for
        # metadata proxy. If not needed, do not declare it.
        # mgt_net_proxy_ips: '10.142.14.251,10.142.14.252'

        # (Optional) mgt_net_proxy_netmask: management network netmask for
        # metadata proxy. If not needed, do not declare it.
        # mgt_net_proxy_netmask: '255.255.255.0'

        # (Optional) mgt_net_moid: Network ID for management network connectivity
        # Do not declare if not used.
        # mgt_net_moid: 'dvportgroup-73'

        # ca_file: Name of the certificate file. If insecure is set to True,
        # then this parameter is ignored. If insecure is set to False and this
        # parameter is not defined, then the system root CAs will be used
        # to verify the server certificate.
        ca_file: a/nsx/certificate/file

        # insecure:
        # If true (default), the NSXv server certificate is not verified.
        # If false, then the default CA truststore is used for verification.
        # This option is ignored if "ca_file" is set
        insecure: True
        # (Optional) edge_ha: if true, will duplicate any edge pool resources
        # Default to False if undeclared.
        # edge_ha: False
        # (Optional) spoofguard_enabled:
        # If True (default), indicates NSXV spoofguard component is used to
        # implement port-security feature.
        # spoofguard_enabled: True
        # (Optional) exclusive_router_appliance_size:
        # Edge appliance size to be used for creating exclusive router.
        # Valid values: 'compact', 'large', 'xlarge', 'quadlarge'
        # Defaults to 'compact' if not declared.  # exclusive_router_appliance_size:
        'compact'</pre></div></div></div><div class="sect6" id="commit-config-processor"><div class="titlepage"><div><div><h6 class="title"><span class="number">28.1.2.5.3.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Commit Changes and Run the Configuration Processor</span> <a title="Permalink" class="permalink" href="#commit-config-processor">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/commit-config_processor.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>commit-config_processor.xml</li><li><span class="ds-label">ID: </span>commit-config-processor</li></ul></div></div></div></div><p>
  Commit your changes with the input model and the required configuration
  values added to the <code class="filename">pass_through.yml</code> and
  <code class="filename">nsx/nsx_config.yml</code> files.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition
<code class="prompt user">ardana &gt; </code>git commit -A -m "Configuration changes for NSX deployment"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml \
 -e \encrypt="" -e rekey=""</pre></div><p>
  If the playbook <code class="filename">config-processor-run.yml</code> fails, there is
  an error in the input model. Fix the error and repeat the above steps.
 </p></div></div><div class="sect5" id="nsx-deploy-os-cobbler"><div class="titlepage"><div><div><h6 class="title"><span class="number">28.1.2.5.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying the Operating System with Cobbler</span> <a title="Permalink" class="permalink" href="#nsx-deploy-os-cobbler">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-deploy-os-cobbler.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-deploy-os-cobbler.xml</li><li><span class="ds-label">ID: </span>nsx-deploy-os-cobbler</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    From the Cloud Lifecycle Manager, run Cobbler to install the operating system on the nodes
    after it has to be deployed:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step "><p>
    Verify the nodes that will have an operating system installed by Cobbler by
    running this command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cobbler system find --netboot-enabled=1</pre></div></li><li class="step "><p>
    Reimage the nodes using Cobbler.  Do not use Cobbler to reimage the nodes
    running as ESX virtual machines. The command below is run on a setup where
    the nova ESX compute proxies are VMs. Controllers 1, 2, and 3 are
    running on physical servers.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml -e \
   nodelist=controller1,controller2,controller3</pre></div></li><li class="step "><p>
    When the playbook has completed, each controller node should have an
    operating system installed with an IP address configured on
    <code class="literal">eth0</code>.
   </p></li><li class="step "><p>
    After your controller nodes have been completed, you should install the
    operating system on your nova compute proxy virtual machines. Each
    configured virtual machine should be able to PXE boot into the operating
    system installer.
   </p></li><li class="step "><p>
    From within the vSphere environment, power on each nova compute proxy
    virtual machine and watch for it to PXE boot into the OS installer via its
    console.
   </p><ol type="a" class="substeps "><li class="step "><p>
    If successful, the virtual machine will have the operating system
    automatically installed and will then automatically power off.
     </p></li><li class="step "><p>
      When the virtual machine has powered off, power it on and let it boot
      into the operating system.
     </p></li></ol></li><li class="step "><p>
    Verify network settings after deploying the operating system to each node.
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Verify that the NIC bus mapping specified in the cloud model input file
      (<code class="filename">~/ardana/my_cloud/definition/data/nic_mappings.yml</code>)
      matches the NIC bus mapping on each <span class="productname">OpenStack</span> node.
     </p><p>
      Check the NIC bus mapping with this command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cobbler system list</pre></div></li><li class="listitem "><p>
      After the playbook has completed, each controller node should have an
      operating system installed with an IP address configured on eth0.
     </p></li></ul></div></li><li class="step "><p>
    When the ESX compute proxy nodes are VMs, install the operating system if
    you have not already done so.
   </p></li></ol></div></div></div><div class="sect5" id="nsx-deploy-cloud"><div class="titlepage"><div><div><h6 class="title"><span class="number">28.1.2.5.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying the Cloud</span> <a title="Permalink" class="permalink" href="#nsx-deploy-cloud">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-deploy-cloud.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-deploy-cloud.xml</li><li><span class="ds-label">ID: </span>nsx-deploy-cloud</li></ul></div></div></div></div><p>
  When the configuration processor has completed successfully, the cloud can be
  deployed. Set the ARDANA_USER_PASSWORD_ENCRYPT_KEY environment
  variable before running <code class="filename">site.yml</code>.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>export ARDANA_USER_PASSWORD_ENCRYPT_KEY=<em class="replaceable ">PASSWORD_KEY</em>
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-cloud-configure.yml</pre></div><p>
<em class="replaceable ">PASSWORD_KEY</em> in the <code class="literal">export</code>
command is the key used to encrypt the passwords for vCenter and NSX Manager.
</p></div></div></div></div></div><div class="sect1" id="nsx-vsphere-baremetal"><div class="titlepage"><div><div><h2 class="title"><span class="number">28.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Integrating with NSX for vSphere on Baremetal</span> <a title="Permalink" class="permalink" href="#nsx-vsphere-baremetal">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span>nsx-vsphere-baremetal</li></ul></div></div></div></div><p>
  This section describes the installation steps and requirements for
  integrating with NSX for vSphere on baremetal physical hardware.
 </p><div class="sect2" id="id-1.3.6.11.10.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">28.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Pre-Integration Checklist</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.10.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following installation and integration instructions assumes an
   understanding of VMware's ESXI and vSphere products for setting up virtual
   environments.
  </p><p>
   Please review the following requirements for the VMware vSphere environment.
  </p><p>
   <span class="bold"><strong>Software Requirements</strong></span>
  </p><p>
   Before you install or upgrade NSX, verify your software versions. The
   following are the required versions.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th>
       <p>
        Software
       </p>
      </th><th>
       <p>
        Version
       </p>
      </th></tr></thead><tbody><tr><td>
       <p>
        <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
       </p>
      </td><td>
       <p>
        8
       </p>
      </td></tr><tr><td>
       <p>
        VMware NSX-v Manager
       </p>
      </td><td>
       <p>
        6.3.4 or higher
       </p>
      </td></tr><tr><td>
       <p>
        VMWare NSX-v neutron Plugin
       </p>
      </td><td>
       <p>
        Pike Release (TAG=11.0.0)
       </p>
      </td></tr><tr><td>
       <p>
        VMWare ESXi and vSphere Appliance (vSphere web Client)
       </p>
      </td><td>
       <p>
        6.0 or higher
       </p>
      </td></tr></tbody></table></div><p>
   A vCenter server (appliance) is required to manage the vSphere environment.
   It is recommended that you install a vCenter appliance as an ESX virtual
   machine.
  </p><div id="id-1.3.6.11.10.3.8" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    Each ESXi compute cluster is required to have shared storage between the
    hosts in the cluster, otherwise attempts to create instances through
    nova-compute will fail.
   </p></div></div><div class="sect2" id="id-1.3.6.11.10.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">28.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing on Baremetal</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.10.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="productname">OpenStack</span> can be deployed in two ways: on baremetal (physical hardware) or in
   an ESXi virtual environment on virtual machines. The following instructions
   describe how to install <span class="productname">OpenStack</span> on baremetal nodes with vCenter and NSX
   Manager running as virtual machines. For instructions on virtual machine
   installation, see <a class="xref" href="#nsx-vsphere-vm" title="28.1. Integrating with NSX for vSphere">Section 28.1, “Integrating with NSX for vSphere”</a>.
  </p><p>
   This deployment example will consist of two ESXi clusters at minimum: a
   <code class="literal">control-plane</code> cluster and a <code class="literal">compute</code>
   cluster. The control-plane cluster must have 3 ESXi hosts minimum (due to
   VMware's recommendation that each NSX controller virtual machine is on a
   separate host). The compute cluster must have 2 ESXi hosts minimum. There
   can be multiple compute clusters. The following table outlines the virtual
   machine specifications to be built in the control-plane cluster:
  </p><div class="table" id="nsx-hw-reqs-bm"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 28.2: </span><span class="name">NSX Hardware Requirements for Baremetal Integration </span><a title="Permalink" class="permalink" href="#nsx-hw-reqs-bm">#</a></h6></div><div class="table-contents"><table class="table" summary="NSX Hardware Requirements for Baremetal Integration" border="1"><colgroup><col class="1" /><col class="2" /><col class="3" /><col class="4" /><col class="5" /><col class="6" /></colgroup><thead><tr><th>
       <p>
        Virtual Machine Role
       </p>
      </th><th>
       <p>
        Required Number
       </p>
      </th><th>
       <p>
        Disk
       </p>
      </th><th>
       <p>
        Memory
       </p>
      </th><th>
       <p>
        Network
       </p>
      </th><th>
       <p>
        CPU
       </p>
      </th></tr></thead><tbody><tr><td>
       <p>
        Compute virtual machines
       </p>
      </td><td>
       <p>
        1 per compute cluster
       </p>
      </td><td>
       <p>
        80GB
       </p>
      </td><td>
       <p>
        4GB
       </p>
      </td><td>
       <p>
        3 VMXNET Virtual Network Adapters
       </p>
      </td><td>
       <p>
        2 vCPU
       </p>
      </td></tr><tr><td>
       <p>
        NSX Edge Gateway/DLR/Metadata-proxy appliances
       </p>
      </td><td>
       
      </td><td>
       <p>
        Autogenerated by NSXv
       </p>
      </td><td>
       <p>
        Autogenerated by NSXv
       </p>
      </td><td>
       <p>
        Autogenerated by NSXv
       </p>
      </td><td>
       <p>
        Autogenerated by NSXv
       </p>
      </td></tr></tbody></table></div></div><p>
   In addition to the ESXi hosts, it is recommended that there is one physical
   host for the Cloud Lifecycle Manager node and three physical hosts for the controller nodes.
  </p><div class="sect3" id="id-1.3.6.11.10.4.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">28.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Requirements</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.10.4.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    NSX-v requires the following for networking:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      The ESXi hosts, vCenter, and the NSX Manager appliance must resolve DNS
      lookup.
     </p></li><li class="listitem "><p>
      The ESXi host must have the NTP service configured and enabled.
     </p></li><li class="listitem "><p>
      Jumbo frames must be enabled on the switch ports that the ESXi hosts are
      connected to.
     </p></li><li class="listitem "><p>
      The ESXi hosts must have at least 2 physical network cards each.
     </p></li></ul></div></div><div class="sect3" id="id-1.3.6.11.10.4.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">28.2.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Model</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.10.4.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    The model in these instructions requires the following networks:
   </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.6.11.10.4.7.3.1"><span class="term ">ESXi Hosts and vCenter</span></dt><dd><p>
       This is the network that the ESXi hosts and vCenter use to route traffic
       with.
      </p></dd><dt id="id-1.3.6.11.10.4.7.3.2"><span class="term ">NSX Management</span></dt><dd><p>
       The network which the NSX controllers and NSX Manager will use.
      </p></dd><dt id="id-1.3.6.11.10.4.7.3.3"><span class="term ">NSX VTEP Pool</span></dt><dd><p>
       The network that NSX uses to create endpoints for VxLAN tunnels.
      </p></dd><dt id="id-1.3.6.11.10.4.7.3.4"><span class="term ">Management</span></dt><dd><p>
       The network that <span class="productname">OpenStack</span> uses for deployment and maintenance of the
       cloud.
      </p></dd><dt id="id-1.3.6.11.10.4.7.3.5"><span class="term ">Internal API (optional)</span></dt><dd><p>
       The network group that will be used for management (private API) traffic
       within the cloud.
      </p></dd><dt id="id-1.3.6.11.10.4.7.3.6"><span class="term ">External API</span></dt><dd><p>
       This is the network that users will use to make requests to the cloud.
      </p></dd><dt id="id-1.3.6.11.10.4.7.3.7"><span class="term ">External VM</span></dt><dd><p>
       VLAN-backed provider network for external access to guest VMs (floating
       IPs).
      </p></dd></dl></div></div><div class="sect3" id="id-1.3.6.11.10.4.8"><div class="titlepage"><div><div><h4 class="title"><span class="number">28.2.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">vSphere port security settings</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.10.4.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    Even though the <span class="productname">OpenStack</span> deployment is on baremetal, it is still necessary
    to define each VLAN within a vSphere Distributed Switch for the nova
    compute proxy virtual machine. Therefore, the vSphere port security
    settings are shown in the table below.
   </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="1" /><col class="2" /><col class="3" /><col class="4" /></colgroup><thead><tr><th>
        <p>
         Network Group
        </p>
       </th><th>
        <p>
         VLAN Type
        </p>
       </th><th>
        <p>
         Interface
        </p>
       </th><th>
        <p>
         vSphere Port Group Security Settings
        </p>
       </th></tr></thead><tbody><tr><td>
        <p>
         IPMI
        </p>
       </td><td>
        <p>
         Untagged
        </p>
       </td><td>
        <p>
         N/A
        </p>
       </td><td>
        <p>
         N/A
        </p>
       </td></tr><tr><td>
        <p>
         ESXi Hosts and vCenter
        </p>
       </td><td>
        <p>
         Tagged
        </p>
       </td><td>
        <p>
         N/A
        </p>
       </td><td>
        <p>
         Defaults
        </p>
       </td></tr><tr><td>
        <p>
         NSX Manager
        </p>
        <p>
         Must be able to reach ESXi Hosts and vCenter
        </p>
       </td><td>
        <p>
         Tagged
        </p>
       </td><td>
        <p>
         N/A
        </p>
       </td><td>
        <p>
         Defaults
        </p>
       </td></tr><tr><td>
        <p>
         NSX VTEP Pool
        </p>
       </td><td>
        <p>
         Tagged
        </p>
       </td><td>
        <p>
         N/A
        </p>
       </td><td>
        <p>
         Defaults
        </p>
       </td></tr><tr><td>
        <p>
         Management
        </p>
       </td><td>
        <p>
         Tagged or Untagged
        </p>
       </td><td>
        <p>
         bond0
        </p>
       </td><td>
        <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
           <span class="bold"><strong>Promiscuous Mode</strong></span>: Accept
          </p></li><li class="listitem "><p>
           <span class="bold"><strong>MAC Address Changes</strong></span>: Reject
          </p></li><li class="listitem "><p>
           <span class="bold"><strong>Forged Transmits</strong></span>:Reject
          </p></li></ul></div>
       </td></tr><tr><td>
        <p>
         Internal API (Optional, may be combined with the Management Network.
         If network segregation is required for security reasons, you can keep
         this as a separate network.)
        </p>
       </td><td>
        <p>
         Tagged
        </p>
       </td><td>
        <p>
         bond0
        </p>
       </td><td>
        <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
           <span class="bold"><strong>Promiscuous Mode</strong></span>: Accept
          </p></li><li class="listitem "><p>
           <span class="bold"><strong>MAC Address Changes</strong></span>: Reject
          </p></li><li class="listitem "><p>
           <span class="bold"><strong>Forged Transmits</strong></span>: Accept
          </p></li></ul></div>
       </td></tr><tr><td>
        <p>
         External API (Public)
        </p>
       </td><td>
        <p>
         Tagged
        </p>
       </td><td>
        <p>
         N/A
        </p>
       </td><td>
        <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
           <span class="bold"><strong>Promiscuous Mode</strong></span>: Accept
          </p></li><li class="listitem "><p>
           <span class="bold"><strong>MAC Address Changes</strong></span>: Reject
          </p></li><li class="listitem "><p>
           <span class="bold"><strong>Forged Transmits</strong></span>: Accept
          </p></li></ul></div>
       </td></tr><tr><td>
        <p>
         External VM
        </p>
       </td><td>
        <p>
         Tagged
        </p>
       </td><td>
        <p>
         N/A
        </p>
       </td><td>
        <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
           <span class="bold"><strong>Promiscuous Mode</strong></span>: Accept
          </p></li><li class="listitem "><p>
           <span class="bold"><strong>MAC Address Changes</strong></span>: Reject
          </p></li><li class="listitem "><p>
           <span class="bold"><strong>Forged Transmits</strong></span>: Accept
          </p></li></ul></div>
       </td></tr></tbody></table></div></div><div class="sect3" id="id-1.3.6.11.10.4.9"><div class="titlepage"><div><div><h4 class="title"><span class="number">28.2.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the vSphere Environment</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.10.4.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    Before deploying <span class="productname">OpenStack</span> with NSX-v, the VMware vSphere environment must
    be properly configured, including setting up vSphere distributed switches
    and port groups. For detailed instructions, see
    <a class="xref" href="#install-esx-ovsvapp" title="Chapter 27. Installing ESX Computes and OVSvAPP">Chapter 27, <em>Installing ESX Computes and OVSvAPP</em></a>.
   </p><p>
    Installing and configuring the VMware NSX Manager and creating the NSX
    network within the vSphere environment is covered below.
   </p><p>
    Before proceeding with the installation, ensure that the following are
    configured in the vSphere environment.
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      The vSphere datacenter is configured with at least two clusters, one
      <span class="bold"><strong>control-plane</strong></span> cluster and one
      <span class="bold"><strong>compute</strong></span> cluster.
     </p></li><li class="listitem "><p>
      Verify that all software, hardware, and networking requirements have been
      met.
     </p></li><li class="listitem "><p>
      Ensure the vSphere distributed virtual switches (DVS) are configured for
      each cluster.
     </p></li></ul></div><div id="id-1.3.6.11.10.4.9.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     The MTU setting for each DVS should be set to 1600. NSX should
     automatically apply this setting to each DVS during the setup process.
     Alternatively, the setting can be manually applied to each DVS before
     setup if desired.
    </p></div><p>
    Make sure there is a copy of the SUSE Linux Enterprise Server 12 SP4 <code class="filename">.iso</code> in the
    <code class="literal">ardana</code> home directory,
    <code class="filename">var/lib/ardana</code>, and that it is called
    <code class="filename">sles12sp4.iso</code>.
   </p><p>
    Install the <code class="literal">open-vm-tools</code> package.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper install open-vm-tools</pre></div><div class="sect4" id="id-1.3.6.11.10.4.9.10"><div class="titlepage"><div><div><h5 class="title"><span class="number">28.2.2.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Install NSX Manager</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.10.4.9.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
     The NSX Manager is the centralized network management component of NSX. It
     provides a single point of configuration and REST API entry-points.
    </p><p>
     The NSX Manager is installed as a virtual appliance on one of the ESXi
     hosts within the vSphere environment. This guide will cover installing the
     appliance on one of the ESXi hosts within the control-plane cluster. For
     more detailed information, refer to
     <a class="link" href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-D8578F6E-A40C-493A-9B43-877C2B75ED52.html" target="_blank">VMware's
     NSX Installation Guide.</a>
    </p><p>
     To install the NSX Manager, download the virtual appliance from
     <a class="link" href="https://www.vmware.com/go/download-nsx-vsphere" target="_blank">VMware</a>
     and deploy the appliance within vCenter onto one of the ESXi hosts. For
     information on deploying appliances within vCenter, refer to VMware's
     documentation for ESXi
     <a class="link" href="https://pubs.vmware.com/vsphere-55/index.jsp?topic=%2Fcom.vmware.vsphere.vm_admin.doc%2FGUID-AFEDC48B-C96F-4088-9C1F-4F0A30E965DE.html" target="_blank">5.5</a>
     or
     <a class="link" href="https://pubs.vmware.com/vsphere-60/index.jsp?topic=%2Fcom.vmware.vsphere.vm_admin.doc%2FGUID-AFEDC48B-C96F-4088-9C1F-4F0A30E965DE.html" target="_blank">6.0</a>.
    </p><p>
     During the deployment of the NSX Manager appliance, be aware of the
     following:
    </p><p>
     When prompted, select <span class="guimenu ">Accept extra configuration
     options</span>. This will present options for configuring IPv4 and IPv6
     addresses, the default gateway, DNS, NTP, and SSH properties during the
     installation, rather than configuring these settings manually after the
     installation.
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Choose an ESXi host that resides within the control-plane cluster.
      </p></li><li class="listitem "><p>
       Ensure that the network mapped port group is the DVS port group that
       represents the VLAN the NSX Manager will use for its networking (in this
       example it is labeled as the <code class="literal">NSX Management</code> network).
      </p></li></ul></div><div id="id-1.3.6.11.10.4.9.10.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The IP address assigned to the NSX Manager must be able to resolve
      reverse DNS.
     </p></div><p>
     Power on the NSX Manager virtual machine after it finishes deploying and
     wait for the operating system to fully load. When ready, carry out the
     following steps to have the NSX Manager use single sign-on (SSO) and to
     register the NSX Manager with vCenter:
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Open a web browser and enter the hostname or IP address that was
       assigned to the NSX Manager during setup.
      </p></li><li class="step "><p>
       Log in with the username <code class="literal">admin</code> and the password set
       during the deployment.
      </p></li><li class="step "><p>
       After logging in, click on <span class="guimenu ">Manage vCenter
       Registration</span>.
      </p></li><li class="step "><p>
       Configure the NSX Manager to connect to the vCenter server.
      </p></li><li class="step "><p>
       Configure NSX manager for single sign on (SSO) under the <span class="guimenu ">Lookup
       Server URL</span> section.
      </p></li></ol></div></div><div id="id-1.3.6.11.10.4.9.10.11" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      When configuring SSO, use <code class="literal">Lookup Service Port 443</code> for
      vCenter version 6.0. Use <code class="literal">Lookup Service Port 7444</code> for
      vCenter version 5.5.
     </p><p>
      SSO makes vSphere and NSX more secure by allowing the various components
      to communicate with each other through a secure token exchange mechanism,
      instead of requiring each component to authenticate a user separately.
      For more details, refer to VMware's documentation on
      <a class="link" href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-523B0D77-AAB9-4535-B326-1716967EC0D2.html" target="_blank">Configure
      Single Sign-On</a>.
     </p></div><p>
     Both the <code class="literal">Lookup Service URL</code> and the <code class="literal">vCenter
     Server</code> sections should have a status of
     <code class="literal">connected</code> when configured properly.
    </p><p>
     Log into the vSphere Web Client (log out and and back in if already logged
     in). The NSX Manager will appear under the <span class="guimenu ">Networking &amp;
     Security</span> section of the client.
    </p><div id="id-1.3.6.11.10.4.9.10.14" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The <span class="guimenu ">Networking &amp; Security</span> section will not appear
      under the vSphere desktop client. Use of the web client is required for
      the rest of this process.
     </p></div></div><div class="sect4" id="id-1.3.6.11.10.4.9.11"><div class="titlepage"><div><div><h5 class="title"><span class="number">28.2.2.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Add NSX Controllers</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.10.4.9.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
     The NSX controllers serve as the central control point for all logical
     switches within the vSphere environment's network, and they maintain
     information about all hosts, logical switches (VXLANs), and distributed
     logical routers.
    </p><p>
     NSX controllers will each be deployed as a virtual appliance on the ESXi
     hosts within the control-plane cluster to form the NSX Controller cluster.
     For details about NSX controllers and the NSX control plane in general,
     refer to
     <a class="link" href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-4E0FEE83-CF2C-45E0-B0E6-177161C3D67C.html" target="_blank">VMware's
     NSX documentation</a>.
    </p><div id="id-1.3.6.11.10.4.9.11.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      Whatever the size of the NSX deployment, the following conditions must be
      met:
     </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
        Each NSX Controller cluster must contain three controller nodes. Having
        a different number of controller nodes is not supported.
       </p></li><li class="listitem "><p>
        Before deploying NSX Controllers, you must deploy an NSX Manager
        appliance and register vCenter with NSX Manager.
       </p></li><li class="listitem "><p>
        Determine the IP pool settings for your controller cluster, including
        the gateway and IP address range. DNS settings are optional.
       </p></li><li class="listitem "><p>
        The NSX Controller IP network must have connectivity to the NSX Manager
        and to the management interfaces on the ESXi hosts.
       </p></li></ul></div></div><p>
     Log in to the vSphere web client and do the following steps to add the NSX
     controllers:
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       In vCenter, navigate to <span class="guimenu ">Home</span>, select
       <span class="guimenu ">Networking &amp;
       Security</span> › <span class="guimenu ">Installation</span>, and then
       select the <span class="guimenu ">Management</span> tab.
      </p></li><li class="step "><p>
       In the <span class="guimenu ">NSX Controller nodes</span> section, click the
       <span class="guimenu ">Add Node</span> icon represented by a green plus sign.
      </p></li><li class="step "><p>
       Enter the NSX Controller settings appropriate to your environment. If
       you are following this example, use the control-plane clustered ESXi
       hosts and control-plane DVS port group for the controller settings.
      </p></li><li class="step "><p>
       If it has not already been done, create an IP pool for the NSX
       Controller cluster with at least three IP addressess by clicking
       <span class="guimenu ">New IP Pool</span>. Individual controllers can be in
       separate IP subnets, if necessary.
      </p></li><li class="step "><p>
       Click <span class="guimenu ">OK</span> to deploy the controller. After the first
       controller is completely deployed, deploy two additional controllers.
      </p></li></ol></div></div><div id="id-1.3.6.11.10.4.9.11.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      Three NSX controllers is mandatory. VMware recommends configuring a DRS
      anti-affinity rule to prevent the controllers from residing on the same
      ESXi host. See more information about
      <a class="link" href="https://pubs.vmware.com/vsphere-51/index.jsp?topic=%2Fcom.vmware.vsphere.resmgmt.doc%2FGUID-FF28F29C-8B67-4EFF-A2EF-63B3537E6934.html" target="_blank">DRS
      Affinity Rules</a>.
     </p></div></div><div class="sect4" id="id-1.3.6.11.10.4.9.12"><div class="titlepage"><div><div><h5 class="title"><span class="number">28.2.2.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prepare Clusters for NSX Management</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.10.4.9.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
     During <span class="guimenu ">Host Preparation</span>, the NSX Manager:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Installs the NSX kernel modules on ESXi hosts that are members of
       vSphere clusters
      </p></li><li class="listitem "><p>
       Builds the NSX control-plane and management-plane infrastructure
      </p></li></ul></div><p>
     The NSX kernel modules are packaged in <code class="filename">VIB</code> (vSphere
     Installation Bundle) files. They run within the hypervisor kernel and
     provide services such as distributed routing, distributed firewall, and
     VXLAN bridging capabilities. These files are installed on a per-cluster
     level, and the setup process deploys the required software on all ESXi
     hosts in the target cluster. When a new ESXi host is added to the cluster,
     the required software is automatically installed on the newly added host.
    </p><p>
     Before beginning the NSX host preparation process, make sure of the
     following in your environment:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Register vCenter with NSX Manager and deploy the NSX controllers.
      </p></li><li class="listitem "><p>
       Verify that DNS reverse lookup returns a fully qualified domain name
       when queried with the IP address of NSX Manager.
      </p></li><li class="listitem "><p>
       Verify that the ESXi hosts can resolve the DNS name of vCenter server.
      </p></li><li class="listitem "><p>
       Verify that the ESXi hosts can connect to vCenter Server on port 80.
      </p></li><li class="listitem "><p>
       Verify that the network time on vCenter Server and the ESXi hosts is
       synchronized.
      </p></li><li class="listitem "><p>
       For each vSphere cluster that will participate in NSX, verify that the
       ESXi hosts within each respective cluster are attached to a common VDS.
      </p><p>
       For example, given a deployment with two clusters named Host1 and Host2.
       Host1 is attached to VDS1 and VDS2. Host2 is attached to VDS1 and VDS3.
       When you prepare a cluster for NSX, you can only associate NSX with VDS1
       on the cluster. If you add another host (Host3) to the cluster and Host3
       is not attached to VDS1, it is an invalid configuration, and Host3 will
       not be ready for NSX functionality.
      </p></li><li class="listitem "><p>
       If you have vSphere Update Manager (VUM) in your environment, you must
       disable it before preparing clusters for network virtualization. For
       information on how to check if VUM is enabled and how to disable it if
       necessary, see the
       <a class="link" href="http://kb.vmware.com/kb/2053782" target="_blank">VMware knowledge
       base</a>.
      </p></li><li class="listitem "><p>
       In the vSphere web client, ensure that the cluster is in the resolved
       state (listed under the <span class="guimenu ">Host Preparation</span> tab). If the
       Resolve option does not appear in the cluster's Actions list, then it is
       in a resolved state.
      </p></li></ul></div><p>
     To prepare the vSphere clusters for NSX:
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       In vCenter, select
       <span class="guimenu ">Home</span> › <span class="guimenu ">Networking &amp;
       Security</span> › <span class="guimenu ">Installation</span>, and then
       select the <span class="guimenu ">Host Preparation</span> tab.
      </p></li><li class="step "><p>
       Continuing with the example in these instructions, click on the
       <span class="guimenu ">Actions</span> button (gear icon) and select
       <span class="guimenu ">Install</span> for both the control-plane cluster and
       compute cluster (if you are using something other than this example,
       then only install on the clusters that require NSX logical switching,
       routing, and firewalls).
      </p></li><li class="step "><p>
       Monitor the installation until the <code class="literal">Installation
       Status</code> column displays a green check mark.
      </p><div id="id-1.3.6.11.10.4.9.12.8.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
        While installation is in progress, do not deploy, upgrade, or uninstall
        any service or component.
       </p></div><div id="id-1.3.6.11.10.4.9.12.8.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
        If the <code class="literal">Installation Status</code> column displays a red
        warning icon and says <code class="literal">Not Ready</code>, click
        <span class="guimenu ">Resolve</span>. Clicking <span class="guimenu ">Resolve</span> might
        result in a reboot of the host. If the installation is still not
        successful, click the warning icon. All errors will be displayed. Take
        the required action and click <span class="guimenu ">Resolve</span> again.
       </p></div></li><li class="step "><p>
       To verify the VIBs (<code class="filename">esx-vsip</code> and
       <code class="filename">esx-vxlan</code>) are installed and registered, SSH into
       an ESXi host within the prepared cluster. List the names and versions of
       the VIBs installed by running the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>esxcli software vib list | grep esx</pre></div><div class="verbatim-wrap"><pre class="screen">...
esx-vsip      6.0.0-0.0.2732470    VMware  VMwareCertified   2015-05-29
esx-vxlan     6.0.0-0.0.2732470    VMware  VMwareCertified   2015-05-29
...</pre></div></li></ol></div></div><div id="id-1.3.6.11.10.4.9.12.9" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      After host preparation:
     </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
        A host reboot is not required
       </p></li><li class="listitem "><p>
        If you add a host to a prepared cluster, the NSX VIBs are automatically
        installed on the host.
       </p></li><li class="listitem "><p>
        If you move a host to an unprepared cluster, the NSX VIBs are
        automatically uninstalled from the host. In this case, a host reboot is
        required to complete the uninstall process.
       </p></li></ul></div></div></div><div class="sect4" id="id-1.3.6.11.10.4.9.13"><div class="titlepage"><div><div><h5 class="title"><span class="number">28.2.2.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure VXLAN Transport Parameters</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.10.4.9.13">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
     VXLAN is configured on a per-cluster basis, where each vSphere cluster
     that is to participate in NSX is mapped to a vSphere Distributed Virtual
     Switch (DVS). When mapping a vSphere cluster to a DVS, each ESXi host in
     that cluster is enabled for logical switches. The settings chosen in this
     section will be used in creating the VMkernel interface.
    </p><p>
     Configuring transport parameters involves selecting a DVS, a VLAN ID, an
     MTU size, an IP addressing mechanism, and a NIC teaming policy. The MTU
     for each switch must be set to 1550 or higher. By default, it is set to
     1600 by NSX. This is also the recommended setting for integration with
     <span class="productname">OpenStack</span>.
    </p><p>
     To configure the VXLAN transport parameters:
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       In the vSphere web client, navigate to
       <span class="guimenu ">Home</span> › <span class="guimenu ">Networking &amp;
       Security</span> › <span class="guimenu ">Installation</span>.
      </p></li><li class="step "><p>
       Select the <span class="guimenu ">Host Preparation</span> tab.
      </p></li><li class="step "><p>
       Click the <span class="guimenu ">Configure</span> link in the VXLAN column.
      </p></li><li class="step "><p>
       Enter the required information.
      </p></li><li class="step "><p>
       If you have not already done so, create an IP pool for the VXLAN tunnel
       end points (VTEP) by clicking <span class="guimenu ">New IP Pool</span>:
      </p></li><li class="step "><p>
       Click <span class="guimenu ">OK</span> to create the VXLAN network.
      </p></li></ol></div></div><p>
     When configuring the VXLAN transport network, consider the following:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Use a NIC teaming policy that best suits the environment being built.
       <code class="literal">Load Balance - SRCID</code> as the VMKNic teaming policy is
       usually the most flexible out of all the available options. This allows
       each host to have a VTEP vmkernel interface for each dvuplink on the
       selected distributed switch (two dvuplinks gives two VTEP interfaces per
       ESXi host).
      </p></li><li class="listitem "><p>
       Do not mix different teaming policies for different portgroups on a VDS
       where some use Etherchannel or Link Aggregation Control Protocol (LACPv1
       or LACPv2) and others use a different teaming policy. If uplinks are
       shared in these different teaming policies, traffic will be interrupted.
       If logical routers are present, there will be routing problems. Such a
       configuration is not supported and should be avoided.
      </p></li><li class="listitem "><p>
       For larger environments it may be better to use DHCP for the VMKNic IP
       Addressing.
      </p></li><li class="listitem "><p>
       For more information and further guidance, see the
       <a class="link" href="https://communities.vmware.com/docs/DOC-27683" target="_blank">VMware
       NSX for vSphere Network Virtualization Design Guide</a>.
      </p></li></ul></div></div><div class="sect4" id="id-1.3.6.11.10.4.9.14"><div class="titlepage"><div><div><h5 class="title"><span class="number">28.2.2.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Assign Segment ID Pool</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.10.4.9.14">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
     Each VXLAN tunnel will need a segment ID to isolate its network traffic.
     Therefore, it is necessary to configure a segment ID pool for the NSX
     VXLAN network to use. If an NSX controller is not deployed within the
     vSphere environment, a multicast address range must be added to spread
     traffic across the network and avoid overloading a single multicast
     address.
    </p><p>
     For the purposes of the example in these instructions, do the following
     steps to assign a segment ID pool. Otherwise, follow best practices as
     outlined in
     <a class="link" href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-7B33DE72-78A7-448C-A61C-9B41D1EB12AD.html" target="_blank">VMware's
     documentation</a>.
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       In the vSphere web client, navigate to
       <span class="guimenu ">Home</span> › <span class="guimenu ">Networking &amp;
       Security</span> › <span class="guimenu ">Installation</span>.
      </p></li><li class="step "><p>
       Select the <span class="guimenu ">Logical Network Preparation</span> tab.
      </p></li><li class="step "><p>
       Click <span class="guimenu ">Segment ID</span>, and then <span class="guimenu ">Edit</span>.
      </p></li><li class="step "><p>
       Click <span class="guimenu ">OK</span> to save your changes.
      </p></li></ol></div></div></div><div class="sect4" id="id-1.3.6.11.10.4.9.15"><div class="titlepage"><div><div><h5 class="title"><span class="number">28.2.2.4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Assign Segment ID Pool</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.10.4.9.15">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
     Each VXLAN tunnel will need a segment ID to isolate its network traffic.
     Therefore, it is necessary to configure a segment ID pool for the NSX
     VXLAN network to use. If an NSX controller is not deployed within the
     vSphere environment, a multicast address range must be added to spread
     traffic across the network and avoid overloading a single multicast
     address.
    </p><p>
     For the purposes of the example in these instructions, do the following
     steps to assign a segment ID pool. Otherwise, follow best practices as
     outlined in
     <a class="link" href="https://pubs.vmware.com/NSX-62/index.jsp?topic=%2Fcom.vmware.nsx.install.doc%2FGUID-7B33DE72-78A7-448C-A61C-9B41D1EB12AD.html" target="_blank">VMware's
     documentation</a>.
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       In the vSphere web client, navigate to
       <span class="guimenu ">Home</span> › <span class="guimenu ">Networking &amp;
       Security</span> › <span class="guimenu ">Installation</span>.
      </p></li><li class="step "><p>
       Select the <span class="guimenu ">Logical Network Preparation</span> tab.
      </p></li><li class="step "><p>
       Click <span class="guimenu ">Segment ID</span>, and then <span class="guimenu ">Edit</span>.
      </p></li><li class="step "><p>
       Click <span class="guimenu ">OK</span> to save your changes.
      </p></li></ol></div></div></div><div class="sect4" id="id-1.3.6.11.10.4.9.16"><div class="titlepage"><div><div><h5 class="title"><span class="number">28.2.2.4.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create a Transport Zone</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.10.4.9.16">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
     A transport zone controls which hosts a logical switch can reach and has
     the following characteristics.
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       It can span one or more vSphere clusters.
      </p></li><li class="listitem "><p>
       Transport zones dictate which clusters can participate in the use of a
       particular network. Therefore they dictate which VMs can participate in
       the use of a particular network.
      </p></li><li class="listitem "><p>
       A vSphere NSX environment can contain one or more transport zones based
       on the environment's requirements.
      </p></li><li class="listitem "><p>
       A host cluster can belong to multiple transport zones.
      </p></li><li class="listitem "><p>
       A logical switch can belong to only one transport zone.
      </p></li></ul></div><div id="id-1.3.6.11.10.4.9.16.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      <span class="productname">OpenStack</span> has only been verified to work with a single transport zone
      within a vSphere NSX-v environment. Other configurations are currently
      not supported.
     </p></div><p>
     For more information on transport zones, refer to
     <a class="link" href="https://pubs.vmware.com/NSX-62/topic/com.vmware.nsx.install.doc/GUID-0B3BD895-8037-48A8-831C-8A8986C3CA42.html" target="_blank">VMware's
     Add A Transport Zone</a>.
    </p><p>
     To create a transport zone:
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       In the vSphere web client, navigate to
       <span class="guimenu ">Home</span> › <span class="guimenu ">Networking &amp;
       Security</span> › <span class="guimenu ">Installation</span>.
      </p></li><li class="step "><p>
       Select the <span class="guimenu ">Logical Network Preparation</span> tab.
      </p></li><li class="step "><p>
       Click <span class="guimenu ">Transport Zones</span>, and then click the
       <span class="guimenu ">New Transport Zone</span> (New Logical Switch) icon.
      </p></li><li class="step "><p>
       In the <span class="guimenu ">New Transport Zone</span> dialog box, type a name and
       an optional description for the transport zone.
      </p></li><li class="step "><p>
       For these example instructions, select the control plane mode as
       <code class="literal">Unicast</code>.
      </p><div id="id-1.3.6.11.10.4.9.16.7.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
        Whether there is a controller in the environment or if the environment
        is going to use multicast addresses will determine the control plane
        mode to select:
       </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          <code class="literal">Unicast</code> (what this set of instructions uses): The
          control plane is handled by an NSX controller. All unicast traffic
          leverages optimized headend replication. No multicast IP addresses or
          special network configuration is required.
         </p></li><li class="listitem "><p>
          <code class="literal">Multicast</code>: Multicast IP addresses in the physical
          network are used for the control plane. This mode is recommended only
          when upgrading from older VXLAN deployments. Requires PIM/IGMP in the
          physical network.
         </p></li><li class="listitem "><p>
          <code class="literal">Hybrid</code>: Offloads local traffic replication to the
          physical network (L2 multicast). This requires IGMP snooping on the
          first-hop switch and access to an IGMP querier in each VTEP subnet,
          but does not require PIM. The first-hop switch handles traffic
          replication for the subnet.
         </p></li></ul></div></div></li><li class="step "><p>
       Select the clusters to be added to the transport zone.
      </p></li><li class="step "><p>
       Click <span class="guimenu ">OK</span> to save your changes.
      </p></li></ol></div></div></div><div class="sect4" id="id-1.3.6.11.10.4.9.17"><div class="titlepage"><div><div><h5 class="title"><span class="number">28.2.2.4.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.10.4.9.17">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
     With vSphere environment setup completed, the <span class="productname">OpenStack</span> can be deployed.
     The following sections will cover creating virtual machines within the
     vSphere environment, configuring the cloud model and integrating NSX-v
     neutron core plugin into the <span class="productname">OpenStack</span>:
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Create the virtual machines
      </p></li><li class="step "><p>
       Deploy the Cloud Lifecycle Manager
      </p></li><li class="step "><p>
       Configure the neutron environment with NSX-v
      </p></li><li class="step "><p>
       Modify the cloud input model
      </p></li><li class="step "><p>
       Set up the parameters
      </p></li><li class="step "><p>
       Deploy the Operating System with Cobbler
      </p></li><li class="step "><p>
       Deploy the cloud
      </p></li></ol></div></div></div><div class="sect4" id="id-1.3.6.11.10.4.9.18"><div class="titlepage"><div><div><h5 class="title"><span class="number">28.2.2.4.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> on Baremetal</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.10.4.9.18">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
     Within the vSphere environment, create the <span class="productname">OpenStack</span> compute proxy virtual
     machines. There needs to be one neutron compute proxy virtual machine per
     ESXi compute cluster.
    </p><p>
     For the minimum NSX hardware requirements, refer to
     <a class="xref" href="#nsx-hw-reqs-bm" title="NSX Hardware Requirements for Baremetal Integration">Table 28.2, “NSX Hardware Requirements for Baremetal Integration”</a>. Also be aware of the networking
     model to use for the VM network interfaces, see
     <a class="xref" href="#nsx-interface-reqs" title="NSX Interface Requirements">Table 28.3, “NSX Interface Requirements”</a>:
    </p><p>
     If ESX VMs are to be used as nova compute proxy nodes, set up three
     LAN interfaces in each virtual machine as shown in the table below. There
     is at least one nova compute proxy node per cluster.
    </p><div class="table" id="nsx-interface-reqs"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 28.3: </span><span class="name">NSX Interface Requirements </span><a title="Permalink" class="permalink" href="#nsx-interface-reqs">#</a></h6></div><div class="table-contents"><table class="table" summary="NSX Interface Requirements" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th>
         <p>
          Network Group
         </p>
        </th><th>
         <p>
          Interface
         </p>
        </th></tr></thead><tbody><tr><td>
         <p>
          Management
         </p>
        </td><td>
         <p>
          <code class="literal">eth0</code>
         </p>
        </td></tr><tr><td>
         <p>
          External API
         </p>
        </td><td>
         <p>
          <code class="literal">eth1</code>
         </p>
        </td></tr><tr><td>
         <p>
          Internal API
         </p>
        </td><td>
         <p>
          <code class="literal">eth2</code>
         </p>
        </td></tr></tbody></table></div></div><div class="sect5" id="id-1.3.6.11.10.4.9.18.6"><div class="titlepage"><div><div><h6 class="title"><span class="number">28.2.2.4.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Advanced Configuration Option</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.10.4.9.18.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div id="id-1.3.6.11.10.4.9.18.6.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
       Within vSphere for each in the virtual machine:
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         In the <span class="guimenu ">Options</span> section, under <span class="guimenu ">Advanced
         configuration parameters</span>, ensure that
         <code class="literal">disk.EnableUUIDoption</code> is set to
         <code class="literal">true</code>.
        </p></li><li class="listitem "><p>
         If the option does not exist, it must be added. This option is
         required for the <span class="productname">OpenStack</span> deployment.
        </p></li><li class="listitem "><p>
         If the option is not specified, then the deployment will fail when
         attempting to configure the disks of each virtual machine.
        </p></li></ul></div></div></div><div class="sect5" id="id-1.3.6.11.10.4.9.18.7"><div class="titlepage"><div><div><h6 class="title"><span class="number">28.2.2.4.9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting Up the Cloud Lifecycle Manager</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.10.4.9.18.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="sect6" id="id-1.3.6.11.10.4.9.18.7.2"><div class="titlepage"><div><div><h6 class="title"><span class="number">28.2.2.4.9.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing the Cloud Lifecycle Manager</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.10.4.9.18.7.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    Running the <code class="command">ARDANA_INIT_AUTO=1</code> command is optional to
    avoid stopping for authentication at any step. You can also run
    <code class="command">ardana-init</code>to launch the Cloud Lifecycle Manager.  You will be prompted to
    enter an optional SSH passphrase, which is used to protect the key used by
    Ansible when connecting to its client nodes.  If you do not want to use a
    passphrase, press <span class="keycap">Enter</span> at the prompt.
   </p><p>
    If you have protected the SSH key with a passphrase, you can avoid having
    to enter the passphrase on every attempt by Ansible to connect to its
    client nodes with the following commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>eval $(ssh-agent)
<code class="prompt user">ardana &gt; </code>ssh-add ~/.ssh/id_rsa</pre></div><p>
    The Cloud Lifecycle Manager will contain the installation scripts and configuration files to
    deploy your cloud. You can set up the Cloud Lifecycle Manager on a dedicated node or you do
    so on your first controller node. The default choice is to use the first
    controller node as the Cloud Lifecycle Manager.
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Download the product from:
     </p><ol type="a" class="substeps "><li class="step "><p>
        <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a>
       </p></li></ol></li><li class="step "><p>
      Boot your Cloud Lifecycle Manager from the SLES ISO contained in the download.
     </p></li><li class="step "><p>
      Enter <code class="literal">install</code> (all lower-case, exactly as spelled out
      here) to start installation.
     </p></li><li class="step "><p>
      Select the language. Note that only the English language selection is
      currently supported.
     </p></li><li class="step "><p>
      Select the location.
     </p></li><li class="step "><p>
      Select the keyboard layout.
     </p></li><li class="step "><p>
      Select the primary network interface, if prompted:
     </p><ol type="a" class="substeps "><li class="step "><p>
        Assign IP address, subnet mask, and default gateway
       </p></li></ol></li><li class="step "><p>
      Create new account:
     </p><ol type="a" class="substeps "><li class="step "><p>
        Enter a username.
       </p></li><li class="step "><p>
        Enter a password.
       </p></li><li class="step "><p>
        Enter time zone.
       </p></li></ol></li></ol></div></div><p>
    Once the initial installation is finished, complete the Cloud Lifecycle Manager setup with
    these steps:
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Ensure your Cloud Lifecycle Manager has a valid DNS nameserver specified in
      <code class="literal">/etc/resolv.conf</code>.
     </p></li><li class="step "><p>
      Set the environment variable LC_ALL:
     </p><div class="verbatim-wrap"><pre class="screen">export LC_ALL=C</pre></div><div id="id-1.3.6.11.10.4.9.18.7.2.8.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
       This can be added to <code class="filename">~/.bashrc</code> or
       <code class="filename">/etc/bash.bashrc</code>.
      </p></div></li></ol></div></div><p>
    The node should now have a working SLES setup.
   </p></div></div><div class="sect5" id="id-1.3.6.11.10.4.9.18.8"><div class="titlepage"><div><div><h6 class="title"><span class="number">28.2.2.4.9.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure the Neutron Environment with NSX-v</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.10.4.9.18.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
      In summary, integrating NSX with vSphere has four major steps:
     </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
        Modify the input model to define the server roles, servers, network
        roles and networks. <a class="xref" href="#nsx-modify-input-model" title="28.1.2.5.3.2. Modify the Input Model">Section 28.1.2.5.3.2, “Modify the Input Model”</a>
       </p></li><li class="step "><p>
        Set up the parameters needed for neutron and nova to communicate
        with the ESX and NSX Manager. <a class="xref" href="#nsx-deploy-os-cobbler" title="28.1.2.5.3.3. Deploying the Operating System with Cobbler">Section 28.1.2.5.3.3, “Deploying the Operating System with Cobbler”</a>
       </p></li><li class="step "><p>
        Do the steps to deploy the cloud. <a class="xref" href="#nsx-deploy-cloud" title="28.1.2.5.3.4. Deploying the Cloud">Section 28.1.2.5.3.4, “Deploying the Cloud”</a>
       </p></li></ol></div></div><div class="sect6" id="id-1.3.6.11.10.4.9.18.8.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">28.2.2.4.9.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Third-Party Import of VMware NSX-v Into neutron and python-neutronclient</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.10.4.9.18.8.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
       To import the NSX-v neutron core-plugin into Cloud Lifecycle Manager, run the third-party
       import playbook.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost third-party-import.yml</pre></div></div><div class="sect6" id="id-1.3.6.11.10.4.9.18.8.5"><div class="titlepage"><div><div><h6 class="title"><span class="number">28.2.2.4.9.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Modify the Input Model</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.10.4.9.18.8.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
       After the third-party import has completed successfully, modify the
       input model:
      </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Prepare for input model changes
        </p></li><li class="step "><p>
         Define the servers and server roles needed for a NSX-v cloud.
        </p></li><li class="step "><p>
         Define the necessary networks and network groups
        </p></li><li class="step "><p>
         Specify the services needed to be deployed on the Cloud Lifecycle Manager controllers
         and the nova ESX compute proxy nodes.
        </p></li><li class="step "><p>
         Commit the changes and run the configuration processor.
        </p></li></ol></div></div><div class="sect6" id="id-1.3.6.11.10.4.9.18.8.5.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">28.2.2.4.9.3.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prepare for Input Model Changes</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.10.4.9.18.8.5.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
        The previous steps created a modified <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> tarball with the
        NSX-v core plugin in the neutron and <code class="literal">neutronclient</code>
        venvs. The <code class="filename">tar</code> file can now be extracted and the
        <code class="filename">ardana-init.bash</code> script can be run to set up the
        deployment files and directories. If a modified
        <code class="filename">tar</code> file was not created, then extract the tar
        from the /media/cdrom/ardana location.
       </p><p>
        To run the <code class="filename">ardana-init.bash</code> script which is
        included in the build, use this commands:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>~/ardana/hos-init.bash</pre></div></div><div class="sect6" id="id-1.3.6.11.10.4.9.18.8.5.5"><div class="titlepage"><div><div><h6 class="title"><span class="number">28.2.2.4.9.3.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create the Input Model</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.10.4.9.18.8.5.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
        Copy the example input model to
        <code class="filename">~/openstack/my_cloud/definition/</code> directory:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/ardana-extensions/ardana-extensions-nsx/vmware/examples/models/entry-scale-nsx
<code class="prompt user">ardana &gt; </code>cp -R entry-scale-nsx ~/openstack/my_cloud/definition</pre></div><p>
        Refer to the reference input model in
        <code class="filename">ardana-extensions/ardana-extensions-nsx/vmware/examples/models/entry-scale-nsx/</code>
        for details about how these definitions should be made. The main
        differences between this model and the standard Cloud Lifecycle Manager input models are:
       </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          Only the neutron-server is deployed. No other neutron agents are
          deployed.
         </p></li><li class="listitem "><p>
          Additional parameters need to be set in
          <code class="filename">pass_through.yml</code> and
          <code class="filename">nsx/nsx_config.yml</code>.
         </p></li><li class="listitem "><p>
          nova ESX compute proxy nodes may be ESX virtual machines.
         </p></li></ul></div><div class="sect6" id="id-1.3.6.11.10.4.9.18.8.5.5.6"><div class="titlepage"><div><div><h6 class="title"><span class="number">28.2.2.4.9.3.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Set up the Parameters</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.10.4.9.18.8.5.5.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
         The special parameters needed for the NSX-v integrations are set in
         the files <code class="filename">pass_through.yml</code> and
         <code class="filename">nsx/nsx_config.yml</code>. They are in the
         <code class="filename">~/openstack/my_cloud/definition/data</code> directory.
        </p><p>
         Parameters in <code class="filename">pass_through.yml</code> are in the sample
         input model in the
         <code class="filename">ardana-extensions/ardana-extensions-nsx/vmware/examples/models/entry-scale-nsx/</code>
         directory. The comments in the sample input model file describe how to
         locate the values of the required parameters.
        </p><div class="verbatim-wrap"><pre class="screen">#
# (c) Copyright 2017 SUSE LLC
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---
product:
  version: 2
pass-through:
  global:
    vmware:
      - username: <em class="replaceable ">VCENTER_ADMIN_USERNAME</em>
        ip: <em class="replaceable ">VCENTER_IP</em>
        port: 443
        cert_check: false
        # The password needs to be encrypted using the script
        # openstack/ardana/ansible/ardanaencrypt.py on the deployer:
        #
        # $ cd ~/openstack/ardana/ansible
        # $ export ARDANA_USER_PASSWORD_ENCRYPT_KEY=<em class="replaceable ">ENCRYPTION_KEY</em>
        # $ ./ardanaencrypt.py
        #
        # The script will prompt for the vCenter password. The string
        # generated is the encrypted password. Enter the string
        # enclosed by double-quotes below.
        password: "<em class="replaceable ">ENCRYPTED_PASSWD_FROM_ARDANAENCRYPT</em>"

        # The id is is obtained by the URL
        # https://<em class="replaceable ">VCENTER_IP</em>/mob/?moid=ServiceInstance&amp;doPath=content%2eabout,
        # field instanceUUID.
        id: <em class="replaceable ">VCENTER_UUID</em>
  servers:
    -
      # Here the 'id' refers to the name of the node running the
      # esx-compute-proxy. This is identical to the 'servers.id' in
      # servers.yml. There should be one esx-compute-proxy node per ESX
      # resource pool.
      id: esx-compute1
      data:
        vmware:
          vcenter_cluster: <em class="replaceable ">VMWARE_CLUSTER1_NAME</em>
          vcenter_id: <em class="replaceable ">VCENTER_UUID</em>
    -
      id: esx-compute2
      data:
        vmware:
          vcenter_cluster: <em class="replaceable ">VMWARE_CLUSTER2_NAME</em>
          vcenter_id: <em class="replaceable ">VCENTER_UUID</em></pre></div><p>
         There are parameters in <code class="filename">nsx/nsx_config.yml</code>. The
         comments describes how to retrieve the values.
        </p><div class="verbatim-wrap"><pre class="screen"># (c) Copyright 2017 SUSE LLC
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---
  product:
    version: 2
  configuration-data:
    - name: NSX-CONFIG-CP1
      services:
        - nsx
      data:
        # (Required) URL for NSXv manager (e.g - https://management_ip).
        manager_uri: 'https://<em class="replaceable ">NSX_MGR_IP</em>

        # (Required) NSXv username.
        user: 'admin'

        # (Required) Encrypted NSX Manager password.
        # Password encryption is done by the script
        # ~/openstack/ardana/ansible/ardanaencrypt.py on the deployer:
        #
        # $ cd ~/openstack/ardana/ansible
        # $ export ARDANA_USER_PASSWORD_ENCRYPT_KEY=<em class="replaceable ">ENCRYPTION_KEY</em>
        # $ ./ardanaencrypt.py
        #
        # NOTE: Make sure that the NSX Manager password is encrypted with the same key
        # used to encrypt the VCenter password.
        #
        # The script will prompt for the NSX Manager password. The string
        # generated is the encrypted password. Enter the string enclosed
        # by double-quotes below.
        password: "<em class="replaceable ">ENCRYPTED_NSX_MGR_PASSWD_FROM_ARDANAENCRYPT</em>"
        # (Required) datacenter id for edge deployment.
        # Retrieved using
        #    http://<em class="replaceable ">VCENTER_IP_ADDR</em>/mob/?moid=ServiceInstance&amp;doPath=content
        # click on the value from the rootFolder property. The datacenter_moid is
        # the value of the childEntity property.
        # The vCenter-ip-address comes from the file pass_through.yml in the
        # input model under "pass-through.global.vmware.ip".
        datacenter_moid: 'datacenter-21'
        # (Required) id of logic switch for physical network connectivity.
        # How to retrieve
        # 1. Get to the same page where the datacenter_moid is found.
        # 2. Click on the value of the rootFolder property.
        # 3. Click on the value of the childEntity property
        # 4. Look at the network property. The external network is
        #    network associated with EXTERNAL VM in VCenter.
        external_network: 'dvportgroup-74'
        # (Required) clusters ids containing OpenStack hosts.
        # Retrieved using http://<em class="replaceable ">VCENTER_IP_ADDR</em>/mob, click on the value
        # from the rootFolder property. Then click on the value of the
        # hostFolder property. Cluster_moids are the values under childEntity
        # property of the compute clusters.
        cluster_moid: 'domain-c33,domain-c35'
        # (Required) resource-pool id for edge deployment.
        resource_pool_id: 'resgroup-67'
        # (Optional) datastore id for edge deployment. If not needed,
        # do not declare it.
        # datastore_id: 'datastore-117'

        # (Required) network scope id of the transport zone.
        # To get the vdn_scope_id, in the vSphere web client from the Home
        # menu:
        #   1. click on Networking &amp; Security
        #   2. click on installation
        #   3. click on the Logical Netowrk Preparation tab.
        #   4. click on the Transport Zones button.
        #   5. Double click on the transport zone being configure.
        #   6. Select Manage tab.
        #   7. The vdn_scope_id will appear at the end of the URL.
        vdn_scope_id: 'vdnscope-1'

        # (Optional) Dvs id for VLAN based networks. If not needed,
        # do not declare it.
        # dvs_id: 'dvs-68'

        # (Required) backup_edge_pool: backup edge pools management range,
        # - edge_type&gt;[edge_size]:<em class="replaceable ">MINIMUM_POOLED_EDGES</em>:<em class="replaceable ">MAXIMUM_POOLED_EDGES</em>
        # - edge_type: service (service edge) or  vdr (distributed edge)
        # - edge_size:  compact ,  large (by default),  xlarge  or  quadlarge
        backup_edge_pool: 'service:compact:4:10,vdr:compact:4:10'

        # (Optional) mgt_net_proxy_ips: management network IP address for
        # metadata proxy. If not needed, do not declare it.
        # mgt_net_proxy_ips: '10.142.14.251,10.142.14.252'

        # (Optional) mgt_net_proxy_netmask: management network netmask for
        # metadata proxy. If not needed, do not declare it.
        # mgt_net_proxy_netmask: '255.255.255.0'

        # (Optional) mgt_net_moid: Network ID for management network connectivity
        # Do not declare if not used.
        # mgt_net_moid: 'dvportgroup-73'

        # ca_file: Name of the certificate file. If insecure is set to True,
        # then this parameter is ignored. If insecure is set to False and this
        # parameter is not defined, then the system root CAs will be used
        # to verify the server certificate.
        ca_file: a/nsx/certificate/file

        # insecure:
        # If true (default), the NSXv server certificate is not verified.
        # If false, then the default CA truststore is used for verification.
        # This option is ignored if "ca_file" is set
        insecure: True
        # (Optional) edge_ha: if true, will duplicate any edge pool resources
        # Default to False if undeclared.
        # edge_ha: False
        # (Optional) spoofguard_enabled:
        # If True (default), indicates NSXV spoofguard component is used to
        # implement port-security feature.
        # spoofguard_enabled: True
        # (Optional) exclusive_router_appliance_size:
        # Edge appliance size to be used for creating exclusive router.
        # Valid values: 'compact', 'large', 'xlarge', 'quadlarge'
        # Defaults to 'compact' if not declared.  # exclusive_router_appliance_size:
        'compact'</pre></div></div></div><div class="sect6" id="id-1.3.6.11.10.4.9.18.8.5.6"><div class="titlepage"><div><div><h6 class="title"><span class="number">28.2.2.4.9.3.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Commit Changes and Run the Configuration Processor</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.10.4.9.18.8.5.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
        Commit your changes with the input model and the required configuration
        values added to the <code class="filename">pass_through.yml</code> and
        <code class="filename">nsx/nsx_config.yml</code> files.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition
<code class="prompt user">ardana &gt; </code>git commit -A -m "Configuration changes for NSX deployment"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml \
 -e \encrypt="" -e rekey=""</pre></div><p>
        If the playbook <code class="filename">config-processor-run.yml</code> fails,
        there is an error in the input model. Fix the error and repeat the
        above steps.
       </p></div></div><div class="sect6" id="id-1.3.6.11.10.4.9.18.8.6"><div class="titlepage"><div><div><h6 class="title"><span class="number">28.2.2.4.9.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying the Operating System with Cobbler</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.10.4.9.18.8.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         From the Cloud Lifecycle Manager, run Cobbler to install the operating system on the
         nodes after it has to be deployed:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step "><p>
         Verify the nodes that will have an operating system installed by
         Cobbler by running this command:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cobbler system find --netboot-enabled=1</pre></div></li><li class="step "><p>
         Reimage the nodes using Cobbler. Do not use Cobbler to reimage the
         nodes running as ESX virtual machines. The command below is run on a
         setup where the nova ESX compute proxies are VMs. Controllers 1,
         2, and 3 are running on physical servers.
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml -e \
   nodelist=controller1,controller2,controller3</pre></div></li><li class="step "><p>
         When the playbook has completed, each controller node should have an
         operating system installed with an IP address configured on
         <code class="literal">eth0</code>.
        </p></li><li class="step "><p>
         After your controller nodes have been completed, you should install
         the operating system on your nova compute proxy virtual machines.
         Each configured virtual machine should be able to PXE boot into the
         operating system installer.
        </p></li><li class="step "><p>
         From within the vSphere environment, power on each nova compute
         proxy virtual machine and watch for it to PXE boot into the OS
         installer via its console.
        </p><ol type="a" class="substeps "><li class="step "><p>
           If successful, the virtual machine will have the operating system
           automatically installed and will then automatically power off.
          </p></li><li class="step "><p>
           When the virtual machine has powered off, power it on and let it
           boot into the operating system.
          </p></li></ol></li><li class="step "><p>
         Verify network settings after deploying the operating system to each
         node.
        </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
           Verify that the NIC bus mapping specified in the cloud model input
           file
           (<code class="filename">~/ardana/my_cloud/definition/data/nic_mappings.yml</code>)
           matches the NIC bus mapping on each <span class="productname">OpenStack</span> node.
          </p><p>
           Check the NIC bus mapping with this command:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cobbler system list</pre></div></li><li class="listitem "><p>
           After the playbook has completed, each controller node should have
           an operating system installed with an IP address configured on eth0.
          </p></li></ul></div></li><li class="step "><p>
         When the ESX compute proxy nodes are VMs, install the operating system
         if you have not already done so.
        </p></li></ol></div></div></div><div class="sect6" id="id-1.3.6.11.10.4.9.18.8.7"><div class="titlepage"><div><div><h6 class="title"><span class="number">28.2.2.4.9.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying the Cloud</span> <a title="Permalink" class="permalink" href="#id-1.3.6.11.10.4.9.18.8.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-vsphere-baremetal.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-vsphere-baremetal.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
       When the configuration processor has completed successfully, the cloud
       can be deployed. Set the ARDANA_USER_PASSWORD_ENCRYPT_KEY environment
       variable before running <code class="filename">site.yml</code>.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>export ARDANA_USER_PASSWORD_ENCRYPT_KEY=<em class="replaceable ">PASSWORD_KEY</em>
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-cloud-configure.yml</pre></div><p>
       <em class="replaceable ">PASSWORD_KEY</em> in the <code class="literal">export</code>
       command is the key used to encrypt the passwords for vCenter and NSX
       Manager.
      </p></div></div></div></div></div></div><div class="sect1" id="nsx-verification"><div class="titlepage"><div><div><h2 class="title"><span class="number">28.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Verifying the NSX-v Functionality After Integration</span> <a title="Permalink" class="permalink" href="#nsx-verification">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/nsx-verification.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>nsx-verification.xml</li><li><span class="ds-label">ID: </span>nsx-verification</li></ul></div></div></div></div><p>
  After you have completed your <span class="productname">OpenStack</span> deployment and integrated the NSX-v
  neutron plugin, you can use these steps to verify that NSX-v is enabled and
  working in the environment.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Validating neutron from the Cloud Lifecycle Manager. All of these commands require that you
    authenticate by <code class="filename">service.osrc</code> file.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc</pre></div></li><li class="step "><p>
    List your neutron networks:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network list
+--------------------------------------+----------------+-------------------------------------------------------+
| id                                   | name           | subnets                                               |
+--------------------------------------+----------------+-------------------------------------------------------+
| 574d5f6c-871e-47f8-86d2-4b7c33d91002 | inter-edge-net | c5e35e22-0c1c-4886-b7f3-9ce3a6ab1512 169.254.128.0/17 |
+--------------------------------------+----------------+-------------------------------------------------------+</pre></div></li><li class="step "><p>
    List your neutron subnets:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack subnet list
+--------------------------------------+-------------------+------------------+------------------------------------------------------+
| id                                   | name              | cidr             | allocation_pools                                     |
+--------------------------------------+-------------------+------------------+------------------------------------------------------+
| c5e35e22-0c1c-4886-b7f3-9ce3a6ab1512 | inter-edge-subnet | 169.254.128.0/17 | {"start": "169.254.128.2", "end": "169.254.255.254"} |
+--------------------------------------+-------------------+------------------+------------------------------------------------------+</pre></div></li><li class="step "><p>
    List your neutron routers:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack router list
+--------------------------------------+-----------------------+-----------------------+-------------+
| id                                   | name                  | external_gateway_info | distributed |
+--------------------------------------+-----------------------+-----------------------+-------------+
| 1c5bf781-5120-4b7e-938b-856e23e9f156 | metadata_proxy_router | null                  | False       |
| 8b5d03bf-6f77-4ea9-bb27-87dd2097eb5c | metadata_proxy_router | null                  | False       |
+--------------------------------------+-----------------------+-----------------------+-------------+</pre></div></li><li class="step "><p>
    List your neutron ports:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port list
+--------------------------------------+------+-------------------+------------------------------------------------------+
| id                                   | name | mac_address       | fixed_ips                                            |
+--------------------------------------+------+-------------------+------------------------------------------------------+
| 7f5f0461-0db4-4b9a-a0c6-faa0010b9be2 |      | fa:16:3e:e5:50:d4 | {"subnet_id":                                        |
|                                      |      |                   | "c5e35e22-0c1c-4886-b7f3-9ce3a6ab1512",              |
|                                      |      |                   | "ip_address": "169.254.128.2"}                       |
| 89f27dff-f38d-4084-b9b0-ded495255dcb |      | fa:16:3e:96:a0:28 | {"subnet_id":                                        |
|                                      |      |                   | "c5e35e22-0c1c-4886-b7f3-9ce3a6ab1512",              |
|                                      |      |                   | "ip_address": "169.254.128.3"}                       |
+--------------------------------------+------+-------------------+------------------------------------------------------+</pre></div></li><li class="step "><p>
    List your neutron security group rules:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack security group rule list
+--------------------------------------+----------------+-----------+-----------+---------------+-----------------+
| id                                   | security_group | direction | ethertype | protocol/port | remote          |
+--------------------------------------+----------------+-----------+-----------+---------------+-----------------+
| 0385bd3a-1050-4bc2-a212-22ddab00c488 | default        | egress    | IPv6      | any           | any             |
| 19f6f841-1a9a-4b4b-bc45-7e8501953d8f | default        | ingress   | IPv6      | any           | default (group) |
| 1b3b5925-7aa6-4b74-9df0-f417ee6218f1 | default        | egress    | IPv4      | any           | any             |
| 256953cc-23d7-404d-b140-2600d55e44a2 | default        | ingress   | IPv4      | any           | default (group) |
| 314c4e25-5822-44b4-9d82-4658ae87d93f | default        | egress    | IPv6      | any           | any             |
| 59d4a71e-9f99-4b3b-b75b-7c9ad34081e0 | default        | ingress   | IPv6      | any           | default (group) |
| 887e25ef-64b7-4b69-b301-e053f88efa6c | default        | ingress   | IPv4      | any           | default (group) |
| 949e9744-75cd-4ae2-8cc6-6c0f578162d7 | default        | ingress   | IPv4      | any           | default (group) |
| 9a83027e-d6d6-4b6b-94fa-7c0ced2eba37 | default        | egress    | IPv4      | any           | any             |
| abf63b79-35ad-428a-8829-8e8d796a9917 | default        | egress    | IPv4      | any           | any             |
| be34b72b-66b6-4019-b782-7d91674ca01d | default        | ingress   | IPv6      | any           | default (group) |
| bf3d87ce-05c8-400d-88d9-a940e43760ca | default        | egress    | IPv6      | any           | any             |
+--------------------------------------+----------------+-----------+-----------+---------------+-----------------+</pre></div></li></ol></div></div><p>
  Verify metadata proxy functionality
 </p><p>
  To test that the metadata proxy virtual machines are working as intended,
  verify that there are at least two metadata proxy virtual machines from
  within vSphere (there will be four if edge high availability was set to
  true).
 </p><p>
  When that is verified, create a new compute instance either with the API,
  CLI, or within the cloud console GUI and log into the instance. From within
  the instance, using curl, grab the metadata instance-id from the
  metadata proxy address.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>curl http://169.254.169.254/latest/meta-data/instance-id
i-00000004</pre></div></div></div><div class="chapter " id="install-ironic-overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">29 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing Baremetal (Ironic)</span> <a title="Permalink" class="permalink" href="#install-ironic-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-install_ironic_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-install_ironic_overview.xml</li><li><span class="ds-label">ID: </span>install-ironic-overview</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#install-ironic"><span class="number">29.1 </span><span class="name">Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Ironic Flat Network</span></a></span></dt><dt><span class="section"><a href="#ironic-multi-control-plane"><span class="number">29.2 </span><span class="name">ironic in Multiple Control Plane</span></a></span></dt><dt><span class="section"><a href="#ironic-provisioning"><span class="number">29.3 </span><span class="name">Provisioning Bare-Metal Nodes with Flat Network Model</span></a></span></dt><dt><span class="section"><a href="#ironic-provisioning-multi-tenancy"><span class="number">29.4 </span><span class="name">Provisioning Baremetal Nodes with Multi-Tenancy</span></a></span></dt><dt><span class="section"><a href="#ironic-system-details"><span class="number">29.5 </span><span class="name">View Ironic System Details</span></a></span></dt><dt><span class="section"><a href="#ironic-toubleshooting"><span class="number">29.6 </span><span class="name">Troubleshooting ironic Installation</span></a></span></dt><dt><span class="section"><a href="#ironic-node-cleaning"><span class="number">29.7 </span><span class="name">Node Cleaning</span></a></span></dt><dt><span class="section"><a href="#ironic-oneview"><span class="number">29.8 </span><span class="name">Ironic and HPE OneView</span></a></span></dt><dt><span class="section"><a href="#ironic-raid-config"><span class="number">29.9 </span><span class="name">RAID Configuration for Ironic</span></a></span></dt><dt><span class="section"><a href="#ironic-audit-support"><span class="number">29.10 </span><span class="name">Audit Support for Ironic</span></a></span></dt></dl></div></div><p>
  Bare Metal as a Service is enabled in this release for deployment of nova
  instances on bare metal nodes using flat networking.
 </p><div class="sect1" id="install-ironic"><div class="titlepage"><div><div><h2 class="title"><span class="number">29.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Ironic Flat Network</span> <a title="Permalink" class="permalink" href="#install-ironic">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-install_entryscale_ironic.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-install_entryscale_ironic.xml</li><li><span class="ds-label">ID: </span>install-ironic</li></ul></div></div></div></div><p>
  This page describes the installation step requirements for the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  Entry-scale Cloud with ironic Flat Network.
 </p><div class="sect2" id="id-1.3.6.12.3.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure Your Environment</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.3.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-install_entryscale_ironic.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-install_entryscale_ironic.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Prior to deploying an operational environment with ironic, operators need to
   be aware of the nature of TLS certificate authentication. As pre-built
   deployment agent ramdisks images are supplied, these ramdisk images will
   only authenticate known third-party TLS Certificate Authorities in the
   interest of end-to-end security. As such, uses of self-signed certificates
   and private certificate authorities will be unable to leverage ironic
   without modifying the supplied ramdisk images.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Set up your configuration files, as follows:
    </p><ol type="a" class="substeps "><li class="step "><p>
       See the sample sets of configuration files in the
       <code class="literal">~/openstack/examples/</code> directory. Each set will have an
       accompanying README.md file that explains the contents of each of the
       configuration files.
      </p></li><li class="step "><p>
       Copy the example configuration files into the required setup directory
       and edit them to contain the details of your environment:
      </p><div class="verbatim-wrap"><pre class="screen">cp -r ~/openstack/examples/entry-scale-ironic-flat-network/* \
  ~/openstack/my_cloud/definition/</pre></div></li></ol></li><li class="step "><p><span class="step-optional">(Optional)</span> 
     You can use the <code class="literal">ardanaencrypt.py</code> script to
     encrypt your IPMI passwords. This script uses OpenSSL.
    </p><ol type="a" class="substeps "><li class="step "><p>
       Change to the Ansible directory:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible</pre></div></li><li class="step "><p>
       Put the encryption key into the following environment variable:
      </p><div class="verbatim-wrap"><pre class="screen">export ARDANA_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</pre></div></li><li class="step "><p>
       Run the python script below and follow the instructions. Enter a
       password that you want to encrypt.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>./ardanaencrypt.py</pre></div></li><li class="step "><p>
       Take the string generated and place it in the
       <code class="literal">ilo-password</code> field in your
       <code class="filename">~/openstack/my_cloud/definition/data/servers.yml</code>
       file, remembering to enclose it in quotes.
      </p></li><li class="step "><p>
       Repeat the above for each server.
      </p><div id="id-1.3.6.12.3.3.3.2.2.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
        Before you run any playbooks, remember that you need to export the
        encryption key in the following environment variable: <code class="literal">export
        ARDANA_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</code>
       </p></div></li></ol></li><li class="step "><p>
     Commit your configuration to the local git repo
     (<a class="xref" href="#using-git" title="Chapter 22. Using Git for Configuration Management">Chapter 22, <em>Using Git for Configuration Management</em></a>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div><div id="id-1.3.6.12.3.3.3.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      This step needs to be repeated any time you make changes to your
      configuration files before you move on to the following steps. See
      <a class="xref" href="#using-git" title="Chapter 22. Using Git for Configuration Management">Chapter 22, <em>Using Git for Configuration Management</em></a> for more information.
     </p></div></li></ol></div></div></div><div class="sect2" id="sec-ironic-provision"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Provisioning Your Baremetal Nodes</span> <a title="Permalink" class="permalink" href="#sec-ironic-provision">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-install_entryscale_ironic.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-install_entryscale_ironic.xml</li><li><span class="ds-label">ID: </span>sec-ironic-provision</li></ul></div></div></div></div><p>
   To provision the baremetal nodes in your cloud deployment you can either use
   the automated operating system installation process provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> or
   you can use the 3rd party installation tooling of your choice. We will
   outline both methods below:
  </p><div class="sect3" id="id-1.3.6.12.3.4.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">29.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using Third Party Baremetal Installers</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.3.4.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-install_entryscale_ironic.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-install_entryscale_ironic.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    If you do not wish to use the automated operating system installation
    tooling included with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> then the requirements that have to be met
    using the installation tooling of your choice are:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      The operating system must be installed via the SLES ISO provided on
      the <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a>.
     </p></li><li class="listitem "><p>
      Each node must have SSH keys in place that allows the same user from the
      Cloud Lifecycle Manager node who will be doing the deployment to SSH to each node without a
      password.
     </p></li><li class="listitem "><p>
      Passwordless sudo needs to be enabled for the user.
     </p></li><li class="listitem "><p>
      There should be a LVM logical volume as <code class="literal">/root</code> on each
      node.
     </p></li><li class="listitem "><p>
      If the LVM volume group name for the volume group holding the
      <code class="literal">root</code> LVM logical volume is
      <code class="literal">ardana-vg</code>, then it will align with the disk input
      models in the examples.
     </p></li><li class="listitem "><p>
      <span class="phrase">Ensure that <code class="literal">openssh-server</code>,
      <code class="literal">python</code>, <code class="literal">python-apt</code>, and
      <code class="literal">rsync</code> are installed.</span>
     </p></li></ul></div><p>
    If you chose this method for installing your baremetal hardware, skip
    forward to the step
    <em class="citetitle ">Running the Configuration Processor</em>.
   </p></div><div class="sect3" id="id-1.3.6.12.3.4.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">29.1.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Automated Operating System Installation Provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.3.4.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-install_entryscale_ironic.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-install_entryscale_ironic.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    If you would like to use the automated operating system installation tools
    provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, complete the steps below.
   </p><div class="sect4" id="id-1.3.6.12.3.4.4.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">29.1.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Cobbler</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.3.4.4.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-install_entryscale_ironic.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-install_entryscale_ironic.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
     This phase of the install process takes the baremetal information that was
     provided in <code class="literal">servers.yml</code> and installs the Cobbler
     provisioning tool and loads this information into Cobbler. This sets each
     node to <code class="literal">netboot-enabled: true</code> in Cobbler. Each node
     will be automatically marked as <code class="literal">netboot-enabled: false</code>
     when it completes its operating system install successfully. Even if the
     node tries to PXE boot subsequently, Cobbler will not serve it. This is
     deliberate so that you cannot reimage a live node by accident.
    </p><p>
     The <code class="literal">cobbler-deploy.yml</code> playbook prompts for a password
     - this is the password that will be encrypted and stored in Cobbler, which
     is associated with the user running the command on the Cloud Lifecycle Manager, that you
     will use to log in to the nodes via their consoles after install. The
     username is the same as the user set up in the initial dialogue when
     installing the Cloud Lifecycle Manager from the ISO, and is the same user that is running
     the cobbler-deploy play.
    </p><div id="id-1.3.6.12.3.4.4.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      When imaging servers with your own tooling, it is still necessary to have
      ILO/IPMI settings for all nodes. Even if you are not using Cobbler, the
      username and password fields in <code class="filename">servers.yml</code> need to
      be filled in with dummy settings. For example, add the following to
      <code class="filename">servers.yml</code>:
     </p><div class="verbatim-wrap"><pre class="screen">ilo-user: manual
ilo-password: deployment</pre></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Run the following playbook which confirms that there is IPMI connectivity
       for each of your nodes so that they are accessible to be re-imaged in a
       later step:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-power-status.yml</pre></div></li><li class="step "><p>
       Run the following playbook to deploy Cobbler:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li></ol></div></div></div><div class="sect4" id="id-1.3.6.12.3.4.4.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">29.1.2.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Imaging the Nodes</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.3.4.4.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-install_entryscale_ironic.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-install_entryscale_ironic.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
     This phase of the install process goes through a number of distinct steps:
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Powers down the nodes to be installed
      </p></li><li class="step "><p>
       Sets the nodes hardware boot order so that the first option is a network
       boot.
      </p></li><li class="step "><p>
       Powers on the nodes. (The nodes will then boot from the network and be
       installed using infrastructure set up in the previous phase)
      </p></li><li class="step "><p>
       Waits for the nodes to power themselves down (this indicates a
       successful install). This can take some time.
      </p></li><li class="step "><p>
       Sets the boot order to hard disk and powers on the nodes.
      </p></li><li class="step "><p>
       Waits for the nodes to be reachable by SSH and verifies that they have the
       signature expected.
      </p></li></ol></div></div><p>
     Deploying nodes has been automated in the Cloud Lifecycle Manager and requires the
     following:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       All of your nodes using SLES must already be installed, either
       manually or via Cobbler.
      </p></li><li class="listitem "><p>
       Your input model should be configured for your SLES nodes.
      </p></li><li class="listitem "><p>
       You should have run the configuration processor and the
       <code class="filename">ready-deployment.yml</code> playbook.
      </p></li></ul></div><p>
     Execute the following steps to re-image one or more nodes after you have
     run the <code class="filename">ready-deployment.yml</code> playbook.
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Run the following playbook, specifying your SLES nodes using the
       nodelist. This playbook will reconfigure Cobbler for the nodes listed.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook prepare-sles-grub2.yml -e \
      nodelist=node1[,node2,node3]</pre></div></li><li class="step "><p>
       Re-image the node(s) with the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml \
      -e nodelist=node1[,node2,node3]</pre></div></li></ol></div></div><p>
     If a nodelist is not specified then the set of nodes in Cobbler with
     <code class="literal">netboot-enabled: True</code> is selected. The playbook pauses
     at the start to give you a chance to review the set of nodes that it is
     targeting and to confirm that it is correct.
    </p><p>
     You can use the command below which will list all of your nodes with the
     <code class="literal">netboot-enabled: True</code> flag set:
    </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system find --netboot-enabled=1</pre></div></div></div></div><div class="sect2" id="sec-ironic-config-processor"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Running the Configuration Processor</span> <a title="Permalink" class="permalink" href="#sec-ironic-config-processor">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-install_entryscale_ironic.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-install_entryscale_ironic.xml</li><li><span class="ds-label">ID: </span>sec-ironic-config-processor</li></ul></div></div></div></div><p>
   Once you have your configuration files setup, you need to run the
   configuration processor to complete your configuration.
  </p><p>
   When you run the configuration processor, you will be prompted for two
   passwords. Enter the first password to make the configuration processor
   encrypt its sensitive data, which consists of the random inter-service
   passwords that it generates and the ansible <code class="literal">group_vars</code>
   and <code class="literal">host_vars</code> that it produces for subsequent deploy
   runs. You will need this password for subsequent Ansible deploy and
   configuration processor runs. If you wish to change an encryption password
   that you have already used when running the configuration processor then
   enter the new password at the second prompt, otherwise just press
   <span class="keycap">Enter</span> to bypass this.
  </p><p>
   Run the configuration processor with this command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
   For automated installation (for example CI), you can specify the required
   passwords on the ansible command line. For example, the command below will
   disable encryption by the configuration processor:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml \
  -e encrypt="" -e rekey=""</pre></div><p>
   If you receive an error during this step, there is probably an issue with
   one or more of your configuration files. Verify that all information in each
   of your configuration files is correct for your environment. Then commit
   those changes to Git using the instructions in the previous section before
   re-running the configuration processor again.
  </p><p>
   For any troubleshooting information regarding these steps, see
   <a class="xref" href="#sec-trouble-config-processor" title="36.2. Issues while Updating Configuration Files">Section 36.2, “Issues while Updating Configuration Files”</a>.
  </p></div><div class="sect2" id="sec-ironic-deploy"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying the Cloud</span> <a title="Permalink" class="permalink" href="#sec-ironic-deploy">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-install_entryscale_ironic.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-install_entryscale_ironic.xml</li><li><span class="ds-label">ID: </span>sec-ironic-deploy</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Use the playbook below to create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     [OPTIONAL] - Run the <code class="literal">wipe_disks.yml</code> playbook to ensure
     all of your non-OS partitions on your nodes are completely wiped before
     continuing with the installation. The <code class="filename">wipe_disks.yml</code>
     playbook is only meant to be run on systems immediately after running
     <code class="filename">bm-reimage.yml</code>. If used for any other case, it may
     not wipe all of the expected partitions.
    </p><p>
     If you are using fresh machines this step may not be necessary.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml</pre></div><p>
     If you have used an encryption password when running the configuration
     processor use the command below and enter the encryption password when
     prompted:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --ask-vault-pass</pre></div></li><li class="step "><p>
     Run the <code class="literal">site.yml</code> playbook below:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml</pre></div><p>
     If you have used an encryption password when running the configuration
     processor use the command below and enter the encryption password when
     prompted:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --ask-vault-pass</pre></div><div id="id-1.3.6.12.3.6.2.3.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The step above runs <code class="literal">osconfig</code> to configure the cloud
      and <code class="literal">ardana-deploy</code> to deploy the cloud. Therefore, this
      step may run for a while, perhaps 45 minutes or more, depending on the
      number of nodes in your environment.
     </p></div></li><li class="step "><p>
     Verify that the network is working correctly. Ping each IP in the
     <code class="literal">/etc/hosts</code> file from one of the controller nodes.
    </p></li></ol></div></div><p>
   For any troubleshooting information regarding these steps, see
   <a class="xref" href="#sec-trouble-deploy-cloud" title="36.3. Issues while Deploying the Cloud">Section 36.3, “Issues while Deploying the Cloud”</a>.
  </p></div><div class="sect2" id="id-1.3.6.12.3.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ironic configuration</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.3.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-install_entryscale_ironic.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-install_entryscale_ironic.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Run the <code class="literal">ironic-cloud-configure.yml</code> playbook below:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-cloud-configure.yml</pre></div><p>
   This step configures ironic flat network, uploads glance images and sets the
   ironic configuration.
  </p><p>
   To see the images uploaded to glance, run:
  </p><div class="verbatim-wrap"><pre class="screen">$ source ~/service.osrc
$ openstack image list</pre></div><p>
   This will produce output like the following example, showing three images
   that have been added by ironic:
  </p><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+--------------------------+
| ID                                   | Name                     |
+--------------------------------------+--------------------------+
| d4e2a0ff-9575-4bed-ac5e-5130a1553d93 | ir-deploy-iso-HOS3.0     |
| b759a1f0-3b33-4173-a6cb-be5706032124 | ir-deploy-kernel-HOS3.0  |
| ce5f4037-e368-46f2-941f-c01e9072676c | ir-deploy-ramdisk-HOS3.0 |
+--------------------------------------+--------------------------+</pre></div><p>
   To see the network created by ironic, run:
  </p><div class="verbatim-wrap"><pre class="screen">$ openstack network list</pre></div><p>
   This returns details of the "flat-net" generated by the ironic install:
  </p><div class="verbatim-wrap"><pre class="screen"> +---------------+----------+-------------------------------------------------------+
 | id            | name     | subnets                                               |
 +---------------+----------+-------------------------------------------------------+
 | f9474...11010 | flat-net | ca8f8df8-12c8-4e58-b1eb-76844c4de7e8 192.168.245.0/24 |
 +---------------+----------+-------------------------------------------------------+</pre></div></div><div class="sect2" id="ironic-node-config"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Node Configuration</span> <a title="Permalink" class="permalink" href="#ironic-node-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-node_config.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-node_config.xml</li><li><span class="ds-label">ID: </span>ironic-node-config</li></ul></div></div></div></div><div class="sect3" id="id-1.3.6.12.3.8.2"><div class="titlepage"><div><div><h4 class="title"><span class="number">29.1.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DHCP</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.3.8.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-node_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-node_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Once booted, nodes obtain network configuration via DHCP. If multiple
   interfaces are to be utilized, you may want to pre-build images with
   settings to execute DHCP on all interfaces. An easy way to build custom
   images is with KIWI, the command line utility to build Linux system
   appliances.
  </p><p>
   For information about building custom KIWI images, see
   <a class="xref" href="#sec-ironic-provision-kiwi" title="29.3.13. Building glance Images Using KIWI">Section 29.3.13, “Building glance Images Using KIWI”</a>.
   For more information, see the KIWI documentation at
   <a class="link" href="https://osinside.github.io/kiwi/" target="_blank">https://osinside.github.io/kiwi/</a>.
  </p></div><div class="sect3" id="id-1.3.6.12.3.8.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">29.1.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuration Drives</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.3.8.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-node_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-node_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div id="id-1.3.6.12.3.8.3.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
    Configuration Drives are stored unencrypted and should not include any
    sensitive data.
   </p></div><p>
   You can use Configuration Drives to store metadata for initial boot
   setting customization. Configuration Drives are extremely useful for
   initial machine configuration. However, as a general security practice,
   they should not include any
   sensitive data. Configuration Drives should only be trusted upon the initial
   boot of an instance. <code class="literal">cloud-init</code> utilizes a lock file for
   this purpose. Custom instance images should not rely upon the integrity of a
   Configuration Drive beyond the initial boot of a host as an administrative
   user within a deployed instance can potentially modify a configuration drive
   once written to disk and released for use.
  </p><p>
   For more information about Configuration Drives, see
   <a class="link" href="http://docs.openstack.org/user-guide/cli_config_drive.html" target="_blank">http://docs.openstack.org/user-guide/cli_config_drive.html</a>.
  </p></div></div><div class="sect2" id="ironic-tls"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.1.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">TLS Certificates with Ironic Python Agent (IPA) Images</span> <a title="Permalink" class="permalink" href="#ironic-tls">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_tls.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_tls.xml</li><li><span class="ds-label">ID: </span>ironic-tls</li></ul></div></div></div></div><p>
  As part of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9, ironic Python Agent, better known as IPA in the
  OpenStack community, images are supplied and loaded into glance. Two types of
  image exist. One is a traditional boot ramdisk which is used by the
  <code class="literal">agent_ipmitool</code>, <code class="literal">pxe_ipmitool</code>, and
  <code class="literal">pxe_ilo</code> drivers. The other is an ISO image that is
  supplied as virtual media to the host when using the
  <code class="literal">agent_ilo</code> driver.
 </p><p>
  As these images are built in advance, they are unaware of any private
  certificate authorities. Users attempting to utilize self-signed certificates
  or a private certificate authority will need to inject their signing
  certificate(s) into the image in order for IPA to be able to boot on a remote
  node, and ensure that the TLS endpoints being connected to in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> can be
  trusted. This is not an issue with publicly signed certificates.
 </p><p>
  As two different types of images exist, below are instructions for
  disassembling the image ramdisk file or the ISO image. Once this has been
  done, you will need to re-upload the files to glance, and update any impacted
  node's <code class="literal">driver_info</code>, for example, the
  <code class="literal">deploy_ramdisk</code> and <code class="literal">ilo_deploy_iso</code>
  settings that were set when the node was first defined. Respectively, this
  can be done with the
 </p><div class="verbatim-wrap"><pre class="screen">ironic node-update &lt;node&gt; replace driver_info/deploy_ramdisk=&lt;glance_id&gt;</pre></div><p>
  or
 </p><div class="verbatim-wrap"><pre class="screen">ironic node-update &lt;node&gt; replace driver_info/ilo_deploy_iso=&lt;glance_id&gt;</pre></div><div class="sect3" id="cert-ramdisk"><div class="titlepage"><div><div><h4 class="title"><span class="number">29.1.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Add New Trusted CA Certificate Into Deploy Images</span> <a title="Permalink" class="permalink" href="#cert-ramdisk">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-cert_ramdisk.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-cert_ramdisk.xml</li><li><span class="ds-label">ID: </span>cert-ramdisk</li></ul></div></div></div></div><p>
  Perform the following steps.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    To upload your trusted CA certificate to the Cloud Lifecycle Manager, follow the directions
    in <a class="xref" href="#sec-upload-toclm" title="41.7. Upload to the Cloud Lifecycle Manager">Section 41.7, “Upload to the Cloud Lifecycle Manager”</a>.
   </p></li><li class="step "><p>
    Delete the deploy images.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack image delete ir-deploy-iso-ARDANA5.0
<code class="prompt user">ardana &gt; </code>openstack image delete ir-deploy-ramdisk-ARDANA5.0</pre></div></li><li class="step "><p>
    On the deployer, run <code class="filename">ironic-reconfigure.yml</code> playbook
    to re-upload the images that include the new trusted CA bundle.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd /var/lib/ardana/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml</pre></div></li><li class="step "><p>
    Update the existing ironic nodes with the new image IDs accordingly. For
    example,
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack baremetal node set --driver-info \
deploy_ramdisk=<em class="replaceable ">NEW_RAMDISK_ID</em> <em class="replaceable ">NODE_ID</em></pre></div></li></ol></div></div></div></div></div><div class="sect1" id="ironic-multi-control-plane"><div class="titlepage"><div><div><h2 class="title"><span class="number">29.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">ironic in Multiple Control Plane</span> <a title="Permalink" class="permalink" href="#ironic-multi-control-plane">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_multi_control_plane.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_multi_control_plane.xml</li><li><span class="ds-label">ID: </span>ironic-multi-control-plane</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 introduces the concept of multiple control planes - see the
  Input Model documentation for the relevant <a class="xref" href="#concept-controlplanes-regions" title="5.2.2.1. Control Planes and Regions">Section 5.2.2.1, “Control Planes and Regions”</a> and <a class="xref" href="#configobj-multiple-control-planes" title="6.2.3. Multiple Control Planes">Section 6.2.3, “Multiple Control Planes”</a>. This document covers the use
  of an ironic region in a multiple control plane cloud model in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p><div class="sect2" id="id-1.3.6.12.4.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networking for Baremetal in Multiple Control Plane</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.4.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_multi_control_plane.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_multi_control_plane.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="bold"><strong>IRONIC-FLAT-NET</strong></span> is the network
   configuration for baremetal control plane.
  </p><p>
   You need to set the environment variable <span class="bold"><strong>OS_REGION_NAME</strong></span> to the ironic region in baremetal
   control plane. This will set up the ironic flat networking in
   neutron.
  </p><div class="verbatim-wrap"><pre class="screen">export OS_REGION_NAME=&lt;ironic_region&gt;</pre></div><p>
   To see details of the <code class="literal">IRONIC-FLAT-NETWORK</code> created during
   configuration, use the following command:
  </p><div class="verbatim-wrap"><pre class="screen">openstack network list</pre></div><p>
   Referring to the diagram below, the Cloud Lifecycle Manager is a shared service that runs in a
   Core API Controller in a Core API Cluster. ironic Python Agent (IPA) must
   be able to make REST API calls to the ironic API (the connection is
   represented by the green line to Internal routing). The IPA connect to
   swift to get user images (the gray line connecting to swift routing).
  </p><div class="figure" id="id-1.3.6.12.4.3.8"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/media-ironic-ironic_multi_control_plane.png" target="_blank"><img src="images/media-ironic-ironic_multi_control_plane.png" width="" alt="Architecture of Multiple Control Plane with ironic" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 29.1: </span><span class="name">Architecture of Multiple Control Plane with ironic </span><a title="Permalink" class="permalink" href="#id-1.3.6.12.4.3.8">#</a></h6></div></div></div><div class="sect2" id="id-1.3.6.12.4.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Handling Optional swift Service</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.4.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_multi_control_plane.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_multi_control_plane.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   swift is resource-intensive and as a result, it is now optional in the
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> control plane. A number of services depend on swift, and if it is
   not present, they must provide a fallback strategy. For example, glance can
   use the filesystem in place of swift for its backend store.
  </p><p>
   In ironic, agent-based drivers require swift. If it is not present, it is
   necessary to disable access to this ironic feature in the control plane. The
   <code class="literal">enable_agent_driver</code> flag has been added to the ironic
   configuration data and can have values of <code class="literal">true</code> or
   <code class="literal">false</code>. Setting this flag to <code class="literal">false</code> will
   disable swift configurations and the agent based drivers in the ironic
   control plane.
  </p></div><div class="sect2" id="id-1.3.6.12.4.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Instance Provisioning</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.4.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_multi_control_plane.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_multi_control_plane.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   In a multiple control plane cloud setup, changes for glance container name
   in the swift namespace of <code class="literal">ironic-conductor.conf</code>
   introduces a conflict with the one in <code class="literal">glance-api.conf</code>.
   Provisioning with agent-based drivers requires the container name to be the
   same in ironic and glance. Hence, on instance provisioning with agent-based
   drivers (swift-enabled), the agent is not able to fetch the images from
   glance store and fails at that point.
  </p><p>
   You can resolve this issue using the following steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Copy the value of <code class="literal">swift_store_container</code> from the file
     <code class="filename">/opt/stack/service/glance-api/etc/glance-api.conf</code>
    </p></li><li class="step "><p>
     Log in to the Cloud Lifecycle Manager and use the value for
     <code class="literal">swift_container</code> in glance namespace of
     <code class="filename">~/scratch/ansible/next/ardana/ansible/roles/ironic-common/templates/ironic-conductor.conf.j2</code>
    </p></li><li class="step "><p>
     Run the following playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml</pre></div></li></ol></div></div></div></div><div class="sect1" id="ironic-provisioning"><div class="titlepage"><div><div><h2 class="title"><span class="number">29.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Provisioning Bare-Metal Nodes with Flat Network Model</span> <a title="Permalink" class="permalink" href="#ironic-provisioning">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_provisioning.xml</li><li><span class="ds-label">ID: </span>ironic-provisioning</li></ul></div></div></div></div><div id="id-1.3.6.12.5.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
   Providing bare-metal resources to an untrusted third party is not advised
   as a malicious user can potentially modify hardware firmware.
  </p></div><div id="id-1.3.6.12.5.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   The steps outlined in <a class="xref" href="#ironic-tls" title="29.1.7. TLS Certificates with Ironic Python Agent (IPA) Images">Section 29.1.7, “TLS Certificates with Ironic Python Agent (IPA) Images”</a>
   <span class="emphasis"><em>must</em></span> be performed.
  </p></div><p>
  A number of drivers are available to provision and manage bare-metal
  machines. The drivers are named based on the deployment mode and the power
  management interface. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> has been tested with the following drivers:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    agent_ilo
   </p></li><li class="listitem "><p>
    agent_ipmi
   </p></li><li class="listitem "><p>
    pxe_ilo
   </p></li><li class="listitem "><p>
    pxe_ipmi
   </p></li><li class="listitem "><p>
    Redfish
   </p></li></ul></div><p>
  Before you start, you should be aware that:
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Node Cleaning is enabled for all the drivers in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9.
   </p></li><li class="listitem "><p>
    Node parameter settings must have matching flavors in terms of
    <code class="literal">cpus</code>, <code class="literal">local_gb</code>, and
    <code class="literal">memory_mb</code>, <code class="literal">boot_mode</code> and
    <code class="literal">cpu_arch</code>.
   </p></li><li class="listitem "><p>
    It is advisable that nodes enrolled for ipmitool drivers are pre-validated
    in terms of BIOS settings, in terms of boot mode, prior to setting
    capabilities.
   </p></li><li class="listitem "><p>
    Network cabling and interface layout should also be pre-validated in any
    given particular boot mode or configuration that is registered.
   </p></li><li class="listitem "><p>
    The use of <code class="literal">agent_</code> drivers is predicated upon glance
    images being backed by a swift image store, specifically the need for the
    temporary file access features. Using the file system as a glance back-end
    image store means that the <code class="literal">agent_</code> drivers cannot be
    used.
   </p></li><li class="listitem "><p>
    Manual Cleaning (RAID) and Node inspection is supported by ilo drivers
    (<code class="literal">agent_ilo</code> and <code class="literal">pxe_ilo)</code>
   </p></li></ol></div><div class="sect2" id="id-1.3.6.12.5.8"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Redfish Protocol Support</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.5.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_provisioning.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Redfish is a successor to the Intelligent Platform Management Interface
   (IPMI) with the ability to scale to larger and more diverse cloud
   deployments. It has an API that allows users to collect performance data
   from heterogeneous server installations and more data sources than could be
   handled previously. It is based on an industry standard protocol with a
   RESTful interface for managing cloud assets that are compliant with the
   <a class="link" href="https://redfish.dmtf.org/" target="_blank">Redfish protocol</a>.
  </p><div id="id-1.3.6.12.5.8.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    There are two known limitations to using Redfish.
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      RAID configuration does not work due to missing HPE Smart Storage
      Administrator CLI (HPE SSACLI) in the default deploy RAM disk. This is a
      licensing issue.
     </p></li><li class="listitem "><p>
      The ironic <code class="literal">inspector</code> inspect interface is not supported.
     </p></li></ul></div></div><p>
   Enable the Redfish driver with the following steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Install the Sushy library on the ironic-conductor nodes. Sushy is a Python
     library for communicating with Redfish-based systems with ironic. More
     information is available at <a class="link" href="https://opendev.org/openstack/sushy/" target="_blank">https://opendev.org/openstack/sushy/</a>.
    </p><div class="verbatim-wrap"><pre class="screen">sudo pip install sushy</pre></div></li><li class="step "><p>
     Add <code class="literal">redfish</code> to the list of
     <code class="literal">enabled_hardware_types</code>,
     <code class="literal">enabled_power_interfaces</code> and
     <code class="literal">enabled_management_interfaces</code> in
     <code class="filename">/etc/ironic/ironic.conf</code> as shown below:
    </p><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
...
enabled_hardware_types = ipmi,redfish
enabled_power_interfaces = ipmitool,redfish
enabled_management_interfaces = ipmitool,redfish</pre></div></li><li class="step "><p>
     Restart the ironic-conductor service:
    </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl restart openstack-ironic-conductor</pre></div></li></ol></div></div><p>
   To continue with Redfish, see <a class="xref" href="#register-redfish-node" title="29.3.4. Registering a Node with the Redfish Driver">Section 29.3.4, “Registering a Node with the Redfish Driver”</a>.
  </p></div><div class="sect2" id="sec-ironic-provision-image"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Supplied Images</span> <a title="Permalink" class="permalink" href="#sec-ironic-provision-image">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_provisioning.xml</li><li><span class="ds-label">ID: </span>sec-ironic-provision-image</li></ul></div></div></div></div><p>
   As part of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale ironic Cloud installation,
   ironic Python Agent (IPA) images are supplied and loaded into glance.
   To see the images that have been loaded, execute the following commands on
   the deployer node:
  </p><div class="verbatim-wrap"><pre class="screen">$ source ~/service.osrc
openstack image list</pre></div><p>
   This will display three images that have been added by ironic:
  </p><div class="verbatim-wrap"><pre class="screen">Deploy_iso : openstack-ironic-image.x86_64-8.0.0.kernel.4.4.120-94.17-default
Deploy_kernel : openstack-ironic-image.x86_64-8.0.0.xz
Deploy_ramdisk : openstack-ironic-image.x86_64-8.0.0.iso</pre></div><p>
   The <code class="literal">ir-deploy-ramdisk</code> image is a traditional boot ramdisk
   used by the <code class="literal">agent_ipmitool</code>,
   <code class="literal">pxe_ipmitool</code>, and <code class="literal">pxe_ilo</code> drivers
   while <code class="literal">ir-deploy-iso</code> is an ISO image that is supplied as
   virtual media to the host when using the <code class="literal">agent_ilo</code>
   driver.
  </p></div><div class="sect2" id="sec-ironic-provision-provision"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Provisioning a Node</span> <a title="Permalink" class="permalink" href="#sec-ironic-provision-provision">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_provisioning.xml</li><li><span class="ds-label">ID: </span>sec-ironic-provision-provision</li></ul></div></div></div></div><p>
   The information required to provision a node varies slightly depending on
   the driver used. In general the following details are required.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Network access information and credentials to connect to the management
     interface of the node.
    </p></li><li class="listitem "><p>
     Sufficient properties to allow for nova flavor matching.
    </p></li><li class="listitem "><p>
     A deployment image to perform the actual deployment of the guest operating
     system to the bare-metal node.
    </p></li></ul></div><p>
   A combination of the <code class="literal">ironic node-create</code> and
   <code class="literal">ironic node-update</code> commands are used for registering a
   node's characteristics with the ironic service. In particular,
   <code class="literal">ironic node-update &lt;nodeid&gt;
   <em class="replaceable ">add</em></code> and <code class="literal">ironic node-update
   &lt;nodeid&gt; <em class="replaceable ">replace</em></code> can be used to
   modify the properties of a node after it has been created while
   <code class="literal">ironic node-update &lt;nodeid&gt;
   <em class="replaceable ">remove</em></code> will remove a property.
  </p></div><div class="sect2" id="register-redfish-node"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Registering a Node with the Redfish Driver</span> <a title="Permalink" class="permalink" href="#register-redfish-node">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_provisioning.xml</li><li><span class="ds-label">ID: </span>register-redfish-node</li></ul></div></div></div></div><p>
   Nodes configured to use the Redfish driver should have the driver property
   set to <code class="literal">redfish</code>.
  </p><p>
   The following properties are specified in the <code class="literal">driver_info</code>
   field of the node:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.6.12.5.11.4.1"><span class="term "><code class="literal">redfish_address</code> (required)</span></dt><dd><p>
      The URL address to the Redfish controller. It must include the authority
      portion of the URL, and can optionally include the scheme. If the scheme
      is missing, HTTPS is assumed.
     </p></dd><dt id="id-1.3.6.12.5.11.4.2"><span class="term "><code class="literal">redfish_system_id</code> (required)</span></dt><dd><p>
      The canonical path to the system resource that the driver interacts
      with. It should include the root service, version and the unique path to
      the system resource. For example,<code class="literal">
      /redfish/v1/Systems/1</code>.
     </p></dd><dt id="id-1.3.6.12.5.11.4.3"><span class="term "><code class="literal">redfish_user</code> name (recommended)</span></dt><dd><p>
      User account with admin and server-profile access privilege.
     </p></dd><dt id="id-1.3.6.12.5.11.4.4"><span class="term "><code class="literal">redfish_password</code> (recommended)</span></dt><dd><p>
      User account password.
     </p></dd><dt id="id-1.3.6.12.5.11.4.5"><span class="term "><code class="literal">redfish_verify_ca</code> (optional)</span></dt><dd><p>
      If <code class="literal">redfish_address</code> has the HTTPS scheme, the driver
      will use a secure (TLS) connection when talking to the Redfish
      controller. By default (if this is not set or set to
      <code class="literal">True</code>), the driver will try to verify the host
      certificates. This can be set to the path of a certificate file or
      directory with trusted certificates that the driver will use for
      verification. To disable verifying TLS, set this to
      <code class="literal">False</code>.
     </p></dd></dl></div><p>
   The <code class="command">openstack baremetal node create</code> command is used
   to enroll a node with the Redfish driver. For example:
  </p><div class="verbatim-wrap"><pre class="screen">openstack baremetal node create --driver redfish --driver-info \
redfish_address=https://example.com --driver-info \
redfish_system_id=/redfish/v1/Systems/CX34R87 --driver-info \
redfish_username=admin --driver-info redfish_password=password</pre></div></div><div class="sect2" id="sec-ironic-provision-create-ilo"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a Node Using <code class="command">agent_ilo</code></span> <a title="Permalink" class="permalink" href="#sec-ironic-provision-create-ilo">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_provisioning.xml</li><li><span class="ds-label">ID: </span>sec-ironic-provision-create-ilo</li></ul></div></div></div></div><p>
   If you want to use a boot mode of BIOS as opposed to UEFI, then you need to
   ensure that the boot mode has been set correctly on the IPMI:
  </p><p>
   While the iLO driver can automatically set a node to boot in UEFI mode via
   the <code class="literal">boot_mode</code> defined capability, it cannot set BIOS boot
   mode once UEFI mode has been set.
  </p><p>
   Use the <code class="literal">ironic node-create</code> command to specify the
   <code class="literal">agent_ilo</code> driver, network access and credential
   information for the IPMI, properties of the node and the glance ID of the
   supplied ISO IPA image. Note that memory size is specified in megabytes while
   disk size is specified in gigabytes.
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-create -d agent_ilo -i ilo_address=<em class="replaceable ">IP_ADDRESS</em> -i \
  ilo_username=Administrator -i ilo_password=<em class="replaceable ">PASSWORD</em> \
  -p cpus=2 -p cpu_arch=x86_64 -p memory_mb=64000 -p local_gb=99 \
  -i ilo_deploy_iso=<em class="replaceable ">DEPLOY_UUID</em></pre></div><p>
   This will generate output similar to the following:
  </p><div class="verbatim-wrap"><pre class="screen">+--------------+---------------------------------------------------------------+
| Property     | Value                                                         |
+--------------+---------------------------------------------------------------+
| uuid         | <em class="replaceable ">NODE_UUID</em>                                                     |
| driver_info  | {u'ilo_address': u'<em class="replaceable ">IP_ADDRESS</em>', u'ilo_password': u'******',   |
|              | u'ilo_deploy_iso': u'<em class="replaceable ">DEPLOY_UUID</em>',                            |
|              | u'ilo_username': u'Administrator'}                            |
| extra        | {}                                                            |
| driver       | agent_ilo                                                     |
| chassis_uuid |                                                               |
| properties   | {u'memory_mb': 64000, u'local_gb': 99, u'cpus': 2,            |
|              | u'cpu_arch': u'x86_64'}                                       |
| name         | None                                                          |
+--------------+---------------------------------------------------------------+</pre></div><p>
   Now update the node with <code class="literal">boot_mode</code> and
   <code class="literal">boot_option</code> properties:
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-update <em class="replaceable ">NODE_UUID</em> add \
  properties/capabilities="boot_mode:bios,boot_option:local"</pre></div><p>
   The <code class="literal">ironic node-update</code> command returns details for all of
   the node's characteristics.
  </p><div class="verbatim-wrap"><pre class="screen">+------------------------+------------------------------------------------------------------+
| Property               | Value                                                            |
+------------------------+------------------------------------------------------------------+
| target_power_state     | None                                                             |
| extra                  | {}                                                               |
| last_error             | None                                                             |
| updated_at             | None                                                             |
| maintenance_reason     | None                                                             |
| provision_state        | available                                                        |
| clean_step             | {}                                                               |
| uuid                   | <em class="replaceable ">NODE_UUID</em>                                                        |
| console_enabled        | False                                                            |
| target_provision_state | None                                                             |
| provision_updated_at   | None                                                             |
| maintenance            | False                                                            |
| inspection_started_at  | None                                                             |
| inspection_finished_at | None                                                             |
| power_state            | None                                                             |
| driver                 | agent_ilo                                                        |
| reservation            | None                                                             |
| properties             | {u'memory_mb': 64000, u'cpu_arch': u'x86_64', u'local_gb': 99,   |
|                        | u'cpus': 2, u'capabilities': u'boot_mode:bios,boot_option:local'}|
| instance_uuid          | None                                                             |
| name                   | None                                                             |
| driver_info            | {u'ilo_address': u'10.1.196.117', u'ilo_password': u'******',    |
|                        | u'ilo_deploy_iso': u'<em class="replaceable ">DEPLOY_UUID</em>',                               |
|                        | u'ilo_username': u'Administrator'}                               |
| created_at             | 2016-03-11T10:17:10+00:00                                        |
| driver_internal_info   | {}                                                               |
| chassis_uuid           |                                                                  |
| instance_info          | {}                                                               |
+------------------------+------------------------------------------------------------------+</pre></div></div><div class="sect2" id="sec-ironic-provision-create-ipmi"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.3.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a Node Using <code class="command">agent_ipmi</code></span> <a title="Permalink" class="permalink" href="#sec-ironic-provision-create-ipmi">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_provisioning.xml</li><li><span class="ds-label">ID: </span>sec-ironic-provision-create-ipmi</li></ul></div></div></div></div><p>
   Use the <code class="literal">ironic node-create</code> command to specify the
   <code class="literal">agent_ipmi</code> driver, network access and credential
   information for the IPMI, properties of the node and the glance IDs of the
   supplied kernel and ramdisk images. Note that memory size is specified in
   megabytes while disk size is specified in gigabytes.
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-create -d <span class="bold"><strong>agent_ipmitool</strong></span> \
  -i ipmi_address=<em class="replaceable ">IP_ADDRESS</em> \
  -i ipmi_username=Administrator -i ipmi_password=<em class="replaceable ">PASSWORD</em> \
  -p cpus=2 -p memory_mb=64000 -p local_gb=99 -p cpu_arch=x86_64 \
  -i deploy_kernel=<em class="replaceable ">KERNEL_UUID</em> \
  -i deploy_ramdisk=<em class="replaceable ">RAMDISK_UUID</em></pre></div><p>
   This will generate output similar to the following:
  </p><div class="verbatim-wrap"><pre class="screen">+--------------+-----------------------------------------------------------------------+
| Property     | Value                                                                 |
+--------------+-----------------------------------------------------------------------+
| uuid         | <em class="replaceable ">NODE2_UUID</em>                                                            |
| driver_info  | {u'deploy_kernel': u'<em class="replaceable ">KERNEL_UUID</em>',                                    |
|              | u'ipmi_address': u'<em class="replaceable ">IP_ADDRESS</em>', u'ipmi_username': u'Administrator',   |
|              | u'ipmi_password': u'******',                                          |
|              | u'deploy_ramdisk': u'<em class="replaceable ">RAMDISK_UUID</em>'}                                   |
| extra        | {}                                                                    |
| driver       | agent_ipmitool                                                        |
| chassis_uuid |                                                                       |
| properties   | {u'memory_mb': 64000, u'cpu_arch': u'x86_64', u'local_gb': 99,        |
|              | u'cpus': 2}                                                           |
| name         | None                                                                  |
+--------------+-----------------------------------------------------------------------+</pre></div><p>
   Now update the node with <code class="literal">boot_mode</code> and
   <code class="literal">boot_option</code> properties:
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-update <em class="replaceable ">NODE_UUID</em> add \
  properties/capabilities="boot_mode:bios,boot_option:local"</pre></div><p>
   The <code class="literal">ironic node-update</code> command returns details for all of
   the node's characteristics.
  </p><div class="verbatim-wrap"><pre class="screen">+------------------------+-----------------------------------------------------------------+
| Property               | Value                                                           |
+------------------------+-----------------------------------------------------------------+
| target_power_state     | None                                                            |
| extra                  | {}                                                              |
| last_error             | None                                                            |
| updated_at             | None                                                            |
| maintenance_reason     | None                                                            |
| provision_state        | available                                                       |
| clean_step             | {}                                                              |
| uuid                   | <em class="replaceable ">NODE2_UUID</em>                                                      |
| console_enabled        | False                                                           |
| target_provision_state | None                                                            |
| provision_updated_at   | None                                                            |
| maintenance            | False                                                           |
| inspection_started_at  | None                                                            |
| inspection_finished_at | None                                                            |
| power_state            | None                                                            |
| driver                 | agent_ipmitool                                                  |
| reservation            | None                                                            |
| properties             | {u'memory_mb': 64000, u'cpu_arch': u'x86_64',                   |
|                        | u'local_gb': 99, u'cpus': 2,                                    |
|                        | u'capabilities': u'boot_mode:bios,boot_option:local'}           |
| instance_uuid          | None                                                            |
| name                   | None                                                            |
| driver_info            | {u'ipmi_password': u'******', u'ipmi_address': u'<em class="replaceable ">IP_ADDRESS</em>',   |
|                        | u'ipmi_username': u'Administrator', u'deploy_kernel':           |
|                        | u'<em class="replaceable ">KERNEL_UUID</em>',                                                 |
|                        | u'deploy_ramdisk': u'<em class="replaceable ">RAMDISK_UUID</em>'}                             |
| created_at             | 2016-03-11T14:19:18+00:00                                       |
| driver_internal_info   | {}                                                              |
| chassis_uuid           |                                                                 |
| instance_info          | {}                                                              |
+------------------------+-----------------------------------------------------------------+</pre></div><p>
   For more information on node enrollment, see the <span class="productname">OpenStack</span> documentation at
   <a class="link" href="http://docs.openstack.org/developer/ironic/deploy/install-guide.html#enrollment" target="_blank">http://docs.openstack.org/developer/ironic/deploy/install-guide.html#enrollment</a>.
  </p></div><div class="sect2" id="sec-ironic-provision-flavor"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.3.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a Flavor</span> <a title="Permalink" class="permalink" href="#sec-ironic-provision-flavor">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_provisioning.xml</li><li><span class="ds-label">ID: </span>sec-ironic-provision-flavor</li></ul></div></div></div></div><p>
   nova uses flavors when fulfilling requests for bare-metal nodes. The nova
   scheduler attempts to match the requested flavor against the properties of
   the created ironic nodes. So an administrator needs to set up flavors that
   correspond to the available bare-metal nodes using the command
   <code class="command">openstack flavor create</code>:
  </p><div class="verbatim-wrap"><pre class="screen">openstack flavor create bmtest auto 64000  99 2

+----------------+--------+--------+------+-----------+------+-------+-------------+-----------+
| ID             | Name   | Mem_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public |
+----------------+--------+--------+------+-----------+------+-------+-------------+-----------+
| 645de0...b1348 | bmtest | 64000  | 99   | 0         |      | 2     | 1.0         | True      |
+----------------+--------+--------+------+-----------+------+-------+-------------+-----------+</pre></div><p>
   To see a list of all the available flavors, run <code class="command">openstack flavor
   list</code>:
  </p><div class="verbatim-wrap"><pre class="screen">openstack flavor list

+-------------+--------------+--------+------+-----------+------+-------+--------+-----------+
| ID          | Name         | Mem_MB | Disk | Ephemeral | Swap | VCPUs |  RXTX  | Is_Public |
|             |              |        |      |           |      |       | Factor |           |
+-------------+--------------+--------+------+-----------+------+-------+--------+-----------+
| 1           | m1.tiny      | 512    | 1    | 0         |      | 1     | 1.0    | True      |
| 2           | m1.small     | 2048   | 20   | 0         |      | 1     | 1.0    | True      |
| 3           | m1.medium    | 4096   | 40   | 0         |      | 2     | 1.0    | True      |
| 4           | m1.large     | 8192   | 80   | 0         |      | 4     | 1.0    | True      |
| 5           | m1.xlarge    | 16384  | 160  | 0         |      | 8     | 1.0    | True      |
| 6           | m1.baremetal | 4096   | 80   | 0         |      | 2     | 1.0    | True      |
| 645d...1348 | bmtest       | 64000  | 99   | 0         |      | 2     | 1.0    | True      |
+-------------+--------------+--------+------+-----------+------+-------+--------+-----------+</pre></div><p>
   Now set the CPU architecture and boot mode and boot option capabilities:
  </p><div class="verbatim-wrap"><pre class="screen">openstack flavor set 645de08d-2bc6-43f1-8a5f-2315a75b1348 set cpu_arch=x86_64
openstack flavor set 645de08d-2bc6-43f1-8a5f-2315a75b1348 set capabilities:boot_option="local"
openstack flavor set 645de08d-2bc6-43f1-8a5f-2315a75b1348 set capabilities:boot_mode="bios"</pre></div><p>
   For more information on flavor creation, see the <span class="productname">OpenStack</span> documentation at
   <a class="link" href="http://docs.openstack.org/developer/ironic/deploy/install-guide.html#flavor-creation" target="_blank">http://docs.openstack.org/developer/ironic/deploy/install-guide.html#flavor-creation</a>.
  </p></div><div class="sect2" id="sec-ironic-provision-net"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.3.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a Network Port</span> <a title="Permalink" class="permalink" href="#sec-ironic-provision-net">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_provisioning.xml</li><li><span class="ds-label">ID: </span>sec-ironic-provision-net</li></ul></div></div></div></div><p>
   Register the MAC addresses of all connected physical network interfaces
   intended for use with the bare-metal node.
  </p><div class="verbatim-wrap"><pre class="screen">ironic port-create -a 5c:b9:01:88:f0:a4 -n ea7246fd-e1d6-4637-9699-0b7c59c22e67</pre></div></div><div class="sect2" id="sec-ironic-provision-glance-image"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.3.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a glance Image</span> <a title="Permalink" class="permalink" href="#sec-ironic-provision-glance-image">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_provisioning.xml</li><li><span class="ds-label">ID: </span>sec-ironic-provision-glance-image</li></ul></div></div></div></div><p>
   You can create a complete disk image using the instructions at
   <a class="xref" href="#sec-ironic-provision-kiwi" title="29.3.13. Building glance Images Using KIWI">Section 29.3.13, “Building glance Images Using KIWI”</a>.
  </p><p>
   The image you create can then be loaded into glance:
  </p><div class="verbatim-wrap"><pre class="screen">openstack image create --name='leap' --disk-format=raw \
  --container-format=bare \
  --file /tmp/myimage/LimeJeOS-Leap-42.3.x86_64-1.42.3.raw

+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | 45a4a06997e64f7120795c68beeb0e3c     |
| container_format | bare                                 |
| created_at       | 2018-02-17T10:42:14Z                 |
| disk_format      | raw                                  |
| id               | <span class="bold"><strong>17e4915a-ada0-4b95-bacf-ba67133f39a7</strong></span> |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | leap                                 |
| owner            | 821b7bb8148f439191d108764301af64     |
| protected        | False                                |
| size             | 372047872                            |
| status           | active                               |
| tags             | []                                   |
| updated_at       | 2018-02-17T10:42:23Z                 |
| virtual_size     | None                                 |
| visibility       | private                              |
+------------------+--------------------------------------+</pre></div><p>
   This image will subsequently be used to boot the bare-metal node.
  </p></div><div class="sect2" id="sec-ironic-provision-key"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.3.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Generating a Key Pair</span> <a title="Permalink" class="permalink" href="#sec-ironic-provision-key">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_provisioning.xml</li><li><span class="ds-label">ID: </span>sec-ironic-provision-key</li></ul></div></div></div></div><p>
   Create a key pair that you will use when you login to the newly booted node:
  </p><div class="verbatim-wrap"><pre class="screen">openstack keypair create <span class="bold"><strong>ironic_kp</strong></span> &gt; ironic_kp.pem</pre></div></div><div class="sect2" id="sec-ironic-provision-neutron-id"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.3.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Determining the neutron Network ID</span> <a title="Permalink" class="permalink" href="#sec-ironic-provision-neutron-id">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_provisioning.xml</li><li><span class="ds-label">ID: </span>sec-ironic-provision-neutron-id</li></ul></div></div></div></div><div class="verbatim-wrap"><pre class="screen">openstack network list

+---------------+----------+----------------------------------------------------+
| id            | name     | subnets                                            |
+---------------+----------+----------------------------------------------------+
| <span class="bold"><strong>c0102...1ca8c </strong></span>| flat-net | 709ee2a1-4110-4b26-ba4d-deb74553adb9 192.3.15.0/24 |
+---------------+----------+----------------------------------------------------+</pre></div></div><div class="sect2" id="sec-ironic-provision-boot"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.3.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Booting the Node</span> <a title="Permalink" class="permalink" href="#sec-ironic-provision-boot">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_provisioning.xml</li><li><span class="ds-label">ID: </span>sec-ironic-provision-boot</li></ul></div></div></div></div><p>
   Before booting, it is advisable to power down the node:
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-set-power-state ea7246fd-e1d6-4637-9699-0b7c59c22e67 off</pre></div><p>
   You can now boot the bare-metal node with the information compiled in the
   preceding steps, using the neutron network ID, the whole disk image ID, the
   matching flavor and the key name:
  </p><div class="verbatim-wrap"><pre class="screen">openstack server create --nic net-id=c010267c-9424-45be-8c05-99d68531ca8c \
  --image 17e4915a-ada0-4b95-bacf-ba67133f39a7 \
  --flavor 645de08d-2bc6-43f1-8a5f-2315a75b1348 \
  --key-name ironic_kp leap</pre></div><p>
   This command returns information about the state of the node that is booting:
  </p><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+------------------------+
| Property                             | Value                  |
+--------------------------------------+------------------------+
| OS-EXT-AZ:availability_zone          |                        |
| OS-EXT-SRV-ATTR:host                 | -                      |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | -                      |
| OS-EXT-SRV-ATTR:instance_name        | instance-00000001      |
| OS-EXT-STS:power_state               | 0                      |
| OS-EXT-STS:task_state                | scheduling             |
| OS-EXT-STS:vm_state                  | building               |
| OS-SRV-USG:launched_at               | -                      |
| OS-SRV-USG:terminated_at             | -                      |
| accessIPv4                           |                        |
| accessIPv6                           |                        |
| adminPass                            | adpHw3KKTjHk           |
| config_drive                         |                        |
| created                              | 2018-03-11T11:00:28Z   |
| flavor                               | bmtest (645de...b1348) |
| hostId                               |                        |
| id                                   | a9012...3007e          |
| image                                | leap (17e49...f39a7)   |
| key_name                             | ironic_kp              |
| metadata                             | {}                     |
| name                                 | leap                   |
| os-extended-volumes:volumes_attached | []                     |
| progress                             | 0                      |
| security_groups                      | default                |
| status                               | BUILD                  |
| tenant_id                            | d53bcaf...baa60dd      |
| updated                              | 2016-03-11T11:00:28Z   |
| user_id                              | e580c64...4aaf990      |
+--------------------------------------+------------------------+</pre></div><p>
   The boot process can take up to 10 minutes. Monitor the progress with the
   IPMI console or with <code class="literal">openstack server list</code>,
   <code class="literal">openstack server show
   &lt;nova_node_id&gt;</code>, and <code class="literal">ironic node-show
   &lt;ironic_node_id&gt;</code> commands.
  </p><div class="verbatim-wrap"><pre class="screen">openstack server list

+---------------+--------+--------+------------+-------------+----------------------+
| ID            | Name   | Status | Task State | Power State | Networks             |
+---------------+--------+--------+------------+-------------+----------------------+
| a9012...3007e | leap   | BUILD  | spawning   | NOSTATE     | flat-net=192.3.15.12 |
+---------------+--------+--------+------------+-------------+----------------------+</pre></div><p>
   During the boot procedure, a login prompt will appear for SLES:
  </p><p>
   Ignore this login screen and wait for the login screen of your target
   operating system to appear:
  </p><p>
   If you now run the command <code class="command">openstack server list</code>, it should show the
   node in the running state:
  </p><div class="verbatim-wrap"><pre class="screen">openstack server list
+---------------+--------+--------+------------+-------------+----------------------+
| ID            | Name   | Status | Task State | Power State | Networks             |
+---------------+--------+--------+------------+-------------+----------------------+
| a9012...3007e | leap   | ACTIVE | -          | Running     | flat-net=<span class="bold"><strong>192.3.15.14</strong></span> |
+---------------+--------+--------+------------+-------------+----------------------+</pre></div><p>
   You can now log in to the booted node using the key you generated earlier.
   (You may be prompted to change the permissions of your private key files, so
   that they are not accessible by others).
  </p><div class="verbatim-wrap"><pre class="screen">ssh leap@192.3.15.14 -i ironic_kp.pem</pre></div></div><div class="sect2" id="sec-ironic-provision-kiwi"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.3.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Building glance Images Using KIWI</span> <a title="Permalink" class="permalink" href="#sec-ironic-provision-kiwi">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_provisioning.xml</li><li><span class="ds-label">ID: </span>sec-ironic-provision-kiwi</li></ul></div></div></div></div><p>
   The following sections show you how to create your own images using KIWI,
   the command line utility to build Linux system appliances. For information
   on installing KIWI, see <a class="link" href="https://osinside.github.io/kiwi/installation.html" target="_blank">https://osinside.github.io/kiwi/installation.html</a>.
  </p><p>
   KIWI creates images in a two-step process:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     The <code class="literal">prepare</code> operation generates an unpacked image tree
     using the information provided in the image description.
    </p></li><li class="listitem "><p>
     The <code class="literal">create</code> operation creates the packed image based on
     the unpacked image and the information provided in the configuration file
     (<code class="filename">config.xml</code>).
    </p></li></ol></div><p>
   Instructions for installing KIWI are available at
   <a class="link" href="https://osinside.github.io/kiwi/installation.html" target="_blank">https://osinside.github.io/kiwi/installation.html</a>.
  </p><p>
   Image creation with KIWI is automated and does not require any user
   interaction. The information required for the image creation process is
   provided by the image description.
  </p><p>
   To use and run KIWI requires:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     A recent Linux distribution such as:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       openSUSE Leap 42.3
      </p></li><li class="listitem "><p>
       SUSE Linux Enterprise 12 SP4
      </p></li><li class="listitem "><p>
       openSUSE Tumbleweed
      </p></li></ul></div></li><li class="listitem "><p>
     Enough free disk space to build and store the image (a minimum of 10 GB is
     recommended).
    </p></li><li class="listitem "><p>
     Python version 2.7, 3.4 or higher. KIWI supports both Python 2 and 3
     versions
    </p></li><li class="listitem "><p>
     Git (package <span class="package ">git-core</span>) to clone a repository.
    </p></li><li class="listitem "><p>
     Virtualization technology to start the image (QEMU is recommended).
    </p></li></ul></div></div><div class="sect2" id="sec-ironic-provision-opensuse"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.3.14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating an openSUSE Image with KIWI</span> <a title="Permalink" class="permalink" href="#sec-ironic-provision-opensuse">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_provisioning.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_provisioning.xml</li><li><span class="ds-label">ID: </span>sec-ironic-provision-opensuse</li></ul></div></div></div></div><p>
   The following example shows how to build an openSUSE Leap image that is
   ready to run in QEMU.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Retrieve the example image descriptions.
    </p><div class="verbatim-wrap"><pre class="screen">git clone https://github.com/SUSE/kiwi-descriptions</pre></div></li><li class="step "><p>
     Build the image with KIWI:
    </p><div class="verbatim-wrap"><pre class="screen">sudo kiwi-ng --type vmx system build \
  --description kiwi-descriptions/suse/x86_64/suse-leap-42.3-JeOS \
  --target-dir /tmp/myimage</pre></div><p>
     A <code class="filename">.raw</code> image will be built in the
     <code class="filename">/tmp/myimage</code> directory.
    </p></li><li class="step "><p>
     Test the live image with QEMU:
    </p><div class="verbatim-wrap"><pre class="screen">qemu \
  -drive file=LimeJeOS-Leap-42.3.x86_64-1.42.3.raw,format=raw,if=virtio \
  -m 4096</pre></div></li><li class="step "><p>
     With a successful test, the image is complete.
    </p></li></ol></div></div><p>
   By default, KIWI generates a file in the <code class="filename">.raw</code> format.
   The <code class="filename">.raw</code> file is a disk image with a structure
   equivalent to a physical hard disk. <code class="filename">.raw</code> images are
   supported by any hypervisor, but are not compressed and do not offer the
   best performance.
  </p><p>
   Virtualization systems support their own formats (such as
   <code class="literal">qcow2</code> or <code class="literal">vmdk</code>) with compression and
   improved I/O performance. To build an image in a format other than
   <code class="filename">.raw</code>, add the format attribute to the type definition
   in the preferences section of <code class="filename">config.xml</code>. Using
   <code class="literal">qcow2</code> for example:
  </p><div class="verbatim-wrap"><pre class="screen">&lt;image ...&gt;
  &lt;preferences&gt;
    &lt;type format="qcow2" .../&gt;
    ...
  &lt;/preferences&gt;
  ...
&lt;/image</pre></div><p>
   More information about KIWI is at
   <a class="link" href="https://osinside.github.io/kiwi/" target="_blank">https://osinside.github.io/kiwi/</a>.
  </p></div></div><div class="sect1" id="ironic-provisioning-multi-tenancy"><div class="titlepage"><div><div><h2 class="title"><span class="number">29.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Provisioning Baremetal Nodes with Multi-Tenancy</span> <a title="Permalink" class="permalink" href="#ironic-provisioning-multi-tenancy">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_provisioning_multi_tenancy.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_provisioning_multi_tenancy.xml</li><li><span class="ds-label">ID: </span>ironic-provisioning-multi-tenancy</li></ul></div></div></div></div><p>
  To enable ironic multi-tenancy, you must first manually install the
  <code class="literal">python-networking-generic-switch</code> package along with all
  its dependents on all neutron nodes.
 </p><p>
  To manually enable the <code class="literal">genericswitch</code> mechanism driver in
  neutron, the <code class="literal">networking-generic-switch</code> package must be
  installed first. Do the following steps in each of the controllers where
  neutron is running.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Comment out the <code class="literal">multi_tenancy_switch_config</code> section in
    <code class="filename">~/openstack/my_cloud/definition/data/ironic/ironic_config.yml</code>.
   </p></li><li class="step "><p>
    SSH into the controller node
   </p></li><li class="step "><p>
    Change to root
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo -i</pre></div></li><li class="step "><p>
    Activate the neutron venv
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo . /opt/stack/venv/neutron-20180528T093206Z/bin/activate</pre></div></li><li class="step "><p>
    Install netmiko package
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo pip install netmiko</pre></div></li><li class="step "><p>
    Clone the <code class="literal">networking-generic-switch</code> source code into
    <code class="filename">/tmp</code>
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cd /tmp
<code class="prompt user">tux &gt; </code>sudo git clone
   https://github.com/openstack/networking-generic-switch.git</pre></div></li><li class="step "><p>
    Install <code class="literal">networking_generic_switch</code> package
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo python setup.py install</pre></div></li></ol></div></div><p>
  After the <code class="literal">networking_generic_switch</code> package is installed,
  the <code class="literal">genericswitch</code> settings must be enabled in the input
  model. The following process must be run again any time a maintenance update
  is installed that updates the neutron venv.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    SSH into the deployer node as the user <code class="literal">ardana</code>.
   </p></li><li class="step "><p>
    Edit the ironic configuration data in the input model
    <code class="filename">~/openstack/my_cloud/definition/data/ironic/ironic_config.yml</code>. Make
    sure the <code class="literal">multi_tenancy_switch_config:</code> section is
    uncommented and has the appropriate settings. <code class="literal">driver_type</code> should be <code class="literal">genericswitch</code> and
    <code class="literal">device_type</code> should be
    <code class="literal">netmiko_hp_comware</code>.
   </p><div class="verbatim-wrap"><pre class="screen">multi_tenancy_switch_config:
  -
    id: switch1
    driver_type: genericswitch
    device_type: netmiko_hp_comware
    ip_address: 192.168.75.201
    username: IRONICSHARE
    password: 'k27MwbEDGzTm'</pre></div></li><li class="step "><p>
    Run the configure process to generate the model
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
    Run <code class="filename">neutron-reconfigure.yml</code>
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost neutron-reconfigure.yml</pre></div></li><li class="step "><p>
    Run <code class="filename">neutron-status.yml</code> to make sure everything is OK
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-status.yml</pre></div></li></ol></div></div><p>
  With the <code class="literal">networking-generic-switch</code> package installed and
  enabled, you can proceed with provisioning baremetal nodes with multi-tenancy.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Create a network and a subnet:
   </p><div class="verbatim-wrap"><pre class="screen">$ openstack network create guest-net-1
Created a new network:
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | True                                 |
| availability_zone_hints   |                                      |
| availability_zones        |                                      |
| created_at                | 2017-06-10T02:49:56Z                 |
| description               |                                      |
| id                        | 256d55a6-9430-4f49-8a4c-cc5192f5321e |
| ipv4_address_scope        |                                      |
| ipv6_address_scope        |                                      |
| mtu                       | 1500                                 |
| name                      | guest-net-1                          |
| project_id                | 57b792cdcdd74d16a08fc7a396ee05b6     |
| provider:network_type     | vlan                                 |
| provider:physical_network | physnet1                             |
| provider:segmentation_id  | 1152                                 |
| revision_number           | 2                                    |
| router:external           | False                                |
| shared                    | False                                |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tags                      |                                      |
| tenant_id                 | 57b792cdcdd74d16a08fc7a396ee05b6     |
| updated_at                | 2017-06-10T02:49:57Z                 |
+---------------------------+--------------------------------------+

$ openstack subnet create guest-net-1 200.0.0.0/24
Created a new subnet:
+-------------------+----------------------------------------------+
| Field             | Value                                        |
+-------------------+----------------------------------------------+
| allocation_pools  | {"start": "200.0.0.2", "end": "200.0.0.254"} |
| cidr              | 200.0.0.0/24                                 |
| created_at        | 2017-06-10T02:53:08Z                         |
| description       |                                              |
| dns_nameservers   |                                              |
| enable_dhcp       | True                                         |
| gateway_ip        | 200.0.0.1                                    |
| host_routes       |                                              |
| id                | 53accf35-ae02-43ae-95d8-7b5efed18ae9         |
| ip_version        | 4                                            |
| ipv6_address_mode |                                              |
| ipv6_ra_mode      |                                              |
| name              |                                              |
| network_id        | 256d55a6-9430-4f49-8a4c-cc5192f5321e         |
| project_id        | 57b792cdcdd74d16a08fc7a396ee05b6             |
| revision_number   | 2                                            |
| service_types     |                                              |
| subnetpool_id     |                                              |
| tenant_id         | 57b792cdcdd74d16a08fc7a396ee05b6             |
| updated_at        | 2017-06-10T02:53:08Z                         |
+-------------------+----------------------------------------------+</pre></div></li><li class="step "><p>
    Review glance image list
   </p><div class="verbatim-wrap"><pre class="screen">$ openstack image list
+--------------------------------------+--------------------------+
| ID                                   | Name                     |
+--------------------------------------+--------------------------+
| 0526d2d7-c196-4c62-bfe5-a13bce5c7f39 | cirros-0.4.0-x86_64      |
+--------------------------------------+--------------------------+</pre></div></li><li class="step "><p>
    Create ironic node
   </p><div class="verbatim-wrap"><pre class="screen">$ ironic --ironic-api-version 1.22 node-create -d agent_ipmitool \
  -n test-node-1 -i ipmi_address=192.168.9.69 -i ipmi_username=ipmi_user \
  -i ipmi_password=XXXXXXXX --network-interface neutron -p  memory_mb=4096 \
  -p cpu_arch=x86_64 -p local_gb=80 -p cpus=2 \
  -p capabilities=boot_mode:bios,boot_option:local \
  -p root_device='{"name":"/dev/sda"}' \
  -i deploy_kernel=db3d131f-2fb0-4189-bb8d-424ee0886e4c \
  -i deploy_ramdisk=304cae15-3fe5-4f1c-8478-c65da5092a2c

+-------------------+-------------------------------------------------------------------+
| Property          | Value                                                             |
+-------------------+-------------------------------------------------------------------+
| chassis_uuid      |                                                                   |
| driver            | agent_ipmitool                                                    |
| driver_info       | {u'deploy_kernel': u'db3d131f-2fb0-4189-bb8d-424ee0886e4c',       |
|                   | u'ipmi_address': u'192.168.9.69',                                 |
|                   | u'ipmi_username': u'gozer', u'ipmi_password': u'******',          |
|                   | u'deploy_ramdisk': u'304cae15-3fe5-4f1c-8478-c65da5092a2c'}       |
| extra             | {}                                                                |
| name              | test-node-1                                                       |
| network_interface | neutron                                                           |
| properties        | {u'cpu_arch': u'x86_64', u'root_device': {u'name': u'/dev/sda'},  |
|                   | u'cpus': 2, u'capabilities': u'boot_mode:bios,boot_option:local', |
|                   | u'memory_mb': 4096, u'local_gb': 80}                              |
| resource_class    | None                                                              |
| uuid              | cb4dda0d-f3b0-48b9-ac90-ba77b8c66162                              |
+-------------------+-------------------------------------------------------------------+</pre></div><p>
    ipmi_address, ipmi_username and ipmi_password are IPMI access parameters for
    baremetal ironic node. Adjust memory_mb, cpus, local_gb to your node size
    requirements. They also need to be reflected in flavor setting (see below).
    Use capabilities boot_mode:bios for baremetal nodes operating in Legacy
    BIOS mode. For UEFI baremetal nodes, use boot_mode:uefi lookup
    deploy_kernel and deploy_ramdisk in glance image list output above.
   </p><div id="id-1.3.6.12.6.8.3.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
     Since we are using ironic API version 1.22, node is created initial state
     <span class="bold"><strong>enroll</strong></span>. It needs to be explicitly moved
     to <span class="bold"><strong>available</strong></span> state. This behavior changed
     in API version 1.11
    </p></div></li><li class="step "><p>
    Create port
   </p><div class="verbatim-wrap"><pre class="screen">$ ironic --ironic-api-version 1.22 port-create --address f0:92:1c:05:6c:40 \
  --node cb4dda0d-f3b0-48b9-ac90-ba77b8c66162 -l switch_id=e8:f7:24:bf:07:2e -l \
  switch_info=hp59srv1-a-11b -l port_id="Ten-GigabitEthernet 1/0/34" \
  --pxe-enabled true
+-----------------------+--------------------------------------------+
| Property              | Value                                      |
+-----------------------+--------------------------------------------+
| address               | f0:92:1c:05:6c:40                          |
| extra                 | {}                                         |
| local_link_connection | {u'switch_info': u'hp59srv1-a-11b',        |
|                       | u'port_id': u'Ten-GigabitEthernet 1/0/34', |
|                       | u'switch_id': u'e8:f7:24:bf:07:2e'}        |
| node_uuid             | cb4dda0d-f3b0-48b9-ac90-ba77b8c66162       |
| pxe_enabled           | True                                       |
| uuid                  | a49491f3-5595-413b-b4a7-bb6f9abec212       |
+-----------------------+--------------------------------------------+</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      for <code class="option">--address</code>, use MAC of 1st NIC of ironic baremetal
      node, which will be used for PXE boot
     </p></li><li class="listitem "><p>
      for <code class="option">--node</code>, use ironic node uuid (see above)
     </p></li><li class="listitem "><p>
      for <code class="option">-l switch_id</code>, use switch management interface MAC
      address. It can be
      retrieved by pinging switch management IP and looking up MAC address in
      'arp -l -n' command output.
     </p></li><li class="listitem "><p>
      for <code class="option">-l switch_info</code>, use switch_id from
      <code class="filename">data/ironic/ironic_config.yml</code>
      file. If you have several switch config definitions, use the right switch
      your baremetal node is connected to.
     </p></li><li class="listitem "><p>
      for -l port_id, use port ID on the switch
     </p></li></ul></div></li><li class="step "><p>
    Move ironic node to manage and then available state
   </p><div class="verbatim-wrap"><pre class="screen">$ ironic node-set-provision-state test-node-1 manage
$ ironic node-set-provision-state test-node-1 provide</pre></div></li><li class="step "><p>
    Once node is successfully moved to available state, its resources should
    be included into nova hypervisor statistics
   </p><div class="verbatim-wrap"><pre class="screen">$ openstack hypervisor stats show
+----------------------+-------+
| Property             | Value |
+----------------------+-------+
| count                | 1     |
| current_workload     | 0     |
| disk_available_least | 80    |
| free_disk_gb         | 80    |
| free_ram_mb          | 4096  |
| local_gb             | 80    |
| local_gb_used        | 0     |
| memory_mb            | 4096  |
| memory_mb_used       | 0     |
| running_vms          | 0     |
| vcpus                | 2     |
| vcpus_used           | 0     |
+----------------------+-------+</pre></div></li><li class="step "><p>
    Prepare a keypair, which will be used for logging into the node
   </p><div class="verbatim-wrap"><pre class="screen">$ openstack keypair create ironic_kp &gt; ironic_kp.pem</pre></div></li><li class="step "><p>
    Obtain user image and upload it to glance. Please refer to OpenStack
    documentation on user image creation:
    <a class="link" href="https://docs.openstack.org/project-install-guide/baremetal/draft/configure-glance-images.html" target="_blank">https://docs.openstack.org/project-install-guide/baremetal/draft/configure-glance-images.html</a>.
   </p><div id="id-1.3.6.12.6.8.8.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     Deployed images are already populated by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installer.
    </p></div><div class="verbatim-wrap"><pre class="screen">$ openstack image create --name='Ubuntu Trusty 14.04' --disk-format=qcow2 \
  --container-format=bare --file ~/ubuntu-trusty.qcow2
+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | d586d8d2107f328665760fee4c81caf0     |
| container_format | bare                                 |
| created_at       | 2017-06-13T22:38:45Z                 |
| disk_format      | qcow2                                |
| id               | 9fdd54a3-ccf5-459c-a084-e50071d0aa39 |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | Ubuntu Trusty 14.04                  |
| owner            | 57b792cdcdd74d16a08fc7a396ee05b6     |
| protected        | False                                |
| size             | 371508736                            |
| status           | active                               |
| tags             | []                                   |
| updated_at       | 2017-06-13T22:38:55Z                 |
| virtual_size     | None                                 |
| visibility       | private                              |
+------------------+--------------------------------------+

$ openstack image list
+--------------------------------------+---------------------------+
| ID                                   | Name                      |
+--------------------------------------+---------------------------+
| 0526d2d7-c196-4c62-bfe5-a13bce5c7f39 | cirros-0.4.0-x86_64       |
| 83eecf9c-d675-4bf9-a5d5-9cf1fe9ee9c2 | ir-deploy-iso-<em class="replaceable ">EXAMPLE</em>     |
| db3d131f-2fb0-4189-bb8d-424ee0886e4c | ir-deploy-kernel-<em class="replaceable ">EXAMPLE</em>  |
| 304cae15-3fe5-4f1c-8478-c65da5092a2c | ir-deploy-ramdisk-<em class="replaceable "> EXAMPLE</em> |
| 9fdd54a3-ccf5-459c-a084-e50071d0aa39 | Ubuntu Trusty 14.04       |
+--------------------------------------+---------------------------+</pre></div></li><li class="step "><p>
    Create a baremetal flavor and set flavor keys specifying requested node
    size, architecture and boot mode. A flavor can be re-used for several nodes
    having the same size, architecture and boot mode
   </p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor create m1.ironic auto 4096 80 2
+-------------+-----------+--------+------+---------+------+-------+-------------+-----------+
| ID          | Name      | Mem_MB | Disk | Ephemrl | Swap | VCPUs | RXTX_Factor | Is_Public |
+-------------+-----------+--------+------+---------+------+-------+-------------+-----------+
| ab69...87bf | m1.ironic | 4096   | 80   | 0       |      | 2     | 1.0         | True      |
+-------------+-----------+--------+------+---------+------+-------+-------------+-----------+

$ openstack flavor set ab6988...e28694c87bf set cpu_arch=x86_64
$ openstack flavor set ab6988...e28694c87bf set capabilities:boot_option="local"
$ openstack flavor set ab6988...e28694c87bf set capabilities:boot_mode="bios"</pre></div><p>
    Parameters must match parameters of ironic node above. Use
    <code class="literal">capabilities:boot_mode="bios"</code> for Legacy BIOS nodes. For
    UEFI nodes, use <code class="literal">capabilities:boot_mode="uefi"</code>
   </p></li><li class="step "><p>
    Boot the node
   </p><div class="verbatim-wrap"><pre class="screen">$ openstack server create --flavor m1.ironic --image 9fdd54a3-ccf5-459c-a084-e50071d0aa39 \
  --key-name ironic_kp --nic net-id=256d55a6-9430-4f49-8a4c-cc5192f5321e \
  test-node-1
+--------------------------------------+-------------------------------------------------+
| Property                             | Value                                           |
+--------------------------------------+-------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                          |
| OS-EXT-AZ:availability_zone          |                                                 |
| OS-EXT-SRV-ATTR:host                 | -                                               |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | -                                               |
| OS-EXT-SRV-ATTR:instance_name        |                                                 |
| OS-EXT-STS:power_state               | 0                                               |
| OS-EXT-STS:task_state                | scheduling                                      |
| OS-EXT-STS:vm_state                  | building                                        |
| OS-SRV-USG:launched_at               | -                                               |
| OS-SRV-USG:terminated_at             | -                                               |
| accessIPv4                           |                                                 |
| accessIPv6                           |                                                 |
| adminPass                            | XXXXXXXXXXXX                                    |
| config_drive                         |                                                 |
| created                              | 2017-06-14T21:25:18Z                            |
| flavor                               | m1.ironic (ab69881...5a-497d-93ae-6e28694c87bf) |
| hostId                               |                                                 |
| id                                   | f1a8c63e-da7b-4d9a-8648-b1baa6929682            |
| image                                | Ubuntu Trusty 14.04 (9fdd54a3-ccf5-4a0...0aa39) |
| key_name                             | ironic_kp                                       |
| metadata                             | {}                                              |
| name                                 | test-node-1                                     |
| os-extended-volumes:volumes_attached | []                                              |
| progress                             | 0                                               |
| security_groups                      | default                                         |
| status                               | BUILD                                           |
| tenant_id                            | 57b792cdcdd74d16a08fc7a396ee05b6                |
| updated                              | 2017-06-14T21:25:17Z                            |
| user_id                              | cc76d7469658401fbd4cf772278483d9                |
+--------------------------------------+-------------------------------------------------+</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      for <code class="option">--image</code>, use the ID of user image created at
      previous step
     </p></li><li class="listitem "><p>
      for <code class="option">--nic net-id</code>, use ID of
      the tenant network created at the beginning
     </p></li></ul></div><div id="id-1.3.6.12.6.8.10.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     During the node provisioning, the following is happening in the
     background:
    </p><p>
     neutron connects to switch management interfaces and assigns provisioning
     VLAN to baremetal node port on the switch. ironic powers up the node using
     IPMI interface. Node is booting IPA image via PXE. IPA image is writing
     provided user image onto specified root device
     (<code class="filename">/dev/sda</code>) and powers node
     down. neutron connects to switch management interfaces and assigns tenant
     VLAN to baremetal node port on the switch. A VLAN ID is selected from
     provided range. ironic powers up the node using IPMI interface. Node is
     booting user image from disk.
    </p></div></li><li class="step "><p>
    Once provisioned, node will join the private tenant network. Access to
    private network from other networks is defined by switch configuration.
   </p></li></ol></div></div></div><div class="sect1" id="ironic-system-details"><div class="titlepage"><div><div><h2 class="title"><span class="number">29.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">View Ironic System Details</span> <a title="Permalink" class="permalink" href="#ironic-system-details">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-system_details.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-system_details.xml</li><li><span class="ds-label">ID: </span>ironic-system-details</li></ul></div></div></div></div><div class="sect2" id="id-1.3.6.12.7.2"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">View details about the server using <code class="command">openstack server show &lt;nova-node-id&gt;</code></span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.7.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-system_details.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-system_details.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="verbatim-wrap"><pre class="screen">openstack server show a90122ce-bba8-496f-92a0-8a7cb143007e

+--------------------------------------+-----------------------------------------------+
| Property                             | Value                                         |
+--------------------------------------+-----------------------------------------------+
| OS-EXT-AZ:availability_zone          | nova                                          |
| OS-EXT-SRV-ATTR:host                 | ardana-cp1-ir-compute0001-mgmt                |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | ea7246fd-e1d6-4637-9699-0b7c59c22e67          |
| OS-EXT-SRV-ATTR:instance_name        | instance-0000000a                             |
| OS-EXT-STS:power_state               | 1                                             |
| OS-EXT-STS:task_state                | -                                             |
| OS-EXT-STS:vm_state                  | active                                        |
| OS-SRV-USG:launched_at               | 2016-03-11T12:26:25.000000                    |
| OS-SRV-USG:terminated_at             | -                                             |
| accessIPv4                           |                                               |
| accessIPv6                           |                                               |
| config_drive                         |                                               |
| created                              | 2016-03-11T12:17:54Z                          |
| flat-net network                     | 192.3.15.14                                   |
| flavor                               | bmtest (645de08d-2bc6-43f1-8a5f-2315a75b1348) |
| hostId                               | ecafa4f40eb5f72f7298...3bad47cbc01aa0a076114f |
| id                                   | a90122ce-bba8-496f-92a0-8a7cb143007e          |
| image                                | ubuntu (17e4915a-ada0-4b95-bacf-ba67133f39a7) |
| key_name                             | ironic_kp                                     |
| metadata                             | {}                                            |
| name                                 | ubuntu                                        |
| os-extended-volumes:volumes_attached | []                                            |
| progress                             | 0                                             |
| security_groups                      | default                                       |
| status                               | ACTIVE                                        |
| tenant_id                            | d53bcaf15afb4cb5aea3adaedbaa60dd              |
| updated                              | 2016-03-11T12:26:26Z                          |
| user_id                              | e580c645bfec4faeadef7dbd24aaf990              |
+--------------------------------------+-----------------------------------------------+</pre></div></div><div class="sect2" id="id-1.3.6.12.7.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">View detailed information about a node using <code class="command">ironic node-show &lt;ironic-node-id&gt;</code></span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.7.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-system_details.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-system_details.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="verbatim-wrap"><pre class="screen">ironic node-show  ea7246fd-e1d6-4637-9699-0b7c59c22e67

+------------------------+--------------------------------------------------------------------------+
| Property               | Value                                                                    |
+------------------------+--------------------------------------------------------------------------+
| target_power_state     | None                                                                     |
| extra                  | {}                                                                       |
| last_error             | None                                                                     |
| updated_at             | 2016-03-11T12:26:25+00:00                                                |
| maintenance_reason     | None                                                                     |
| provision_state        | active                                                                   |
| clean_step             | {}                                                                       |
| uuid                   | ea7246fd-e1d6-4637-9699-0b7c59c22e67                                     |
| console_enabled        | False                                                                    |
| target_provision_state | None                                                                     |
| provision_updated_at   | 2016-03-11T12:26:25+00:00                                                |
| maintenance            | False                                                                    |
| inspection_started_at  | None                                                                     |
| inspection_finished_at | None                                                                     |
| power_state            | power on                                                                 |
| driver                 | agent_ilo                                                                |
| reservation            | None                                                                     |
| properties             | {u'memory_mb': 64000, u'cpu_arch': u'x86_64', u'local_gb': 99,           |
|                        | u'cpus': 2, u'capabilities': u'boot_mode:bios,boot_option:local'}        |
| instance_uuid          | a90122ce-bba8-496f-92a0-8a7cb143007e                                     |
| name                   | None                                                                     |
| driver_info            | {u'ilo_address': u'10.1.196.117', u'ilo_password': u'******',            |
|                        | u'ilo_deploy_iso': u'b9499494-7db3-4448-b67f-233b86489c1f',              |
|                        | u'ilo_username': u'Administrator'}                                       |
| created_at             | 2016-03-11T10:17:10+00:00                                                |
| driver_internal_info   | {u'agent_url': u'http://192.3.15.14:9999',                               |
|                        | u'is_whole_disk_image': True, u'agent_last_heartbeat': 1457699159}       |
| chassis_uuid           |                                                                          |
| instance_info          | {u'root_gb': u'99', u'display_name': u'ubuntu', u'image_source': u       |
|                        | '17e4915a-ada0-4b95-bacf-ba67133f39a7', u'capabilities': u'{"boot_mode": |
|                        | "bios", "boot_option": "local"}', u'memory_mb': u'64000', u'vcpus':      |
|                        | u'2', u'image_url': u'http://192.168.12.2:8080/v1/AUTH_ba121db7732f4ac3a |
|                        | 50cc4999a10d58d/glance/17e4915a-ada0-4b95-bacf-ba67133f39a7?temp_url_sig |
|                        | =ada691726337805981ac002c0fbfc905eb9783ea&amp;temp_url_expires=1457699878',  |
|                        | u'image_container_format': u'bare', u'local_gb': u'99',                  |
|                        | u'image_disk_format': u'qcow2', u'image_checksum':                       |
|                        | u'2d7bb1e78b26f32c50bd9da99102150b', u'swap_mb': u'0'}                   |
+------------------------+--------------------------------------------------------------------------+</pre></div></div><div class="sect2" id="id-1.3.6.12.7.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">View detailed information about a port using <code class="command">ironic port-show &lt;ironic-port-id&gt;</code></span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.7.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-system_details.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-system_details.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="verbatim-wrap"><pre class="screen">ironic port-show a17a4ef8-a711-40e2-aa27-2189c43f0b67

+------------+-----------------------------------------------------------+
| Property   | Value                                                     |
+------------+-----------------------------------------------------------+
| node_uuid  | ea7246fd-e1d6-4637-9699-0b7c59c22e67                      |
| uuid       | a17a4ef8-a711-40e2-aa27-2189c43f0b67                      |
| extra      | {u'vif_port_id': u'82a5ab28-76a8-4c9d-bfb4-624aeb9721ea'} |
| created_at | 2016-03-11T10:40:53+00:00                                 |
| updated_at | 2016-03-11T12:17:56+00:00                                 |
| address    | 5c:b9:01:88:f0:a4                                         |
+------------+-----------------------------------------------------------+</pre></div></div><div class="sect2" id="id-1.3.6.12.7.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">View detailed information about a hypervisor using <code class="command">openstack
  hypervisor list</code> and <code class="command">openstack hypervisor show</code></span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.7.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-system_details.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-system_details.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="verbatim-wrap"><pre class="screen">openstack hypervisor list

+-----+--------------------------------------+-------+---------+
| ID  | Hypervisor hostname                  | State | Status  |
+-----+--------------------------------------+-------+---------+
| 541 | ea7246fd-e1d6-4637-9699-0b7c59c22e67 | up    | enabled |
+-----+--------------------------------------+-------+---------+</pre></div><div class="verbatim-wrap"><pre class="screen">openstack hypervisor show ea7246fd-e1d6-4637-9699-0b7c59c22e67

+-------------------------+--------------------------------------+
| Property                | Value                                |
+-------------------------+--------------------------------------+
| cpu_info                |                                      |
| current_workload        | 0                                    |
| disk_available_least    | 0                                    |
| free_disk_gb            | 0                                    |
| free_ram_mb             | 0                                    |
| host_ip                 | 192.168.12.6                         |
| hypervisor_hostname     | ea7246fd-e1d6-4637-9699-0b7c59c22e67 |
| hypervisor_type         | ironic                               |
| hypervisor_version      | 1                                    |
| id                      | 541                                  |
| local_gb                | 99                                   |
| local_gb_used           | 99                                   |
| memory_mb               | 64000                                |
| memory_mb_used          | 64000                                |
| running_vms             | 1                                    |
| service_disabled_reason | None                                 |
| service_host            | ardana-cp1-ir-compute0001-mgmt       |
| service_id              | 25                                   |
| state                   | up                                   |
| status                  | enabled                              |
| vcpus                   | 2                                    |
| vcpus_used              | 2                                    |
+-------------------------+--------------------------------------+</pre></div></div><div class="sect2" id="id-1.3.6.12.7.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.5.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">View a list of all running services using <code class="command">openstack compute
  service list</code></span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.7.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-system_details.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-system_details.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="verbatim-wrap"><pre class="screen">openstack compute service list

+----+------------------+-----------------------+----------+---------+-------+------------+----------+
| Id | Binary           | Host                  | Zone     | Status  | State | Updated_at | Disabled |
|    |                  |                       |          |         |       |            | Reason   |
+----+------------------+-----------------------+----------+---------+-------+------------+----------+
| 1  | nova-conductor   | ardana-cp1-c1-m1-mgmt | internal | enabled | up    | date:time  | -        |
| 7  | nova-conductor   |  " -cp1-c1-m2-mgmt    | internal | enabled | up    | date:time  | -        |
| 10 | nova-conductor   |  " -cp1-c1-m3-mgmt    | internal | enabled | up    | date:time  | -        |
| 13 | nova-scheduler   |  " -cp1-c1-m1-mgmt    | internal | enabled | up    | date:time  | -        |
| 16 | nova-scheduler   |  " -cp1-c1-m3-mgmt    | internal | enabled | up    | date:time  | -        |
| 19 | nova-scheduler   |  " -cp1-c1-m2-mgmt    | internal | enabled | up    | date:time  | -        |
| 25 | nova-compute     |  " -cp1-ir- | nova    |          | enabled | up    | date:time  | -        |
|    |                  |      compute0001-mgmt |          |         |       |            |          |
+----+------------------+-----------------------+----------+---------+-------+------------+----------+</pre></div></div></div><div class="sect1" id="ironic-toubleshooting"><div class="titlepage"><div><div><h2 class="title"><span class="number">29.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting ironic Installation</span> <a title="Permalink" class="permalink" href="#ironic-toubleshooting">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_troubleshooting.xml</li><li><span class="ds-label">ID: </span>ironic-toubleshooting</li></ul></div></div></div></div><p>
  Sometimes the <code class="literal">openstack server create</code> command does not
  succeed and when you do a <code class="literal">openstack server list</code>, you will see output
  like the following:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list

+------------------+--------------+--------+------------+-------------+----------+
| ID               | Name         | Status | Task State | Power State | Networks |
+------------------+--------------+--------+------------+-------------+----------+
| ee08f82...624e5f | OpenSUSE42.3 | ERROR  | -          | NOSTATE     |          |
+------------------+--------------+--------+------------+-------------+----------+</pre></div><p>
  You should execute the <code class="literal">openstack server show &lt;nova-node-id&gt;</code> and
  <code class="literal">ironic node-show &lt;ironic-node-id&gt;</code> commands to get
  more information about the error.
 </p><div class="sect2" id="id-1.3.6.12.8.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Error: No valid host was found.</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.8.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_troubleshooting.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The error <code class="literal">No valid host was found. There are not enough
   hosts.</code> is typically seen when performing the <code class="literal">openstack
   server create</code> where there is a mismatch between the properties set
   on the node and the flavor used. For example, the output from a
   <code class="literal">openstack server show</code> command may look like this:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server show ee08f82e-8920-4360-be51-a3f995624e5f

+------------------------+------------------------------------------------------------------------------+
| Property               | Value                                                                        |
+------------------------+------------------------------------------------------------------------------+
| OS-EXT-AZ:             |                                                                              |
|   availability_zone    |                                                                              |
| OS-EXT-SRV-ATTR:host   | -                                                                            |
| OS-EXT-SRV-ATTR:       |                                                                              |
|   hypervisor_hostname  | -                                                                            |
| OS-EXT-SRV-ATTR:       |                                                                              |
|   instance_name        | instance-00000001                                                            |
| OS-EXT-STS:power_state | 0                                                                            |
| OS-EXT-STS:task_state  | -                                                                            |
| OS-EXT-STS:vm_state    | error                                                                        |
| OS-SRV-USG:launched_at | -                                                                            |
| OS-SRV-USG:            |                                                                              |
|    terminated_at       | -                                                                            |
| accessIPv4             |                                                                              |
| accessIPv6             |                                                                              |
| config_drive           |                                                                              |
| created                | 2016-03-11T11:00:28Z                                                         |
| fault                  | {"message": "<span class="bold"><strong>No valid host was found. There are not enough hosts             |
|                        |  available.</strong></span>", "code": 500, "details": "  File \<span class="bold"><strong>"/opt/stack/                  |
|                        |  venv/nova-20160308T002421Z/lib/python2.7/site-packages/nova/                |
|                        |  conductor/manager.py\"</strong></span>, line 739, in build_instances                        |
|                        |     request_spec, filter_properties)                                         |
|                        |   File \<span class="bold"><strong>"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/nova/scheduler/utils.py\"</strong></span>, line 343, in wrapped              |
|                        |     return func(*args, **kwargs)                                             |
|                        |   File \<span class="bold"><strong>"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/nova/scheduler/client/__init__.py\"</strong></span>, line 52,                |
|                        |     in select_destinations context, request_spec, filter_properties)         |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/nova/scheduler/client/__init__.py\",line 37,in __run_method  |
|                        |     return getattr(self.instance, __name)(*args, **kwargs)                   |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/nova/scheduler/client/query.py\", line 34,                   |
|                        |     in select_destinations context, request_spec, filter_properties)         |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/nova/scheduler/rpcapi.py\", line 120, in select_destinations |
|                        |     request_spec=request_spec, filter_properties=filter_properties)          |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/oslo_messaging/rpc/client.py\", line 158, in call            |
|                        |     retry=self.retry)                                                        |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/oslo_messaging/transport.py\", line 90, in _send             |
|                        |     timeout=timeout, retry=retry)                                            |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/oslo_messaging/_drivers/amqpdriver.py\", line 462, in send   |
|                        |     retry=retry)                                                             |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/oslo_messaging/_drivers/amqpdriver.py\", line 453, in _send  |
|                        |     raise result                                                             |
|                        | ", "created": "2016-03-11T11:00:29Z"}                                        |
| flavor                 | bmtest (645de08d-2bc6-43f1-8a5f-2315a75b1348)                                |
| hostId                 |                                                                              |
| id                     | ee08f82e-8920-4360-be51-a3f995624e5f                                         |
| image                  | opensuse (17e4915a-ada0-4b95-bacf-ba67133f39a7)                              |
| key_name               | ironic_kp                                                                    |
| metadata               | {}                                                                           |
| name                   | opensuse                                                                     |
| os-extended-volumes:   |                                                                              |
|    volumes_attached    | []                                                                           |
| status                 | ERROR                                                                        |
| tenant_id              | d53bcaf15afb4cb5aea3adaedbaa60dd                                             |
| updated                | 2016-03-11T11:00:28Z                                                         |
| user_id                | e580c645bfec4faeadef7dbd24aaf990                                             |
+------------------------+------------------------------------------------------------------------------+</pre></div><p>
   You can find more information about the error by inspecting the log file at
   <code class="literal">/var/log/nova/nova-scheduler.log</code> or alternatively by
   viewing the error location in the source files listed in the stack-trace (in
   bold above).
  </p><p>
   To find the mismatch, compare the properties of the ironic node:
  </p><div class="verbatim-wrap"><pre class="screen">+------------------------+---------------------------------------------------------------------+
| Property               | Value                                                               |
+------------------------+---------------------------------------------------------------------+
| target_power_state     | None                                                                |
| extra                  | {}                                                                  |
| last_error             | None                                                                |
| updated_at             | None                                                                |
| maintenance_reason     | None                                                                |
| provision_state        | available                                                           |
| clean_step             | {}                                                                  |
| uuid                   | ea7246fd-e1d6-4637-9699-0b7c59c22e67                                |
| console_enabled        | False                                                               |
| target_provision_state | None                                                                |
| provision_updated_at   | None                                                                |
| maintenance            | False                                                               |
| inspection_started_at  | None                                                                |
| inspection_finished_at | None                                                                |
| power_state            | None                                                                |
| driver                 | agent_ilo                                                           |
| reservation            | None                                                                |
| properties             | <span class="bold"><strong>{u'memory_mb': 64000, u'local_gb': 99, u'cpus': 2, u'capabilities':</strong></span> |
|                        | <span class="bold"><strong>u'boot_mode:bios,boot_option:local'} </strong></span>                               |
| instance_uuid          | None                                                                |
| name                   | None                                                                |
| driver_info            | {u'ilo_address': u'10.1.196.117', u'ilo_password': u'******',       |
|                        | u'ilo_deploy_iso': u'b9499494-7db3-4448-b67f-233b86489c1f',         |
|                        | u'ilo_username': u'Administrator'}                                  |
| created_at             | 2016-03-11T10:17:10+00:00                                           |
| driver_internal_info   | {}                                                                  |
| chassis_uuid           |                                                                     |
| instance_info          | {}                                                                  |
+------------------------+---------------------------------------------------------------------+</pre></div><p>
   with the flavor characteristics:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack flavor show

+----------------------------+-------------------------------------------------------------------+
| Property                   | Value                                                             |
+----------------------------+-------------------------------------------------------------------+
| OS-FLV-DISABLED:disabled   | False                                                             |
| OS-FLV-EXT-DATA:ephemeral  | 0                                                                 |
| disk                       | <span class="bold"><strong>99 </strong></span>                                                               |
| extra_specs                | <span class="bold"><strong>{"capabilities:boot_option": "local", "cpu_arch": "x86_64",       |
|                            | "capabilities:boot_mode": "bios"}</strong></span>                                 |
| id                         | 645de08d-2bc6-43f1-8a5f-2315a75b1348                              |
| name                       | bmtest                                                            |
| os-flavor-access:is_public | True                                                              |
| ram                        | <span class="bold"><strong>64000</strong></span>                                                             |
| rxtx_factor                | 1.0                                                               |
| swap                       |                                                                   |
| vcpus                      | <span class="bold"><strong>2</strong></span>                                                                 |
+----------------------------+-------------------------------------------------------------------+</pre></div><p>
   In this instance, the problem is caused by the absence of the
   <span class="bold"><strong>"cpu_arch": "x86_64"</strong></span> property on the ironic
   node. This can be resolved by updating the ironic node, adding the missing
   property:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ironic node-update ea7246fd-e1d6-4637-9699-0b7c59c22e67 \
  <span class="bold"><strong>add properties/cpu_arch=x86_64</strong></span></pre></div><p>
   and then re-running the <code class="literal">openstack server create</code> command.
  </p></div><div class="sect2" id="id-1.3.6.12.8.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Node fails to deploy because it has timed out</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.8.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_troubleshooting.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="bold"><strong>Possible cause: </strong></span> The neutron API session
   timed out before port creation was completed.
  </p><p>
   <span class="bold"><strong>Resolution: </strong></span> Switch response time varies
   by vendor; the value of <code class="literal">url_timeout</code> must be increased to
   allow for switch response.
  </p><p>
   Check ironic Conductor logs
   (<code class="filename">/var/log/ironic/ironic-conductor.log</code>) for
   <code class="literal">ConnectTimeout</code> errors while connecting to neutron for
   port creation. For example:
  </p><div class="verbatim-wrap"><pre class="screen">19-03-20 19:09:14.557 11556 ERROR ironic.conductor.utils
[req-77f3a7b...1b10c5b - default default] Unexpected error while preparing
to deploy to node 557316...84dbdfbe8b0: ConnectTimeout: Request to
https://192.168.75.1:9696/v2.0/ports timed out</pre></div><p>
   Use the following steps to increase the value of
   <code class="literal">url_timeout</code>.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the deployer node.
    </p></li><li class="step "><p>
     Edit <code class="filename">./roles/ironic-common/defaults/main.yml</code>,
     increasing the value of <code class="literal">url_timeout</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd /var/lib/ardana/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>vi ./roles/ironic-common/defaults/main.yml</pre></div><p>
     Increase the value of the <code class="literal">url_timeout</code> parameter in the
     <code class="literal">ironic_neutron:</code> section. Increase the parameter from
     the default (60 seconds) to 120 and then in increments of 60 seconds until
     the node deploys successfully.
    </p></li><li class="step "><p>
     Reconfigure ironic.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect2" id="id-1.3.6.12.8.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment to a node fails and in "ironic node-list" command, the power_state column for the node is shown as "None"</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.8.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_troubleshooting.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="bold"><strong>Possible cause: </strong></span> The IPMI commands to the
   node take longer to change the power state of the server.
  </p><p>
   <span class="bold"><strong>Resolution: </strong></span> Check if the node power state
   can be changed using the following command
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ironic node-set-power-state $NODEUUID on</pre></div><p>
   If the above command succeeds and the power_state column is updated
   correctly, then the following steps are required to increase the power sync
   interval time.
  </p><p>
   On the first controller, reconfigure ironic to increase the power sync
   interval time. In the example below, it is set to 120 seconds. This value
   may have to be tuned based on the setup.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Go to the <code class="literal">~/openstack/my_cloud/config/ironic/</code> directory
     and edit <code class="literal">ironic-conductor.conf.j2</code> to set the
     <code class="literal">sync_power_state_interval</code> value:
    </p><div class="verbatim-wrap"><pre class="screen">[conductor]
sync_power_state_interval = 120</pre></div></li><li class="step "><p>
     Save the file and then run the following playbooks:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect2" id="id-1.3.6.12.8.8"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Error Downloading Image</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.8.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_troubleshooting.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If you encounter the error below during the deployment:
  </p><div class="verbatim-wrap"><pre class="screen">"u'message': u'Error downloading image: Download of image id 77700...96551 failed:
Image download failed for all URLs.',
u'code': 500,
u'type': u'ImageDownloadError',
u'details': u'Download of image id 77700b53-9e15-406c-b2d5-13e7d9b96551 failed:
Image download failed for all URLs.'"</pre></div><p>
   you should visit the Single Sign-On Settings in the Security page of IPMI and
   change the Single Sign-On Trust Mode setting from the default of "Trust None
   (SSO disabled)" to "Trust by Certificate".
  </p></div><div class="sect2" id="id-1.3.6.12.8.9"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.6.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using <code class="literal">node-inspection</code> can cause temporary claim of IP addresses</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.8.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_troubleshooting.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="bold"><strong>Possible cause: </strong></span> Running
   <code class="literal">node-inspection</code> on a node discovers all the NIC ports
   including the NICs that do not have any connectivity. This causes a
   temporary consumption of the network IPs and increased usage of the
   allocated quota. As a result, other nodes are deprived of IP addresses and
   deployments can fail.
  </p><p>
   <span class="bold"><strong>Resolution:</strong></span>You can add node properties
   manually added instead of using the inspection tool.
  </p><p>
   Note: Upgrade <code class="literal">ipmitool</code> to a version &gt;= 1.8.15 or it
   may not return detailed information about the NIC interface for
   <code class="literal">node-inspection</code>.
  </p></div><div class="sect2" id="id-1.3.6.12.8.10"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.6.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Node permanently stuck in deploying state</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.8.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_troubleshooting.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="bold"><strong>Possible causes:</strong></span>
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     ironic conductor service associated with the node could go down.
    </p></li><li class="listitem "><p>
     There might be a properties mismatch. MAC address registered for the node
     could be incorrect.
    </p></li></ul></div><p>
   <span class="bold"><strong>Resolution:</strong></span> To recover from this
   condition, set the provision state of the node to <code class="literal">Error</code>
   and maintenance to <code class="literal">True</code>. Delete the node and re-register
   again.
  </p></div><div class="sect2" id="id-1.3.6.12.8.11"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.6.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">The NICs in the baremetal node should come first in boot order</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.8.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_troubleshooting.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="bold"><strong>Possible causes:</strong></span> By default, the boot
   order of baremetal node is set as NIC1, HDD and NIC2. If NIC1 fails, the
   nodes starts booting from HDD and the provisioning fails.
  </p><p>
   <span class="bold"><strong>Resolution:</strong></span> Set boot order so that all the
   NICs appear before the hard disk of the baremetal as NIC1, NIC2…, HDD.
  </p></div><div class="sect2" id="id-1.3.6.12.8.12"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.6.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Increase in the number of nodes can cause power commands to fail</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.8.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_troubleshooting.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="bold"><strong>Possible causes:</strong></span>ironic periodically
   performs a power state sync with all the baremetal nodes. When the number of
   nodes increase, ironic does not get sufficient time to perform power
   operations.
  </p><p>
   <span class="bold"><strong>Resolution:</strong></span> The following procedure gives a
   way to increase <code class="literal">sync_power_state_interval</code>:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Edit the file
     <code class="literal">~/openstack/my_cloud/config/ironic/ironic-conductor.conf.j2</code>
     and navigate to the section for <code class="literal">[conductor]</code>
    </p></li><li class="step "><p>
     Increase the <code class="literal">sync_power_state_interval</code>. For example,
     for 100 nodes, set <code class="literal">sync_power_state_interval = 90</code> and
     save the file.
    </p></li><li class="step "><p>
     Execute the following set of commands to reconfigure ironic:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect2" id="id-1.3.6.12.8.13"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.6.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DHCP succeeds with PXE but times out with iPXE</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.8.13">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_troubleshooting.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If you see DHCP error "No configuration methods succeeded" in iPXE right
   after successful DHCP performed by embedded NIC firmware, there may be an
   issue with Spanning Tree Protocol on the switch.
  </p><p>
   To avoid this error, Rapid Spanning Tree Protocol needs to be enabled on the
   switch. If this is not an option due to conservative loop detection
   strategies, use the steps outlined below to install the iPXE binary with
   increased DHCP timeouts.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Clone iPXE source code
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git clone git://git.ipxe.org/ipxe.git
<code class="prompt user">tux &gt; </code>cd ipxe/src</pre></div></li><li class="step "><p>
     Modify lines 22-25 in file <code class="literal">config/dhcp.h</code>, which declare
     reduced DHCP timeouts (1-10 secs). Comment out lines with reduced timeouts
     and uncomment normal PXE timeouts (4-32)
    </p><div class="verbatim-wrap"><pre class="screen">//#define DHCP_DISC_START_TIMEOUT_SEC     1
//#define DHCP_DISC_END_TIMEOUT_SEC       10
#define DHCP_DISC_START_TIMEOUT_SEC   4       /* as per PXE spec */
#define DHCP_DISC_END_TIMEOUT_SEC     32      /* as per PXE spec */</pre></div></li><li class="step "><p>
     Make <code class="literal">undionly.kpxe</code> (BIOS) and
     <code class="literal">ipxe.efi</code> (UEFI) images
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>make bin/undionly.kpxe
<code class="prompt user">tux &gt; </code>make bin-x86_64-efi/ipxe.efi</pre></div></li><li class="step "><p>
     Copy iPXE images to Cloud Lifecycle Manager
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>scp bin/undionly.kpxe bin-x86_64-efi/ipxe.efi stack@10.0.0.4:
stack@10.0.0.4's password:
undionly.kpxe                                    100%   66KB  65.6KB/s   00:00
ipxe.efi                                         100%  918KB 918.2KB/s   00:00</pre></div></li><li class="step "><p>
     From deployer, distribute image files onto all 3 controllers
    </p><div class="verbatim-wrap"><pre class="screen">stack@ardana-cp1-c1-m1-mgmt:<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/

stack@ardana-cp1-c1-m1-mgmt:<code class="prompt user">ardana &gt; </code>~/scratch/ansible/next/ardana/ansible$ ansible -i hosts/verb_hosts \
IRN-CND -m copy -b -a 'src=~/ipxe.efi dest=/tftpboot'
...
stack@ardana-cp1-c1-m1-mgmt:<code class="prompt user">ardana &gt; </code>~/scratch/ansible/next/ardana/ansible$ ansible -i hosts/verb_hosts \
IRN-CND -m copy -b -a 'src=~/undionly.kpxe dest=/tftpboot'
...</pre></div></li></ol></div></div><p>
   There is no need to restart services. With next PXE boot attempt, iPXE
   binary with the increased timeout will be downloaded to the target node via
   TFTP.
  </p><div class="sect3" id="id-1.3.6.12.8.13.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">29.6.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">ironic Support and Limitations</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.8.13.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_troubleshooting.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following drivers are supported and tested:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      <code class="systemitem">pxe_ipmitool</code> (UEFI and Legacy BIOS mode, flat-network)
     </p></li><li class="listitem "><p>
      <code class="systemitem">pxe_ipmitool</code> (UEFI and Legacy BIOS mode, flat-network)
     </p></li><li class="listitem "><p>
      <code class="systemitem">pxe_ilo</code> (UEFI and Legacy BIOS mode, flat-network)
     </p></li><li class="listitem "><p>
      <code class="systemitem">agent_ipmitool</code> (UEFI and Legacy BIOS mode, flat-network)
     </p></li><li class="listitem "><p>
      <code class="systemitem">agent_ilo</code> (UEFI and Legacy BIOS mode, flat-network)
     </p></li></ul></div><p>
    <span class="bold"><strong>ISO Image Exceeds Free Space</strong></span>
   </p><p>
   When using the <code class="systemitem">agent_ilo</code> driver, provisioning will
   fail if the size of the user ISO image exceeds the free space available on
   the ramdisk partition. This will produce an error in the ironic Conductor
   logs that may look like as follows:
  </p><div class="verbatim-wrap"><pre class="screen">"ERROR root [-] Command failed: prepare_image, error: Error downloading
image: Download of image id 0c4d74e4-58f1-4f8d-8c1d-8a49129a2163 failed: Unable
to write image to /tmp/0c4d74e4-58f1-4f8d-8c1d-8a49129a2163. Error: [Errno 28]
No space left on device: ImageDownloadError: Error downloading image: Download
of image id 0c4d74e4-58f1-4f8d-8c1d-8a49129a2163 failed: Unable to write image
to /tmp/0c4d74e4-58f1-4f8d-8c1d-8a49129a2163. Error: [Errno 28] No space left
on device"</pre></div><p>
   By default, the total amount of space allocated to ramdisk is 4GB. To
   increase the space allocated for the ramdisk, you can update the deploy
   ISO image using the following workaround.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Save the deploy ISO to a file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack image save --file deploy.iso<em class="replaceable ">IMAGE_ID</em></pre></div><p>
    Replace <em class="replaceable ">IMAGE_ID</em> with the ID of the deploy ISO
    stored in glance. The ID can be obtained using the <code class="command">openstack image list</code>.
   </p></li><li class="step "><p>
     Mount the saved ISO:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>mkdir /tmp/mnt
<code class="prompt user">tux &gt; </code>sudo mount -t iso9660 -o loop deploy.iso /tmp/mnt</pre></div><p>
     Since the mount directory is read-only, it is necessary to copy its
     content to be able to make modifications.
    </p></li><li class="step "><p>
     Copy the content of the mount directory to a custom directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>mkdir /tmp/custom
<code class="prompt user">tux &gt; </code>cp -aRvf /tmp/mnt/* /tmp/custom/</pre></div></li><li class="step "><p>
     Modify the bootloader files to increase the size of the ramdisk:
    </p><div class="verbatim-wrap"><pre class="screen">/tmp/custom/boot/x86_64/loader/isolinux.cfg
/tmp/custom/EFI/BOOT/grub.cfg
/tmp/custom/boot/grub2/grub.cfg</pre></div><p>
     Find the <code class="literal">openstack-ironic-image</code> label and modify the
     <code class="literal">ramdisk_size</code> parameter in the <code class="literal">append</code>
     property. The <code class="literal">ramdisk_size</code> value must be specified in Kilobytes.
    </p><div class="verbatim-wrap"><pre class="screen">label openstack-ironic-image
  kernel linux
  append initrd=initrd ramdisk_size=10485760 ramdisk_blocksize=4096 \
boot_method=vmedia showopts</pre></div><p>
      Make sure that your baremetal node has the
      amount of RAM that equals or exceeds  the <code class="literal">ramdisk_size</code> value.
     </p></li><li class="step "><p>
     Repackage the ISO using the genisoimage tool:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>cd /tmp/custom
<code class="prompt user">tux &gt; </code>genisoimage -b boot/x86_64/loader/isolinux.bin -R -J -pad -joliet-long \
-iso-level 4 -A '0xaa2dab53' -no-emul-boot -boot-info-table \
-boot-load-size 4 -c boot/x86_64/boot.catalog -hide boot/x86_64/boot.catalog \
-hide-joliet boot/x86_64/boot.catalog -eltorito-alt-boot -b boot/x86_64/efi \
-no-emul-boot -joliet-long -hide glump -hide-joliet glump -o /tmp/custom_deploy.iso ./</pre></div><div id="id-1.3.6.12.8.13.6.8.5.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
       When repackaging the ISO, make sure that you use the same label. You can
       find the label file in the <code class="filename">/tmp/custom/boot/</code>
       directory. The label begins with <code class="literal">0x</code>. For example, <code class="literal">0x51e568cb</code>.
      </p></div></li><li class="step "><p>
     Delete the existing deploy ISO in glance:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack image delete <em class="replaceable ">IMAGE_ID</em></pre></div></li><li class="step "><p>
     Create a new image with <code class="literal">custom_deploy.iso</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack image create --container-format bare \
--disk-format iso --public --file custom_deploy.iso ir-deploy-iso-ARDANA5.0</pre></div></li><li class="step "><p>
     Re-deploy the ironic node.
    </p></li></ol></div></div><p>
    <span class="bold"><strong>Partition Image Exceeds Free Space</strong></span>
   </p><p>
   The previous procedure applies to ISO images. It does not apply to
   <code class="literal">partition images</code>, although there will be a similar error
   in the ironic logs. However the resolution is different. An option must be
   added to the <code class="literal">PXE</code> line in the
   <code class="filename">main.yml</code> file to increase the <code class="filename">/tmp</code>
   disk size with the following workaround:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Edit
     <code class="filename">/openstack/ardana/ansible/roles/ironic-common/defaults/main.yml</code>.
    </p></li><li class="step "><p>
     Add <code class="literal">suse.tmpsize=4G</code> to
     <code class="literal">pxe_append_params</code>. Adjust the size of
     <code class="literal">suse.tmpsize</code> as needed for the partition image.
    </p><div class="verbatim-wrap"><pre class="screen">pxe_append_params : "nofb nomodeset vga=normal elevator=deadline
                     security=apparmor crashkernel=256M console=tty0
                     console=ttyS0 suse.tmpsize=4G"</pre></div></li><li class="step "><p>
     Update Git and run playbooks:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Add suse.tmpsize variable"
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd /var/lib/ardana/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml</pre></div></li><li class="step "><p>
     Re-deploy the ironic node.
    </p></li></ol></div></div></div></div></div><div class="sect1" id="ironic-node-cleaning"><div class="titlepage"><div><div><h2 class="title"><span class="number">29.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Node Cleaning</span> <a title="Permalink" class="permalink" href="#ironic-node-cleaning">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-node_cleaning.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-node_cleaning.xml</li><li><span class="ds-label">ID: </span>ironic-node-cleaning</li></ul></div></div></div></div><p>
  Cleaning is the process by which data is removed after a previous tenant has
  used the node. Cleaning requires use of ironic's agent_ drivers. It is
  extremely important to note that if the pxe_ drivers are utilized, no node
  cleaning operations will occur, and a previous tenant's data could be found
  on the node. The same risk of a previous tenant's data possibly can occur if
  cleaning is explicitly disabled as part of the installation.
 </p><p>
  By default, cleaning attempts to utilize ATA secure erase to wipe the
  contents of the disk. If secure erase is unavailable, the cleaning
  functionality built into the ironic Python Agent falls back to an operation
  referred to as "shred" where random data is written over the contents of the
  disk, and then followed up by writing "0"s across the disk. This can be a
  time-consuming process.
 </p><p>
  An additional feature of cleaning is the ability to update firmware or
  potentially assert new hardware configuration, however, this is an advanced
  feature that must be built into the ironic Python Agent image. Due to the
  complex nature of such operations, and the fact that no one size fits all,
  this requires a custom ironic Python Agent image to be constructed with an
  appropriate hardware manager. For more information on hardware managers, see
  <a class="link" href="http://docs.openstack.org/developer/ironic-python-agent/#hardware-managers" target="_blank">http://docs.openstack.org/developer/ironic-python-agent/#hardware-managers</a>
 </p><p>
  ironic's upstream documentation for cleaning may be found here:
  <a class="link" href="http://docs.openstack.org/developer/ironic/deploy/cleaning.html" target="_blank">http://docs.openstack.org/developer/ironic/deploy/cleaning.html</a>
 </p><div class="sect2" id="id-1.3.6.12.9.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setup</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.9.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-node_cleaning.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-node_cleaning.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Cleaning is enabled by default in ironic when installed via the Cloud Lifecycle Manager.
   You can verify this by examining the ironic-conductor.conf file.
   Look for:
  </p><div class="verbatim-wrap"><pre class="screen">[conductor]
clean_nodes=true</pre></div></div><div class="sect2" id="id-1.3.6.12.9.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">In use</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.9.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-node_cleaning.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-node_cleaning.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   When enabled, cleaning will be run automatically when nodes go from active
   to available state or from manageable to available. To monitor what step of
   cleaning the node is in, run <code class="literal">ironic node-show</code>:
  </p><div class="verbatim-wrap"><pre class="screen">stack@ardana-cp1-c1-m1-mgmt:~$ ironic node-show 4e6d4273-2535-4830-a826-7f67e71783ed
+------------------------+-----------------------------------------------------------------------+
| Property               | Value                                                                 |
+------------------------+-----------------------------------------------------------------------+
| target_power_state     | None                                                                  |
| extra                  | {}                                                                    |
| last_error             | None                                                                  |
| updated_at             | 2016-04-15T09:33:16+00:00                                             |
| maintenance_reason     | None                                                                  |
| provision_state        | cleaning                                                              |
| clean_step             | {}                                                                    |
| uuid                   | 4e6d4273-2535-4830-a826-7f67e71783ed                                  |
| console_enabled        | False                                                                 |
| target_provision_state | available                                                             |
| provision_updated_at   | 2016-04-15T09:33:16+00:00                                             |
| maintenance            | False                                                                 |
| inspection_started_at  | None                                                                  |
| inspection_finished_at | None                                                                  |
| power_state            | power off                                                             |
| driver                 | agent_ilo                                                             |
| reservation            | ardana-cp1-c1-m1-mgmt                                                 |
| properties             | {u'memory_mb': 4096, u'cpu_arch': u'amd64', u'local_gb': 80,          |
|                        | u'cpus': 2, u'capabilities': u'boot_mode:uefi,boot_option:local'}     |
| instance_uuid          | None                                                                  |
| name                   | None                                                                  |
| driver_info            | {u'ilo_deploy_iso': u'249bf095-e741-441d-bc28-0f44a9b8cd80',          |
|                        | u'ipmi_username': u'Administrator', u'deploy_kernel':                 |
|                        | u'3a78c0a9-3d8d-4764-9300-3e9c00e167a1', u'ilo_address':              |
|                        | u'10.1.196.113', u'ipmi_address': u'10.1.196.113', u'deploy_ramdisk': |
|                        | u'd02c811c-e521-4926-9f26-0c88bbd2ee6d', u'ipmi_password': u'******', |
|                        | u'ilo_password': u'******', u'ilo_username': u'Administrator'}        |
| created_at             | 2016-04-14T08:30:08+00:00                                             |
| driver_internal_info   | {<span class="bold"><strong>u'clean_steps': None</strong></span>,                      |
|                        | u'hardware_manager_version': {u'generic_hardware_manager': u'1.0'},   |
|                        | u'is_whole_disk_image': True, u'agent_erase_devices_iterations': 1,   |
|                        | u'agent_url': u'http://192.168.246.245:9999',                         |
|                        | u'agent_last_heartbeat': 1460633166}                                  |
| chassis_uuid           |                                                                       |
| instance_info          | {}                                                                    |
+------------------------+-----------------------------------------------------------------------+</pre></div><p>
   The status will be in the <code class="literal">driver_internal_info</code> field. You
   will also be able to see the <code class="literal">clean_steps</code> list there.
  </p></div><div class="sect2" id="id-1.3.6.12.9.8"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.7.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.9.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-node_cleaning.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-node_cleaning.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If an error occurs during the cleaning process, the node will enter the
   clean failed state so that it is not deployed. The node remains powered on
   for debugging purposes. The node can be moved to the manageable state to
   attempt a fix using the following command:
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-set-provision-state &lt;node id&gt; manage</pre></div><p>
   Once you have identified and fixed the issue, you can return the node to
   available state by executing the following commands:
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-set-maintenance &lt;node id&gt; false
ironic node-set-provision-state &lt;node id&gt; provide</pre></div><p>
   This will retry the cleaning steps and set the node to available state upon
   their successful completion.
  </p></div><div class="sect2" id="id-1.3.6.12.9.9"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.7.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Disabling Node Cleaning</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.9.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-node_cleaning.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-node_cleaning.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To disable node cleaning, edit
   <code class="literal">~/openstack/my_cloud/definition/data/ironic/ironic_config.yml</code>
   and set <code class="literal">enable_node_cleaning</code> to <code class="literal">false</code>.
  </p><p>
   Commit your changes:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "disable node cleaning"</pre></div><p>
   Deploy these changes by re-running the configuration processor and
   reconfigure the ironic installation:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml
ansible-playbook -i hosts/localhost ready-deployment.yml
cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml</pre></div></div></div><div class="sect1" id="ironic-oneview"><div class="titlepage"><div><div><h2 class="title"><span class="number">29.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ironic and HPE OneView</span> <a title="Permalink" class="permalink" href="#ironic-oneview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_oneview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_oneview.xml</li><li><span class="ds-label">ID: </span>ironic-oneview</li></ul></div></div></div></div><div class="sect2" id="id-1.3.6.12.10.2"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling Ironic HPE OneView driver in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.10.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_oneview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_oneview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Edit the file
   <code class="literal">~/openstack/my_cloud/definition/data/ironic/ironicconfig.yml</code>
   and set the value
  </p><div class="verbatim-wrap"><pre class="screen">enable_oneview: true</pre></div><p>
   This will enable the HPE OneView driver for ironic in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  </p></div><div class="sect2" id="id-1.3.6.12.10.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding HPE OneView Appliance Credentials</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.10.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_oneview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_oneview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="verbatim-wrap"><pre class="screen">manage_url: https://&lt;Onview appliance URL&gt;

oneview_username: "&lt;Appliance username&gt;"

oneview_encrypted_password: "&lt;Encrypted password&gt;"

oneview_allow_insecure_connections: &lt;true/false&gt;

tls_cacert_file: &lt;CA certificate for connection&gt;</pre></div></div><div class="sect2" id="id-1.3.6.12.10.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.8.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Encrypting the HPE OneView Password</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.10.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_oneview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_oneview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Encryption can be applied using <code class="literal">ardanaencrypt.py</code> or using
   <code class="literal">openssl</code>. On the Cloud Lifecycle Manager node, export the key
   used for encryption as environment variable:
  </p><div class="verbatim-wrap"><pre class="screen">export ARDANA_USER_PASSWORD_ENCRYPT_KEY="<em class="replaceable ">ENCRYPTION_KEY</em>"</pre></div><p>
   And then execute the following commands:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
python ardanaencrypt.py</pre></div><p>
   Enter password to be encrypted when prompted. The script uses the key that
   was exported in the <code class="literal">ARDANA_USER_PASSWORD_ENCRYPT_KEY</code> to do
   the encryption.
  </p><p>
   For more information, see <span class="intraxref">Book “Security Guide”, Chapter 10 “Encryption of Passwords and Sensitive Data”</span>.
  </p></div><div class="sect2" id="id-1.3.6.12.10.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.8.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Decrypting the HPE OneView Password</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.10.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_oneview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_oneview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Before running the <code class="literal">site.yml</code> playbook, export the key used
   for encryption as environment variable:
  </p><div class="verbatim-wrap"><pre class="screen">export ARDANA_USER_PASSWORD_ENCRYPT_KEY="<em class="replaceable ">ENCRYPTION_KEY</em>"</pre></div><p>
   The decryption of the password is then automatically handled in
   ironic-ansible playbooks.
  </p></div><div class="sect2" id="id-1.3.6.12.10.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.8.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Registering Baremetal Node for HPE OneView Driver</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.10.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_oneview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_oneview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="verbatim-wrap"><pre class="screen">ironic node-create -d agent_pxe_oneview</pre></div><p>
   Update node driver-info:
  </p><div class="verbatim-wrap"><pre class="screen"> ironic node-update $NODE_UUID add driver_info/server_hardware_uri=$SH_URI</pre></div></div><div class="sect2" id="id-1.3.6.12.10.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.8.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Updating Node Properties</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.10.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_oneview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_oneview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="verbatim-wrap"><pre class="screen">ironic node-update $NODE_UUID add \
  properties/capabilities=server_hardware_type_uri:$SHT_URI,\
        enclosure_group_uri:$EG_URI,server_profile_template_uri=$SPT_URI</pre></div></div><div class="sect2" id="id-1.3.6.12.10.8"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.8.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating Port for Driver</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.10.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_oneview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_oneview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="verbatim-wrap"><pre class="screen">ironic port-create -n $NODE_UUID -a $MAC_ADDRESS</pre></div></div><div class="sect2" id="id-1.3.6.12.10.9"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.8.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a Node</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.10.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_oneview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_oneview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Create Node:
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-create -n ovbay7 -d agent_pxe_oneview</pre></div><p>
   Update driver info:
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-update $ID add driver_info/server_hardware_uri="/rest/server-hardware/3037...464B" \
driver_info/deploy_kernel="$KERNELDISK" driver_info/deploy_ramdisk="$RAMDISK"</pre></div><p>
   Update node properties:
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-update $ID add properties/local_gb=10
ironic node-update $ID add properties/cpus=24 properties/memory_mb=262144 \
properties/cpu_arch=x86_64</pre></div><div class="verbatim-wrap"><pre class="screen">ironic node-update \
$ID add properties/capabilities=server_hardware_type_uri:'/rest/server-hardware-types/B31...F69E',\
enclosure_group_uri:'/rest/enclosure-groups/80efe...b79fa',\
server_profile_template_uri:'/rest/server-profile-templates/faafc3c0-6c81-47ca-a407-f67d11262da5'</pre></div></div><div class="sect2" id="id-1.3.6.12.10.10"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.8.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Getting Data using REST API</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.10.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_oneview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_oneview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   GET login session auth id:
  </p><div class="verbatim-wrap"><pre class="screen">curl -k https://<em class="replaceable ">ONEVIEW_MANAGER_URL</em>/rest/login-sessions \
  -H "content-type:application/json" \
  -X POST \
  -d '{"userName":"<em class="replaceable ">USER_NAME</em>", "password":"<em class="replaceable ">PASSWORD</em>"}'</pre></div><p>
   Get the complete node details in JSON format:
  </p><div class="verbatim-wrap"><pre class="screen">curl -k "https://<em class="replaceable ">ONEVIEW_MANAGER_URL</em>;/rest/server-hardware/30373237-3132-4753-4835-32325652464B" -H "content-type:application/json" -H "Auth:&lt;auth_session_id&gt;"| python -m json.tool</pre></div></div><div class="sect2" id="id-1.3.6.12.10.11"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.8.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ironic HPE OneView CLI</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.10.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_oneview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_oneview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <code class="literal">ironic-oneview-cli</code> is already installed in
   <code class="literal">ironicclient</code> venv with a symbolic link to it. To generate
   an <code class="literal">rc</code> file for the HPE OneView CLI, follow these steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Run the following commands:
    </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc
openstack image list</pre></div></li><li class="step "><p>
     Note the <code class="literal">deploy-kernel</code> and
     <code class="literal">deploy-ramdisk</code> UUID and then run the following command
     to generate the <code class="literal">rc</code> file:
    </p><div class="verbatim-wrap"><pre class="screen">ironic-oneview genrc</pre></div><p>
     You will be prompted to enter:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       HPE OneView Manager URL
      </p></li><li class="listitem "><p>
       Username
      </p></li><li class="listitem "><p>
       deploy-kernel
      </p></li><li class="listitem "><p>
       deploy-ramdisk
      </p></li><li class="listitem "><p>
       allow_insecure_connection
      </p></li><li class="listitem "><p>
       cacert file
      </p></li></ul></div><p>
     The <code class="literal">ironic-oneview.rc</code> file will be generated in the
     current directory, by default. It is possible to specify a different
     location.
    </p></li><li class="step "><p>
     Source the generated file:
    </p><div class="verbatim-wrap"><pre class="screen">source ironic-oneview.rc</pre></div><p>
     Now enter the password of the HPE OneView appliance.
    </p></li><li class="step "><p>
     You can now use the CLI for node and flavor creation as follows:
    </p><div class="verbatim-wrap"><pre class="screen">ironic-oneview node-create
ironic-oneview flavor-create</pre></div></li></ol></div></div></div></div><div class="sect1" id="ironic-raid-config"><div class="titlepage"><div><div><h2 class="title"><span class="number">29.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">RAID Configuration for Ironic</span> <a title="Permalink" class="permalink" href="#ironic-raid-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_raid_config.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_raid_config.xml</li><li><span class="ds-label">ID: </span>ironic-raid-config</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Node Creation:
   </p><p>
    Check the raid capabilities of the driver:
   </p><div class="verbatim-wrap"><pre class="screen">ironic --ironic-api-version 1.15 driver-raid-logical-disk-properties pxe_ilo</pre></div><p>
    This will generate output similar to the following:
   </p><div class="verbatim-wrap"><pre class="screen">+----------------------+-------------------------------------------------------------------------+
| Property             | Description                                                             |
+----------------------+-------------------------------------------------------------------------+
| controller           | Controller to use for this logical disk. If not specified, the          |
|                      | driver will choose a suitable RAID controller on the bare metal node.   |
|                      | Optional.                                                               |
| disk_type            | The type of disk preferred. Valid values are 'hdd' and 'ssd'. If this   |
|                      | is not specified, disk type will not be a selection criterion for       |
|                      | choosing backing physical disks. Optional.                              |
| interface_type       | The interface type of disk. Valid values are 'sata', 'scsi' and 'sas'.  |
|                      | If this is not specified, interface type will not be a selection        |
|                      | criterion for choosing backing physical disks. Optional.                |
| is_root_volume       | Specifies whether this disk is a root volume. By default, this is False.|
|                      | Optional.                                                               |
| #_of_physical_disks  | Number of physical disks to use for this logical disk. By default, the  |
|                      | driver uses the minimum number of disks required for that RAID level.   |
|                      | Optional.                                                               |
| physical_disks       | The physical disks to use for this logical disk. If not specified, the  |
|                      | driver will choose suitable physical disks to use. Optional.            |
| <span class="bold"><strong>raid_level           | RAID level for the logical disk. Valid values are '0', '1', '2', '5', </strong></span>  |
|                      | <span class="bold"><strong>'6', '1+0', '5+0' and '6+0'. Required.</strong></span>                                  |
| share_physical_disks | Specifies whether other logical disks can share physical disks with this|
|                      | logical disk. By default, this is False. Optional.                      |
| <span class="bold"><strong>size_gb              | Size in GiB (Integer) for the logical disk. Use 'MAX' as size_gb if </strong></span>    |
|                      | <span class="bold"><strong>this logical disk is supposed to use the rest of</strong></span>                        |
|                      | <span class="bold"><strong>the space available. Required.</strong></span>                                          |
| volume_name          | Name of the volume to be created. If this is not specified, it will be  |
|                      | auto-generated. Optional.                                               |
+----------------------+-------------------------------------------------------------------------+</pre></div><p>
    Node State will be <span class="bold"><strong>Available</strong></span>
   </p><div class="verbatim-wrap"><pre class="screen">ironic node-create -d pxe_ilo -i ilo_address=&lt;ip_address&gt; \
  -i ilo_username=&lt;username&gt; -i ilo_password=&lt;password&gt; \
  -i ilo_deploy_iso=&lt;iso_id&gt; -i deploy_kernel=&lt;kernel_id&gt; \
  -i deploy_ramdisk=&lt;ramdisk_id&gt; -p cpus=2 -p memory_mb=4096 \
  -p local_gb=80  -p cpu_arch=amd64 \
  -p capabilities="boot_option:local,boot_mode:bios"</pre></div><div class="verbatim-wrap"><pre class="screen">ironic port-create -a &lt;port&gt; -n &lt;node-uuid&gt;</pre></div></li><li class="step "><p>
    Apply the target raid configuration on the node:
   </p><p>
    See the OpenStack documentation for RAID configuration at
    <a class="link" href="http://docs.openstack.org/developer/ironic/deploy/raid.html" target="_blank">http://docs.openstack.org/developer/ironic/deploy/raid.html</a>.
   </p><p>
    Set the target RAID configuration by editing the file
    <code class="literal">raid_conf.json</code> and setting the appropriate values, for
    example:
   </p><div class="verbatim-wrap"><pre class="screen">{ "logical_disks": [ {"size_gb": 5, "raid_level": "0", "is_root_volume": true} ] }</pre></div><p>
    and then run the following command:
   </p><div class="verbatim-wrap"><pre class="screen">ironic --ironic-api-version 1.15 node-set-target-raid-config &lt;node-uuid&gt; raid_conf.json</pre></div><p>
    The output produced should be similar to the following:
   </p><div class="verbatim-wrap"><pre class="screen">+-----------------------+-------------------------------------------------------------------------+
| Property              | Value                                                                   |
+-----------------------+-------------------------------------------------------------------------+
| chassis_uuid          |                                                                         |
| clean_step            | {}                                                                      |
| console_enabled       | False                                                                   |
| created_at            | 2016-06-14T14:58:07+00:00                                               |
| driver                | pxe_ilo                                                                 |
| driver_info           | {u'ilo_deploy_iso': u'd43e589a-07db-4fce-a06e-98e2f38340b4',            |
|                       | u'deploy_kernel': u'915c5c74-1ceb-4f78-bdb4-8547a90ac9c0',              |
|                       | u'ilo_address': u'10.1.196.116', u'deploy_ramdisk':                     |
|                       | u'154e7024-bf18-4ad2-95b0-726c09ce417a', u'ilo_password': u'******',    |
|                       | u'ilo_username': u'Administrator'}                                      |
| driver_internal_info  | {u'agent_cached_clean_steps_refreshed': u'2016-06-15 07:16:08.264091',  |
|                       | u'agent_cached_clean_steps': {u'raid': [{u'interface': u'raid',         |
|                       | u'priority': 0, u'step': u'delete_configuration'}, {u'interface':       |
|                       | u'raid', u'priority': 0, u'step': u'create_configuration'}], u'deploy': |
|                       | [{u'priority': 10, u'interface': u'deploy', u'reboot_requested': False, |
|                       | u'abortable': True, u'step': u'erase_devices'}]}, u'clean_steps': None, |
|                       | u'hardware_manager_version': {u'generic_hardware_manager': u'3'},       |
|                       | u'agent_erase_devices_iterations': 1, u'agent_url':                     |
|                       | u'http://192.168.245.143:9999', u'agent_last_heartbeat': 1465974974}    |
| extra                 | {}                                                                      |
| inspection_finished_at| None                                                                    |
| inspection_started_at | None                                                                    |
| instance_info         | {u'deploy_key': u'XXN2ON0V9ER429MECETJMUG5YHTKOQOZ'}                    |
| instance_uuid         | None                                                                    |
| last_error            | None                                                                    |
| maintenance           | False                                                                   |
| maintenance_reason    | None                                                                    |
| name                  | None                                                                    |
| power_state           | power off                                                               |
| properties            | {u'cpu_arch': u'amd64', u'root_device': {u'wwn': u'0x600508b1001ce286'},|
|                       | u'cpus': 2, u'capabilities':                                            |
|                       | u'boot_mode:bios,raid_level:6,boot_option:local', u'memory_mb': 4096,   |
|                       | u'local_gb': 4}                                                         |
| provision_state       | available                                                               |
| provision_updated_at  | 2016-06-15T07:16:27+00:00                                               |
| reservation           | padawan-ironic-cp1-c1-m2-mgmt                                           |
| target_power_state    | power off                                                               |
| target_provision_state| None                                                                    |
| <span class="bold"><strong>target_raid_config</strong></span>    | {u'logical_disks': [{u'size_gb': 5, <span class="bold"><strong>u'raid_level': u'6',</strong></span>                |
|                       | u'is_root_volume': True}]}                                              |
| updated_at            | 2016-06-15T07:44:22+00:00                                               |
| uuid                  | 22ab9f85-71a1-4748-8d6b-f6411558127e                                    |
+-----------------------+-------------------------------------------------------------------------+</pre></div><p>
    Now set the state of the node to
    <span class="bold"><strong>manageable</strong></span>:
   </p><div class="verbatim-wrap"><pre class="screen">ironic --ironic-api-version latest node-set-provision-state &lt;node-uuid&gt; manage</pre></div></li><li class="step "><p>
    Manual cleaning steps:
   </p><p>
    Manual cleaning is enabled by default in production - the following are the
    steps to enable cleaning if the manual cleaning has been disabled.
   </p><ol type="a" class="substeps "><li class="step "><p>
      Provide <code class="literal">cleaning_network_uuid</code> in
      <code class="literal">ironic-conductor.conf</code>
     </p></li><li class="step "><p>
      Edit the file
      <code class="literal">~/openstack/my_cloud/definition/data/ironic/ironic_config.yml</code>
      and set <code class="literal">enable_node_cleaning</code> to
      <code class="literal">true</code>.
     </p></li><li class="step "><p>
      Then run the following set of commands:
     </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "enabling node cleaning"
cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml
ansible-playbook -i hosts/localhost ready-deployment.yml
cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml</pre></div><p>
      After performing these steps, the state of the node will become
      <span class="bold"><strong>Cleaning</strong></span>.
     </p></li></ol><p>
    Run the following command:
   </p><div class="verbatim-wrap"><pre class="screen">ironic --ironic-api-version latest node-set-provision-state 2123254e-8b31...aa6fd \
  clean --clean-steps '[{ "interface": "raid","step": "delete_configuration"}, \
  { "interface": "raid" ,"step": "create_configuration"}]'</pre></div><p>
    Node-information after a Manual cleaning:
   </p><div class="verbatim-wrap"><pre class="screen">+-----------------------+-------------------------------------------------------------------------+
| Property              | Value                                                                   |
+-----------------------+-------------------------------------------------------------------------+
| chassis_uuid          |                                                                         |
| clean_step            | {}                                                                      |
| console_enabled       | False                                                                   |
| created_at            | 2016-06-14T14:58:07+00:00                                               |
| driver                | pxe_ilo                                                                 |
| driver_info           | {u'ilo_deploy_iso': u'd43e589a-07db-4fce-a06e-98e2f38340b4',            |
|                       | u'deploy_kernel': u'915c5c74-1ceb-4f78-bdb4-8547a90ac9c0',              |
|                       | u'ilo_address': u'10.1.196.116', u'deploy_ramdisk':                     |
|                       | u'154e7024-bf18-4ad2-95b0-726c09ce417a', u'ilo_password': u'******',    |
|                       | u'ilo_username': u'Administrator'}                                      |
| driver_internal_info  | {u'agent_cached_clean_steps_refreshed': u'2016-06-15 07:16:08.264091',  |
|                       | u'agent_cached_clean_steps': {u'raid': [{u'interface': u'raid',         |
|                       | u'priority': 0, u'step': u'delete_configuration'}, {u'interface':       |
|                       | u'raid', u'priority': 0, u'step': u'create_configuration'}], u'deploy': |
|                       | [{u'priority': 10, u'interface': u'deploy', u'reboot_requested': False, |
|                       | u'abortable': True, u'step': u'erase_devices'}]}, u'clean_steps': None, |
|                       | u'hardware_manager_version': {u'generic_hardware_manager': u'3'},       |
|                       | u'agent_erase_devices_iterations': 1, u'agent_url':                     |
|                       | u'http://192.168.245.143:9999', u'agent_last_heartbeat': 1465974974}    |
| extra                 | {}                                                                      |
| inspection_finished_at| None                                                                    |
| inspection_started_at | None                                                                    |
| instance_info         | {u'deploy_key': u'XXN2ON0V9ER429MECETJMUG5YHTKOQOZ'}                    |
| instance_uuid         | None                                                                    |
| last_error            | None                                                                    |
| maintenance           | False                                                                   |
| maintenance_reason    | None                                                                    |
| name                  | None                                                                    |
| power_state           | power off                                                               |
| properties            | {u'cpu_arch': u'amd64', u'root_device': {u'wwn': u'0x600508b1001ce286'},|
|                       | u'cpus': 2, u'capabilities':                                            |
|                       | u'boot_mode:bios,raid_level:6,boot_option:local', u'memory_mb': 4096,   |
|                       | u'local_gb': 4}                                                         |
| provision_state       | manageable                                                              |
| provision_updated_at  | 2016-06-15T07:16:27+00:00                                               |
| raid_config           | {u'last_updated': u'2016-06-15 07:16:14.584014', u'physical_disks':     |
|                       | [{u'status': u'ready', u'size_gb': 1024, u'interface_type': u'sata',    |
|                       | u'firmware': u'HPGC', u'controller': u'Smart Array P440ar in Slot 0     |
|                       | (Embedded)', u'model': u'ATA     MM1000GBKAL', u'disk_type': u'hdd',    |
|                       | u'id': u'1I:3:3'}, {u'status': u'ready', u'size_gb': 1024,              |
|                       | u'interface_type': u'sata', u'firmware': u'HPGC', u'controller': u'Smart|
|                       | Array P440ar in Slot 0 (Embedded)', u'model': u'ATA     MM1000GBKAL',   |
|                       | u'disk_type': u'hdd', u'id': u'1I:3:1'}, {u'status': u'active',         |
|                       | u'size_gb': 1024, u'interface_type': u'sata', u'firmware': u'HPGC',     |
|                       | u'controller': u'Smart Array P440ar in Slot 0 (Embedded)', u'model':    |
|                       | u'ATA     MM1000GBKAL', u'disk_type': u'hdd', u'id': u'1I:3:2'},        |
|                       | {u'status': u'active', u'size_gb': 1024, u'interface_type': u'sata',    |
|                       | u'firmware': u'HPGC', u'controller': u'Smart Array P440ar in Slot 0     |
|                       | (Embedded)', u'model': u'ATA     MM1000GBKAL', u'disk_type': u'hdd',    |
|                       | u'id': u'2I:3:6'}, {u'status': u'active', u'size_gb': 1024,             |
|                       | u'interface_type': u'sata', u'firmware': u'HPGC', u'controller': u'Smart|
|                       | Array P440ar in Slot 0 (Embedded)', u'model': u'ATA     MM1000GBKAL',   |
|                       | u'disk_type': u'hdd', u'id': u'2I:3:5'}, {u'status': u'active',         |
|                       | u'size_gb': 1024, u'interface_type': u'sata', u'firmware': u'HPGC',     |
|                       | u'controller': u'Smart Array P440ar in Slot 0 (Embedded)', u'model':    |
|                       | u'ATA     MM1000GBKAL', u'disk_type': u'hdd', u'id': u'1I:3:4'}],       |
|                       | u'logical_disks': [{u'size_gb': 4, u'physical_disks': [u'1I:3:2',       |
|                       | u'2I:3:6', u'2I:3:5', u'1I:3:4'], u'raid_level': u'6',                  |
|                       | u'is_root_volume': True, u'root_device_hint': {u'wwn':                  |
|                       | u'0x600508b1001ce286'}, u'controller': u'Smart Array P440ar in Slot 0   |
|                       | (Embedded)', u'volume_name': u'015E795CPDNLH0BRH8N406AAB7'}]}           |
| reservation           | padawan-ironic-cp1-c1-m2-mgmt                                           |
| target_power_state    | power off                                                               |
| target_provision_state| None                                                                    |
| target_raid_config    | {u'logical_disks': [{u'size_gb': 5, u'raid_level': u'6',                |
|                       | u'is_root_volume': True}]}                                              |
| updated_at            | 2016-06-15T07:44:22+00:00                                               |
| uuid                  | 22ab9f85-71a1-4748-8d6b-f6411558127e                                    |
+-----------------------+-------------------------------------------------------------------------+</pre></div><p>
    After the manual cleaning, run the following command to change the state of
    a node to <span class="bold"><strong>available</strong></span>:
   </p><div class="verbatim-wrap"><pre class="screen">ironic --ironic-api-version latest node-set-provision-state &lt;node-uuid&gt; \
  provide</pre></div></li></ol></div></div></div><div class="sect1" id="ironic-audit-support"><div class="titlepage"><div><div><h2 class="title"><span class="number">29.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Audit Support for Ironic</span> <a title="Permalink" class="permalink" href="#ironic-audit-support">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_audit_support.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_audit_support.xml</li><li><span class="ds-label">ID: </span>ironic-audit-support</li></ul></div></div></div></div><div class="sect2" id="id-1.3.6.12.12.2"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">API Audit Logging</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.12.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_audit_support.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_audit_support.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Audit middleware supports delivery of CADF audit events via Oslo messaging
   notifier capability. Based on <code class="literal">notification_driver</code>
   configuration, audit events can be routed to messaging infrastructure
   (<code class="literal">notification_driver = messagingv2</code>) or can be routed to a
   log file (<code class="literal">notification_driver = log</code>).
  </p><p>
   Audit middleware creates two events per REST API interaction. The first
   event has information extracted from request data and the second one
   contains information on the request outcome (response).
  </p></div><div class="sect2" id="id-1.3.6.12.12.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.10.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling API Audit Logging</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.12.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_audit_support.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_audit_support.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   You can enable audit logging for ironic by changing the configuration in the
   input model. Edit the file
   <code class="literal">~/openstack/my_cloud/definition/cloudConfig.yml</code> and in the
   <code class="literal">audit-settings</code> section, change the
   <code class="literal">default</code> value to <code class="literal">enabled</code>. The
   ironic-ansible playbooks will now enable audit support for ironic.
  </p><p>
   API audit events will be logged in the corresponding audit directory, for
   example, <code class="literal">/var/audit/ironic/ironic-api-audit.log</code>. An audit
   event will be logged in the log file for every request and response for an
   API call.
  </p></div><div class="sect2" id="id-1.3.6.12.12.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">29.10.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Sample Audit Event</span> <a title="Permalink" class="permalink" href="#id-1.3.6.12.12.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic-ironic_audit_support.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic-ironic_audit_support.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following output is an example of an audit event for an <code class="literal">ironic
   node-list</code> command:
  </p><div class="verbatim-wrap"><pre class="screen">{
   "event_type":"audit.http.request",
   "timestamp":"2016-06-15 06:04:30.904397",
   "payload":{
      "typeURI":"http://schemas.dmtf.org/cloud/audit/1.0/event",
      "eventTime":"2016-06-15T06:04:30.903071+0000",
      "target":{
         "id":"ironic",
         "typeURI":"unknown",
         "addresses":[
            {
               "url":"http://{ironic_admin_host}:6385",
               "name":"admin"
            },
           {
               "url":"http://{ironic_internal_host}:6385",
               "name":"private"
           },
           {
               "url":"http://{ironic_public_host}:6385",
               "name":"public"
           }
         ],
         "name":"ironic"
      },
      "observer":{
         "id":"target"
      },
      "tags":[
         "correlation_id?value=685f1abb-620e-5d5d-b74a-b4135fb32373"
      ],
      "eventType":"activity",
      "initiator":{
         "typeURI":"service/security/account/user",
         "name":"admin",
         "credential":{
            "token":"***",
            "identity_status":"Confirmed"
         },
         "host":{
            "agent":"python-ironicclient",
            "address":"10.1.200.129"
         },
         "project_id":"d8f52dd7d9e1475dbbf3ba47a4a83313",
         "id":"8c1a948bad3948929aa5d5b50627a174"
      },
      "action":"read",
      "outcome":"pending",
      "id":"061b7aa7-5879-5225-a331-c002cf23cb6c",
      "requestPath":"/v1/nodes/?associated=True"
   },
   "priority":"INFO",
   "publisher_id":"ironic-api",
   "message_id":"2f61ebaa-2d3e-4023-afba-f9fca6f21fc2"
}</pre></div></div></div></div><div class="chapter " id="install-swift"><div class="titlepage"><div><div><h2 class="title"><span class="number">30 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Swift Only</span> <a title="Permalink" class="permalink" href="#install-swift">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_swift_object_storage.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_swift_object_storage.xml</li><li><span class="ds-label">ID: </span>install-swift</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#sec-swift-important-notes"><span class="number">30.1 </span><span class="name">Important Notes</span></a></span></dt><dt><span class="section"><a href="#sec-swift-prereqs"><span class="number">30.2 </span><span class="name">Prepare for Cloud Installation</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.13.5"><span class="number">30.3 </span><span class="name">Configure Your Environment</span></a></span></dt><dt><span class="section"><a href="#sec-swift-provision"><span class="number">30.4 </span><span class="name">Provisioning Your Baremetal Nodes</span></a></span></dt><dt><span class="section"><a href="#sec-swift-config-processor"><span class="number">30.5 </span><span class="name">Running the Configuration Processor</span></a></span></dt><dt><span class="section"><a href="#sec-swift-deploy"><span class="number">30.6 </span><span class="name">Deploying the Cloud</span></a></span></dt><dt><span class="section"><a href="#sec-swift-post-installation"><span class="number">30.7 </span><span class="name">Post-Installation Verification and Administration</span></a></span></dt></dl></div></div><p>
  This page describes the installation step requirements for the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  Entry-scale Cloud with swift Only model.
 </p><div class="sect1" id="sec-swift-important-notes"><div class="titlepage"><div><div><h2 class="title"><span class="number">30.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Important Notes</span> <a title="Permalink" class="permalink" href="#sec-swift-important-notes">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_swift_object_storage.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_swift_object_storage.xml</li><li><span class="ds-label">ID: </span>sec-swift-important-notes</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     For information about when to use the GUI installer and when to use the
     command line (CLI), see <a class="xref" href="#preinstall-overview" title="Chapter 13. Overview">Chapter 13, <em>Overview</em></a>.
    </p></li><li class="listitem "><p>
     Review the <a class="xref" href="#min-hardware" title="Chapter 2. Hardware and Software Support Matrix">Chapter 2, <em>Hardware and Software Support Matrix</em></a> that we have listed.
    </p></li><li class="listitem "><p>
     Review the release notes to make yourself aware of any known issues and
     limitations.
    </p></li><li class="listitem "><p>
     The installation process can occur in different phases. For example, you
     can install the control plane only and then add Compute nodes afterwards
     if you would like.
    </p></li><li class="listitem "><p>
     If you run into issues during installation, we have put together a list of
     <a class="xref" href="#troubleshooting-installation" title="Chapter 36. Troubleshooting the Installation">Chapter 36, <em>Troubleshooting the Installation</em></a> you can reference.
    </p></li><li class="listitem "><p>
     Make sure all disks on the system(s) are wiped before you begin the
     install. (For swift, refer to <a class="xref" href="#topic-d1s-hht-tt" title="11.6. Swift Requirements for Device Group Drives">Section 11.6, “Swift Requirements for Device Group Drives”</a>.)
    </p></li><li class="listitem "><p>
     There is no requirement to have a dedicated network for OS-install and
     system deployment, this can be shared with the management network. More
     information can be found in <a class="xref" href="#example-configurations" title="Chapter 9. Example Configurations">Chapter 9, <em>Example Configurations</em></a>.
    </p></li><li class="listitem "><p>
     The terms deployer and Cloud Lifecycle Manager are used interchangeably. They refer to the
     same nodes in your cloud environment.
    </p></li><li class="listitem "><p>
     When running the Ansible playbook in this installation guide, if a runbook
     fails you will see in the error response to use the
     <code class="literal">--limit</code> switch when retrying a playbook. This should be
     avoided. You can simply re-run any playbook without this switch.
    </p></li><li class="listitem "><p>
     DVR is not supported with ESX compute.
    </p></li><li class="listitem "><p>
     When you attach a cinder volume to the VM running on the ESXi host, the
     volume will not get detected automatically. Make sure to set the image
     metadata <span class="bold"><strong>vmware_adaptertype=lsiLogicsas</strong></span>
     for image before launching the instance. This will help to discover the
     volume change appropriately.
    </p></li><li class="listitem "><p>
     The installation process will create several <span class="productname">OpenStack</span> roles. Not all roles
     will be relevant for a cloud with swift only, but they will not cause
     problems.
    </p></li></ul></div></div><div class="sect1" id="sec-swift-prereqs"><div class="titlepage"><div><div><h2 class="title"><span class="number">30.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prepare for Cloud Installation</span> <a title="Permalink" class="permalink" href="#sec-swift-prereqs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_swift_object_storage.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_swift_object_storage.xml</li><li><span class="ds-label">ID: </span>sec-swift-prereqs</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Review the <a class="xref" href="#preinstall-checklist" title="Chapter 14. Pre-Installation Checklist">Chapter 14, <em>Pre-Installation Checklist</em></a> about recommended
     pre-installation tasks.
    </p></li><li class="step "><p>
     Prepare the Cloud Lifecycle Manager node. The Cloud Lifecycle Manager must be accessible either directly or via
     <code class="literal">ssh</code>, and have SUSE Linux Enterprise Server 12 SP4 installed. All nodes must
     be accessible to the Cloud Lifecycle Manager. If the nodes do not have direct access to
     online Cloud subscription channels, the Cloud Lifecycle Manager node will need to host the
     Cloud repositories.
    </p><ol type="a" class="substeps "><li class="step "><p>
       If you followed the installation instructions for Cloud Lifecycle Manager server (see <a class="xref" href="#cha-depl-dep-inst" title="Chapter 15. Installing the Cloud Lifecycle Manager server">Chapter 15, <em>Installing the Cloud Lifecycle Manager server</em></a>), <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> software should already be
       installed. Double-check whether SUSE Linux Enterprise and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are properly
       registered at the SUSE Customer Center by starting YaST and running <span class="guimenu ">Software</span> › <span class="guimenu ">Product
       Registration</span>.
      </p><p>
       If you have not yet installed <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, do so by starting YaST and
       running <span class="guimenu ">Software</span> › <span class="guimenu ">Product
       Registration</span> › <span class="guimenu ">Select
       Extensions</span>. Choose <span class="guimenu "><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span>
       and follow the on-screen instructions. Make sure to register
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> during the installation process and to install the software
       pattern <code class="literal">patterns-cloud-ardana</code>.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper -n in patterns-cloud-ardana</pre></div></li><li class="step "><p>
       Ensure the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> media repositories and updates repositories are
       made available to all nodes in your deployment. This can be accomplished
       either by configuring the Cloud Lifecycle Manager server as an SMT mirror as described in
       <a class="xref" href="#app-deploy-smt-lcm" title="Chapter 16. Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)">Chapter 16, <em>Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</em></a> or by syncing or mounting the
       Cloud and updates repositories to the Cloud Lifecycle Manager server as described in <a class="xref" href="#cha-depl-repo-conf-lcm" title="Chapter 17. Software Repository Setup">Chapter 17, <em>Software Repository Setup</em></a>.
      </p></li><li class="step "><p>
       Configure passwordless <code class="command">sudo</code> for the user created when
       setting up the node (as described in <a class="xref" href="#sec-depl-adm-inst-user" title="15.4. Creating a User">Section 15.4, “Creating a User”</a>). Note that this is
       <span class="emphasis"><em>not</em></span> the user <code class="systemitem">ardana</code> that will be used later in this
       procedure. In the following we assume you named the user <code class="systemitem">cloud</code>. Run the command
       <code class="command">visudo</code> as user <code class="systemitem">root</code> and add the following line
       to the end of the file:
      </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable ">CLOUD</em> ALL = (root) NOPASSWD:ALL</pre></div><p>
       Make sure to replace <em class="replaceable ">CLOUD</em> with your user name
       choice.
      </p></li><li class="step "><p>
       Set the password for the user
       <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo passwd ardana</pre></div></li><li class="step "><p>
       Become the user <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>su - ardana</pre></div></li><li class="step "><p>
       Place a copy of the SUSE Linux Enterprise Server 12 SP4 <code class="filename">.iso</code> in the
       <code class="systemitem">ardana</code> home directory,
       <code class="filename">var/lib/ardana</code>, and rename it to
       <code class="filename">sles12sp4.iso</code>.
      </p></li><li class="step "><p>
       Install the templates, examples, and working model directories:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>/usr/bin/ardana-init</pre></div></li></ol></li></ol></div></div></div><div class="sect1" id="id-1.3.6.13.5"><div class="titlepage"><div><div><h2 class="title"><span class="number">30.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure Your Environment</span> <a title="Permalink" class="permalink" href="#id-1.3.6.13.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_swift_object_storage.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_swift_object_storage.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   This part of the install is going to depend on the specific cloud
   configuration you are going to use.
  </p><p>
     Setup your configuration files, as follows:
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       See the sample sets of configuration files in the
       <code class="literal">~/openstack/examples/</code> directory. Each set will have an
       accompanying README.md file that explains the contents of each of the
       configuration files.
      </p></li><li class="step "><p>
       Copy the example configuration files into the required setup directory
       and edit them to contain the details of your environment:
      </p><div class="verbatim-wrap"><pre class="screen">cp -r ~/openstack/examples/entry-scale-swift/* \
  ~/openstack/my_cloud/definition/</pre></div></li><li class="step "><p>
       Begin inputting your environment information into the configuration
       files in the <code class="literal">~/openstack/my_cloud/definition</code>
       directory.
      </p><p>
       Full details of how to do this can be found here:
       <a class="xref" href="#input-model" title="11.10.1. Ring Specifications in the Input Model">Section 11.10.1, “Ring Specifications in the Input Model”</a>.
      </p><p>
       In many cases, the example models provide most of the data you need to
       create a valid input model. However, there are two important aspects you
       must plan and configure before starting a deploy as follows:
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Check the disk model used by your nodes. Specifically, check that all
         disk drives are correctly named and used as described in
         <a class="xref" href="#topic-d1s-hht-tt" title="11.6. Swift Requirements for Device Group Drives">Section 11.6, “Swift Requirements for Device Group Drives”</a>.
        </p></li><li class="listitem "><p>
         Select an appropriate partition power for your rings. Detailed
         information about this is provided at
         <a class="xref" href="#ring-specification" title="11.10. Understanding Swift Ring Specifications">Section 11.10, “Understanding Swift Ring Specifications”</a>.
        </p></li></ul></div></li></ol></div></div><p>
     Optionally, you can use the <code class="literal">ardanaencrypt.py</code> script to
     encrypt your IPMI passwords. This script uses OpenSSL.
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Change to the Ansible directory:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible</pre></div></li><li class="step "><p>
       Put the encryption key into the following environment variable:
      </p><div class="verbatim-wrap"><pre class="screen">export ARDANA_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</pre></div></li><li class="step "><p>
       Run the python script below and follow the instructions. Enter a
       password that you want to encrypt.
      </p><div class="verbatim-wrap"><pre class="screen">ardanaencrypt.py</pre></div></li><li class="step "><p>
       Take the string generated and place it in the
       <code class="literal">"ilo_password"</code> field in your
       <code class="literal">~/openstack/my_cloud/definition/data/servers.yml</code> file,
       remembering to enclose it in quotes.
      </p></li><li class="step "><p>
       Repeat the above for each server.
      </p></li></ol></div></div><div id="id-1.3.6.13.5.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      Before you run any playbooks, remember that you need to export the
      encryption key in the following environment variable: <code class="literal">export
      ARDANA_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</code>
     </p></div><p>
   Commit your configuration to the local git repo
   (<a class="xref" href="#using-git" title="Chapter 22. Using Git for Configuration Management">Chapter 22, <em>Using Git for Configuration Management</em></a>), as follows:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div><div id="id-1.3.6.13.5.10" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    This step needs to be repeated any time you make changes to your
    configuration files before you move onto the following steps. See
    <a class="xref" href="#using-git" title="Chapter 22. Using Git for Configuration Management">Chapter 22, <em>Using Git for Configuration Management</em></a> for more information.
   </p></div></div><div class="sect1" id="sec-swift-provision"><div class="titlepage"><div><div><h2 class="title"><span class="number">30.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Provisioning Your Baremetal Nodes</span> <a title="Permalink" class="permalink" href="#sec-swift-provision">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_swift_object_storage.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_swift_object_storage.xml</li><li><span class="ds-label">ID: </span>sec-swift-provision</li></ul></div></div></div></div><p>
   To provision the baremetal nodes in your cloud deployment you can either use
   the automated operating system installation process provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> or
   you can use the 3rd party installation tooling of your choice. We will
   outline both methods below:
  </p><div class="sect2" id="id-1.3.6.13.6.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">30.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using Third Party Baremetal Installers</span> <a title="Permalink" class="permalink" href="#id-1.3.6.13.6.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_swift_object_storage.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_swift_object_storage.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    If you do not wish to use the automated operating system installation
    tooling included with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> then the requirements that have to be met
    using the installation tooling of your choice are:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      The operating system must be installed via the SLES ISO provided on
      the <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a>.
     </p></li><li class="listitem "><p>
      Each node must have SSH keys in place that allows the same user from the
      Cloud Lifecycle Manager node who will be doing the deployment to SSH to each node without a
      password.
     </p></li><li class="listitem "><p>
      Passwordless sudo needs to be enabled for the user.
     </p></li><li class="listitem "><p>
      There should be a LVM logical volume as <code class="literal">/root</code> on each
      node.
     </p></li><li class="listitem "><p>
      If the LVM volume group name for the volume group holding the
      <code class="literal">root</code> LVM logical volume is
      <code class="literal">ardana-vg</code>, then it will align with the disk input
      models in the examples.
     </p></li><li class="listitem "><p>
      <span class="phrase">Ensure that <code class="literal">openssh-server</code>,
      <code class="literal">python</code>, <code class="literal">python-apt</code>, and
      <code class="literal">rsync</code> are installed.</span>
     </p></li></ul></div><p>
    If you chose this method for installing your baremetal hardware, skip
    forward to the step
    <em class="citetitle ">Running the Configuration Processor</em>.
   </p></div><div class="sect2" id="id-1.3.6.13.6.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">30.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Automated Operating System Installation Provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <a title="Permalink" class="permalink" href="#id-1.3.6.13.6.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_swift_object_storage.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_swift_object_storage.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    If you would like to use the automated operating system installation tools
    provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, complete the steps below.
   </p><div class="sect3" id="id-1.3.6.13.6.4.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">30.4.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Cobbler</span> <a title="Permalink" class="permalink" href="#id-1.3.6.13.6.4.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_swift_object_storage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_swift_object_storage.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
     This phase of the install process takes the baremetal information that was
     provided in <code class="literal">servers.yml</code> and installs the Cobbler
     provisioning tool and loads this information into Cobbler. This sets each
     node to <code class="literal">netboot-enabled: true</code> in Cobbler. Each node
     will be automatically marked as <code class="literal">netboot-enabled: false</code>
     when it completes its operating system install successfully. Even if the
     node tries to PXE boot subsequently, Cobbler will not serve it. This is
     deliberate so that you cannot reimage a live node by accident.
    </p><p>
     The <code class="literal">cobbler-deploy.yml</code> playbook prompts for a password
     - this is the password that will be encrypted and stored in Cobbler, which
     is associated with the user running the command on the Cloud Lifecycle Manager, that you
     will use to log in to the nodes via their consoles after install. The
     username is the same as the user set up in the initial dialogue when
     installing the Cloud Lifecycle Manager from the ISO, and is the same user that is running
     the cobbler-deploy play.
    </p><div id="id-1.3.6.13.6.4.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      When imaging servers with your own tooling, it is still necessary to have
      ILO/IPMI settings for all nodes. Even if you are not using Cobbler, the
      username and password fields in <code class="filename">servers.yml</code> need to
      be filled in with dummy settings. For example, add the following to
      <code class="filename">servers.yml</code>:
     </p><div class="verbatim-wrap"><pre class="screen">ilo-user: manual
ilo-password: deployment</pre></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Run the following playbook which confirms that there is IPMI connectivity
       for each of your nodes so that they are accessible to be re-imaged in a
       later step:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-power-status.yml</pre></div></li><li class="step "><p>
       Run the following playbook to deploy Cobbler:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li></ol></div></div></div><div class="sect3" id="id-1.3.6.13.6.4.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">30.4.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Imaging the Nodes</span> <a title="Permalink" class="permalink" href="#id-1.3.6.13.6.4.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_swift_object_storage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_swift_object_storage.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
     This phase of the install process goes through a number of distinct steps:
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Powers down the nodes to be installed
      </p></li><li class="step "><p>
       Sets the nodes hardware boot order so that the first option is a network
       boot.
      </p></li><li class="step "><p>
       Powers on the nodes. (The nodes will then boot from the network and be
       installed using infrastructure set up in the previous phase)
      </p></li><li class="step "><p>
       Waits for the nodes to power themselves down (this indicates a
       successful install). This can take some time.
      </p></li><li class="step "><p>
       Sets the boot order to hard disk and powers on the nodes.
      </p></li><li class="step "><p>
       Waits for the nodes to be reachable by SSH and verifies that they have the
       signature expected.
      </p></li></ol></div></div><p>
     Deploying nodes has been automated in the Cloud Lifecycle Manager and requires the
     following:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       All of your nodes using SLES must already be installed, either
       manually or via Cobbler.
      </p></li><li class="listitem "><p>
       Your input model should be configured for your SLES nodes.
      </p></li><li class="listitem "><p>
       You should have run the configuration processor and the
       <code class="filename">ready-deployment.yml</code> playbook.
      </p></li></ul></div><p>
     Execute the following steps to re-image one or more nodes after you have
     run the <code class="filename">ready-deployment.yml</code> playbook.
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Run the following playbook, specifying your SLES nodes using the
       nodelist. This playbook will reconfigure Cobbler for the nodes listed.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook prepare-sles-grub2.yml -e \
      nodelist=node1[,node2,node3]</pre></div></li><li class="step "><p>
       Re-image the node(s) with the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml \
      -e nodelist=node1[,node2,node3]</pre></div></li></ol></div></div><p>
     If a nodelist is not specified then the set of nodes in Cobbler with
     <code class="literal">netboot-enabled: True</code> is selected. The playbook pauses
     at the start to give you a chance to review the set of nodes that it is
     targeting and to confirm that it is correct.
    </p><p>
     You can use the command below which will list all of your nodes with the
     <code class="literal">netboot-enabled: True</code> flag set:
    </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system find --netboot-enabled=1</pre></div></div></div></div><div class="sect1" id="sec-swift-config-processor"><div class="titlepage"><div><div><h2 class="title"><span class="number">30.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Running the Configuration Processor</span> <a title="Permalink" class="permalink" href="#sec-swift-config-processor">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_swift_object_storage.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_swift_object_storage.xml</li><li><span class="ds-label">ID: </span>sec-swift-config-processor</li></ul></div></div></div></div><p>
   Once you have your configuration files setup, you need to run the
   configuration processor to complete your configuration.
  </p><p>
   When you run the configuration processor, you will be prompted for two
   passwords. Enter the first password to make the configuration processor
   encrypt its sensitive data, which consists of the random inter-service
   passwords that it generates and the ansible <code class="literal">group_vars</code>
   and <code class="literal">host_vars</code> that it produces for subsequent deploy
   runs. You will need this password for subsequent Ansible deploy and
   configuration processor runs. If you wish to change an encryption password
   that you have already used when running the configuration processor then
   enter the new password at the second prompt, otherwise just press
   <span class="keycap">Enter</span> to bypass this.
  </p><p>
   Run the configuration processor with this command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
   For automated installation (for example CI), you can specify the required
   passwords on the ansible command line. For example, the command below will
   disable encryption by the configuration processor:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml \
  -e encrypt="" -e rekey=""</pre></div><p>
   If you receive an error during this step, there is probably an issue with
   one or more of your configuration files. Verify that all information in each
   of your configuration files is correct for your environment. Then commit
   those changes to Git using the instructions in the previous section before
   re-running the configuration processor again.
  </p><p>
   For any troubleshooting information regarding these steps, see
   <a class="xref" href="#sec-trouble-config-processor" title="36.2. Issues while Updating Configuration Files">Section 36.2, “Issues while Updating Configuration Files”</a>.
  </p></div><div class="sect1" id="sec-swift-deploy"><div class="titlepage"><div><div><h2 class="title"><span class="number">30.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying the Cloud</span> <a title="Permalink" class="permalink" href="#sec-swift-deploy">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_swift_object_storage.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_swift_object_storage.xml</li><li><span class="ds-label">ID: </span>sec-swift-deploy</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Use the playbook below to create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     [OPTIONAL] - Run the <code class="literal">wipe_disks.yml</code> playbook to ensure
     all of your non-OS partitions on your nodes are completely wiped before
     continuing with the installation. The <code class="filename">wipe_disks.yml</code>
     playbook is only meant to be run on systems immediately after running
     <code class="filename">bm-reimage.yml</code>. If used for any other case, it may
     not wipe all of the expected partitions.
    </p><p>
     If you are using fresh machines this step may not be necessary.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml</pre></div><p>
     If you have used an encryption password when running the configuration
     processor use the command below and enter the encryption password when
     prompted:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --ask-vault-pass</pre></div></li><li class="step "><p>
     Run the <code class="literal">site.yml</code> playbook below:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml</pre></div><p>
     If you have used an encryption password when running the configuration
     processor use the command below and enter the encryption password when
     prompted:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --ask-vault-pass</pre></div><div id="id-1.3.6.13.8.2.3.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The step above runs <code class="literal">osconfig</code> to configure the cloud
      and <code class="literal">ardana-deploy</code> to deploy the cloud. Therefore, this
      step may run for a while, perhaps 45 minutes or more, depending on the
      number of nodes in your environment.
     </p></div></li><li class="step "><p>
     Verify that the network is working correctly. Ping each IP in the
     <code class="literal">/etc/hosts</code> file from one of the controller nodes.
    </p></li></ol></div></div><p>
   For any troubleshooting information regarding these steps, see
   <a class="xref" href="#sec-trouble-deploy-cloud" title="36.3. Issues while Deploying the Cloud">Section 36.3, “Issues while Deploying the Cloud”</a>.
  </p></div><div class="sect1" id="sec-swift-post-installation"><div class="titlepage"><div><div><h2 class="title"><span class="number">30.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Post-Installation Verification and Administration</span> <a title="Permalink" class="permalink" href="#sec-swift-post-installation">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installing_swift_object_storage.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installing_swift_object_storage.xml</li><li><span class="ds-label">ID: </span>sec-swift-post-installation</li></ul></div></div></div></div><p>
   We recommend verifying the installation using the instructions in
   <a class="xref" href="#cloud-verification" title="Chapter 38. Post Installation Tasks">Chapter 38, <em>Post Installation Tasks</em></a>.
  </p><p>
   There are also a list of other common post-installation administrative tasks
   listed in the <a class="xref" href="#postinstall-checklist" title="Chapter 44. Other Common Post-Installation Tasks">Chapter 44, <em>Other Common Post-Installation Tasks</em></a> list.
  </p></div></div><div class="chapter " id="install-sles-compute"><div class="titlepage"><div><div><h2 class="title"><span class="number">31 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing SLES Compute</span> <a title="Permalink" class="permalink" href="#install-sles-compute">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-sles-install_sles_compute.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-sles-install_sles_compute.xml</li><li><span class="ds-label">ID: </span>install-sles-compute</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#sles-overview"><span class="number">31.1 </span><span class="name">SLES Compute Node Installation Overview</span></a></span></dt><dt><span class="section"><a href="#sles-support"><span class="number">31.2 </span><span class="name">SLES Support</span></a></span></dt><dt><span class="section"><a href="#install-sles"><span class="number">31.3 </span><span class="name">Using the Cloud Lifecycle Manager to Deploy SLES Compute Nodes</span></a></span></dt><dt><span class="section"><a href="#provisioning-sles"><span class="number">31.4 </span><span class="name">Provisioning SLES Yourself</span></a></span></dt></dl></div></div><div class="sect1" id="sles-overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">31.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SLES Compute Node Installation Overview</span> <a title="Permalink" class="permalink" href="#sles-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-sles-sles_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-sles-sles_overview.xml</li><li><span class="ds-label">ID: </span>sles-overview</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 supports SLES compute nodes, specifically SUSE Linux Enterprise Server 12 SP4. SUSE
  does not ship a SLES ISO with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> so you will need to download a copy of
  the SLES ISO (<code class="filename">SLE-12-SP4-Server-DVD-x86_64-GM-DVD1.iso</code>)
  from SUSE. You can use the following
  link to download the ISO. To do so, either log in or create a SUSE
  account before downloading:
  <a class="link" href="https://www.suse.com/products/server/download/" target="_blank">https://www.suse.com/products/server/download/</a>.
 </p><p>
  There are two approaches for deploying SLES compute nodes in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Using the Cloud Lifecycle Manager to automatically deploy SLES Compute Nodes.
   </p></li><li class="listitem "><p>
    Provisioning SLES nodes yourself, either manually or using a third-party
    tool, and then providing the relevant information to the Cloud Lifecycle Manager.
   </p></li></ul></div><p>
  These two approaches can be used whether you are installing a cloud for the
  first time or adding a compute node to an existing cloud. Regardless of your
  approach, you should be certain to register your SLES compute nodes in order
  to get product updates as they come available. For more information, see
  <a class="xref" href="#register-suse-overview" title="Chapter 1. Registering SLES">Chapter 1, <em>Registering SLES</em></a>.
 </p></div><div class="sect1" id="sles-support"><div class="titlepage"><div><div><h2 class="title"><span class="number">31.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SLES Support</span> <a title="Permalink" class="permalink" href="#sles-support">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-sles-sles_support.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-sles-sles_support.xml</li><li><span class="ds-label">ID: </span>sles-support</li></ul></div></div></div></div><p>
  SUSE Linux Enterprise Server (SLES) Host OS KVM and/or supported SLES guests
  have been tested and qualified by SUSE to run on <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p></div><div class="sect1" id="install-sles"><div class="titlepage"><div><div><h2 class="title"><span class="number">31.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Cloud Lifecycle Manager to Deploy SLES Compute Nodes</span> <a title="Permalink" class="permalink" href="#install-sles">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-sles-install_sles.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-sles-install_sles.xml</li><li><span class="ds-label">ID: </span>install-sles</li></ul></div></div></div></div><p>
  The method used for deploying SLES compute nodes using Cobbler on the
  Cloud Lifecycle Manager uses legacy BIOS.
 </p><div class="sect2" id="id-1.3.6.14.4.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">31.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying legacy BIOS SLES Compute nodes</span> <a title="Permalink" class="permalink" href="#id-1.3.6.14.4.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-sles-install_sles.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-sles-install_sles.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The installation process for legacy BIOS SLES Compute nodes is similar to
   that described in <a class="xref" href="#install-kvm" title="Chapter 24. Installing Mid-scale and Entry-scale KVM">Chapter 24, <em>Installing Mid-scale and Entry-scale KVM</em></a> with some additional requirements:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The standard SLES ISO (SLE-12-SP4-Server-DVD-x86_64-GM-DVD1.iso) must be
     accessible as <code class="literal">~/sles12sp4.iso</code>. Rename the ISO or
     create a symbolic link:
    </p><div class="verbatim-wrap"><pre class="screen">mv SLE-12-SP4-Server-DVD-x86_64-GM-DVD1.iso ~/sles12sp4.iso</pre></div></li><li class="listitem "><p>
     You must identify the node(s) on which you want to install SLES, by
     adding the key/value pair <code class="literal">distro-id: sles12sp4-x86_64</code>
     to server details in <code class="literal">servers.yml</code>. If there are any
     network interface or disk layout differences in the new server compared to
     the servers already in the model, you may also need to update
     <code class="literal">net_interfaces.yml</code>,
     <code class="literal">server_roles.yml</code>, <code class="literal">disk_compute.yml</code>
     and <code class="literal">control_plane.yml</code>. For more information on
     configuration of the Input Model for SLES, see <a class="xref" href="#sles-compute-model" title="10.1. SLES Compute Nodes">Section 10.1, “SLES Compute Nodes”</a>.
    </p></li><li class="listitem "><p>
     Run the playbook <code class="filename">config-processor-run.yml</code> to check
     for errors in the updated model.
    </p></li><li class="listitem "><p>
     Run the <code class="filename">ready-deployment.yml</code> playbook to build the
     new <code class="filename">scratch</code> directory.
    </p></li><li class="listitem "><p>
     Record the management network IP address that is used for the new
     server. It will be used in the installation process.
    </p></li></ul></div></div><div class="sect2" id="sles-uefi-overview"><div class="titlepage"><div><div><h3 class="title"><span class="number">31.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying UEFI SLES compute nodes</span> <a title="Permalink" class="permalink" href="#sles-uefi-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-sles-install_sles.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-sles-install_sles.xml</li><li><span class="ds-label">ID: </span>sles-uefi-overview</li></ul></div></div></div></div><p>
   Deploying UEFI nodes has been automated in the Cloud Lifecycle Manager and requires the
   following to be met:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     All of your nodes using SLES must already be installed, either manually
     or via Cobbler.
    </p></li><li class="listitem "><p>
     Your input model should be configured for your SLES nodes, per the
     instructions at <a class="xref" href="#sles-compute-model" title="10.1. SLES Compute Nodes">Section 10.1, “SLES Compute Nodes”</a>.
    </p></li><li class="listitem "><p>
     You should have run the configuration processor and the
     <code class="filename">ready-deployment.yml</code> playbook.
    </p></li></ul></div><p>
   Execute the following steps to re-image one or more nodes after you have run
   the <code class="filename">ready-deployment.yml</code> playbook.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Run the following playbook, ensuring that you specify only your UEFI
     SLES nodes using the nodelist. This playbook will reconfigure Cobbler
     for the nodes listed.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook prepare-sles-grub2.yml -e nodelist=node1[,node2,node3]</pre></div></li><li class="step "><p>
     Re-image the node(s), ensuring that you only specify your UEFI SLES
     nodes using the nodelist.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml \
-e nodelist=node1[,node2,node3]</pre></div></li><li class="step "><p>
     Back up the <code class="filename">grub.cfg-*</code> files in
     <code class="filename">/srv/tftpboot/</code> as they will be overwritten when
     running the cobbler-deploy playbook on the next step. You will need these
     files if you need to reimage the nodes in the future.
    </p></li><li class="step "><p>
     Run the <code class="filename">cobbler-deploy.yml</code> playbook, which will reset
     Cobbler back to the default values:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li></ol></div></div><div class="sect3" id="id-1.3.6.14.4.4.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">31.3.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">UEFI Secure Boot</span> <a title="Permalink" class="permalink" href="#id-1.3.6.14.4.4.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-sles-install_sles.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-sles-install_sles.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    Secure Boot is a method used to restrict binary execution for booting the
    system. With this option enabled, system BIOS will only allow boot loaders
    with trusted cryptographic signatures to be executed, thus preventing
    malware from hiding embedded code in the boot chain. Each boot loader
    launched during the boot process is digitally signed and that signature is
    validated against a set of trusted certificates embedded in the UEFI BIOS.
    Secure Boot is completely implemented in the BIOS and does not require
    special hardware.
   </p><p>Thus Secure Boot is:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Intended to prevent boot-sector malware or kernel code injection.
     </p></li><li class="listitem "><p>Hardware-based code signing.</p></li><li class="listitem "><p>Extension of the UEFI BIOS architecture.</p></li><li class="listitem "><p>
      Optional with the ability to enable or disable it through the BIOS.
     </p></li></ul></div><p>
    In Boot Options of RBSU, <span class="guimenu ">Boot Mode</span> needs to be set to
    <code class="literal">UEFI Mode</code> and <span class="guimenu ">UEFI Optimized Boot</span>
    should be <code class="literal">Enabled</code>&gt;.
   </p><p>
    Secure Boot is enabled at <span class="guimenu ">System
    Configuration</span> › <span class="guimenu ">BIOS/Platform Configuration (RBSU)</span> › <span class="guimenu ">Server
         Security</span> › <span class="guimenu ">Secure Boot Configuration</span> › <span class="guimenu ">Secure Boot Enforcement</span>.
   </p></div></div></div><div class="sect1" id="provisioning-sles"><div class="titlepage"><div><div><h2 class="title"><span class="number">31.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Provisioning SLES Yourself</span> <a title="Permalink" class="permalink" href="#provisioning-sles">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-sles-provisioning_sles.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-sles-provisioning_sles.xml</li><li><span class="ds-label">ID: </span>provisioning-sles</li></ul></div></div></div></div><p>
  This section outlines the steps needed to manually provision a SLES node so
  that it can be added to a new or existing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 cloud.
 </p><div class="sect2" id="id-1.3.6.14.5.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">31.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure Cloud Lifecycle Manager to Enable SLES</span> <a title="Permalink" class="permalink" href="#id-1.3.6.14.5.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-sles-provisioning_sles.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-sles-provisioning_sles.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Take note of the IP address of the Cloud Lifecycle Manager node. It will be used below
     during <a class="xref" href="#sec-provisioning-sles-add-zypper" title="31.4.6. Add zypper repository">Section 31.4.6, “Add zypper repository”</a>.
    </p></li><li class="step "><p>
     Mount or copy the contents of
     <code class="filename">SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso</code> to
     <code class="literal">/srv/www/suse-12.3/x86_64/repos/ardana/sles12/zypper/OS/</code>
    </p></li></ol></div></div><div id="id-1.3.6.14.5.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    If you choose to mount an ISO, we recommend creating an <code class="filename">/etc/fstab</code> entry to
    ensure the ISO is mounted after a reboot.
   </p></div></div><div class="sect2" id="id-1.3.6.14.5.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">31.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Install SUSE Linux Enterprise Server 12 SP4</span> <a title="Permalink" class="permalink" href="#id-1.3.6.14.5.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-sles-provisioning_sles.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-sles-provisioning_sles.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Install SUSE Linux Enterprise Server 12 SP4 using the standard iso
   (<code class="filename">SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso</code>)
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Boot the SUSE Linux Enterprise Server 12 SP4 ISO.
    </p></li><li class="step "><p>
     Agree to the license
    </p></li><li class="step "><p>
     Edit the network settings, enter the the management network IP address
     recorded earlier. It is not necessary to enter a
     <span class="guimenu ">Hostname</span>/. For product registration to work correctly,
     you must provide a DNS server. Enter the <span class="guimenu ">Name Server</span> IP
     address and the <span class="guimenu ">Default IPv4 Gateway</span>.
    </p></li><li class="step "><p>
     Additional <code class="literal">System Probing</code> will occur.
    </p></li><li class="step "><p>
     On the <code class="literal">Registration</code> page, you can skip registration if
     the database server does not have a external interface or if there is no
     SMT server on the MGMT LAN.
    </p></li><li class="step "><p>
     No <code class="literal">Add On Products</code> are needed.
    </p></li><li class="step "><p>
     For <code class="literal">System Role</code>, select <span class="guimenu ">Default
     System</span>. Do not select <span class="guimenu ">KVM Virtualization
     Host</span>.
    </p></li><li class="step "><p>
     Partitioning
    </p><ol type="a" class="substeps "><li class="step "><p>
       Select <span class="guimenu ">Expert Partitioner</span> and <span class="guimenu ">Rescan
       Devices</span> to clear <span class="guimenu ">Proposed Settings</span>.
      </p></li><li class="step "><p>
       Delete all <code class="literal">Volume Groups</code>.
      </p></li><li class="step "><p>
       Under the root of the directory tree, delete
       <code class="literal">/dev/sda</code>.
      </p></li><li class="step "><p>
       Delete any other partitions on any other drives.
      </p></li><li class="step "><p>
       <span class="guimenu ">Add Partition</span> under <code class="literal">sda</code>, called
       <code class="literal">ardana</code>, with a <span class="guimenu ">Custom Size</span>
       of 250MB.
      </p></li><li class="step "><p>
       Add an <span class="guimenu ">EFI Boot Partition</span>. Partition should be
       formatted as <code class="literal">FAT</code> and mounted at
       <span class="guimenu ">/boot/efi</span>.
      </p></li><li class="step "><p>
       <span class="guimenu ">Add Partition</span> with all the remaining space
       (<span class="guimenu ">Maximum Size</span>). The role for this partition is
       <span class="guimenu ">Raw Volume (unformatted)</span>. It should not be
       mounted. It should not be formatted.
      </p></li><li class="step "><p>
       Select <span class="guimenu ">Volume Management</span> and add a volume group to <code class="literal">/dev/sda2</code>
       called <code class="literal">ardana-vg</code>.
      </p></li><li class="step "><p>
       Add an LV to <code class="literal">ardana-vg</code> called <code class="literal">root</code>,
       <code class="literal">Type</code> of <span class="guimenu ">Normal Volume</span>,
       <span class="guimenu ">Custom Size</span> of 50GB, <span class="guimenu ">Raw Volume
       (unformatted)</span>. Format as <span class="guimenu ">Ext4 File System</span>
       and mount at <code class="literal">/</code>.
      </p></li><li class="step "><p>
       Acknowledge the warning about having no swap partition.
      </p></li><li class="step "><p>
       Press <span class="guimenu ">Next</span> on the <code class="literal">Suggested
       Partitioning</code> page.
      </p></li></ol></li><li class="step "><p>
     Pick your <span class="guimenu ">Time Zone</span> and check <code class="literal">Hardware Clock
     Set to UTC</code>.
    </p></li><li class="step "><p>
     Create a user named <code class="literal">ardana</code> and a password for
     <code class="literal">system administrator</code>. Do not check <span class="guimenu ">Automatic
     Login</span>.
    </p></li><li class="step "><p>
     On the <code class="literal">Installation Settings</code> page:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Disable firewall
      </p></li><li class="listitem "><p>
       Enable SSH service
      </p></li><li class="listitem "><p>
       Set <code class="literal">text</code> as the <span class="guimenu ">Default systemd
       target</span>.
      </p></li></ul></div></li><li class="step "><p>
     Press <span class="guimenu ">Install</span> and <code class="literal">Confirm
     Installation</code> with the <span class="guimenu ">Install</span> button.
    </p></li><li class="step "><p>
     Installation will begin and the system will reboot automatically when
     installation is complete.
    </p></li><li class="step "><p>
     When the system is booted, log in as <code class="literal">root</code>, using the
     system administrator set during installation.
    </p></li><li class="step "><p>
     Set up the <code class="literal">ardana</code> user and add
     <code class="literal">ardana</code> to the <code class="literal">sudoers</code> group.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>useradd -s /bin/bash -d /var/lib/ardana -m
    ardana
<code class="prompt user">root # </code>passwd ardana</pre></div><p>
     Enter and retype the password for user <code class="literal">ardana</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>echo "ardana ALL=(ALL) NOPASSWD:ALL" | sudo tee -a \
    /etc/sudoers.d/ardana</pre></div></li><li class="step "><p>
     Add an ardana group (id 1000) and change group owner to
     <code class="literal">ardana</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>groupadd --gid 1000 ardana
<code class="prompt user">root # </code>chown -R ardana:ardana /var/lib/ardana</pre></div></li><li class="step "><p>
     Disconnect the installation ISO. List repositories and remove the repository that was used
     for the installation.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper lr</pre></div><p>
     Identify the <code class="literal">Name</code> of the repository to remove.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper rr <em class="replaceable ">REPOSITORY_NAME</em></pre></div></li><li class="step "><p>
     Copy the SSH key from the Cloud Lifecycle Manager.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ssh-copy-id ardana@<em class="replaceable ">DEPLOYER_IP_ADDRESS</em></pre></div></li><li class="step "><p>
     Log in to the SLES via SSH.
    </p></li><li class="step "><p>
     Continue with the <code class="filename">site.yml</code> playbook to scale out
     the node.
    </p></li></ol></div></div></div><div class="sect2" id="id-1.3.6.14.5.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">31.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Assign a static IP</span> <a title="Permalink" class="permalink" href="#id-1.3.6.14.5.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-sles-provisioning_sles.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-sles-provisioning_sles.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Use the <code class="literal">ip addr</code> command to find out what network
     devices are on your system:
    </p><div class="verbatim-wrap"><pre class="screen">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: <span class="bold"><strong>eno1</strong></span>: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000
    link/ether <span class="bold"><strong>f0:92:1c:05:89:70</strong></span> brd ff:ff:ff:ff:ff:ff
    inet 10.13.111.178/26 brd 10.13.111.191 scope global eno1
       valid_lft forever preferred_lft forever
    inet6 fe80::f292:1cff:fe05:8970/64 scope link
       valid_lft forever preferred_lft forever
3: eno2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000
    link/ether f0:92:1c:05:89:74 brd ff:ff:ff:ff:ff:ff</pre></div></li><li class="step "><p>
     Identify the one that matches the MAC address of your server and edit the
     corresponding config file in
     <code class="literal">/etc/sysconfig/network-scripts</code>.
    </p><div class="verbatim-wrap"><pre class="screen">vi /etc/sysconfig/network-scripts/<span class="bold"><strong>ifcfg-eno1</strong></span></pre></div></li><li class="step "><p>
     Edit the <code class="literal">IPADDR</code> and <code class="literal">NETMASK</code> values
     to match your environment. Note that the <code class="literal">IPADDR</code> is used
     in the corresponding stanza in <code class="literal">servers.yml</code>. You may
     also need to set <code class="literal">BOOTPROTO</code> to <code class="literal">none</code>.
    </p><div class="verbatim-wrap"><pre class="screen">TYPE=Ethernet
<span class="bold"><strong>BOOTPROTO=none</strong></span>
DEFROUTE=yes
PEERDNS=yes
PEERROUTES=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_PEERDNS=yes
IPV6_PEERROUTES=yes
IPV6_FAILURE_FATAL=no
NAME=eno1
UUID=36060f7a-12da-469b-a1da-ee730a3b1d7c
DEVICE=eno1
ONBOOT=yes
<span class="bold"><strong>NETMASK=255.255.255.192</strong></span>
<span class="bold"><strong>IPADDR=10.13.111.14</strong></span></pre></div></li><li class="step "><p>
     [OPTIONAL] Reboot your SLES node and ensure that it can be accessed from
     the Cloud Lifecycle Manager.
    </p></li></ol></div></div></div><div class="sect2" id="id-1.3.6.14.5.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">31.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Add <code class="literal">ardana</code> user and home directory</span> <a title="Permalink" class="permalink" href="#id-1.3.6.14.5.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-sles-provisioning_sles.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-sles-provisioning_sles.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="verbatim-wrap"><pre class="screen">useradd -m ardana
passwd ardana</pre></div></div><div class="sect2" id="id-1.3.6.14.5.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">31.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Allow user <code class="literal">ardana</code> to <code class="literal">sudo</code> without password</span> <a title="Permalink" class="permalink" href="#id-1.3.6.14.5.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-sles-provisioning_sles.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-sles-provisioning_sles.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    Setting up sudo on SLES is covered in the <em class="citetitle ">SLES Administration Guide</em> at
    <a class="link" href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-admin/#sec-sudo-conf" target="_blank">https://documentation.suse.com/sles/15-SP1/single-html/SLES-admin/#sec-sudo-conf</a>.
   </p><p>
    The recommendation is to create user specific <code class="command">sudo</code> config files under
    <code class="filename">/etc/sudoers.d</code>, therefore creating an <code class="filename">/etc/sudoers.d/ardana</code> config file with
    the following content will allow sudo commands without the requirement of a
    password.
   </p><div class="verbatim-wrap"><pre class="screen">ardana ALL=(ALL) NOPASSWD:ALL</pre></div></div><div class="sect2" id="sec-provisioning-sles-add-zypper"><div class="titlepage"><div><div><h3 class="title"><span class="number">31.4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Add zypper repository</span> <a title="Permalink" class="permalink" href="#sec-provisioning-sles-add-zypper">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-sles-provisioning_sles.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-sles-provisioning_sles.xml</li><li><span class="ds-label">ID: </span>sec-provisioning-sles-add-zypper</li></ul></div></div></div></div><p>
   Using the ISO-based repositories created above, add the zypper repositories.
  </p><p>
   Follow these steps. Update the value of deployer_ip as necessary.
  </p><div class="verbatim-wrap"><pre class="screen">deployer_ip=192.168.10.254
<code class="prompt user">tux &gt; </code>sudo zypper addrepo --no-gpgcheck --refresh http://$deployer_ip:79/ardana/sles12/zypper/OS SLES-OS
<code class="prompt user">tux &gt; </code>sudo zypper addrepo --no-gpgcheck --refresh http://$deployer_ip:79/ardana/sles12/zypper/SDK SLES-SDK</pre></div><p>
   To verify that the repositories have been added, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper repos --detail</pre></div><p>
   For more information about Zypper, see the
   <em class="citetitle ">SLES Administration Guide</em> at
   <a class="link" href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-admin/#sec-zypper" target="_blank">https://documentation.suse.com/sles/15-SP1/single-html/SLES-admin/#sec-zypper</a>.
  </p><div id="id-1.3.6.14.5.8.8" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
    If you intend on attaching encrypted volumes to any of your SLES
    Compute nodes, install the cryptographic libraries through cryptsetup on
    each node. Run the following command to install the necessary
    cryptographic libraries:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper in cryptsetup</pre></div></div></div><div class="sect2" id="id-1.3.6.14.5.9"><div class="titlepage"><div><div><h3 class="title"><span class="number">31.4.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Add Required Packages</span> <a title="Permalink" class="permalink" href="#id-1.3.6.14.5.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-sles-provisioning_sles.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-sles-provisioning_sles.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   As documented in <a class="xref" href="#sec-kvm-provision" title="24.4. Provisioning Your Baremetal Nodes">Section 24.4, “Provisioning Your Baremetal Nodes”</a>,
   you need to add extra packages.
   <span class="phrase">Ensure that <code class="literal">openssh-server</code>,
   <code class="literal">python</code>,
   and <code class="literal">rsync</code> are installed.</span>
  </p></div><div class="sect2" id="id-1.3.6.14.5.10"><div class="titlepage"><div><div><h3 class="title"><span class="number">31.4.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Set up passwordless SSH access</span> <a title="Permalink" class="permalink" href="#id-1.3.6.14.5.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-sles-provisioning_sles.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-sles-provisioning_sles.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Once you have started your installation using the Cloud Lifecycle Manager, or if
   you are adding a SLES node to an existing cloud, you need to copy the
   Cloud Lifecycle Manager public key to the SLES node. One way of doing this is to
   copy the <code class="literal">/home/ardana/.ssh/authorized_keys</code> from another
   node in the cloud to the same location on the SLES node. If you are
   installing a new cloud, this file will be available on the nodes after
   running the <code class="literal">bm-reimage.yml</code> playbook.
  </p><div id="id-1.3.6.14.5.10.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    Ensure that there is global read access to the file
    <code class="filename">/home/ardana/.ssh/authorized_keys</code>.
   </p></div><p>
   Now test passwordless SSH from the deployer and check your ability to
   remotely execute sudo commands:
  </p><div class="verbatim-wrap"><pre class="screen">ssh ardana@<em class="replaceable ">IP_OF_SLES_NODE</em> "sudo tail -5 /var/log/messages"</pre></div></div></div></div><div class="chapter " id="install-ardana-manila"><div class="titlepage"><div><div><h2 class="title"><span class="number">32 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing manila and Creating manila Shares</span> <a title="Permalink" class="permalink" href="#install-ardana-manila">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-ardana-manila.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-ardana-manila.xml</li><li><span class="ds-label">ID: </span>install-ardana-manila</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#id-1.3.6.15.2"><span class="number">32.1 </span><span class="name">Installing manila</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.15.3"><span class="number">32.2 </span><span class="name">Adding manila to an Existing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Environment</span></a></span></dt><dt><span class="section"><a href="#configure-manila-backend"><span class="number">32.3 </span><span class="name">Configure manila Backend</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.15.5"><span class="number">32.4 </span><span class="name">Creating manila Shares</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.15.6"><span class="number">32.5 </span><span class="name">Troubleshooting</span></a></span></dt></dl></div></div><div class="sect1" id="id-1.3.6.15.2"><div class="titlepage"><div><div><h2 class="title"><span class="number">32.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing manila</span> <a title="Permalink" class="permalink" href="#id-1.3.6.15.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-ardana-manila.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-ardana-manila.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The <span class="productname">OpenStack</span> Shared File Systems service (manila) provides file storage
   to a virtual machine. The Shared File Systems service provides a storage
   provisioning control plane for shared or distributed file systems. The
   service enables management of share types and share snapshots if you have a
   driver that supports them.
  </p><p>
   The manila service consists of the following components:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     manila-api
    </p></li><li class="listitem "><p>
     manila-data
    </p></li><li class="listitem "><p>
     manila-scheduler
    </p></li><li class="listitem "><p>
     manila-share
    </p></li><li class="listitem "><p>
     messaging queue
    </p></li></ul></div><p>
   These manila components are included in example <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> models
   based on nova KVM, such as <code class="literal">entry-scale-kvm</code>,
   <code class="literal">entry-scale-kvm-mml</code>, and
   <code class="literal">mid-scale-kvm</code>. General installation instructions are
   available at <a class="xref" href="#install-kvm" title="Chapter 24. Installing Mid-scale and Entry-scale KVM">Chapter 24, <em>Installing Mid-scale and Entry-scale KVM</em></a>.
  </p><p>
   If you modify one of these cloud models to set up a dedicated Cloud Lifecycle Manager, add
   <code class="literal">manila-client</code> item to the list of service components for
   the Cloud Lifecycle Manager cluster.
  </p><p>
   The following steps install manila if it is not already present in your
   cloud data model:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Apply manila changes in <code class="filename">control_plane.yml</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd /var/lib/ardana/openstack/my_cloud/definition/data/</pre></div><p>
     Add <code class="literal">manila-client</code> to the list of service components for
     Cloud Lifecycle Manager, and <code class="literal">manila-api</code> to the Control Node.
    </p></li><li class="step "><p>
     Run the Configuration Processor.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "manila config"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Deploy manila
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts percona-deploy.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts manila-deploy.yml</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-gen-hosts-file.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts clients-deploy.yml</pre></div><p>
     If manila has already been installed and is being reconfigured, run
     the following for the changes to take effect:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts manila-stop.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts manila-start.yml</pre></div></li><li class="step "><p>
     Verify the manila installation
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd
<code class="prompt user">ardana &gt; </code>. manila.osrc
<code class="prompt user">ardana &gt; </code>. service.osrc
<code class="prompt user">ardana &gt; </code>manila api-version
<code class="prompt user">ardana &gt; </code>manila service-list</pre></div><p>
     The manila CLI can be run from Cloud Lifecycle Manager or controller nodes.
    </p></li></ol></div></div><div id="id-1.3.6.15.2.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    The <code class="literal">manila-share</code> service component is not started by the
    <code class="filename">manila-deploy.yml</code> playbook when run under default
    conditions. This component requires that a valid backend be configured,
    which is described in <a class="xref" href="#configure-manila-backend" title="32.3. Configure manila Backend">Section 32.3, “Configure manila Backend”</a>.
   </p></div></div><div class="sect1" id="id-1.3.6.15.3"><div class="titlepage"><div><div><h2 class="title"><span class="number">32.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding manila to an Existing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Environment</span> <a title="Permalink" class="permalink" href="#id-1.3.6.15.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-ardana-manila.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-ardana-manila.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Add manila to an existing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 installation or as part of an
   upgrade with the following steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Add the items listed below to the list of service components in
     <code class="filename">~/openstack/my_cloud/definition/data/control_plane.yml</code>.
     Add them to clusters that have <code class="literal">server-role</code> set to
     <code class="literal">CONTROLLER-ROLE</code> (applies to entry-scale models).
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       manila-client
      </p></li><li class="listitem "><p>
       manila-api
      </p></li></ul></div></li><li class="step "><p>
     If your environment uses a dedicated Cloud Lifecycle Manager, add
     <code class="literal">magnum-client</code> to the list of service components for the
     Cloud Lifecycle Manager in
     <code class="filename">~/openstack/my_cloud/definition/data/control_plane.yml</code>.
    </p></li><li class="step "><p>
     Commit your configuration to the local git repo.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     Run the configuration processor.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Update deployment directory.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Deploy manila.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts percona-deploy.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts manila-deploy.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-gen-hosts-file.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts clients-deploy.yml</pre></div></li><li class="step "><p>
     Verify the manila installation.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd
<code class="prompt user">ardana &gt; </code>. manila.osrc
<code class="prompt user">ardana &gt; </code>. service.osrc
<code class="prompt user">ardana &gt; </code>manila api-version
<code class="prompt user">ardana &gt; </code>manila service-list</pre></div></li></ol></div></div><p>
   The manila CLI can be run from the Cloud Lifecycle Manager or controller nodes.
  </p><p>
   Proceed to <a class="xref" href="#configure-manila-backend" title="32.3. Configure manila Backend">Section 32.3, “Configure manila Backend”</a>.
  </p></div><div class="sect1" id="configure-manila-backend"><div class="titlepage"><div><div><h2 class="title"><span class="number">32.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure manila Backend</span> <a title="Permalink" class="permalink" href="#configure-manila-backend">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-ardana-manila.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-ardana-manila.xml</li><li><span class="ds-label">ID: </span>configure-manila-backend</li></ul></div></div></div></div><div class="sect2" id="id-1.3.6.15.4.2"><div class="titlepage"><div><div><h3 class="title"><span class="number">32.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure NetaApp manila Back-end</span> <a title="Permalink" class="permalink" href="#id-1.3.6.15.4.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-ardana-manila.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-ardana-manila.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div id="id-1.3.6.15.4.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     An account with cluster administrator privileges must be used with the
     <code class="literal">netapp_login</code> option when using Share Server management.
     Share Server management creates Storage Virtual Machines (SVM), thus SVM
     administrator privileges are insufficient.
    </p><p>
     There are two modes for the NetApp manila back-end:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <code class="literal">driver_handles_share_servers = True</code>
      </p></li><li class="listitem "><p>
       <code class="literal">driver_handles_share_servers = False</code> This value must
       be set to <code class="literal">False</code> if you want the driver to operate
       without managing share servers.
      </p></li></ul></div><p>
     More information is available from
     <a class="link" href="https://netapp-openstack-dev.github.io/openstack-docs/rocky/manila/configuration/manila_config_files/section_unified-driver-without-share-server.html" target="_blank">NetApp
     OpenStack</a>
    </p></div><p>
    The steps to configure a NetApp manila back-end are:
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Configure a back-end in the manila configuration file, following the
      directions and comments in the file.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud
<code class="prompt user">ardana &gt; </code>vi config/manila/manila.conf.j2</pre></div></li><li class="step "><p>
      Commit your configuration to the local Git repo.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
      Run the configuration processor.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
      Update deployment directory.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
      Run reconfiguration playbook.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts manila-reconfigure.yml</pre></div></li><li class="step "><p>
      Restart manila services.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts manila-stop.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts manila-start.yml</pre></div></li></ol></div></div><div id="id-1.3.6.15.4.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     After the <code class="literal">manila-share</code> service has been initialized
     with a backend, it can be controlled independently of
     <code class="literal">manila-api</code> by using the playbooks
     <code class="filename">manila-share-start.yml</code>,
     <code class="filename">manila-share-stop.yml</code>, and
     <code class="filename">manila-share-status.yml</code>.
    </p></div></div><div class="sect2" id="id-1.3.6.15.4.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">32.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure CephFS manila Backend</span> <a title="Permalink" class="permalink" href="#id-1.3.6.15.4.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-ardana-manila.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-ardana-manila.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    Configure a back-end in the manila configuration file,
    <code class="filename">~/openstack/my_cloud vi config/manila/manila.conf.j2</code>.
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      To define a CephFS native back-end, create a section like the following:
     </p><div class="verbatim-wrap"><pre class="screen">[cephfsnative1]
driver_handles_share_servers = False
share_backend_name = CEPHFSNATIVE1
share_driver = manila.share.drivers.cephfs.driver.CephFSDriver
cephfs_conf_path = /etc/ceph/ceph.conf
cephfs_protocol_helper_type = CEPHFS
cephfs_auth_id = manila
cephfs_cluster_name = ceph
cephfs_enable_snapshots = False</pre></div></li><li class="step "><p>
      Add CephFS to <code class="literal">enabled_share_protocols</code>:
     </p><div class="verbatim-wrap"><pre class="screen">enabled_share_protocols = NFS,CIFS,CEPHFS</pre></div></li><li class="step "><p>
      Edit the <code class="literal">enabled_share_backends</code> option in the
      <code class="literal">DEFAULT</code> section to point to the driver’s back-end
      section name.
     </p></li><li class="step "><p>
      According to the environment, modify back-end specific lines in
      <code class="filename">~/openstack/my_cloud vi
      config/manila/manila.conf.j2</code>.
     </p></li><li class="step "><p>
      Commit your configuration to the local Git repo.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
      Run the configuration processor.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
      Update deployment directory.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
      Run reconfiguration playbook.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts manila-reconfigure.yml</pre></div></li><li class="step "><p>
      Restart manila services.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts manila-stop.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts manila-start.yml</pre></div></li></ol></div></div><div id="id-1.3.6.15.4.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     After the <code class="literal">manila-share</code> service has been initialized
     with a back-end, it can be controlled independently of
     <code class="literal">manila-api</code> by using the playbooks
     <code class="filename">manila-share-start.yml</code>,
     <code class="filename">manila-share-stop.yml</code>, and
     <code class="filename">manila-share-status.yml</code>.
    </p></div><p>
    For more details of the CephFS manila back-end, see
    <a class="link" href="https://docs.openstack.org/manila/rocky/admin/cephfs_driver.html" target="_blank">OpenStack
    CephFS driver</a>.
   </p></div></div><div class="sect1" id="id-1.3.6.15.5"><div class="titlepage"><div><div><h2 class="title"><span class="number">32.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating manila Shares</span> <a title="Permalink" class="permalink" href="#id-1.3.6.15.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-ardana-manila.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-ardana-manila.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   manila can support two modes, with and without the handling of share
   servers. The mode depends on driver support.
  </p><p>
   Mode 1: The back-end is a generic driver,
   <code class="literal">driver_handles_share_servers = False</code> (DHSS is disabled).
   The following example creates a VM using manila share image.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>wget http://tarballs.openstack.org/manila-image-elements/images/manila-service-image-master.qcow2
<code class="prompt user">ardana &gt; </code>. service.osrc;openstack image create --name
"manila-service-image-new" \
--file manila-service-image-master.qcow2 --disk-format qcow2 \
--container-format bare --visibility public --progress
<code class="prompt user">ardana &gt; </code>openstack image list (verify manila image)
<code class="prompt user">ardana &gt; </code>openstack security group create manila-security-group \
--description "Allows web and NFS traffic to manila server."
<code class="prompt user">ardana &gt; </code>openstack security group rule create manila-security-group \
--protocol tcp --dst-port 2049
<code class="prompt user">ardana &gt; </code>openstack security group rule create manila-security-group \
--protocol udp --dst-port 2049
<code class="prompt user">ardana &gt; </code>openstack security group rule create manila-security-group \
--protocol tcp --dst-port 22
<code class="prompt user">ardana &gt; </code>openstack security group rule create manila-security-group \
--protocol icmp
<code class="prompt user">ardana &gt; </code>openstack security group rule list manila-security-group (verify manila security group)
<code class="prompt user">ardana &gt; </code>openstack keypair create --public-key ~/.ssh/id_rsa.pub mykey
<code class="prompt user">ardana &gt; </code>openstack network create n1
<code class="prompt user">ardana &gt; </code>openstack subnet create s1 --network n1 <em class="replaceable ">--subnet-range 11.11.11.0/24</em>
<code class="prompt user">ardana &gt; </code>openstack router create r1
<code class="prompt user">ardana &gt; </code>openstack router add subnet r1 s1
<code class="prompt user">ardana &gt; </code>openstack router set r1 ext-net
<code class="prompt user">ardana &gt; </code>openstack network list
<code class="prompt user">ardana &gt; </code>openstack server create manila-vm --flavor m1.small \
--image IMAGE_ID --nic net-id=N1_ID --security-group manila-security-group \
--key-name myKey
<code class="prompt user">ardana &gt; </code>oenstack floating ip create <em class="replaceable ">EXT-NET_ID</em>
<code class="prompt user">ardana &gt; </code>openstack server add floating ip manila-vm <em class="replaceable ">EXT-NET_ID</em></pre></div></li><li class="step "><p>
     Validate your ability to ping or connect by SSH to
     <code class="literal">manila-vm</code> with credentials
     <code class="literal">manila/manila</code>.
    </p></li><li class="step "><p>
     Modify the configuration:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>vi
    /etc/manila/manila.conf.d/100-manila.conf</pre></div><p>
     Make changes in [generic1] section
    </p><div class="verbatim-wrap"><pre class="screen">service_instance_name_or_id = <em class="replaceable ">MANILA_VM_ID</em>
service_net_name_or_ip = <em class="replaceable ">MANILA_VM_FLOATING_IP</em>
tenant_net_name_or_ip = <em class="replaceable ">MANILA_VM_FLOATING_IP</em></pre></div></li><li class="step "><p>
     Create a share type. OpenStack docs has
     <a class="link" href="https://docs.openstack.org/manila/rocky/install/post-install.html" target="_blank">detailed
     instructions</a>. Use the instructions for <code class="literal">manila type-create
     default_share_type False</code>
    </p></li><li class="step "><p>
     Restart manila services and verify they are up.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>systemctl restart openstack-manila-api \
openstack-manila-share openstack-manila-scheduler
<code class="prompt user">ardana &gt; </code>manila service-list</pre></div></li><li class="step "><p>
     Continue creating a share
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>manila create NFS 1 --name <em class="replaceable ">SHARE</em>
<code class="prompt user">ardana &gt; </code>manila list (status will change from  creating to available)
<code class="prompt user">ardana &gt; </code>manila show share1
<code class="prompt user">ardana &gt; </code>manila access-allow <em class="replaceable ">SHARE</em> ip <em class="replaceable ">INSTANCE_IP</em></pre></div></li><li class="step "><p>
     Mount the share on a Compute instance
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>mkdir ~/test_directory
<code class="prompt user">tux &gt; </code>sudo mount -vt nfs <em class="replaceable ">EXT-NET_ID</em>:/shares/<em class="replaceable ">SHARE-SHARE-ID</em> ~/test_folder</pre></div></li></ol></div></div><p>
   Mode 2: The back-end is <code class="literal">NetApp</code>,
   <code class="literal">driver_handles_share_servers = True</code> (DHSS is enabled).
   Procedure for driver_handles_share_servers = False is similar to Mode 1.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Modify the configuration
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>vi /etc/manila/manila.conf.d/100-manila.conf</pre></div><p>
     Add a <code class="literal">backendNetApp</code> section
    </p><div class="verbatim-wrap"><pre class="screen">share_driver = manila.share.drivers.netapp.common.NetAppDriver
driver_handles_share_servers = True
share_backend_name=backendNetApp
netapp_login=<em class="replaceable ">NetApp_USERNAME</em>
netapp_password=<em class="replaceable ">NetApp_PASSWORD</em>
netapp_server_hostname=<em class="replaceable ">NETAPP_HOSTNAME</em>
netapp_root_volume_aggregate=<em class="replaceable ">AGGREGATE_NAME</em></pre></div><p>
     Add to [DEFAULT] section
    </p><div class="verbatim-wrap"><pre class="screen">enabled_share_backends = backendNetApp
default_share_type = default1</pre></div></li><li class="step "><p>
     Create a manila share image and verify it
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>wget http://tarballs.openstack.org/manila-image-elements/images/manila-service-image-master.qcow2
<code class="prompt user">ardana &gt; </code>. service.osrc;openstack image create <code class="literal">manila-service-image-new</code> \
--file manila-service-image-master.qcow2 --disk-format qcow2 \
--container-format bare --visibility public --progress
<code class="prompt user">ardana &gt; </code>openstack image list (verify a manila image)</pre></div></li><li class="step "><p>
     Create a share type. OpenStack docs has
     <a class="link" href="https://docs.openstack.org/manila/rocky/install/post-install.html" target="_blank">detailed
     instructions</a>. Use the instructions for <code class="literal">manila type-create
     default_share_type True</code> .
    </p></li><li class="step "><p>
     Restart services
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>systemctl restart openstack-manila-api openstack-manila-share \openstack-manila-scheduler
<code class="prompt user">ardana &gt; </code>manila service-list (verify services are up)</pre></div></li><li class="step "><p>
     Continue creating a share. <code class="literal">OCTAVIA-MGMT-NET</code> can be used
     as <em class="replaceable ">PRIVATE_NETWORK</em> in this example.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>manila share-network-create --name demo-share-network1 \
--neutron-net-id PRIVATE_NETWORK_ID --neutron-subnet-id PRIVATE_NETWORK_SUBNET_ID
<code class="prompt user">ardana &gt; </code>manila create NFS 1 --name share2 --share-network demo-share-network1</pre></div></li></ol></div></div></div><div class="sect1" id="id-1.3.6.15.6"><div class="titlepage"><div><div><h2 class="title"><span class="number">32.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting</span> <a title="Permalink" class="permalink" href="#id-1.3.6.15.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-ardana-manila.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-ardana-manila.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If manila-list shows share status in error, use <code class="literal">storage aggregate
   show</code> to list available aggregates. Errors may be found in
   <code class="filename">/var/log/manila/manila-share.log</code>
  </p><p>
   if the compute nodes do not have access to manila back-end server, use
   the <code class="literal">manila-share</code> service on controller nodes instead. You
   can do so by either running <code class="literal">sudo systemctl stop
   openstack-manila-share</code> on compute to turn off its share service or
   skipping adding "manila-share" to compute hosts in the input-model
   (<code class="filename">control_plane.yml</code> in
   <code class="filename">/var/lib/ardana/openstack/my_cloud/definition/data</code>).
  </p></div></div><div class="chapter " id="install-heat-templates"><div class="titlepage"><div><div><h2 class="title"><span class="number">33 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing SUSE CaaS Platform heat Templates</span> <a title="Permalink" class="permalink" href="#install-heat-templates">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/install_caasp_heat_templates.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>install_caasp_heat_templates.xml</li><li><span class="ds-label">ID: </span>install-heat-templates</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#sec-heat-templates-install"><span class="number">33.1 </span><span class="name">SUSE CaaS Platform heat Installation Procedure</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.16.4"><span class="number">33.2 </span><span class="name">Installing SUSE CaaS Platform with Multiple Masters</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.16.5"><span class="number">33.3 </span><span class="name">Deploy SUSE CaaS Platform Stack Using heat SUSE CaaS Platform Playbook</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.16.6"><span class="number">33.4 </span><span class="name">Deploy SUSE CaaS Platform Cluster with Multiple Masters Using heat CaasP
  Playbook</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.16.7"><span class="number">33.5 </span><span class="name">SUSE CaaS Platform <span class="productname">OpenStack</span> Image for heat SUSE CaaS Platform Playbook</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.16.8"><span class="number">33.6 </span><span class="name">Enabling the Cloud Provider Integration (CPI) Feature</span></a></span></dt><dt><span class="section"><a href="#sec-heat-templates-register"><span class="number">33.7 </span><span class="name">Register SUSE CaaS Platform Cluster for Software Updates</span></a></span></dt></dl></div></div><p>
  This chapter describes how to install SUSE CaaS Platform v3 using heat template on
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p><div class="sect1" id="sec-heat-templates-install"><div class="titlepage"><div><div><h2 class="title"><span class="number">33.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE CaaS Platform heat Installation Procedure</span> <a title="Permalink" class="permalink" href="#sec-heat-templates-install">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/install_caasp_heat_templates.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>install_caasp_heat_templates.xml</li><li><span class="ds-label">ID: </span>sec-heat-templates-install</li></ul></div></div></div></div><div class="procedure " id="id-1.3.6.16.3.2"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 33.1: </span><span class="name">Preparation </span><a title="Permalink" class="permalink" href="#id-1.3.6.16.3.2">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Download the latest SUSE CaaS Platform for <span class="productname">OpenStack</span> image (for example,
     <code class="filename">SUSE-CaaS-Platform-3.0-OpenStack-Cloud.x86_64-1.0.0-GM.qcow2</code>)
     from <a class="link" href="https://download.suse.com" target="_blank">https://download.suse.com</a>.
    </p></li><li class="step "><p>
     Upload the image to glance:
    </p><div class="verbatim-wrap"><pre class="screen">openstack image create --public --disk-format qcow2 --container-format \
bare --file SUSE-CaaS-Platform-3.0-OpenStack-Cloud.x86_64-1.0.0-GM.qcow2 \
CaaSP-3</pre></div></li><li class="step "><p>
     Install the <span class="package ">caasp-openstack-heat-templates</span> package on a
     machine with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> repositories:
    </p><div class="verbatim-wrap"><pre class="screen">zypper in caasp-openstack-heat-templates</pre></div><p>
     The installed templates are located in
     <code class="filename">/usr/share/caasp-openstack-heat-templates</code>.
    </p><p>
     Alternatively, you can get official heat templates by cloning the
     appropriate Git repository:
    </p><div class="verbatim-wrap"><pre class="screen">git clone https://github.com/SUSE/caasp-openstack-heat-templates</pre></div></li></ol></div></div><div class="procedure " id="id-1.3.6.16.3.3"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 33.2: </span><span class="name">Installing Templates via horizon </span><a title="Permalink" class="permalink" href="#id-1.3.6.16.3.3">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     In horizon, go to
     <span class="guimenu ">Project</span> › <span class="guimenu ">Stacks</span> › <span class="guimenu ">Launch
     Stack</span>.
    </p></li><li class="step "><p>
     Select <span class="guimenu ">File</span> from the <span class="guimenu ">Template Source</span>
     drop-down box and upload the <code class="filename">caasp-stack.yaml</code> file.
    </p></li><li class="step "><p>
     In the <span class="guimenu ">Launch Stack</span> dialog, provide the required
     information (stack name, password, flavor size, external network of your
     environment, etc.).
    </p></li><li class="step "><p>
     Click <span class="guimenu ">Launch</span> to launch the stack. This creates all
     required resources for running SUSE CaaS Platform in an <span class="productname">OpenStack</span> environment. The
     stack creates one Admin Node, one Master Node, and server worker nodes as
     specified.
    </p></li></ol></div></div><div class="procedure " id="id-1.3.6.16.3.4"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 33.3: </span><span class="name">Install Templates from the Command Line </span><a title="Permalink" class="permalink" href="#id-1.3.6.16.3.4">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Specify the appropriate flavor and network settings in the
     <code class="filename">caasp-environment.yaml</code> file.
    </p></li><li class="step "><p>
     Create a stack in heat by passing the template, environment file, and
     parameters:
    </p><div class="verbatim-wrap"><pre class="screen">openstack stack create -t caasp-stack.yaml -e caasp-environment.yaml \
--parameter image=CaaSP-3 caasp-stack</pre></div></li></ol></div></div><div class="procedure " id="id-1.3.6.16.3.5"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 33.4: </span><span class="name">Accessing Velum SUSE CaaS Platform dashboard </span><a title="Permalink" class="permalink" href="#id-1.3.6.16.3.5">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     After the stack has been created, the Velum SUSE CaaS Platform dashboard runs on the Admin Node.
     You can access it using the Admin Node's floating IP address.
    </p></li><li class="step "><p>
     Create an account and follow the steps in the Velum SUSE CaaS Platform dashboard to complete the
     SUSE CaaS Platform installation.
    </p></li></ol></div></div><p>
   When you have successfully accessed the admin node web interface via the
   floating IP, follow the instructions at <a class="link" href="https://documentation.suse.com/suse-caasp/4.1/single-html/caasp-deployment/" target="_blank">https://documentation.suse.com/suse-caasp/4.1/single-html/caasp-deployment/</a> to
   continue the setup of SUSE CaaS Platform.
  </p></div><div class="sect1" id="id-1.3.6.16.4"><div class="titlepage"><div><div><h2 class="title"><span class="number">33.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing SUSE CaaS Platform with Multiple Masters</span> <a title="Permalink" class="permalink" href="#id-1.3.6.16.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/install_caasp_heat_templates.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>install_caasp_heat_templates.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div id="id-1.3.6.16.4.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    A heat stack with load balancing and multiple master nodes can only be
    created from the command line, because horizon does not have support for nested
    heat templates.
   </p></div><p>
   Install the <span class="package ">caasp-openstack-heat-templates</span> package on a
   machine with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> repositories:
  </p><div class="verbatim-wrap"><pre class="screen">zypper in caasp-openstack-heat-templates</pre></div><p>
   The installed templates are located in
   <code class="filename">/usr/share/caasp-openstack-heat-templates</code>.
  </p><p>
   A working load balancer is needed in your SUSE <span class="productname">OpenStack</span> Cloud deployment. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   uses HAProxy.
  </p><p>
   Verify that load balancing with HAProxy is working correctly
   in your <span class="productname">OpenStack</span> installation by creating a load balancer manually and
   checking that the <code class="literal">provisioning_status</code> changes to
   <code class="literal">Active</code>.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack loadbalancer show
&lt;<em class="replaceable ">LOAD_BALANCER_ID</em>&gt;</pre></div><p>
   HAProxy is the default load balancer provider in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  </p><p>
   The steps below can be used to set up a network, subnet, router, security
   and IPs for a test <code class="literal">lb_net1</code> network with
   <code class="literal">lb_subnet1</code> subnet.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network create lb_net1
  <code class="prompt user">ardana &gt; </code>openstack subnet create --name lb_subnet1 lb_net1 \
--subnet-range 172.29.0.0/24 --gateway 172.29.0.2
<code class="prompt user">ardana &gt; </code>openstack router create lb_router1
<code class="prompt user">ardana &gt; </code>openstack router add subnet lb_router1 lb_subnet1
<code class="prompt user">ardana &gt; </code>openstack router set lb_router1 --external-gateway ext-net
<code class="prompt user">ardana &gt; </code>openstack network list</pre></div><div class="procedure " id="id-1.3.6.16.4.12"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 33.5: </span><span class="name">Steps to Install SUSE CaaS Platform with Multiple Masters </span><a title="Permalink" class="permalink" href="#id-1.3.6.16.4.12">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Specify the appropriate flavor and network settings in the
     <code class="filename">caasp-multi-master-environment.yaml</code> file.
    </p></li><li class="step "><p>
     Set <code class="literal">master_count</code> to the desired number in the
     <code class="filename">caasp-multi-master-environment.yaml</code> file. The master
     count must be set to an odd number of nodes.
    </p><div class="verbatim-wrap"><pre class="screen">master_count: 3</pre></div></li><li class="step "><p>
     Create a stack in heat by passing the template, environment file, and
     parameters:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack stack create -t caasp-multi-master-stack.yaml \
-e caasp-multi-master-environment.yaml --parameter image=CaaSP-3 caasp-multi-master-stack</pre></div></li><li class="step "><p>
     Find the floating IP address of the load balancer. This is necessary for
     accessing the Velum SUSE CaaS Platform dashboard.
    </p><ol type="a" class="substeps "><li class="step "><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack loadbalancer list --provider</pre></div></li><li class="step "><p>
       From the output, copy the <code class="literal">id</code> and enter it in the
       following command as shown in the following example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack loadbalancer show id</pre></div><div class="verbatim-wrap"><pre class="screen">+---------------------+------------------------------------------------+
| Field               | Value                                          |
+---------------------+------------------------------------------------+
| admin_state_up      | True                                           |
| description         |                                                |
| id                  | 0d973d80-1c79-40a4-881b-42d111ee9625           |
| listeners           | {"id": "c9a34b63-a1c8-4a57-be22-75264769132d"} |
|                     | {"id": "4fa2dae0-126b-4eb0-899f-b2b6f5aab461"} |
| name                | caasp-stack-master_lb-bhr66gtrx3ue             |
| operating_status    | ONLINE                                         |
| pools               | {"id": "8c011309-150c-4252-bb04-6550920e0059"} |
|                     | {"id": "c5f55af7-0a25-4dfa-a088-79e548041929"} |
| provider            | haproxy                                        |
| provisioning_status | ACTIVE                                         |
| tenant_id           | fd7ffc07400642b1b05dbef647deb4c1               |
| vip_address         | 172.28.0.6                                     |
| vip_port_id         | 53ad27ba-1ae0-4cd7-b798-c96b53373e8b           |
| vip_subnet_id       | 87d18a53-ad0c-4d71-b82a-050c229b710a           |
+---------------------+------------------------------------------------+</pre></div></li><li class="step "><p>
       Search the floating IP list for <code class="literal">vip_address</code>
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack floating ip list | grep 172.28.0.6
| d636f3...481b0c | fd7ff...deb4c1 | 172.28.0.6  | 10.84.65.37  | 53ad2...373e8b |</pre></div></li><li class="step "><p>
       The load balancer floating ip address is 10.84.65.37
      </p></li></ol></li></ol></div></div><p>
   <span class="bold"><strong>Accessing the Velum SUSE CaaS Platform Dashboard</strong></span>
  </p><p>
   After the stack has been created, the Velum SUSE CaaS Platform dashboard runs on the
   admin node. You can access it using the floating IP address of the admin
   node.
  </p><p>
   Create an account and follow the steps in the Velum SUSE CaaS Platform dashboard to
   complete the SUSE CaaS Platform installation.
  </p><p>
   SUSE CaaS Platform Admin Node Install: Screen 1
  </p><p>
   If you plan to manage your containers using Helm or Airship (this is common),
   check the box labeled <code class="literal">Install Tiller (Helm's server component)</code>.
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/caasp_1.png" target="_blank"><img src="images/caasp_1.png" width="" /></a></div></div><p>
   SUSE CaaS Platform Admin Node Install: Screen 2
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/caasp_2.png" target="_blank"><img src="images/caasp_2.png" width="" /></a></div></div><p>
   SUSE CaaS Platform Admin Node Install: Screen 3
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/caasp_3.png" target="_blank"><img src="images/caasp_3.png" width="" /></a></div></div><p>
   SUSE CaaS Platform Admin Node Install: Screen 4
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/caasp_4.png" target="_blank"><img src="images/caasp_4.png" width="" /></a></div></div><p>
   SUSE CaaS Platform Admin Node Install: Screen 5
  </p><p>
   Set External Kubernetes API to
   <em class="replaceable ">LOADBALANCER_FLOATING_IP</em>, External Dashboard FQDN
   to <em class="replaceable ">ADMIN_NODE_FLOATING_IP</em>
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/caasp_5.png" target="_blank"><img src="images/caasp_5.png" width="" /></a></div></div><p>
   SUSE CaaS Platform Admin Node Install: Screen 6
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/caasp_6.png" target="_blank"><img src="images/caasp_6.png" width="" /></a></div></div><p>
   SUSE CaaS Platform Admin Node Install: Screen 7
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/caasp_7.png" target="_blank"><img src="images/caasp_7.png" width="" /></a></div></div></div><div class="sect1" id="id-1.3.6.16.5"><div class="titlepage"><div><div><h2 class="title"><span class="number">33.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploy SUSE CaaS Platform Stack Using heat SUSE CaaS Platform Playbook</span> <a title="Permalink" class="permalink" href="#id-1.3.6.16.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/install_caasp_heat_templates.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>install_caasp_heat_templates.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Install the <span class="package ">caasp-openstack-heat-templates</span> package on a
     machine with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> repositories:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>zypper in caasp-openstack-heat-templates</pre></div><p>
     The installed templates are located in
     <code class="filename">/usr/share/caasp-openstack-heat-templates</code>.
    </p></li><li class="step "><p>
     Run <code class="filename">heat-caasp-deploy.yml</code> on the Cloud Lifecycle Manager to create a
     SUSE CaaS Platform cluster with heat templates from the
     <code class="literal">caasp-openstack-heat-templates</code> package.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost heat-caasp-deploy.yml</pre></div></li><li class="step "><p>
     In a browser, navigate to the horizon UI to determine the floating IP
     address assigned to the admin node.
    </p></li><li class="step "><p>
     Go to http://<em class="replaceable ">ADMIN-NODE-FLOATING-IP</em>/ to bring
     up the Velum dashboard.
    </p></li><li class="step "><p>
     Complete the web-based bootstrap process to bring up the SUSE CaaS Platform
     Kubernetes cluster. Refer to the SUSE CaaS Platform documentation for specifics.</p><p>
     When prompted, set the <code class="literal">Internal Dashboard Location</code> to the
     private network IP address (i.e. in the the 172.x.x.x subnet) of the SUSE CaaS Platform
     admin node created during the <code class="filename">heat-caasp-deploy.yml</code>
     playbook. Do not use the floating IP address which is also associated with
     the node.
    </p></li></ol></div></div></div><div class="sect1" id="id-1.3.6.16.6"><div class="titlepage"><div><div><h2 class="title"><span class="number">33.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploy SUSE CaaS Platform Cluster with Multiple Masters Using heat CaasP
  Playbook</span> <a title="Permalink" class="permalink" href="#id-1.3.6.16.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/install_caasp_heat_templates.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>install_caasp_heat_templates.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Install the <span class="package ">caasp-openstack-heat-templates</span> package on a
     machine with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> repositories:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>zypper in caasp-openstack-heat-templates</pre></div><p>
     The installed templates are located in
     <code class="filename">/usr/share/caasp-openstack-heat-templates</code>.
    </p></li><li class="step "><p>
     On the Cloud Lifecycle Manager, run the <code class="filename">heat-caasp-deploy.yml</code> playbook
     and pass parameters for <code class="literal">caasp_stack_name</code>,
     <code class="literal">caasp_stack_yaml_file</code> and
     <code class="literal">caasp_stack_env_yaml_file</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost heat-caasp-deploy.yml -e "caasp_stack_name=caasp_multi-master caasp_stack_yaml_file=caasp-multi-master-stack.yaml caasp_stack_env_yaml_file=caasp-multi-master-environment.yaml"</pre></div></li></ol></div></div></div><div class="sect1" id="id-1.3.6.16.7"><div class="titlepage"><div><div><h2 class="title"><span class="number">33.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE CaaS Platform <span class="productname">OpenStack</span> Image for heat SUSE CaaS Platform Playbook</span> <a title="Permalink" class="permalink" href="#id-1.3.6.16.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/install_caasp_heat_templates.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>install_caasp_heat_templates.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   By default the heat SUSE CaaS Platform playbook downloads the SUSE CaaS Platform image from <a class="link" href="http://download.suse.de/install/SUSE-CaaSP-3-GM/SUSE-CaaS-Platform-3.0-for-OpenStack-Cloud.x86_64-3.0.0-GM.qcow2" target="_blank">http://download.suse.de/install/SUSE-CaaSP-3-GM/SUSE-CaaS-Platform-3.0-for-OpenStack-Cloud.x86_64-3.0.0-GM.qcow2</a>. If
   this URL is not accessible, the SUSE CaaS Platform image can be downloaded from <a class="link" href="https://download.suse.com/Download?buildid=z7ezhywXXRc" target="_blank">https://download.suse.com/Download?buildid=z7ezhywXXRc</a> and
   copied to the deployer.
  </p><p>
   To create a SUSE CaaS Platform cluster and pass the path to the downloaded image, run the
   following commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost heat-caasp-deploy.yml -e "caasp_image_tmp_path=~/SUSE-CaaS-Platform-3.0-for-OpenStack-Cloud.x86_64-3.0.0-GM.qcow2"</pre></div><p>
   To create a SUSE CaaS Platform cluster with multiple masters and pass the path to the
   downloaded image, run the following commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost heat-caasp-deploy.yml -e "caasp_image_tmp_path=caasp_image_tmp_path=~/SUSE-CaaS-Platform-3.0-for-OpenStack-Cloud.x86_64-3.0.0-GM.qcow2
 caasp_stack_name=caasp_multi-master caasp_stack_yaml_file=caasp-multi-master-stack.yaml caasp_stack_env_yaml_file=caasp-multi-master-environment.yaml"</pre></div></div><div class="sect1" id="id-1.3.6.16.8"><div class="titlepage"><div><div><h2 class="title"><span class="number">33.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling the Cloud Provider Integration (CPI) Feature</span> <a title="Permalink" class="permalink" href="#id-1.3.6.16.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/install_caasp_heat_templates.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>install_caasp_heat_templates.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   When deploying a CaaaSP cluster using SUSE CaaS Platform <span class="productname">OpenStack</span> heat
   templates, the following CPI parameters can be set in
   <code class="filename">caasp-environment.yaml</code> or
   <code class="filename">caasp-multi-master-environment.yaml</code>.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.6.16.8.3.1"><span class="term ">cpi_auth_url</span></dt><dd><p>
      The URL of the keystone API used to authenticate the user. This value
      can be found on <span class="productname">OpenStack</span> Dashboard under
      <span class="guimenu ">Access and Security</span> › <span class="guimenu ">API
      Access</span> › <span class="guimenu ">Credentials</span> (for
      example, https://api.keystone.example.net:5000/)
     </p></dd><dt id="id-1.3.6.16.8.3.2"><span class="term ">cpi_domain_name</span></dt><dd><p>
      Name of the domain the user belongs to.
     </p></dd><dt id="id-1.3.6.16.8.3.3"><span class="term ">cpi_tenant_name</span></dt><dd><p>
      Name of the project the user belongs to. This is the project in which
      SUSE CaaS Platform resources are created.
     </p></dd><dt id="id-1.3.6.16.8.3.4"><span class="term ">cpi_region</span></dt><dd><p>
      Name of the region to use when running a multi-region <span class="productname">OpenStack</span>
      cloud. The region is a general division of an <span class="productname">OpenStack</span> deployment.
     </p></dd><dt id="id-1.3.6.16.8.3.5"><span class="term ">cpi_username</span></dt><dd><p>
      Username of a valid user that has been set in keystone. Default: admin
     </p></dd><dt id="id-1.3.6.16.8.3.6"><span class="term ">cpi_password</span></dt><dd><p>
      Password of a valid user that has been set in keystone.
     </p></dd><dt id="id-1.3.6.16.8.3.7"><span class="term ">cpi_monitor_max_retries</span></dt><dd><p>
      neutron load balancer monitoring max retries. Default: 3
     </p></dd><dt id="id-1.3.6.16.8.3.8"><span class="term ">cpi_bs_version</span></dt><dd><p>
      cinder Block Storage API version. Possible values are v1, v2 , v3 or
      auto. Default: <code class="literal">auto</code>
     </p></dd><dt id="id-1.3.6.16.8.3.9"><span class="term ">cpi_ignore_volume_az</span></dt><dd><p>
      Ignore cinder and nova availability zones. Default: <code class="literal">true</code>
     </p></dd><dt id="id-1.3.6.16.8.3.10"><span class="term ">dns_nameserver</span></dt><dd><p>
      Set this to the IP address of a DNS nameserver accessible by the SUSE CaaS Platform
      cluster.
     </p></dd></dl></div><p>
   Immediately after the SUSE CaaS Platform cluster comes online, and before bootstrapping,
   install the latest SUSE CaaS Platform 3.0 Maintenance Update using the following steps.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Register the SUSE CaaS Platform nodes for Maintenance Updates by following the
     instructions in <a class="xref" href="#sec-heat-templates-register" title="33.7. Register SUSE CaaS Platform Cluster for Software Updates">Section 33.7, “Register SUSE CaaS Platform Cluster for Software Updates”</a>.
    </p></li><li class="step "><p>
     On each of the SUSE CaaS Platform nodes, install the latest Maintenance Update:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo transactional-update</pre></div><p>
     Verify that the Velum image packages were updated:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper se --detail velum-image
i | sles12-velum-image | package    | 3.1.7-3.27.3  | x86_64 | update_caasp</pre></div><p>
     Reboot the node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo reboot</pre></div></li><li class="step "><p>
     Finally, when preparing to bootstrap using the SUSE CaaS Platform web interface,
     upload a valid trust certificate that can validate a certificate
     presented by keystone at the specified
     <code class="literal">keystone_auth_url</code> in the <code class="literal">System-wide
     certificate</code> section of Velum. If the SSL certificate provided
     by keystone cannot be verified, bootstrapping fails with the error
     <code class="literal">x509: certificate signed by unknown authority</code>.
    </p><div id="id-1.3.6.16.8.5.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      If your <span class="productname">OpenStack</span> endpoints operate on the Internet, or if the SSL
      certificates in use have been signed by a public authority, no action
      should be needed to enable secure communication with them.
     </p><p>
      If your <span class="productname">OpenStack</span> services operate in a private network using SSL
      certificates signed by an organizational certificate authority, provide
      that CA certificate as the system-wide certificate.
     </p><p>
      If your <span class="productname">OpenStack</span> service SSL infrastructure was self-signed during the
      installation of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 (as is done by default), its CA certificate
      can be retrieved from the Cloud Lifecycle Manager node in the <code class="filename">/etc/ssl/certs/</code>
      directory. The filename should match the node name of your primary
      controller node. For example:
      <code class="filename">/etc/ssl/certs/ardana-stack1-cp1-core-m1.pem</code>
      Download this file and provide it as the system-wide certificate.</p></div><p>
     The CPI configuration settings match the values provided
     via the <code class="filename">caasp-environment.yaml</code> or
     <code class="filename">caasp-multi-master-environment.yaml</code> files.
     Verify that they are correct before proceeding.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/cpi_1.png" target="_blank"><img src="images/cpi_1.png" width="" /></a></div></div></li></ol></div></div></div><div class="sect1" id="sec-heat-templates-register"><div class="titlepage"><div><div><h2 class="title"><span class="number">33.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Register SUSE CaaS Platform Cluster for Software Updates</span> <a title="Permalink" class="permalink" href="#sec-heat-templates-register">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/install_caasp_heat_templates.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>install_caasp_heat_templates.xml</li><li><span class="ds-label">ID: </span>sec-heat-templates-register</li></ul></div></div></div></div><p>
   Software updates are published for all registered users of SUSE CaaS Platform, and
   should always be enabled upon deploying a new cluster.
  </p><p>
   These steps may be performed on cluster nodes one at a time, or in parallel,
   making SSH connections as the <code class="literal">root</code> user with the password
   that was established in your
   <code class="filename">/usr/share/caasp-openstack-heat-templates/caasp-environment.yaml</code> file.
  </p><div id="id-1.3.6.16.9.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    If using a private SMT server for registration, use its hostname or IP
    address when running the commands below. Otherwise, use
    <code class="literal">scc.suse.com</code> to connect to SUSE's public registration
    server.
   </p></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     If this node was previously registered, deactivate its current registration:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo SUSEConnect -d
<code class="prompt user">tux &gt; </code>sudo SUSEConnect --cleanup</pre></div></li><li class="step "><p>
     If you are registering with a private SMT server, install its SSL
     certificate or the related organizational CA in order to perform SMT
     operations securely.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo curl <em class="replaceable ">SMT_SERVER</em>/smt.crt \
  -o /etc/pki/trust/anchors/registration-server.pem
<code class="prompt user">tux &gt; </code>sudo update-ca-certificates</pre></div></li><li class="step "><p>
     Establish the new system registration:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo SUSEConnect --write-config --url https://<em class="replaceable ">SMT_SERVER</em> \
  -r <em class="replaceable ">REGISTRATION_CODE</em> -e <em class="replaceable ">EMAIL_ADDRESS</em></pre></div><p>
     The same registration code may be used for all the nodes in your cluster.
    </p></li><li class="step "><p>
     Test the registration and look for a status of <code class="literal">Registered</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo SUSEConnect --status-text</pre></div></li></ol></div></div></div></div><div class="chapter " id="install-caasp-terraform"><div class="titlepage"><div><div><h2 class="title"><span class="number">34 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing SUSE CaaS Platform v4 using terraform</span> <a title="Permalink" class="permalink" href="#install-caasp-terraform">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/install_caasp_terraform.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>install_caasp_terraform.xml</li><li><span class="ds-label">ID: </span>install-caasp-terraform</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#id-1.3.6.17.2"><span class="number">34.1 </span><span class="name">CaaSP v4 deployment on SOC using terraform.</span></a></span></dt></dl></div></div><div class="sect1" id="id-1.3.6.17.2"><div class="titlepage"><div><div><h2 class="title"><span class="number">34.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">CaaSP v4 deployment on SOC using terraform.</span> <a title="Permalink" class="permalink" href="#id-1.3.6.17.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/install_caasp_terraform.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>install_caasp_terraform.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   More information about the SUSE CaaS Platform v4 is available at <a class="link" href="https://documentation.suse.com/suse-caasp/4.0/html/caasp-deployment/_deployment_instructions.html#_deployment_on_suse_openstack_cloud" target="_blank">https://documentation.suse.com/suse-caasp/4.0/html/caasp-deployment/_deployment_instructions.html#_deployment_on_suse_openstack_cloud</a>
  </p><div id="id-1.3.6.17.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   For SOC deployments that support Octavia, set export OS_USE_OCTAVIA=true in the downloaded openstack rc file in order for the load balancing API requests to the octavia service instead of the networking service.
  </p></div></div></div><div class="chapter " id="integrations"><div class="titlepage"><div><div><h2 class="title"><span class="number">35 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Integrations</span> <a title="Permalink" class="permalink" href="#integrations">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-integrations.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-integrations.xml</li><li><span class="ds-label">ID: </span>integrations</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#config-3par"><span class="number">35.1 </span><span class="name">Configuring for 3PAR Block Storage Backend</span></a></span></dt><dt><span class="section"><a href="#ironic-oneview-integration"><span class="number">35.2 </span><span class="name">Ironic HPE OneView Integration</span></a></span></dt><dt><span class="section"><a href="#ses-integration"><span class="number">35.3 </span><span class="name">SUSE Enterprise Storage Integration</span></a></span></dt></dl></div></div><p>
  Once you have completed your cloud installation, these are some of the common
  integrations you may want to perform.
 </p><div class="sect1" id="config-3par"><div class="titlepage"><div><div><h2 class="title"><span class="number">35.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring for 3PAR Block Storage Backend</span> <a title="Permalink" class="permalink" href="#config-3par">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-configure_3par.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-configure_3par.xml</li><li><span class="ds-label">ID: </span>config-3par</li></ul></div></div></div></div><p>
  This page describes how to configure your 3PAR backend for the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  Entry-scale with KVM cloud model.
 </p><div class="sect2" id="idg-installation-installation-configure-3par-xml-7"><div class="titlepage"><div><div><h3 class="title"><span class="number">35.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#idg-installation-installation-configure-3par-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-configure_3par.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-configure_3par.xml</li><li><span class="ds-label">ID: </span>idg-installation-installation-configure-3par-xml-7</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     You must have the license for the following software before you start your
     3PAR backend configuration for the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale with KVM cloud
     model:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Thin Provisioning
      </p></li><li class="listitem "><p>
       Virtual Copy
      </p></li><li class="listitem "><p>
       System Reporter
      </p></li><li class="listitem "><p>
       Dynamic Optimization
      </p></li><li class="listitem "><p>
       Priority Optimization
      </p></li></ul></div></li><li class="listitem "><p>
     Your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale KVM Cloud should be up and running.
     Installation steps can be found in
     <a class="xref" href="#install-kvm" title="Chapter 24. Installing Mid-scale and Entry-scale KVM">Chapter 24, <em>Installing Mid-scale and Entry-scale KVM</em></a>.
    </p></li><li class="listitem "><p>
     Your 3PAR Storage Array should be available in the cloud management
     network or routed to the cloud management network and the 3PAR FC and
     iSCSI ports configured.
    </p></li><li class="listitem "><p>
     The 3PAR management IP and iSCSI port IPs must have connectivity from the
     controller and compute nodes.
    </p></li><li class="listitem "><p>
     Please refer to the system requirements for 3PAR in the OpenStack
     configuration guide, which can be found here:
     <a class="link" href="http://docs.openstack.org/liberty/config-reference/content/hp-3par-sys-reqs.html" target="_blank">3PAR
     System Requirements</a>.
    </p></li></ul></div></div><div class="sect2" id="idg-installation-installation-configure-3par-xml-9"><div class="titlepage"><div><div><h3 class="title"><span class="number">35.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Notes</span> <a title="Permalink" class="permalink" href="#idg-installation-installation-configure-3par-xml-9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-configure_3par.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-configure_3par.xml</li><li><span class="ds-label">ID: </span>idg-installation-installation-configure-3par-xml-9</li></ul></div></div></div></div><p>
   The <code class="literal">cinder_admin</code> role must be added in order to configure
   3Par ICSI as a volume type in horizon.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>openstack role add --user admin --project admin cinder_admin</pre></div><p>
   <span class="bold"><strong>Encrypted 3Par Volume</strong></span>: Attaching an
   encrypted 3Par volume is possible after installation by setting
   <code class="literal">volume_use_multipath = true</code> under the libvirt stanza in
   the <code class="literal">nova/kvm-hypervisor.conf.j2</code> file and reconfigure
   nova.
  </p><p>
   <span class="bold"><strong>Concerning using multiple backends:</strong></span> If you
   are using multiple backend options, ensure that you specify each of the
   backends you are using when configuring your
   <code class="literal">cinder.conf.j2</code> file using a comma-delimited list.
   Also create multiple volume types so you can specify a backend to utilize
   when creating volumes. Instructions are included below.
   You can also read the OpenStack documentation about <a class="link" href="https://wiki.openstack.org/wiki/Cinder-multi-backend" target="_blank">cinder
   multiple storage backends</a>.
  </p><p>
   <span class="bold"><strong>Concerning iSCSI and Fiber Channel:</strong></span> You
   should not configure cinder backends so that multipath volumes are exported
   over both iSCSI and Fiber Channel from a 3PAR backend to the same nova
   compute server.
  </p><p>
   <span class="bold"><strong>3PAR driver correct name:</strong></span> In a previous
   release, the 3PAR driver used for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> integration had its name
   updated from <code class="literal">HP3PARFCDriver</code> and
   <code class="literal">HP3PARISCSIDriver</code> to <code class="literal">HPE3PARFCDriver</code>
   and <code class="literal">HPE3PARISCSIDriver</code> respectively
   (<code class="literal">HP</code> changed to <code class="literal">HPE</code>). You may get a
   warning or an error if the deprecated filenames are used. The correct values
   are those in
   <code class="filename">~/openstack/my_cloud/config/cinder/cinder.conf.j2</code>.
  </p></div><div class="sect2" id="sec-3par-multipath"><div class="titlepage"><div><div><h3 class="title"><span class="number">35.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Multipath Support</span> <a title="Permalink" class="permalink" href="#sec-3par-multipath">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-configure_3par.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-configure_3par.xml</li><li><span class="ds-label">ID: </span>sec-3par-multipath</li></ul></div></div></div></div><div id="id-1.3.6.18.3.5.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
    If multipath functionality is enabled, ensure that all 3PAR fibre channel
    ports are active and zoned correctly in the 3PAR storage.
   </p></div><p>
   We recommend setting up multipath support for 3PAR FC/iSCSI as a default
   best practice.  For instructions on this process, refer to the
   <code class="filename">~/openstack/ardana/ansible/roles/multipath/README.md</code>
   file on the Cloud Lifecycle Manager. The <code class="filename">README.md</code> file contains
   detailed procedures for configuring multipath for 3PAR FC/iSCSI cinder
   volumes.
  </p><p>
   The following steps are also required to enable 3PAR FC/iSCSI multipath
   support in the <span class="productname">OpenStack</span> configuration files:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Edit the
     <code class="literal">~/openstack/my_cloud/config/nova/kvm-hypervisor.conf.j2</code>
     file and add this line under the <code class="literal">[libvirt]</code> section:
    </p><p>
     Example:
    </p><div class="verbatim-wrap"><pre class="screen">[libvirt]
...
iscsi_use_multipath=true</pre></div><p>
     If you plan to attach encrypted 3PAR volumes, also set
     <code class="literal">volume_use_multipath=true</code> in the same section.
    </p></li><li class="step "><p>
     Edit the file
     <code class="literal">~/openstack/my_cloud/config/cinder/cinder.conf.j2</code>
     and add the following lines in the <code class="literal">[3par]</code> section:
    </p><p>
     Example:
    </p><div class="verbatim-wrap"><pre class="screen">[3par]
...
enforce_multipath_for_image_xfer=True
use_multipath_for_image_xfer=True</pre></div></li><li class="step "><p>
     Commit your configuration to the local git repo
     (<a class="xref" href="#using-git" title="Chapter 22. Using Git for Configuration Management">Chapter 22, <em>Using Git for Configuration Management</em></a>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Use the playbook below to create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the nova reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect2" id="config-fc"><div class="titlepage"><div><div><h3 class="title"><span class="number">35.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure 3PAR FC as a Cinder Backend</span> <a title="Permalink" class="permalink" href="#config-fc">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-configure_3par.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-configure_3par.xml</li><li><span class="ds-label">ID: </span>config-fc</li></ul></div></div></div></div><p>
   You must modify the <code class="literal">cinder.conf.j2</code> file
   to configure the FC details.
  </p><p>
   Perform the following steps to configure 3PAR FC as cinder backend:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Make the following changes to the
     <code class="literal">~/openstack/my_cloud/config/cinder/cinder.conf.j2</code> file:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Add your 3PAR backend to the <code class="literal">enabled_backends</code>
       section:
      </p><div class="verbatim-wrap"><pre class="screen"># Configure the enabled backends
enabled_backends=3par_FC</pre></div><p>
       If you are using multiple backend types, you can use a comma-delimited
        list.
      </p></li><li class="step "><div id="id-1.3.6.18.3.6.4.2.2.2.1" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
         A <code class="literal">default_volume_type</code> is required.
        </p></div><p>
        Use one or the other of the following alternatives as the
        <code class="literal">volume type</code> to specify as the
        <code class="literal">default_volume_type</code>.
       </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          Use a volume type (<em class="replaceable ">YOUR VOLUME TYPE</em>) that
          has already been created to meet the needs of your environment (see
          <span class="intraxref">Book “Operations Guide CLM”, Chapter 8 “Managing Block Storage”, Section 8.1 “Managing Block Storage using Cinder”, Section 8.1.2 “Creating a Volume Type for your Volumes”</span>).
         </p></li><li class="listitem "><p>
          You can create an empty <code class="literal">volume type</code> called
          <code class="literal">default_type</code> with the following:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack volume type create --is-public True \
--description "Default volume type" default_type</pre></div></li></ul></div><p>
        In <code class="filename">cinder.conf.j2</code>, set
        <code class="literal">default_volume_type</code> with one or the other of the
        following:
       </p><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
# Set the default volume type
default_volume_type = default_type</pre></div><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
# Set the default volume type
default_volume_type = <em class="replaceable ">YOUR VOLUME TYPE</em></pre></div></li><li class="step "><p>
       Uncomment the <code class="literal">StoreServ (3par) iscsi cluster</code> section
       and fill the values per your cluster information. Storage performance
       can be improved by enabling the <code class="literal">Image-Volume</code>
       cache. Here is an example:
      </p><div class="verbatim-wrap"><pre class="screen">[3par_FC]
san_ip: &lt;3par-san-ipaddr&gt;
san_login: &lt;3par-san-username&gt;
san_password: &lt;3par-san-password&gt;
hpe3par_username: &lt;3par-username&gt;
hpe3par_password: &lt;hpe3par_password&gt;
hpe3par_api_url: https://&lt;3par-san-ipaddr&gt;:8080/api/v1
hpe3par_cpg: &lt;3par-cpg-name-1&gt;[,&lt;3par-cpg-name-2&gt;, ...]
volume_backend_name: &lt;3par-backend-name&gt;
volume_driver = cinder.volume.drivers.hpe.hpe_3par_iscsi.HPE3PARISCSIDriver
image_volume_cache_enabled = True</pre></div></li></ol><div id="id-1.3.6.18.3.6.4.2.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      Do not use <code class="literal">backend_host</code> variable in
      <code class="literal">cinder.conf.j2</code> file. If <code class="literal">backend_host</code>
      is set, it will override the [DEFAULT]/host value which <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9
      is dependent on.
     </p></div></li><li class="step "><p>
     Commit your configuration to the local git repo
     (<a class="xref" href="#using-git" title="Chapter 22. Using Git for Configuration Management">Chapter 22, <em>Using Git for Configuration Management</em></a>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the following playbook to complete the configuration:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect2" id="config-iscsi"><div class="titlepage"><div><div><h3 class="title"><span class="number">35.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure 3PAR iSCSI as Cinder backend</span> <a title="Permalink" class="permalink" href="#config-iscsi">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-configure_3par.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-configure_3par.xml</li><li><span class="ds-label">ID: </span>config-iscsi</li></ul></div></div></div></div><p>
   You must modify the <code class="literal">cinder.conf.j2</code> to configure the iSCSI
   details.
  </p><p>
   Perform the following steps to configure 3PAR iSCSI as cinder backend:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Make the following changes to the
     <code class="literal">~/openstack/my_cloud/config/cinder/cinder.conf.j2</code> file:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Add your 3PAR backend to the <code class="literal">enabled_backends</code>
       section:
      </p><div class="verbatim-wrap"><pre class="screen"># Configure the enabled backends
enabled_backends=3par_iSCSI</pre></div></li><li class="step "><p>
       Uncomment the <code class="literal">StoreServ (3par) iscsi cluster</code> section
       and fill the values per your cluster information. Here is an example:
      </p><div class="verbatim-wrap"><pre class="screen">[3par_iSCSI]
san_ip: &lt;3par-san-ipaddr&gt;
san_login: &lt;3par-san-username&gt;
san_password: &lt;3par-san-password&gt;
hpe3par_username: &lt;3par-username&gt;
hpe3par_password: &lt;hpe3par_password&gt;
hpe3par_api_url: https://&lt;3par-san-ipaddr&gt;:8080/api/v1
hpe3par_cpg: &lt;3par-cpg-name-1&gt;[,&lt;3par-cpg-name-2&gt;, ...]
volume_backend_name: &lt;3par-backend-name&gt;
volume_driver: cinder.volume.drivers.san.hp.hp_3par_iscsi.hpe3parISCSIDriver
hpe3par_iscsi_ips: &lt;3par-ip-address-1&gt;[,&lt;3par-ip-address-2&gt;,&lt;3par-ip-address-3&gt;, ...]
hpe3par_iscsi_chap_enabled=true</pre></div><div id="id-1.3.6.18.3.7.4.2.2.2.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
        Do not use <code class="literal">backend_host</code> variable in
        <code class="literal">cinder.conf</code> file. If <code class="literal">backend_host</code>
        is set, it will override the [DEFAULT]/host value which <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9
        is dependent on.
       </p></div></li></ol></li><li class="step "><p>
     Commit your configuration your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "&lt;commit message&gt;"</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
     When you run the configuration processor you will be prompted for two
     passwords. Enter the first password to make the configuration processor
     encrypt its sensitive data, which consists of the random inter-service
     passwords that it generates and the Ansible group_vars and host_vars that
     it produces for subsequent deploy runs. You will need this key for
     subsequent Ansible deploy runs and subsequent configuration processor
     runs. If you wish to change an encryption password that you have already
     used when running the configuration processor then enter the new password
     at the second prompt, otherwise press <span class="keycap">Enter</span>.
    </p><p>
     For CI purposes you can specify the required passwords on the ansible
     command line. For example, the command below will disable encryption by
     the configuration processor
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost config-processor-run.yml \
  -e encrypt="" -e rekey=""</pre></div><p>
     If you receive an error during either of these steps then there is an
     issue with one or more of your configuration files. We recommend that you
     verify that all of the information in each of your configuration files is
     correct for your environment and then commit those changes to git using
     the instructions above.
    </p></li><li class="step "><p>
     Run the following command to create a deployment directory.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the following command to complete the configuration:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect2" id="idg-installation-installation-configure-3par-xml-16"><div class="titlepage"><div><div><h3 class="title"><span class="number">35.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Post-Installation Tasks</span> <a title="Permalink" class="permalink" href="#idg-installation-installation-configure-3par-xml-16">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-configure_3par.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-configure_3par.xml</li><li><span class="ds-label">ID: </span>idg-installation-installation-configure-3par-xml-16</li></ul></div></div></div></div><p>
   After configuring 3PAR as your Block Storage backend, perform the
   following tasks:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="intraxref">Book “Operations Guide CLM”, Chapter 8 “Managing Block Storage”, Section 8.1 “Managing Block Storage using Cinder”, Section 8.1.2 “Creating a Volume Type for your Volumes”</span>
    </p></li><li class="listitem "><p>
     <a class="xref" href="#sec-verify-block-storage-volume" title="39.1. Verifying Your Block Storage Backend">Section 39.1, “Verifying Your Block Storage Backend”</a>
    </p></li></ul></div></div></div><div class="sect1" id="ironic-oneview-integration"><div class="titlepage"><div><div><h2 class="title"><span class="number">35.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ironic HPE OneView Integration</span> <a title="Permalink" class="permalink" href="#ironic-oneview-integration">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic_oneview_integration.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic_oneview_integration.xml</li><li><span class="ds-label">ID: </span>ironic-oneview-integration</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 supports integration of ironic (Baremetal) service with
  HPE OneView using <span class="emphasis"><em>agent_pxe_oneview</em></span> driver. Please refer to
  <a class="link" href="https://docs.openstack.org/developer/ironic/drivers/oneview.html" target="_blank">OpenStack
  Documentation</a> for more information.
 </p><div class="sect2" id="id-1.3.6.18.4.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">35.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#id-1.3.6.18.4.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic_oneview_integration.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic_oneview_integration.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="orderedlist " id="prereq-list"><ol class="orderedlist" type="1"><li class="listitem "><p>
     Installed <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 with entry-scale-ironic-flat-network or
     entry-scale-ironic-multi-tenancy model.
    </p></li><li class="listitem "><p>
     HPE OneView 3.0 instance is running and connected to management network.
    </p></li><li class="listitem "><p>
     HPE OneView configuration is set into
     <code class="literal">definition/data/ironic/ironic_config.yml</code> (and
     <code class="literal">ironic-reconfigure.yml</code> playbook ran if needed). This
     should enable <span class="emphasis"><em>agent_pxe_oneview</em></span> driver in ironic
     conductor.
    </p></li><li class="listitem "><p>
     Managed node(s) should support PXE booting in legacy BIOS mode.
    </p></li><li class="listitem "><p>
     Managed node(s) should have PXE boot NIC listed first. That is, embedded
     1Gb NIC must be disabled (otherwise it always goes first).
    </p></li></ol></div></div><div class="sect2" id="id-1.3.6.18.4.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">35.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Integrating with HPE OneView</span> <a title="Permalink" class="permalink" href="#id-1.3.6.18.4.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic_oneview_integration.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic_oneview_integration.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     On the Cloud Lifecycle Manager, open the file
     <code class="literal">~/openstack/my_cloud/definition/data/ironic/ironic_config.yml</code>
    </p><div class="verbatim-wrap"><pre class="screen">~$ cd ~/openstack
vi my_cloud/definition/data/ironic/ironic_config.yml</pre></div></li><li class="step "><p>
     Modify the settings listed below:
    </p><ol type="a" class="substeps "><li class="step "><p>
       <code class="literal">enable_oneview</code>: should be set to "true" for HPE OneView
       integration
      </p></li><li class="step "><p>
       <code class="literal">oneview_manager_url</code>: HTTPS endpoint of HPE OneView
       management interface, for example:
       <span class="bold"><strong>https://10.0.0.10/</strong></span>
      </p></li><li class="step "><p>
       <code class="literal">oneview_username</code>: HPE OneView username, for example:
       <span class="bold"><strong>Administrator</strong></span>
      </p></li><li class="step "><p>
       <code class="literal">oneview_encrypted_password</code>: HPE OneView password in
       encrypted or clear text form. The encrypted form is distinguished by
       presence of <code class="literal">@ardana@</code> at the beginning of the
       string. The encrypted form can be created by running the
       <code class="command">ardanaencrypt.py</code>
       program. This program is shipped as part of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> and can be found in
       <code class="filename">~/openstack/ardana/ansible</code> directory on Cloud Lifecycle Manager.
      </p></li><li class="step "><p>
       <code class="literal">oneview_allow_insecure_connections</code>: should be set to
       "true" if HPE OneView is using self-generated certificate.
      </p></li></ol></li><li class="step "><p>
     Once you have saved your changes and exited the editor, add files, commit
     changes to local git repository, and run
     <code class="literal">config-processor-run.yml</code> and
     <code class="literal">ready-deployment.yml</code> playbooks, as described in
     <a class="xref" href="#using-git" title="Chapter 22. Using Git for Configuration Management">Chapter 22, <em>Using Git for Configuration Management</em></a>.
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack$ git add my_cloud/definition/data/ironic/ironic_config.yml
~/openstack$ cd ardana/ansible
~/openstack/ardana/ansible$ ansible-playbook -i hosts/localhost \
  config-processor-run.yml
...
~/openstack/ardana/ansible$ ansible-playbook -i hosts/localhost \
  ready-deployment.yml</pre></div></li><li class="step "><p>
     Run ironic-reconfigure.yml playbook.
    </p><div class="verbatim-wrap"><pre class="screen">$ cd ~/scratch/ansible/next/ardana/ansible/

# This is needed if password was encrypted in ironic_config.yml file
~/scratch/ansible/next/ardana/ansible$ export ARDANA_USER_PASSWORD_ENCRYPT_KEY=your_password_encrypt_key
~/scratch/ansible/next/ardana/ansible$ ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml
...</pre></div></li></ol></div></div></div><div class="sect2" id="id-1.3.6.18.4.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">35.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Registering Node in HPE OneView</span> <a title="Permalink" class="permalink" href="#id-1.3.6.18.4.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic_oneview_integration.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic_oneview_integration.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   In the HPE OneView web interface:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Navigate to
     <span class="guimenu ">Menu</span> › <span class="guimenu ">Server Hardware</span>.
     Add new <span class="guimenu ">Server Hardware</span> item, using
     managed node IPMI IP and credentials. If this is the first node of this
     type being added, corresponding
     <span class="guimenu ">Server Hardware Type</span> will be created automatically.
    </p></li><li class="step "><p>
     Navigate to
     <span class="guimenu ">Menu</span> › <span class="guimenu ">Server Profile Template</span>.
     Add <span class="guimenu ">Server Profile Template</span>. Use
     <span class="guimenu ">Server Hardware Type</span> corresponding to node being
     registered. In <span class="guimenu ">BIOS Settings</span> section, set
     <span class="guimenu ">Manage Boot Mode</span> and <span class="guimenu ">Manage Boot
     Order</span> options must be turned on:
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-ironic-OneViewWebRegister.png" target="_blank"><img src="images/media-ironic-OneViewWebRegister.png" width="" /></a></div></div></li><li class="step "><p>
     Verify that node is powered off. Power the node off if needed.
    </p></li></ol></div></div></div><div class="sect2" id="id-1.3.6.18.4.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">35.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Provisioning ironic Node</span> <a title="Permalink" class="permalink" href="#id-1.3.6.18.4.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ironic_oneview_integration.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ironic_oneview_integration.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Login to the Cloud Lifecycle Manager and source respective credentials file
     (for example <code class="filename">service.osrc</code> for admin account).
    </p></li><li class="step "><p>
     Review glance images with <code class="literal">openstack image list</code>
    </p><div class="verbatim-wrap"><pre class="screen">$ openstack image list
+--------------------------------------+--------------------------+
| ID                                   | Name                     |
+--------------------------------------+--------------------------+
| c61da588-622c-4285-878f-7b86d87772da | cirros-0.3.4-x86_64      |
+--------------------------------------+--------------------------+</pre></div><p>
     ironic deploy images (boot image,
     <code class="literal">ir-deploy-kernel</code>, <code class="literal">ir-deploy-ramdisk</code>,
     <code class="literal">ir-deploy-iso</code>) are created automatically. The
     <code class="systemitem">agent_pxe_oneview</code> ironic driver requires
     <code class="systemitem">ir-deploy-kernel</code> and
     <code class="systemitem">ir-deploy-ramdisk</code> images.
    </p></li><li class="step "><p>
     Create node using <code class="literal">agent_pxe_oneview</code> driver.
    </p><div class="verbatim-wrap"><pre class="screen">$ ironic --ironic-api-version 1.22 node-create -d agent_pxe_oneview --name test-node-1 \
  --network-interface neutron -p memory_mb=131072 -p cpu_arch=x86_64 -p local_gb=80 -p cpus=2 \
  -p 'capabilities=boot_mode:bios,boot_option:local,server_hardware_type_uri:\
     /rest/server-hardware-types/E5366BF8-7CBF-48DF-A752-8670CF780BB2,server_profile_template_uri:\
     /rest/server-profile-templates/00614918-77f8-4146-a8b8-9fc276cd6ab2' \
  -i 'server_hardware_uri=/rest/server-hardware/32353537-3835-584D-5135-313930373046' \
  -i dynamic_allocation=True \
  -i deploy_kernel=633d379d-e076-47e6-b56d-582b5b977683 \
  -i deploy_ramdisk=d5828785-edf2-49fa-8de2-3ddb7f3270d5

+-------------------+--------------------------------------------------------------------------+
| Property          | Value                                                                    |
+-------------------+--------------------------------------------------------------------------+
| chassis_uuid      |                                                                          |
| driver            | agent_pxe_oneview                                                        |
| driver_info       | {u'server_hardware_uri': u'/rest/server-                                 |
|                   | hardware/32353537-3835-584D-5135-313930373046', u'dynamic_allocation':   |
|                   | u'True', u'deploy_ramdisk': u'd5828785-edf2-49fa-8de2-3ddb7f3270d5',     |
|                   | u'deploy_kernel': u'633d379d-e076-47e6-b56d-582b5b977683'}               |
| extra             | {}                                                                       |
| name              | test-node-1                                                              |
| network_interface | neutron                                                                  |
| properties        | {u'memory_mb': 131072, u'cpu_arch': u'x86_64', u'local_gb': 80, u'cpus': |
|                   | 2, u'capabilities':                                                      |
|                   | u'boot_mode:bios,boot_option:local,server_hardware_type_uri:/rest        |
|                   | /server-hardware-types/E5366BF8-7CBF-                                    |
|                   | 48DF-A752-8670CF780BB2,server_profile_template_uri:/rest/server-profile- |
|                   | templates/00614918-77f8-4146-a8b8-9fc276cd6ab2'}                         |
| resource_class    | None                                                                     |
| uuid              | c202309c-97e2-4c90-8ae3-d4c95afdaf06                                     |
+-------------------+--------------------------------------------------------------------------+</pre></div><div id="id-1.3.6.18.4.6.2.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
        For deployments created via ironic/HPE OneView integration,
        <code class="literal">memory_mb</code> property must reflect physical amount of
        RAM installed in the managed node. That is, for a server with 128 Gb of RAM
        it works out to 132*1024=13072.
       </p></li><li class="listitem "><p>
        Boot mode in capabilities property must reflect boot mode used by the
        server, that is 'bios' for Legacy BIOS and 'uefi' for UEFI.
       </p></li><li class="listitem "><p>
        Values for <code class="literal">server_hardware_type_uri</code>,
        <code class="literal">server_profile_template_uri</code> and
        <code class="literal">server_hardware_uri</code> can be grabbed from browser URL
        field while navigating to respective objects in HPE OneView UI. URI
        corresponds to the part of URL which starts form the token
        <code class="literal">/rest</code>.
        That is, the URL
        <code class="literal">https://oneview.mycorp.net/#/profile-templates/show/overview/r/rest/server-profile-templates/12345678-90ab-cdef-0123-012345678901</code>
        corresponds to the URI
        <code class="literal">/rest/server-profile-templates/12345678-90ab-cdef-0123-012345678901</code>.
       </p></li><li class="listitem "><p>
        Grab IDs of <code class="literal">deploy_kernel</code> and
        <code class="literal">deploy_ramdisk</code> from <span class="bold"><strong>openstack
        image list</strong></span> output above.
       </p></li></ul></div></div></li><li class="step "><p>
     Create port.
    </p><div class="verbatim-wrap"><pre class="screen">$ ironic --ironic-api-version 1.22 port-create \
  --address aa:bb:cc:dd:ee:ff \
  --node c202309c-97e2-4c90-8ae3-d4c95afdaf06 \
  -l switch_id=ff:ee:dd:cc:bb:aa \
  -l switch_info=MY_SWITCH \
  -l port_id="Ten-GigabitEthernet 1/0/1" \
  --pxe-enabled true
+-----------------------+----------------------------------------------------------------+
| Property              | Value                                                          |
+-----------------------+----------------------------------------------------------------+
| address               | 8c:dc:d4:b5:7d:1c                                              |
| extra                 | {}                                                             |
| local_link_connection | {u'switch_info': u'C20DATA', u'port_id': u'Ten-GigabitEthernet |
|                       | 1/0/1',    u'switch_id': u'ff:ee:dd:cc:bb:aa'}                 |
| node_uuid             | c202309c-97e2-4c90-8ae3-d4c95afdaf06                           |
| pxe_enabled           | True                                                           |
| uuid                  | 75b150ef-8220-4e97-ac62-d15548dc8ebe                           |
+-----------------------+----------------------------------------------------------------+</pre></div><div id="id-1.3.6.18.4.6.2.4.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
      ironic Multi-Tenancy networking model is used in this example.
      Therefore, ironic port-create command contains information about the
      physical switch. HPE OneView integration can also be performed using the
      ironic Flat Networking model. For more information, see
      <a class="xref" href="#ironic-examples" title="9.6. Ironic Examples">Section 9.6, “Ironic Examples”</a>.
     </p></div></li><li class="step "><p>
     Move node to manageable provisioning state. The connectivity between
     ironic and HPE OneView will be verified, Server Hardware Template settings
     validated, and Server Hardware power status retrieved from HPE OneView and set
     into the ironic node.
    </p><div class="verbatim-wrap"><pre class="screen">$ ironic node-set-provision-state test-node-1 manage</pre></div></li><li class="step "><p>
     Verify that node power status is populated.
    </p><div class="verbatim-wrap"><pre class="screen">$ ironic node-show test-node-1
+-----------------------+-----------------------------------------------------------------------+
| Property              | Value                                                                 |
+-----------------------+-----------------------------------------------------------------------+
| chassis_uuid          |                                                                       |
| clean_step            | {}                                                                    |
| console_enabled       | False                                                                 |
| created_at            | 2017-06-30T21:00:26+00:00                                             |
| driver                | agent_pxe_oneview                                                     |
| driver_info           | {u'server_hardware_uri': u'/rest/server-                              |
|                       | hardware/32353537-3835-584D-5135-313930373046', u'dynamic_allocation':|
|                       | u'True', u'deploy_ramdisk': u'd5828785-edf2-49fa-8de2-3ddb7f3270d5',  |
|                       | u'deploy_kernel': u'633d379d-e076-47e6-b56d-582b5b977683'}            |
| driver_internal_info  | {}                                                                    |
| extra                 | {}                                                                    |
| inspection_finished_at| None                                                                  |
| inspection_started_at | None                                                                  |
| instance_info         | {}                                                                    |
| instance_uuid         | None                                                                  |
| last_error            | None                                                                  |
| maintenance           | False                                                                 |
| maintenance_reason    | None                                                                  |
| name                  | test-node-1                                                           |
| network_interface     |                                                                       |
| power_state           | power off                                                             |
| properties            | {u'memory_mb': 131072, u'cpu_arch': u'x86_64', u'local_gb': 80,       |
|                       | u'cpus': 2, u'capabilities':                                          |
|                       | u'boot_mode:bios,boot_option:local,server_hardware_type_uri:/rest     |
|                       | /server-hardware-types/E5366BF8-7CBF-                                 |
|                       | 48DF-A752-86...BB2,server_profile_template_uri:/rest/server-profile-  |
|                       | templates/00614918-77f8-4146-a8b8-9fc276cd6ab2'}                      |
| provision_state       | manageable                                                            |
| provision_updated_at  | 2017-06-30T21:04:43+00:00                                             |
| raid_config           |                                                                       |
| reservation           | None                                                                  |
| resource_class        |                                                                       |
| target_power_state    | None                                                                  |
| target_provision_state| None                                                                  |
| target_raid_config    |                                                                       |
| updated_at            | 2017-06-30T21:04:43+00:00                                             |
| uuid                  | c202309c-97e2-4c90-8ae3-d4c95afdaf06                                  |
+-----------------------+-----------------------------------------------------------------------+</pre></div></li><li class="step "><p>
     Move node to available provisioning state. The ironic node will be
     reported to nova as available.
    </p><div class="verbatim-wrap"><pre class="screen">$ ironic node-set-provision-state test-node-1 provide</pre></div></li><li class="step "><p>
     Verify that node resources were added to nova hypervisor stats.
    </p><div class="verbatim-wrap"><pre class="screen">$ openstack hypervisor stats show
+----------------------+--------+
| Property             | Value  |
+----------------------+--------+
| count                | 1      |
| current_workload     | 0      |
| disk_available_least | 80     |
| free_disk_gb         | 80     |
| free_ram_mb          | 131072 |
| local_gb             | 80     |
| local_gb_used        | 0      |
| memory_mb            | 131072 |
| memory_mb_used       | 0      |
| running_vms          | 0      |
| vcpus                | 2      |
| vcpus_used           | 0      |
+----------------------+--------+</pre></div></li><li class="step "><p>
     Create nova flavor.
    </p><div class="verbatim-wrap"><pre class="screen">$ openstack flavor create m1.ironic auto 131072 80 2
+-------------+-----------+--------+------+-----------+------+-------+-------------+-----------+
| ID          | Name      | Mem_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public |
+-------------+-----------+--------+------+-----------+------+-------+-------------+-----------+
| 33c8...f8d8 | m1.ironic | 131072 | 80   | 0         |      | 2     | 1.0         | True      |
+-------------+-----------+--------+------+-----------+------+-------+-------------+-----------+
$ openstack flavor set m1.ironic set capabilities:boot_mode="bios"
$ openstack flavor set m1.ironic set capabilities:boot_option="local"
$ openstack flavor set m1.ironic set cpu_arch=x86_64</pre></div><div id="id-1.3.6.18.4.6.2.9.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      All parameters (specifically, amount of RAM and boot mode) must
      correspond to ironic node parameters.
     </p></div></li><li class="step "><p>
     Create nova keypair if needed.
    </p><div class="verbatim-wrap"><pre class="screen">$ openstack keypair create ironic_kp --pub-key ~/.ssh/id_rsa.pub</pre></div></li><li class="step "><p>
     Boot nova instance.
    </p><div class="verbatim-wrap"><pre class="screen">$ openstack server create --flavor m1.ironic --image d6b5...e942 --key-name ironic_kp \
  --nic net-id=5f36...dcf3 test-node-1
+-------------------------------+-----------------------------------------------------+
| Property                      | Value                                               |
+-------------------------------+-----------------------------------------------------+
| OS-DCF:diskConfig             | MANUAL                                              |
| OS-EXT-AZ:availability_zone   |                                                     |
| OS-EXT-SRV-ATTR:host          | -                                                   |
| OS-EXT-SRV-ATTR:              |                                                     |
|       hypervisor_hostname     | -                                                   |
| OS-EXT-SRV-ATTR:instance_name |                                                     |
| OS-EXT-STS:power_state        | 0                                                   |
| OS-EXT-STS:task_state         | scheduling                                          |
| OS-EXT-STS:vm_state           | building                                            |
| OS-SRV-USG:launched_at        | -                                                   |
| OS-SRV-USG:terminated_at      | -                                                   |
| accessIPv4                    |                                                     |
| accessIPv6                    |                                                     |
| adminPass                     | pE3m7wRACvYy                                        |
| config_drive                  |                                                     |
| created                       | 2017-06-30T21:08:42Z                                |
| flavor                        | m1.ironic (33c81884-b8aa-46...3b72f8d8)             |
| hostId                        |                                                     |
| id                            | b47c9f2a-e88e-411a-abcd-6172aea45397                |
| image                         | Ubuntu Trusty 14.04 BIOS (d6b5d971-42...5f2d88e942) |
| key_name                      | ironic_kp                                           |
| metadata                      | {}                                                  |
| name                          | test-node-1                                         |
| os-extended-volumes:          |                                                     |
|       volumes_attached        | []                                                  |
| progress                      | 0                                                   |
| security_groups               | default                                             |
| status                        | BUILD                                               |
| tenant_id                     | c8573f7026d24093b40c769ca238fddc                    |
| updated                       | 2017-06-30T21:08:42Z                                |
| user_id                       | 2eae99221545466d8f175eeb566cc1b4                    |
+-------------------------------+-----------------------------------------------------+</pre></div><p>
     During nova instance boot, the following operations will be performed by
     ironic via HPE OneView REST API.
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       In HPE OneView, new Server Profile is generated for specified Server
       Hardware, using specified Server Profile Template. Boot order in Server
       Profile is set to list PXE as the first boot source.
      </p></li><li class="listitem "><p>
       The managed node is powered on and boots IPA image from PXE.
      </p></li><li class="listitem "><p>
       IPA image writes user image onto disk and reports success back to
       ironic.
      </p></li><li class="listitem "><p>
       ironic modifies Server Profile in HPE OneView to list 'Disk' as default boot
       option.
      </p></li><li class="listitem "><p>
       ironic reboots the node (via HPE OneView REST API call).
      </p></li></ul></div></li></ol></div></div></div></div><div class="sect1" id="ses-integration"><div class="titlepage"><div><div><h2 class="title"><span class="number">35.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE Enterprise Storage Integration</span> <a title="Permalink" class="permalink" href="#ses-integration">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ses_integration.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ses_integration.xml</li><li><span class="ds-label">ID: </span>ses-integration</li></ul></div></div></div></div><p>
  The current version of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> supports integration with SUSE Enterprise Storage
  (SES). Integrating SUSE Enterprise Storage enables Ceph to provide RADOS Block Device (RBD),
  block storage, image storage, object storage via RADOS Gateway (RGW),
  and CephFS (file storage) in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. The following documentation
  outlines integration for SUSE Enterprise Storage 5 and 5.5.
 </p><p>
   SUSE Enterprise Storage 5.5 uses a Salt runner that creates users and pools. Salt generates
   a yaml configuration that is needed to integrate with SUSE <span class="productname">OpenStack</span> Cloud.
   The integration runner creates separate users for cinder,
   cinder backup, and glance. Both the cinder
   and nova services have the same user, as cinder needs access
   to create objects that nova uses.
 </p><p>
   SUSE Enterprise Storage 5 uses a manual configuration that requires the creation of users and
   pools.
 </p><p>
  For more information on SUSE Enterprise Storage, see
  the <a class="link" href="https://documentation.suse.com/ses/5.5" target="_blank">https://documentation.suse.com/ses/5.5</a>.
 </p><div class="sect2" id="ses-installation"><div class="titlepage"><div><div><h3 class="title"><span class="number">35.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling SUSE Enterprise Storage 5.5 Integration</span> <a title="Permalink" class="permalink" href="#ses-installation">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ses_integration.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ses_integration.xml</li><li><span class="ds-label">ID: </span>ses-installation</li></ul></div></div></div></div><p>The following instructions detail integrating SUSE Enterprise Storage 5.5 with SUSE <span class="productname">OpenStack</span> Cloud.</p><p>
   Log in as root to run the SES 5.5 Salt runner on the salt admin host:
  </p><p>If no prefix is specified (as the below command shows), by default
    pool names are prefixed with <code class="literal">cloud-</code> and are more
    generic.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>salt-run --out=yaml openstack.integrate</pre></div><div class="verbatim-wrap"><pre class="screen">ceph_conf:
  cluster_network: 10.84.56.0/21
  fsid: d5d7c7cb-5858-3218-a36f-d028df7b0673
  mon_host: 10.84.56.8, 10.84.56.9, 10.84.56.7
  mon_initial_members: ses-osd1, ses-osd2, ses-osd3
  public_network: 10.84.56.0/21
cinder:
  key: AQBI5/xcAAAAABAAFP7ES4gl5tZ9qdLd611AmQ==
  rbd_store_pool: cloud-volumes
  rbd_store_user: cinder
cinder-backup:
  key: AQBI5/xcAAAAABAAVSZmfeuPl3KFvJetCygUmA==
  rbd_store_pool: cloud-backups
  rbd_store_user: cinder-backup
glance:
  key: AQBI5/xcAAAAABAALHgkBxARTZAeuoIWDsC0LA==
  rbd_store_pool: cloud-images
  rbd_store_user: glance
nova:
  rbd_store_pool: cloud-vms
radosgw_urls:
  - http://10.84.56.7:80/swift/v1
  - http://10.84.56.8:80/swift/v1</pre></div><p>If you perform the command with a prefix, the prefix is applied to
   pool names and to key names. This way, multiple cloud deployments can use
   different users and pools on the same SES deployment.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>salt-run --out=yaml openstack.integrate prefix=mycloud</pre></div><div class="verbatim-wrap"><pre class="screen">ceph_conf:
  cluster_network: 10.84.56.0/21
  fsid: d5d7c7cb-5858-3218-a36f-d028df7b0673
  mon_host: 10.84.56.8, 10.84.56.9, 10.84.56.7
  mon_initial_members: ses-osd1, ses-osd2, ses-osd3
  public_network: 10.84.56.0/21
cinder:
  key: AQAM5fxcAAAAABAAIyMeLwclr+5uegp33xdiIw==
  rbd_store_pool: mycloud-cloud-volumes
  rbd_store_user: mycloud-cinder
cinder-backup:
  key: AQAM5fxcAAAAABAAq6ZqKuMNaaJgk6OtFHMnsQ==
  rbd_store_pool: mycloud-cloud-backups
  rbd_store_user: mycloud-cinder-backup
glance:
  key: AQAM5fxcAAAAABAAvhJjxC81IePAtnkye+bLoQ==
  rbd_store_pool: mycloud-cloud-images
  rbd_store_user: mycloud-glance
nova:
  rbd_store_pool: mycloud-cloud-vms
radosgw_urls:
  - http://10.84.56.7:80/swift/v1
  - http://10.84.56.8:80/swift/v1</pre></div></div><div class="sect2" id="ses-config"><div class="titlepage"><div><div><h3 class="title"><span class="number">35.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling SUSE Enterprise Storage 5 Integration</span> <a title="Permalink" class="permalink" href="#ses-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ses_integration.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ses_integration.xml</li><li><span class="ds-label">ID: </span>ses-config</li></ul></div></div></div></div><p>The following instructions detail integrating SUSE Enterprise Storage 5 with SUSE <span class="productname">OpenStack</span> Cloud.</p><p>
  The SUSE Enterprise Storage integration is provided through the <span class="package ">ardana-ses</span>
  RPM package. This package is included in the
   <code class="systemitem">patterns-cloud-ardana</code> pattern and the installation is
  covered in <a class="xref" href="#cha-depl-dep-inst" title="Chapter 15. Installing the Cloud Lifecycle Manager server">Chapter 15, <em>Installing the Cloud Lifecycle Manager server</em></a>. The update repositories and
  the installation covered there are required to support SUSE Enterprise Storage
  integration. The latest updates should be applied before proceeding.
 </p><p>
   After the SUSE Enterprise Storage integration package has been installed, it must be
   configured. Files that contain relevant SUSE Enterprise Storage deployment information
   must be placed into a directory on the deployer node. This includes the
   configuration file that describes various aspects of the Ceph environment
   as well as keyrings for each user and pool created in the Ceph
   environment. In addition to that, you need to edit the
   <code class="filename">settings.yml</code> file to enable the SUSE Enterprise Storage integration to
   run and update all of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> service configuration files.
  </p><p>
   The <code class="filename">settings.yml</code> file must reside in the
   <code class="filename">~/openstack/my_cloud/config/ses/</code> directory. Open the
   file for editing, uncomment the <code class="literal">ses_config_path:</code>
   parameter, and specify the location on the deployer host containing the
   <code class="filename">ses_config.yml</code> and keyring files as the parameter's
   value. After you have done that, the <code class="filename">site.yml</code> and
   <code class="filename">ardana-reconfigure.yml</code> playbooks activate and configure
   the cinder, glance, and nova
   services.
  </p><div id="id-1.3.6.18.5.7.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    For security reasons, you should use a unique UUID in the
    <code class="filename">settings.yml</code> file for
    <code class="literal">ses_secret_id</code>, replacing the fixed, hard-coded UUID in
    that file. You can generate a UUID that is unique to your deployment
    using the command <code class="command">uuidgen</code>.
   </p></div><p>
   After you have run the <code class="literal">openstack.integrate</code> runner, copy
   the yaml into the <code class="filename">ses_config.yml</code> file on the deployer
   node. Then edit the <code class="filename">settings.yml</code> file to enable SUSE Enterprise Storage
   integration to run and update all of the SUSE <span class="productname">OpenStack</span> Cloud service configuration
   files. The <code class="filename">settings.yml</code> file resides in the
   <code class="filename">~/openstack/my_cloud/config/ses</code> directory. Open the
   <code class="filename">settings.yml</code> file for editing, uncomment the
   <code class="literal">ses_config_path:</code> parameter, and specify the location on
   the deployer host containing the <code class="filename">ses_config.yml</code> file.
  </p><p>
    If you are integrating with SUSE Enterprise Storage and want to store nova images in
    Ceph, then set the following:
   </p><div class="verbatim-wrap"><pre class="screen">ses_nova_set_images_type: True</pre></div><p>If you not want to store nova images in Ceph, the following
    setting is required:
   </p><div class="verbatim-wrap"><pre class="screen">ses_nova_set_images_type: False</pre></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Commit your configuration to your local git repo:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "add SES integration"</pre></div></li><li class="step "><p>
      Run the configuration processor:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
      Create a deployment directory:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
      Run a series of reconfiguration playbooks:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ses-deploy.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</pre></div></li><li class="step "><p>
      Reconfigure the Cloud Lifecycle Manager to complete the deployment:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml</pre></div></li></ol></div></div><p>
   In the <code class="filename">control_plane.yml</code> file, the glance
   <code class="literal">default_store</code> option must be adjusted.
  </p><div class="verbatim-wrap"><pre class="screen">- glance-api:
            glance_default_store: 'rbd'</pre></div><div id="id-1.3.6.18.5.7.15" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The following content is only relevant if you are running a
    standalone Ceph cluster (not SUSE Enterprise Storage) or a SUSE Enterprise Storage cluster that is
    before version 5.5.
  </p></div><p>
   For Ceph, it is necessary to create pools and users to allow the
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services to use the SUSE Enterprise Storage/Ceph cluster. Pools and users must
   be created for cinder, cinder backup, nova and
   glance. Instructions for creating and managing pools, users and keyrings is
   covered in the SUSE Enterprise Storage documentation under <a class="link" href="https://documentation.suse.com/en-us/ses/5.5/single-html/ses-admin/#storage-cephx-keymgmt" target="_blank">https://documentation.suse.com/en-us/ses/5.5/single-html/ses-admin/#storage-cephx-keymgmt</a>.
   </p><p>
    After the required pools and users are set up on the Ceph
    cluster, you have to create a <code class="filename">ses_config.yml</code>
    configuration file (see the example below). This file is used during
    deployment to configure all of the services. The
    <code class="filename">ses_config.yml</code> and the keyring files should be placed
    in a separate directory.
   </p><p>
    If you are integrating with SUSE Enterprise Storage and do not want to store nova images in
    Ceph, the following setting is required:
   </p><p>
    Edit <code class="filename">settings.yml</code> and change the line
    <code class="literal">ses_nova_set_images_type: True</code>
    to <code class="literal">ses_nova_set_images_type: False</code>
   </p><div class="example" id="id-1.3.6.18.5.7.20"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 35.1: </span><span class="name">ses_config.yml Example </span><a title="Permalink" class="permalink" href="#id-1.3.6.18.5.7.20">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">ses_cluster_configuration:
    ses_cluster_name: ceph
    ses_radosgw_url: "https://192.168.56.8:8080/swift/v1"

    conf_options:
        ses_fsid: d5d7c7cb-5858-3218-a36f-d028df7b1111
        ses_mon_initial_members: ses-osd2, ses-osd3, ses-osd1
        ses_mon_host: 192.168.56.8, 192.168.56.9, 192.168.56.7
        ses_public_network: 192.168.56.0/21
        ses_cluster_network: 192.168.56.0/21

    cinder:
        rbd_store_pool: cinder
        rbd_store_pool_user: cinder
        keyring_file_name: ceph.client.cinder.keyring

    cinder-backup:
        rbd_store_pool: backups
        rbd_store_pool_user: cinder_backup
        keyring_file_name: ceph.client.cinder-backup.keyring

    # nova uses the cinder user to access the nova pool, cinder pool
    # So all we need here is the nova pool name.
    nova:
        rbd_store_pool: nova

    glance:
        rbd_store_pool: glance
        rbd_store_pool_user: glance
        keyring_file_name: ceph.client.glance.keyring</pre></div></div></div><p>
     The path to this directory must be specified in the
     <code class="filename">settings.yml</code> file, as in the example below. After
     making the changes, follow the steps to complete the configuration.
   </p><div class="verbatim-wrap"><pre class="screen">settings.yml
...
ses_config_path: /var/lib/ardana/ses/
ses_config_file: ses_config.yml

# The unique uuid for use with virsh for cinder and nova
 ses_secret_id: <em class="replaceable ">SES_SECRET_ID</em></pre></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      After modifying these files, commit your configuration to the local git
    repo. For more information, see <a class="xref" href="#using-git" title="Chapter 22. Using Git for Configuration Management">Chapter 22, <em>Using Git for Configuration Management</em></a>.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "configure SES 5"</pre></div></li><li class="step "><p>
      Run the configuration processor:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
      Create a deployment directory:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
      Reconfigure Ardana:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect2" id="id-1.3.6.18.5.8"><div class="titlepage"><div><div><h3 class="title"><span class="number">35.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Add Missing Swift Endpoints</span> <a title="Permalink" class="permalink" href="#id-1.3.6.18.5.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ses_integration.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ses_integration.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    If you deployed Cloud Lifecycle Manager using the SUSE Enterprise Storage integration without swift, the
    integration will not be set up properly. Swift object endpoints will be
    missing. Use the following process to create the necessary endpoints.
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Source the keystone <code class="literal">rc</code> file to have the correct
      permissions to create the swift service and endpoints.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>. ~/keystone.osrc</pre></div></li><li class="step "><p>
      Create the swift service.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack service create --name swift object-store --enable</pre></div></li><li class="step "><p>
      Read the RADOS gateway URL from the <code class="filename">ses_config.yml</code>
      file. For example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>grep http ~/ses/ses_config.yml
https://ses-osd3:8080/swift/v1</pre></div></li><li class="step "><p>
      Create the three swift endpoints.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack endpoint create --enable --region region1 swift \
admin https://ses-osd3:8080/swift/v1
<code class="prompt user">ardana &gt; </code>openstack endpoint create --enable --region region1 swift \
public  https://ses-osd3:8080/swift/v1
<code class="prompt user">ardana &gt; </code>openstack endpoint create --enable --region region1 swift \
internal https://ses-osd3:8080/swift/v1</pre></div></li><li class="step "><p>
      Verify the objects in the endpoint list.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack endpoint list | grep object
5313b...e9412f  region1  swift  object-store  True  public    https://ses-osd3:8080/swift/v1
83faf...1eb602  region1  swift  object-store  True  internal  https://ses-osd3:8080/swift/v1
dc698...715b8c  region1  swift  object-store  True  admin     https://ses-osd3:8080/swift/v1</pre></div></li></ol></div></div></div><div class="sect2" id="id-1.3.6.18.5.9"><div class="titlepage"><div><div><h3 class="title"><span class="number">35.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring SUSE Enterprise Storage for Integration with RADOS Gateway</span> <a title="Permalink" class="permalink" href="#id-1.3.6.18.5.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ses_integration.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ses_integration.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    RADOS gateway integration can be enabled (disabled) by adding (removing)
    the following line in the <code class="filename">ses_config.yml</code>:
   </p><div class="verbatim-wrap"><pre class="screen">ses_radosgw_url: "https://192.168.56.8:8080/swift/v1"</pre></div><p>
    If RADOS gateway integration is enabled, additional SUSE Enterprise Storage configuration is
    needed. RADOS gateway must be configured to use keystone for
    authentication. This is done by adding the configuration statements below
    to the rados section of <code class="filename">ceph.conf</code> on the RADOS node.
   </p><div class="verbatim-wrap"><pre class="screen">[client.rgw.<em class="replaceable ">HOSTNAME</em>]
rgw frontends = "civetweb port=80+443s"
rgw enable usage log = true
rgw keystone url = <em class="replaceable ">KEYSTONE_ENDPOINT</em> (for example:
https://192.168.24.204:5000)
rgw keystone admin user = <em class="replaceable ">KEYSTONE_ADMIN_USER</em>
rgw keystone admin password = <em class="replaceable ">KEYSTONE_ADMIN_PASSWORD</em>
rgw keystone admin project = <em class="replaceable ">KEYSTONE_ADMIN_PROJECT</em>
rgw keystone admin domain = <em class="replaceable ">KEYSTONE_ADMIN_DOMAIN</em>
rgw keystone api version = 3
rgw keystone accepted roles = admin,member
rgw keystone accepted admin roles = admin
rgw keystone revocation interval = 0
rgw keystone verify ssl = false # If keystone is using self-signed
   certificate</pre></div><p>
    After making these changes to <code class="filename">ceph.conf</code>, the RADOS
    gateway service needs to be restarted.
   </p><p>
    Enabling RADOS gateway replaces the existing Object Storage endpoint with the
    RADOS gateway endpoint.
   </p></div><div class="sect2" id="id-1.3.6.18.5.10"><div class="titlepage"><div><div><h3 class="title"><span class="number">35.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling HTTPS, Creating and Importing a Certificate</span> <a title="Permalink" class="permalink" href="#id-1.3.6.18.5.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ses_integration.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ses_integration.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    SUSE Enterprise Storage integration uses the HTTPS protocol to connect to the RADOS
    gateway. However, with SUSE Enterprise Storage 5, HTTPS is not enabled by default. To enable the
    gateway role to communicate securely using SSL, you need to either have a
    CA-issued certificate or create a self-signed one. Instructions for both
    are available in the <a class="link" href="https://documentation.suse.com/en-us/ses/5.5/single-html/ses-admin/#ceph-rgw-https" target="_blank">SUSE Enterprise Storage
    documentation</a>.
   </p><p>
    The certificate needs to be installed on your Cloud Lifecycle Manager. On the Cloud Lifecycle Manager, copy the
    cert to <code class="filename">/tmp/ardana_tls_cacerts</code>. Then deploy it.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts tls-trust-deploy.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts tls-reconfigure.yml</pre></div><p>
    When creating the certificate, the <code class="literal">subjectAltName</code> must
    match the <code class="literal">ses_radosgw_url</code> entry in
    <code class="filename">ses_config.yml</code>. Either an IP address or FQDN can be
    used, but these values must be the same in both places.
   </p></div><div class="sect2" id="id-1.3.6.18.5.11"><div class="titlepage"><div><div><h3 class="title"><span class="number">35.3.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying SUSE Enterprise Storage Configuration for RADOS Integration</span> <a title="Permalink" class="permalink" href="#id-1.3.6.18.5.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ses_integration.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ses_integration.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    The following steps deploy your configuration.
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Commit your configuration to your local git repo.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "add SES integration"</pre></div></li><li class="step "><p>
      Run the configuration processor.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
      Create a deployment directory.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
      Run a series of reconfiguration playbooks.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ses-deploy.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</pre></div></li><li class="step "><p>
      Reconfigure the Cloud Lifecycle Manager to complete the deployment.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect2" id="id-1.3.6.18.5.12"><div class="titlepage"><div><div><h3 class="title"><span class="number">35.3.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enable Copy-On-Write Cloning of Images</span> <a title="Permalink" class="permalink" href="#id-1.3.6.18.5.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ses_integration.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ses_integration.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    Due to a security issue described in <a class="link" href="http://docs.ceph.com/docs/master/rbd/rbd-openstack/?highlight=uuid#enable-copy-on-write-cloning-of-images" target="_blank">http://docs.ceph.com/docs/master/rbd/rbd-openstack/?highlight=uuid#enable-copy-on-write-cloning-of-images</a>, we do not recommend the copy-on-write cloning of images when
    glance and cinder are both using a Ceph back-end.
    However, if you want to use this feature for faster operation,
    you can enable it as follows.</p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Open the
       <code class="literal">~/openstack/my_cloud/config/glance/glance-api.conf.j2</code>
       file for editing and add <code class="literal">show_image_direct_url = True</code>
       under the <code class="literal">[DEFAULT]</code> section.
      </p></li><li class="step "><p>
       Commit changes:</p><div class="verbatim-wrap"><pre class="screen">git add -A
git commit -m "Enable Copy-on-Write Cloning"</pre></div></li><li class="step "><p>
       Run the required playbooks:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
cd /var/lib/ardana/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml</pre></div></li></ol></div></div><div id="id-1.3.6.18.5.12.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
     Note that this exposes the back-end location via glance's API, so the
     end-point should not be publicly accessible when Copy-On-Write image
     cloning is enabled.
    </p></div></div><div class="sect2" id="id-1.3.6.18.5.13"><div class="titlepage"><div><div><h3 class="title"><span class="number">35.3.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Improve SUSE Enterprise Storage Storage Performance</span> <a title="Permalink" class="permalink" href="#id-1.3.6.18.5.13">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ses_integration.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ses_integration.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    SUSE Enterprise Storage performance can be improved with Image-Volume cache. Be aware that
    Image-Volume cache and Copy-on-Write cloning cannot be used for the same
    storage back-end. For more information, see the <a class="link" href="https://docs.openstack.org/cinder/pike/admin/blockstorage-image-volume-cache.html" target="_blank">OpenStack
    documentation</a>.
   </p><p>
    Enable Image-Volume cache with the following steps:
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Open the
      <code class="filename">~/openstack/my_cloud/config/cinder/cinder.conf.j2</code>
      file for editing.
     </p></li><li class="step "><p>
      Add <code class="literal">image_volume_cache_enabled = True</code> option under the
      <code class="literal">[ses_ceph]</code> section.
     </p></li><li class="step "><p>
      Commit changes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Enable Image-Volume cache"</pre></div></li><li class="step "><p>
      Run the required playbooks:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd /var/lib/ardana/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</pre></div></li></ol></div></div></div></div></div><div class="chapter " id="troubleshooting-installation"><div class="titlepage"><div><div><h2 class="title"><span class="number">36 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting the Installation</span> <a title="Permalink" class="permalink" href="#troubleshooting-installation">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installation_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installation_troubleshooting.xml</li><li><span class="ds-label">ID: </span>troubleshooting-installation</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#sec-trouble-deployer-setup"><span class="number">36.1 </span><span class="name">Issues during Cloud Lifecycle Manager Setup</span></a></span></dt><dt><span class="section"><a href="#sec-trouble-config-processor"><span class="number">36.2 </span><span class="name">Issues while Updating Configuration Files</span></a></span></dt><dt><span class="section"><a href="#sec-trouble-deploy-cloud"><span class="number">36.3 </span><span class="name">Issues while Deploying the Cloud</span></a></span></dt></dl></div></div><p>
  We have gathered some of the common issues that occur during installation and
  organized them by when they occur during the installation. These sections
  will coincide with the steps labeled in the installation instructions.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <a class="xref" href="#sec-trouble-deployer-setup" title="36.1. Issues during Cloud Lifecycle Manager Setup">Section 36.1, “Issues during Cloud Lifecycle Manager Setup”</a>
   </p></li><li class="listitem "><p>
     <a class="xref" href="#sec-trouble-config-processor" title="36.2. Issues while Updating Configuration Files">Section 36.2, “Issues while Updating Configuration Files”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-trouble-deploy-cloud" title="36.3. Issues while Deploying the Cloud">Section 36.3, “Issues while Deploying the Cloud”</a>
   </p></li></ul></div><div class="sect1" id="sec-trouble-deployer-setup"><div class="titlepage"><div><div><h2 class="title"><span class="number">36.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Issues during Cloud Lifecycle Manager Setup</span> <a title="Permalink" class="permalink" href="#sec-trouble-deployer-setup">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installation_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installation_troubleshooting.xml</li><li><span class="ds-label">ID: </span>sec-trouble-deployer-setup</li></ul></div></div></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.6.19.4.2"><span class="name">Issue: Running the ardana-init.bash script when configuring your Cloud Lifecycle Manager does not complete</span><a title="Permalink" class="permalink" href="#id-1.3.6.19.4.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installation_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h5></div><p>
   Part of what the <code class="literal">ardana-init.bash</code> script does is
   install Git. So if your DNS server(s) is/are not specified in your
   <code class="filename">/etc/resolv.conf</code> file, is not valid, or is not
   functioning properly on your Cloud Lifecycle Manager, it will not be able to
   complete.
  </p><p>
   To resolve this issue, double check your nameserver in your
   <code class="filename">/etc/resolv.conf</code> file and then re-run the script.
  </p></div><div class="sect1" id="sec-trouble-config-processor"><div class="titlepage"><div><div><h2 class="title"><span class="number">36.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Issues while Updating Configuration Files</span> <a title="Permalink" class="permalink" href="#sec-trouble-config-processor">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installation_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installation_troubleshooting.xml</li><li><span class="ds-label">ID: </span>sec-trouble-config-processor</li></ul></div></div></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.6.19.5.2"><span class="name">Configuration Processor Fails Due to Wrong yml Format</span><a title="Permalink" class="permalink" href="#id-1.3.6.19.5.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installation_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h5></div><p>
   If you receive the error below when running the configuration processor then
   you may have a formatting error:
  </p><div class="verbatim-wrap"><pre class="screen">TASK: [fail msg="Configuration processor run failed, see log output above for
details"]</pre></div><p>
   First you should check the Ansible log in the location below for more
   details on which yml file in your input model has the error:
  </p><div class="verbatim-wrap"><pre class="screen">~/.ansible/ansible.log</pre></div><p>
   Check the configuration file to locate and fix the error, keeping in mind
   the following tips below.
  </p><p>
   Check your files to ensure that they do not contain the following:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Non-ascii characters
    </p></li><li class="listitem "><p>
     Unneeded spaces
    </p></li></ul></div><p>
   Once you have fixed the formatting error in your files, commit the changes
   with these steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Commit your changes to Git:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     Re-run the configuration processor playbook and confirm the error is not
     received again.
    </p></li></ol></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.6.19.5.12"><span class="name">Configuration processor fails with provider network OCTAVIA-MGMT-NET error</span><a title="Permalink" class="permalink" href="#id-1.3.6.19.5.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installation_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h5></div><p>
   If you receive the error below when running the configuration processor then
   you have not correctly configured your VLAN settings for Octavia.
  </p><div class="verbatim-wrap"><pre class="screen">################################################################################,
# The configuration processor failed.
#   config-data-2.0           ERR: Provider network OCTAVIA-MGMT-NET host_routes:
# destination '192.168.10.0/24' is not defined as a Network in the input model.
# Add 'external: True' to this host_route if this is for an external network.
################################################################################</pre></div><p>
   To resolve the issue, ensure that your settings in
   <code class="literal">~/openstack/my_cloud/definition/data/neutron/neutron_config.yml</code>
   are correct for the VLAN setup for Octavia.
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.6.19.5.16"><span class="name">Changes Made to your Configuration Files</span><a title="Permalink" class="permalink" href="#id-1.3.6.19.5.16">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installation_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h5></div><p>
   If you have made corrections to your configuration files and need to re-run
   the Configuration Processor, the only thing you need to do is commit your
   changes to your local Git repository:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "commit message"</pre></div><p>
   You can then re-run the configuration processor:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.6.19.5.21"><span class="name">Configuration Processor Fails Because Encryption Key Does Not Meet Requirements</span><a title="Permalink" class="permalink" href="#id-1.3.6.19.5.21">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installation_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h5></div><p>
   If you choose to set an encryption password when running the configuration
   processor, you may receive the following error if the chosen password does
   not meet the complexity requirements:
  </p><div class="verbatim-wrap"><pre class="screen">################################################################################
# The configuration processor failed.
#   encryption-key ERR: The Encryption Key does not meet the following requirement(s):
#       The Encryption Key must be at least 12 characters
#       The Encryption Key must contain at least 3 of following classes of characters:
#                           Uppercase Letters, Lowercase Letters, Digits, Punctuation
################################################################################</pre></div><p>
   If you receive the above error, run the configuration processor again and
   select a password that meets the complexity requirements detailed in the
   error message:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></div><div class="sect1" id="sec-trouble-deploy-cloud"><div class="titlepage"><div><div><h2 class="title"><span class="number">36.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Issues while Deploying the Cloud</span> <a title="Permalink" class="permalink" href="#sec-trouble-deploy-cloud">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installation_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-installation_troubleshooting.xml</li><li><span class="ds-label">ID: </span>sec-trouble-deploy-cloud</li></ul></div></div></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.6.19.6.2"><span class="name">Issue: If the site.yml playbook fails, you can query the log for the reason</span><a title="Permalink" class="permalink" href="#id-1.3.6.19.6.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installation_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h5></div><p>
   Ansible is good about outputting the errors into the command line output,
   however if you would like to view the full log for any reason the location is:
  </p><div class="verbatim-wrap"><pre class="screen">~/.ansible/ansible.log</pre></div><p>
   This log is updated real time as you run Ansible playbooks.
  </p><div id="id-1.3.6.19.6.6" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>
    Use grep to parse through the log. Usage: <code class="literal">grep &lt;text&gt;
    ~/.ansible/ansible.log</code>
   </p></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.6.19.6.7"><span class="name">Issue: How to Wipe the Disks of your Machines</span><a title="Permalink" class="permalink" href="#id-1.3.6.19.6.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installation_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h5></div><p>
   If you have re-run the <code class="literal">site.yml</code> playbook, you may need to
   wipe the disks of your nodes
  </p><p>
   You should run the <code class="filename">wipe_disks.yml</code> playbook only after
   re-running the <code class="literal">bm-reimage.yml</code> playbook but before you
   re-run the <code class="literal">site.yml</code> playbook.
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts wipe_disks.yml</pre></div><p>
   The playbook will show you the disks to be wiped in the output and allow you
   to confirm that you want to complete this action or abort it if you do not
   want to proceed. You can optionally use the <code class="literal">--limit
   &lt;NODE_NAME&gt;</code> switch on this playbook to restrict it to
   specific nodes. This action will not affect the OS partitions on the servers.
  </p><p>
   If you receive an error stating that <code class="literal">osconfig</code> has already
   run on your nodes then you will need to remove the
   <code class="literal">/etc/ardana/osconfig-ran</code> file on each of the nodes you want
   to wipe with this command:
  </p><div class="verbatim-wrap"><pre class="screen">sudo rm /etc/ardana/osconfig-ran</pre></div><p>
   That will clear this flag and allow the disk to be wiped.
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.6.19.6.15"><span class="name">Error Received if Root Logical Volume is Too Small</span><a title="Permalink" class="permalink" href="#id-1.3.6.19.6.15">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installation_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h5></div><p>
   When running the <code class="literal">site.yml</code> playbook, you may receive a
   message that includes the error below if your root logical volume is too
   small. This error needs to be parsed out and resolved.
  </p><div class="verbatim-wrap"><pre class="screen">2015-09-29 15:54:03,022 p=26345 u=stack | stderr: New size given (7128 extents)
not larger than existing size (7629 extents)</pre></div><p>
   The error message may also reference the root volume:
  </p><div class="verbatim-wrap"><pre class="screen">"name": "root", "size": "10%"</pre></div><p>
   The problem here is that the root logical volume, as specified in the
   <code class="literal">disks_controller.yml</code> file, is set to
   <code class="literal">10%</code> of the overall physical volume and this value is too
   small.
  </p><p>
   To resolve this issue you need to ensure that the percentage is set properly
   for the size of your logical-volume. The default values in the configuration
   files is based on a 500 GB disk, so if your logical volumes are smaller you
   may need to increase the percentage so there is enough room.
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="id-1.3.6.19.6.22"><span class="name">Multiple Keystone Failures Received during site.yml</span><a title="Permalink" class="permalink" href="#id-1.3.6.19.6.22">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-installation_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h5></div><p>
   If you receive the keystone error below during your
   <code class="literal">site.yml</code> run then follow these steps:
  </p><div class="verbatim-wrap"><pre class="screen">TASK: [OPS-MON | _keystone_conf | Create Ops Console service in keystone] *****
failed:
[...]
msg: An unexpected error prevented the server from fulfilling your request.
(HTTP 500) (Request-ID: req-23a09c72-5991-4685-b09f-df242028d742), failed

FATAL: all hosts have already failed -- aborting</pre></div><p>
   The most likely cause of this error is that the virtual IP address is having
   issues and the keystone API communication through the virtual IP address is
   not working properly. You will want to check the keystone log on the
   controller where you will likely see authorization failure errors.
  </p><p>
   Verify that your virtual IP address is active and listening on the proper
   port on all of your controllers using this command:
  </p><div class="verbatim-wrap"><pre class="screen">netstat -tplan | grep 35357</pre></div><p>
   Ensure that your Cloud Lifecycle Manager did not pick the wrong (unusable) IP
   address from the list of IP addresses assigned to your Management network.
  </p><p>
   The Cloud Lifecycle Manager will take the first available IP address after the
   <code class="literal">gateway-ip</code> defined in your
   <code class="filename">~/openstack/my_cloud/definition/data/networks.yml</code> file.
   This IP will be used as the virtual IP address for that particular network.
   If this IP address is used and reserved for another purpose outside of your
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> deployment then you will receive the error above.
  </p><p>
   To resolve this issue we recommend that you utilize the
   <code class="literal">start-address</code> and possibly the
   <code class="literal">end-address</code> (if needed) options in your
   <code class="filename">networks.yml</code> file to further define which IP addresses
   you want your cloud deployment to use. For more information, see
   <a class="xref" href="#configobj-networks" title="6.14. Networks">Section 6.14, “Networks”</a>.
  </p><p>
   After you have made changes to your <code class="filename">networks.yml</code> file,
   follow these steps to commit the changes:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Ensuring that you stay within the <code class="filename">~/openstack</code> directory,
     commit the changes you just made:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack
git commit -a -m "commit message"</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Re-run the site.yml playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml</pre></div></li></ol></div></div></div></div><div class="chapter " id="esx-troubleshooting-installation"><div class="titlepage"><div><div><h2 class="title"><span class="number">37 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting the ESX</span> <a title="Permalink" class="permalink" href="#esx-troubleshooting-installation">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-esx-esx_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-esx-esx_troubleshooting.xml</li><li><span class="ds-label">ID: </span>esx-troubleshooting-installation</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#id-1.3.6.20.3"><span class="number">37.1 </span><span class="name">Issue: ardana-service.service is not running</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.20.4"><span class="number">37.2 </span><span class="name">Issue: ESX Cluster shows UNKNOWN in Operations Console</span></a></span></dt><dt><span class="section"><a href="#id-1.3.6.20.5"><span class="number">37.3 </span><span class="name">Issue: Unable to view the VM console in Horizon UI</span></a></span></dt></dl></div></div><p>
  This section contains troubleshooting tasks for your <span class="phrase"><span class="phrase">SUSE® <span class="productname">OpenStack</span> Cloud</span></span>
  9 for ESX.
 </p><div class="sect1" id="id-1.3.6.20.3"><div class="titlepage"><div><div><h2 class="title"><span class="number">37.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Issue: ardana-service.service is not running</span> <a title="Permalink" class="permalink" href="#id-1.3.6.20.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-esx-esx_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-esx-esx_troubleshooting.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If you perform any maintenance work or reboot the Cloud Lifecycle Manager/deployer
   node, make sure to restart the Cloud Lifecycle Manager API service for standalone deployer node
   and shared Cloud Lifecycle Manager/controller node based on your environment.
  </p><p>
   For standalone deployer node, execute <code class="literal">ardana-start.yml</code>
   playbook to restart the Cloud Lifecycle Manager API service on the deployer node after a reboot.
  </p><p>
   For shared deployer/controller node, execute
   <code class="literal">ardana-start.yml</code> playbook on all the controllers to
   restart Cloud Lifecycle Manager API service.
  </p><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit <em class="replaceable ">HOST_NAME</em></pre></div><p>
   Replace <em class="replaceable ">HOST_NAME</em> with the host name of the Cloud Lifecycle Manager
   node or the Cloud Lifecycle Manager Node/Shared Controller.
  </p></div><div class="sect1" id="id-1.3.6.20.4"><div class="titlepage"><div><div><h2 class="title"><span class="number">37.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Issue: ESX Cluster shows UNKNOWN in Operations Console</span> <a title="Permalink" class="permalink" href="#id-1.3.6.20.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-esx-esx_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-esx-esx_troubleshooting.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   In the Operations Console Alarms dashboard, if all the alarms for ESX cluster are
   showing UNKNOWN then restart the <code class="literal">openstack-monasca-agent</code> running in
   ESX compute proxy.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     SSH to the respective compute proxy. You can find the hostname of the
     proxy from the dimensions list shown against the respective alarm.
    </p></li><li class="step "><p>
     Restart the <code class="literal">openstack-monasca-agent</code> service.
    </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl restart openstack-monasca-agent</pre></div></li></ol></div></div></div><div class="sect1" id="id-1.3.6.20.5"><div class="titlepage"><div><div><h2 class="title"><span class="number">37.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Issue: Unable to view the VM console in Horizon UI</span> <a title="Permalink" class="permalink" href="#id-1.3.6.20.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-esx-esx_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-esx-esx_troubleshooting.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   By default the gdbserver firewall is disabled in ESXi host which results in
   a Handshake error when accessing the VM instance console in the horizon UI.
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-gdbserver.png" target="_blank"><img src="images/media-esx-gdbserver.png" width="" /></a></div></div><p>
   <span class="bold"><strong>Procedure to enable gdbserver</strong></span>
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Login to vSphere Client.
    </p></li><li class="step "><p>
     Select the ESXi Host and click
     <span class="guimenu ">Configuration</span> tab in the menu bar. You
     must perform the following actions on all the ESXi hosts in the compute
     clusters.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-1.png" target="_blank"><img src="images/media-esx-1.png" width="" /></a></div></div></li><li class="step "><p>
     On the left hand side select <span class="bold"><strong>Security
     Profile</strong></span> from the list of
     <span class="bold"><strong>Software</strong></span>. Click
     <span class="bold"><strong>Properties</strong></span> on the right hand side.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-2.png" target="_blank"><img src="images/media-esx-2.png" width="" /></a></div></div><p>
     Firewall Properties box displays.
    </p></li><li class="step "><p>
     Select <span class="bold"><strong>gdbserver</strong></span> checkbox and click
     <span class="bold"><strong>OK</strong></span>.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-3.png" target="_blank"><img src="images/media-esx-3.png" width="" /></a></div></div></li></ol></div></div></div></div></div><div class="part" id="post-install"><div class="titlepage"><div><div><h1 class="title"><span class="number">Part V </span><span class="name">Post-Installation </span><a title="Permalink" class="permalink" href="#post-install">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-post_install_overview.xml" title="Edit the source file for this section">Edit source</a></h1></div></div></div><div class="toc"><dl><dt><span class="chapter"><a href="#cloud-verification"><span class="number">38 </span><span class="name">Post Installation Tasks</span></a></span></dt><dd class="toc-abstract"><p>
  When you have completed your cloud deployment, these are some of the common
  post-installation tasks you may need to perform to verify your cloud
  installation.
 </p></dd><dt><span class="chapter"><a href="#ui-verification"><span class="number">39 </span><span class="name">UI Verification</span></a></span></dt><dd class="toc-abstract"><p>
  Once you have completed your cloud deployment, these are some of the common
  post-installation tasks you may need to perform to verify your cloud
  installation.
 </p></dd><dt><span class="chapter"><a href="#install-openstack-clients"><span class="number">40 </span><span class="name">Installing OpenStack Clients</span></a></span></dt><dd class="toc-abstract"><p>
  If you have a standalone deployer, the OpenStack CLI and other clients will
  not be installed automatically on that node. If you require access to these
  clients, you will need to follow the procedure below to add the appropriate
  software.
 </p></dd><dt><span class="chapter"><a href="#tls30"><span class="number">41 </span><span class="name">Configuring Transport Layer Security (TLS)</span></a></span></dt><dd class="toc-abstract"><p>
    TLS is enabled by default during the installation of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 and
    additional configuration options are available to secure your environment,
    as described below.
   </p></dd><dt><span class="chapter"><a href="#config-availability-zones"><span class="number">42 </span><span class="name">Configuring Availability Zones</span></a></span></dt><dd class="toc-abstract"><p>
  The Cloud Lifecycle Manager only creates a default availability zone during
  installation. If your system has multiple failure/availability zones defined
  in your input model, these zones will not get created automatically.
 </p></dd><dt><span class="chapter"><a href="#OctaviaInstall"><span class="number">43 </span><span class="name">Configuring Load Balancer as a Service</span></a></span></dt><dd class="toc-abstract"><p>
    The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> neutron LBaaS service supports several load balancing
    providers. By default, both Octavia and the namespace HAProxy driver are
    configured to be used.
   </p></dd><dt><span class="chapter"><a href="#postinstall-checklist"><span class="number">44 </span><span class="name">Other Common Post-Installation Tasks</span></a></span></dt><dd class="toc-abstract"><p>On your Cloud Lifecycle Manager, in the ~/scratch/ansible/next/ardana/ansible/group_vars/ directory you will find several files. In the one labeled as first control plane node you can locate the user credentials for both the Administrator user (admin) and your Demo user (demo) which you will use to …</p></dd></dl></div><div class="chapter " id="cloud-verification"><div class="titlepage"><div><div><h2 class="title"><span class="number">38 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Post Installation Tasks</span> <a title="Permalink" class="permalink" href="#cloud-verification">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-cloud_verification.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-cloud_verification.xml</li><li><span class="ds-label">ID: </span>cloud-verification</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#api-verification"><span class="number">38.1 </span><span class="name">API Verification</span></a></span></dt><dt><span class="section"><a href="#sec-verify-block-storage-swift"><span class="number">38.2 </span><span class="name">Verify the Object Storage (swift) Operations</span></a></span></dt><dt><span class="section"><a href="#upload-image"><span class="number">38.3 </span><span class="name">Uploading an Image for Use</span></a></span></dt><dt><span class="section"><a href="#create-extnet"><span class="number">38.4 </span><span class="name">Creating an External Network</span></a></span></dt></dl></div></div><p>
  When you have completed your cloud deployment, these are some of the common
  post-installation tasks you may need to perform to verify your cloud
  installation.
 </p><div id="id-1.3.7.2.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   Manually back up <code class="filename">/etc/group</code> on the Cloud Lifecycle Manager. It may be
   useful for an emergency recovery.
  </p></div><div class="sect1" id="api-verification"><div class="titlepage"><div><div><h2 class="title"><span class="number">38.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">API Verification</span> <a title="Permalink" class="permalink" href="#api-verification">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-api_verification.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-api_verification.xml</li><li><span class="ds-label">ID: </span>api-verification</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 provides a tool (Tempest) that you can use to verify that
  your cloud deployment completed successfully:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="#sec-api-verification-prereq" title="38.1.1. Prerequisites">Section 38.1.1, “Prerequisites”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-api-verification-tempest" title="38.1.2. Tempest Integration Tests">Section 38.1.2, “Tempest Integration Tests”</a>
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      <a class="xref" href="#sec-api-verification-running" title="38.1.3. Running the Tests">Section 38.1.3, “Running the Tests”</a>
     </p></li><li class="listitem "><p>
      <a class="xref" href="#sec-api-verification-result" title="38.1.4. Viewing Test Results">Section 38.1.4, “Viewing Test Results”</a>
     </p></li><li class="listitem "><p>
      <a class="xref" href="#sec-api-verification-custom" title="38.1.5. Customizing the Test Run">Section 38.1.5, “Customizing the Test Run”</a>
     </p></li></ul></div></li><li class="listitem "><p>
    <a class="xref" href="#sec-verify-block-storage-volume" title="39.1. Verifying Your Block Storage Backend">Section 39.1, “Verifying Your Block Storage Backend”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-verify-block-storage-swift" title="38.2. Verify the Object Storage (swift) Operations">Section 38.2, “Verify the Object Storage (swift) Operations”</a>
   </p></li></ul></div><div class="sect2" id="sec-api-verification-prereq"><div class="titlepage"><div><div><h3 class="title"><span class="number">38.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#sec-api-verification-prereq">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-api_verification.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-api_verification.xml</li><li><span class="ds-label">ID: </span>sec-api-verification-prereq</li></ul></div></div></div></div><p>
   The verification tests rely on you having an external network setup and a
   cloud image in your image (glance) repository. Run the following playbook to
   configure your cloud:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-cloud-configure.yml</pre></div><div id="id-1.3.7.2.4.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9, the EXT_NET_CIDR setting for the external network is
    now specified in the input model - see
    <a class="xref" href="#configobj-neutron-external-networks" title="6.16.2.2. neutron-external-networks">Section 6.16.2.2, “neutron-external-networks”</a>.
   </p></div></div><div class="sect2" id="sec-api-verification-tempest"><div class="titlepage"><div><div><h3 class="title"><span class="number">38.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Tempest Integration Tests</span> <a title="Permalink" class="permalink" href="#sec-api-verification-tempest">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-api_verification.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-api_verification.xml</li><li><span class="ds-label">ID: </span>sec-api-verification-tempest</li></ul></div></div></div></div><p>
   Tempest is a set of integration tests for OpenStack API validation,
   scenarios, and other specific tests to be run against a live OpenStack
   cluster. In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9, Tempest has been modeled as a service and this
   gives you the ability to locate Tempest anywhere in the cloud. It is
   recommended that you install Tempest on your Cloud Lifecycle Manager node - that
   is where it resides by default in a new installation.
  </p><p>
   A version of the upstream
   <a class="link" href="http://docs.openstack.org/developer/tempest/" target="_blank">Tempest</a>
   integration tests is pre-deployed on the Cloud Lifecycle Manager node.
   For details on what Tempest is testing, you can check the contents of this
   file on your Cloud Lifecycle Manager:
  </p><div class="verbatim-wrap"><pre class="screen">/opt/stack/tempest/run_filters/ci.txt</pre></div><p>
   You can use these embedded tests to verify if the deployed cloud is
   functional.
  </p><p>
   For more information on running Tempest tests, see
   <a class="link" href="https://git.openstack.org/cgit/openstack/tempest/tree/README.rst" target="_blank">Tempest
   - The OpenStack Integration Test Suite</a>.
  </p><div id="id-1.3.7.2.4.5.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    Running these tests requires access to the deployed cloud's identity admin
    credentials
   </p></div><p>
   Tempest creates and deletes test accounts and test resources for test
   purposes.
  </p><p>
   In certain cases Tempest might fail to clean-up some of test resources after
   a test is complete, for example in case of failed tests.
  </p></div><div class="sect2" id="sec-api-verification-running"><div class="titlepage"><div><div><h3 class="title"><span class="number">38.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Running the Tests</span> <a title="Permalink" class="permalink" href="#sec-api-verification-running">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-api_verification.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-api_verification.xml</li><li><span class="ds-label">ID: </span>sec-api-verification-running</li></ul></div></div></div></div><p>
   To run the default set of Tempest tests:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Ensure you can access your cloud:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts cloud-client-setup.yml
source /etc/environment</pre></div></li><li class="step "><p>
     Run the tests:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts tempest-run.yml</pre></div></li></ol></div></div><p>
   Optionally, you can <a class="xref" href="#sec-api-verification-custom" title="38.1.5. Customizing the Test Run">Section 38.1.5, “Customizing the Test Run”</a>.
  </p></div><div class="sect2" id="sec-api-verification-result"><div class="titlepage"><div><div><h3 class="title"><span class="number">38.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Viewing Test Results</span> <a title="Permalink" class="permalink" href="#sec-api-verification-result">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-api_verification.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-api_verification.xml</li><li><span class="ds-label">ID: </span>sec-api-verification-result</li></ul></div></div></div></div><p>
   Tempest is deployed under <code class="literal">/opt/stack/tempest</code>. Test
   results are written in a log file in the following directory:
  </p><div class="verbatim-wrap"><pre class="screen">/opt/stack/tempest/logs</pre></div><p>
   A detailed log file is written to:
  </p><div class="verbatim-wrap"><pre class="screen">/opt/stack/tempest/logs/testr_results_region1.log</pre></div><div id="id-1.3.7.2.4.7.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    If you encounter an error saying "local variable 'run_subunit_content'
    referenced before assignment", you may need to log in as the
    <code class="literal">tempest</code> user to run this command. This is due to a known
    issue reported at
    <a class="link" href="https://bugs.launchpad.net/testrepository/+bug/1348970" target="_blank">https://bugs.launchpad.net/testrepository/+bug/1348970</a>.
   </p></div><p>
   See
   <a class="link" href="https://testrepository.readthedocs.org/en/latest/" target="_blank">Test
   Repository Users Manual</a> for more details on how to manage the test
   result repository.
  </p></div><div class="sect2" id="sec-api-verification-custom"><div class="titlepage"><div><div><h3 class="title"><span class="number">38.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Customizing the Test Run</span> <a title="Permalink" class="permalink" href="#sec-api-verification-custom">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-api_verification.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-api_verification.xml</li><li><span class="ds-label">ID: </span>sec-api-verification-custom</li></ul></div></div></div></div><p>
   There are several ways available to customize which tests will be executed.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <a class="xref" href="#sec-api-verification-service" title="38.1.6. Run Tests for Specific Services and Exclude Specific Features">Section 38.1.6, “Run Tests for Specific Services and Exclude Specific Features”</a>
    </p></li><li class="listitem "><p>
     <a class="xref" href="#sec-api-verification-list" title="38.1.7. Run Tests Matching a Series of White and Blacklists">Section 38.1.7, “Run Tests Matching a Series of White and Blacklists”</a>
    </p></li></ul></div></div><div class="sect2" id="sec-api-verification-service"><div class="titlepage"><div><div><h3 class="title"><span class="number">38.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Run Tests for Specific Services and Exclude Specific Features</span> <a title="Permalink" class="permalink" href="#sec-api-verification-service">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-api_verification.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-api_verification.xml</li><li><span class="ds-label">ID: </span>sec-api-verification-service</li></ul></div></div></div></div><p>
   Tempest allows you to test specific services and features using the
   <code class="literal">tempest.conf</code> configuration file.
  </p><p>
   A working configuration file with inline documentation is deployed under
   <code class="literal">/opt/stack/tempest/configs/</code>.
  </p><p>
   To use this, follow these steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Edit the
     <code class="literal">/opt/stack/tempest/configs/tempest_region1.conf</code> file.
    </p></li><li class="step "><p>
     To test specific service, edit the <code class="literal">[service_available]</code>
     section and clear the comment character <code class="literal">#</code> and set a
     line to <code class="literal">true</code> to test that service or
     <code class="literal">false</code> to not test that service.
    </p><div class="verbatim-wrap"><pre class="screen">cinder = true
neutron = false</pre></div></li><li class="step "><p>
     To test specific features, edit any of the
     <code class="literal">*_feature_enabled</code> sections to enable or disable tests
     on specific features of a service.
    </p><div class="verbatim-wrap"><pre class="screen">[volume-feature-enabled]
[compute-feature-enabled]
[identity-feature-enabled]
[image-feature-enabled]
[network-feature-enabled]
[object-storage-feature-enabled]</pre></div><div class="verbatim-wrap"><pre class="screen">#Is the v2 identity API enabled (boolean value)
api_v2 = true
#Is the v3 identity API enabled (boolean value)
api_v3 = false</pre></div></li><li class="step "><p>
     Then run tests normally
    </p></li></ol></div></div></div><div class="sect2" id="sec-api-verification-list"><div class="titlepage"><div><div><h3 class="title"><span class="number">38.1.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Run Tests Matching a Series of White and Blacklists</span> <a title="Permalink" class="permalink" href="#sec-api-verification-list">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-api_verification.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-api_verification.xml</li><li><span class="ds-label">ID: </span>sec-api-verification-list</li></ul></div></div></div></div><p>
   You can run tests against specific scenarios by editing or creating a run
   filter file.
  </p><p>
   Run filter files are deployed under
   <code class="literal">/opt/stack/tempest/run_filters</code>.
  </p><p>
   Use run filters to whitelist or blacklist specific tests or groups of tests:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     lines starting with # or empty are ignored
    </p></li><li class="listitem "><p>
     lines starting with <code class="literal">+</code> are whitelisted
    </p></li><li class="listitem "><p>
     lines starting with <code class="literal">-</code> are blacklisted
    </p></li><li class="listitem "><p>
     lines not matching any of the above conditions are blacklisted
    </p></li></ul></div><p>
   If whitelist is empty, all available tests are fed to blacklist. If
   blacklist is empty, all tests from whitelist are returned.
  </p><p>
   Whitelist is applied first. The blacklist is executed against the set of
   tests returned by the whitelist.
  </p><p>
   To run whitelist and blacklist tests:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Make sure you can access the cloud:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts cloud-client-setup.yml
source /etc/environment</pre></div></li><li class="step "><p>
     Run the tests:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts tempest-run.yml  -e run_filter &lt;run_filter_name&gt;</pre></div></li></ol></div></div><p>
   Note that the run_filter_name is the name of the run_filter file except for
   the extension. For instance, to run using the filter from the file
   /opt/stack/tempest/run_filters/ci.txt, use the following:
  </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts tempest-run.yml -e run_filter=ci</pre></div><p>
   Documentation on the format of white and black-lists is available at:
  </p><div class="verbatim-wrap"><pre class="screen">/opt/stack/tempest/bin/tests2skip.py</pre></div><p>
   Example:
  </p><p>
   The following entries run API tests, exclude tests that are less relevant
   for deployment validation, such as negative, admin, cli and third-party (EC2)
   tests:
  </p><div class="verbatim-wrap"><pre class="screen">+tempest\.api\.*
*[Aa]dmin.*
*[Nn]egative.*
- tempest\.cli.*
- tempest\.thirdparty\.*</pre></div></div></div><div class="sect1" id="sec-verify-block-storage-swift"><div class="titlepage"><div><div><h2 class="title"><span class="number">38.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Verify the Object Storage (swift) Operations</span> <a title="Permalink" class="permalink" href="#sec-verify-block-storage-swift">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-verify_swift.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-verify_swift.xml</li><li><span class="ds-label">ID: </span>sec-verify-block-storage-swift</li></ul></div></div></div></div><p>
   For information about verifying the operations, see
   <span class="intraxref">Book “Operations Guide CLM”, Chapter 9 “Managing Object Storage”, Section 9.1 “Running the swift Dispersion Report”</span>.
  </p></div><div class="sect1" id="upload-image"><div class="titlepage"><div><div><h2 class="title"><span class="number">38.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Uploading an Image for Use</span> <a title="Permalink" class="permalink" href="#upload-image">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-operations-upload_image.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-operations-upload_image.xml</li><li><span class="ds-label">ID: </span>upload-image</li></ul></div></div></div></div><p>
  To create a Compute instance, you need to obtain an image that you can use.
  The Cloud Lifecycle Manager provides an Ansible playbook that will
  download a CirrOS Linux image, and then upload it as a public image to your
  image repository for use across your projects.
 </p><div class="sect2" id="id-1.3.7.2.6.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">38.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Running the Playbook</span> <a title="Permalink" class="permalink" href="#id-1.3.7.2.6.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-operations-upload_image.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-operations-upload_image.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Use the following command to run this playbook:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts glance-cloud-configure.yml -e proxy=&lt;PROXY&gt;</pre></div><p>
   The table below shows the optional switch that you can use as part of this
   playbook to specify environment-specific information:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Switch</th><th>Description</th></tr></thead><tbody><tr><td>
       <p>
        <code class="literal">-e proxy="&lt;proxy_address:port&gt;"</code>
       </p>
      </td><td>
       <p>
        Optional. If your environment requires a proxy for the internet, use
        this switch to specify the proxy information.
       </p>
      </td></tr></tbody></table></div></div><div class="sect2" id="id-1.3.7.2.6.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">38.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How to Curate Your Own Images</span> <a title="Permalink" class="permalink" href="#id-1.3.7.2.6.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-operations-upload_image.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-operations-upload_image.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   OpenStack has created a guide to show you how to obtain, create, and modify
   images that will be compatible with your cloud:
  </p><p>
   <a class="link" href="http://docs.openstack.org/image-guide/content/" target="_blank">OpenStack
   Virtual Machine Image Guide</a>
  </p></div><div class="sect2" id="id-1.3.7.2.6.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">38.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the python-glanceclient CLI to Create Images</span> <a title="Permalink" class="permalink" href="#id-1.3.7.2.6.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-operations-upload_image.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-operations-upload_image.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   You can use the glanceClient on a machine accessible to your cloud or on
   your Cloud Lifecycle Manager where it is automatically installed.
  </p><p>
   The OpenStackClient allows you to create, update, list, and delete images as
   well as manage your image member lists, which allows you to share access to
   images across multiple tenants. As with most of the OpenStack CLI tools, you
   can use the <code class="literal">openstack help</code> command to get a full list of
   commands as well as their syntax.
  </p><p>
   If you would like to use the <code class="literal">--copy-from</code> option when
   creating an image, you will need to have your Administrator enable the http
   store in your environment using the instructions outlined at
   <span class="intraxref">Book “Operations Guide CLM”, Chapter 6 “Managing Compute”, Section 6.7 “Configuring the Image Service”, Section 6.7.2 “Allowing the glance copy-from option in your environment”</span>.
  </p></div></div><div class="sect1" id="create-extnet"><div class="titlepage"><div><div><h2 class="title"><span class="number">38.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating an External Network</span> <a title="Permalink" class="permalink" href="#create-extnet">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-operations-create_extnet.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-operations-create_extnet.xml</li><li><span class="ds-label">ID: </span>create-extnet</li></ul></div></div></div></div><p>
  You must have an external network set up to allow your Compute instances to
  reach the internet. There are multiple methods you can use to create this
  external network and we provide two of them here. The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installer
  provides an Ansible playbook that will create this network for use across
  your projects. We also show you how to create this network via the command
  line tool from your Cloud Lifecycle Manager.
 </p><div class="sect2" id="sec-create-extnet-playbook"><div class="titlepage"><div><div><h3 class="title"><span class="number">38.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Ansible Playbook</span> <a title="Permalink" class="permalink" href="#sec-create-extnet-playbook">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-operations-create_extnet.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-operations-create_extnet.xml</li><li><span class="ds-label">ID: </span>sec-create-extnet-playbook</li></ul></div></div></div></div><p>
   This playbook will query the Networking service for an existing external
   network, and then create a new one if you do not already have one. The
   resulting external network will have the name <code class="literal">ext-net</code>
   with a subnet matching the CIDR you specify in the command below.
  </p><p>
   If you need to specify more granularity, for example specifying an
   allocation pool for the subnet then you should utilize the
   <a class="xref" href="#sec-create-extnet-cli" title="38.4.2. Using the OpenStackClient CLI">Section 38.4.2, “Using the OpenStackClient CLI”</a>.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-cloud-configure.yml -e EXT_NET_CIDR=&lt;CIDR&gt;</pre></div><p>
   The table below shows the optional switch that you can use as part of this
   playbook to specify environment-specific information:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Switch</th><th>Description</th></tr></thead><tbody><tr><td>
       <p>
        <code class="literal">-e EXT_NET_CIDR=&lt;CIDR&gt;</code>
       </p>
      </td><td>
       <p>
        Optional. You can use this switch to specify the external network CIDR.
        If you do not use this switch, or use a wrong value, the VMs
        will not be accessible over the network.
       </p>
       <p>
        This CIDR will be from the <code class="literal">EXTERNAL VM</code> network.
       </p>
       <div id="id-1.3.7.2.7.3.6.1.4.1.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
         If this option is not defined the default value is "172.31.0.0/16"
        </p></div>
      </td></tr></tbody></table></div></div><div class="sect2" id="sec-create-extnet-cli"><div class="titlepage"><div><div><h3 class="title"><span class="number">38.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the OpenStackClient CLI</span> <a title="Permalink" class="permalink" href="#sec-create-extnet-cli">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-operations-create_extnet.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-operations-create_extnet.xml</li><li><span class="ds-label">ID: </span>sec-create-extnet-cli</li></ul></div></div></div></div><p>
   For more granularity you can utilize the OpenStackClient to create
   your external network.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Source the Admin credentials:
    </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc</pre></div></li><li class="step "><p>
     Create the external network and then the subnet using these commands
     below.
    </p><p>
     Creating the network:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network create --router:external &lt;external-network-name&gt;</pre></div><p>
     Creating the subnet:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack subnet create &lt;external-network-name&gt; &lt;CIDR&gt; --gateway &lt;gateway&gt; \
--allocation-pool start=&lt;IP_start&gt;,end=&lt;IP_end&gt; [--disable-dhcp]</pre></div><p>
     Where:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Value</th><th>Description</th></tr></thead><tbody><tr><td>external-network-name</td><td>
         <p>
          This is the name given to your external network. This is a unique
          value that you will choose. The value <code class="literal">ext-net</code> is
          usually used.
         </p>
        </td></tr><tr><td>CIDR</td><td>
         <p>
          You can use this switch to specify the external network CIDR. If you
          choose not to use this switch, or use a wrong value, the VMs will not
          be accessible over the network.
         </p>
         <p>
          This CIDR will be from the EXTERNAL VM network.
         </p>
        </td></tr><tr><td>--gateway</td><td>
         <p>
          Optional switch to specify the gateway IP for your subnet. If this
          is not included then it will choose the first available IP.
         </p>
        </td></tr><tr><td>
         --allocation-pool start end
        </td><td>
         <p>
          Optional switch to specify start and end IP addresses to use as the
          allocation pool for this subnet.
         </p>
        </td></tr><tr><td>--disable-dhcp</td><td>
         <p>
          Optional switch if you want to disable DHCP on this subnet. If this
          is not specified, DHCP will be enabled.
         </p>
        </td></tr></tbody></table></div></li></ol></div></div></div><div class="sect2" id="id-1.3.7.2.7.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">38.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Next Steps</span> <a title="Permalink" class="permalink" href="#id-1.3.7.2.7.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-operations-create_extnet.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-operations-create_extnet.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Once the external network is created, users can create a Private Network to
   complete their networking setup.
  </p></div></div></div><div class="chapter " id="ui-verification"><div class="titlepage"><div><div><h2 class="title"><span class="number">39 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">UI Verification</span> <a title="Permalink" class="permalink" href="#ui-verification">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-ui_verification.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-ui_verification.xml</li><li><span class="ds-label">ID: </span>ui-verification</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#sec-verify-block-storage-volume"><span class="number">39.1 </span><span class="name">Verifying Your Block Storage Backend</span></a></span></dt></dl></div></div><p>
  Once you have completed your cloud deployment, these are some of the common
  post-installation tasks you may need to perform to verify your cloud
  installation.
 </p><div class="sect1" id="sec-verify-block-storage-volume"><div class="titlepage"><div><div><h2 class="title"><span class="number">39.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Verifying Your Block Storage Backend</span> <a title="Permalink" class="permalink" href="#sec-verify-block-storage-volume">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-verify_block_storage.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-verify_block_storage.xml</li><li><span class="ds-label">ID: </span>sec-verify-block-storage-volume</li></ul></div></div></div></div><p>
  The sections below will show you the steps to verify that your Block Storage
  backend was setup properly.
 </p><div class="sect2" id="id-1.3.7.3.3.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">39.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create a Volume</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.3.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-verify_block_storage.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-verify_block_storage.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Perform the following steps to create a volume using horizon dashboard.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log into the horizon dashboard.
    </p></li><li class="step "><p>
     Choose <span class="guimenu ">Project</span> › <span class="guimenu ">Compute</span> › <span class="guimenu ">Volumes</span>.
    </p></li><li class="step "><p>
     On the <span class="guimenu ">Volumes</span> tabs, click the
     <span class="guimenu ">Create Volume</span> button to create a volume.
    </p></li><li class="step "><p>
     In the <span class="guimenu ">Create Volume</span> options, enter the
     required details into the fields and then click the
     <span class="guimenu ">Create Volume</span> button:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Volume Name - This is the name you specify for your volume.
      </p></li><li class="step "><p>
       Description (optional) - This is an optional description for the volume.
      </p></li><li class="step "><p>
       Type - Select the volume type you have created for your volumes from the
       drop down.
      </p></li><li class="step "><p>
       Size (GB) - Enter the size, in GB, you would like the volume to be.
      </p></li><li class="step "><p>
       Availability Zone - You can either leave this at the default option of
       <span class="guimenu ">Any Availability Zone</span> or select a
       specific zone from the drop-down box.
      </p></li></ol></li></ol></div></div><p>
   The dashboard will then show the volume you have just created.
  </p></div><div class="sect2" id="id-1.3.7.3.3.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">39.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Attach Volume to an Instance</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.3.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-verify_block_storage.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-verify_block_storage.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Perform the following steps to attach a volume to an instance:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log into the horizon dashboard.
    </p></li><li class="step "><p>
     Choose <span class="guimenu ">Project</span> › <span class="guimenu ">Compute</span> › <span class="guimenu ">Instances</span>.
    </p></li><li class="step "><p>
     In the <span class="guimenu ">Action</span> column, choose the
     <span class="guimenu ">Edit Attachments</span> in the drop-down
     box next to the instance you want to attach the volume to.
    </p></li><li class="step "><p>
     In the <span class="guimenu ">Attach To Instance</span> drop-down,
     select the volume that you want to attach.
    </p></li><li class="step "><p>
     Edit the <span class="guimenu ">Device Name</span> if necessary.
    </p></li><li class="step "><p>
     Click <span class="guimenu ">Attach Volume</span> to complete the
     action.
    </p></li><li class="step "><p>
     On the <span class="guimenu ">Volumes</span> screen,
     verify that the volume you attached is displayed in the
     <span class="guimenu ">Attached To</span> columns.
    </p></li></ol></div></div></div><div class="sect2" id="id-1.3.7.3.3.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">39.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Detach Volume from Instance</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.3.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-verify_block_storage.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-verify_block_storage.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Perform the following steps to detach the volume from instance:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log into the horizon dashboard.
    </p></li><li class="step "><p>
     Choose <span class="guimenu ">Project</span> › <span class="guimenu ">Compute</span> › <span class="guimenu ">Instances</span>.
    </p></li><li class="step "><p>
     Click the check box next to the name of the volume you want to detach.
    </p></li><li class="step "><p>
     In the <span class="guimenu ">Action</span> column, choose the
     <span class="guimenu ">Edit Attachments</span> in the drop-down
     box next to the instance you want to attach the volume to.
    </p></li><li class="step "><p>
     Click <span class="guimenu ">Detach Attachment</span>. A confirmation
     dialog box appears.
    </p></li><li class="step "><p>
     Click <span class="guimenu ">Detach Attachment</span> to confirm the
     detachment of the volume from the associated instance.
    </p></li></ol></div></div></div><div class="sect2" id="id-1.3.7.3.3.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">39.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Delete Volume</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.3.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-verify_block_storage.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-verify_block_storage.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Perform the following steps to delete a volume using horizon dashboard:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log into the horizon dashboard.
    </p></li><li class="step "><p>
     Choose <span class="guimenu ">Project</span> › <span class="guimenu ">Compute</span> › <span class="guimenu ">Volumes</span>.
    </p></li><li class="step "><p>
     In the <span class="guimenu ">Actions</span> column, click
     <span class="guimenu ">Delete Volume</span> next to the volume you
     would like to delete.
    </p></li><li class="step "><p>
     To confirm and delete the volume, click <span class="guimenu ">Delete Volume</span>
     again.
    </p></li><li class="step "><p>
     Verify that the volume was removed from the
     <span class="guimenu ">Volumes</span> screen.
    </p></li></ol></div></div></div><div class="sect2" id="id-1.3.7.3.3.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">39.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Verifying Your Object Storage (swift)</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.3.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-verify_block_storage.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-verify_block_storage.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following procedure shows how to validate that all servers have been
   added to the swift rings:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Run the swift-compare-model-rings.yml playbook as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml</pre></div></li><li class="step "><p>
     Search for output similar to the following. Specifically, look at the
     number of drives that are proposed to be added.
    </p><div class="verbatim-wrap"><pre class="screen">TASK: [swiftlm-ring-supervisor | validate-input-model | Print report] *********
ok: [ardana-cp1-c1-m1-mgmt] =&gt; {
    "var": {
        "report.stdout_lines": [
            "Rings:",
            "  ACCOUNT:",
            "    ring exists",
            "    no device changes",
            "    ring will be rebalanced",
            "  CONTAINER:",
            "    ring exists",
            "    no device changes",
            "    ring will be rebalanced",
            "  OBJECT-0:",
            "    ring exists",
            "    no device changes",
            "    ring will be rebalanced"
        ]
    }
}</pre></div></li><li class="step "><p>
     If the text contains "no device changes" then the deploy was successful
     and no further action is needed.
    </p></li><li class="step "><p>
     If more drives need to be added, it indicates that the deploy
     failed on some nodes and that you restarted the deploy to include those
     nodes. However, the nodes are not in the swift rings because enough time
     has not elapsed to allow the rings to be rebuilt. You have two options to
     continue:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Repeat the deploy. There are two steps:
      </p><ol type="i" class="substeps "><li class="step "><p>
         Delete the ring builder files as described in
         <span class="intraxref">Book “Operations Guide CLM”, Chapter 18 “Troubleshooting Issues”, Section 18.6 “Storage Troubleshooting”, Section 18.6.2 “swift Storage Troubleshooting”, Section 18.6.2.8 “Restarting the Object Storage Deployment”</span>.
        </p></li><li class="step "><p>
         Repeat the installation process starting by running the
         <code class="filename">site.yml</code> playbook as described in
         <a class="xref" href="#sec-kvm-deploy" title="24.7. Deploying the Cloud">Section 24.7, “Deploying the Cloud”</a>.
        </p></li></ol></li><li class="step "><p>
       Rebalance the rings several times until all drives are incorporated in
       the rings. This process may take several hours to complete (because you
       need to wait one hour between each rebalance). The steps are as follows:
      </p><ol type="i" class="substeps "><li class="step "><p>
         Change the min-part-hours to 1 hour. See
         <span class="intraxref">Book “Operations Guide CLM”, Chapter 9 “Managing Object Storage”, Section 9.5 “Managing swift Rings”, Section 9.5.7 “Changing min-part-hours in Swift”</span>.
        </p></li><li class="step "><p>
         Use the "First phase of ring rebalance" and "Final rebalance phase" as
         described in <span class="intraxref">Book “Operations Guide CLM”, Chapter 9 “Managing Object Storage”, Section 9.5 “Managing swift Rings”, Section 9.5.5 “Applying Input Model Changes to Existing Rings”</span>.
         The <span class="quote">“<span class="quote ">Weight change phase of ring rebalance</span>”</span> does not
         apply because you have not set the weight-step attribute at this
         stage.
        </p></li><li class="step "><p>
         Set the min-part-hours to the recommended 16 hours as described in
         <span class="intraxref">Book “Operations Guide CLM”, Chapter 9 “Managing Object Storage”, Section 9.5 “Managing swift Rings”, Section 9.5.7 “Changing min-part-hours in Swift”</span>.
        </p></li></ol></li></ol></li></ol></div></div><p>
   If you receive errors during the validation, see
   <span class="intraxref">Book “Operations Guide CLM”, Chapter 18 “Troubleshooting Issues”, Section 18.6 “Storage Troubleshooting”, Section 18.6.2 “swift Storage Troubleshooting”, Section 18.6.2.3 “Interpreting Swift Input Model Validation Errors”</span>.
  </p></div></div></div><div class="chapter " id="install-openstack-clients"><div class="titlepage"><div><div><h2 class="title"><span class="number">40 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing OpenStack Clients</span> <a title="Permalink" class="permalink" href="#install-openstack-clients">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-install_openstack_clients.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-install_openstack_clients.xml</li><li><span class="ds-label">ID: </span>install-openstack-clients</li></ul></div></div></div></div><div class="line"></div><p>
  If you have a standalone deployer, the OpenStack CLI and other clients will
  not be installed automatically on that node. If you require access to these
  clients, you will need to follow the procedure below to add the appropriate
  software.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    [OPTIONAL] Connect to your standalone deployer and try to use the OpenStack
    CLI:
   </p><div class="verbatim-wrap"><pre class="screen">source ~/keystone.osrc
<span class="bold"><strong>openstack project list</strong></span>

-bash: openstack: command not found</pre></div></li><li class="step "><p>
    Edit the configuration file containing details of your Control Plane,
    typically
    <code class="literal">~/openstack/my_cloud/definition/data/control_plane.yml</code>.
   </p></li><li class="step "><p>
    Locate the stanza for the cluster where you want to install the client(s).
    For a standalone deployer, this will look like the following extract:
   </p><div class="verbatim-wrap"><pre class="screen">      clusters:
        - name: cluster0
          cluster-prefix: c0
          server-role: LIFECYCLE-MANAGER-ROLE
          member-count: 1
          allocation-policy: strict
          service-components:
            - ntp-server
            - lifecycle-manager</pre></div></li><li class="step "><p>
    Choose the client(s) you wish to install from the following list of
    available clients:
   </p><div class="verbatim-wrap"><pre class="screen"> - barbican-client
 - ceilometer-client
 - cinder-client
 - designate-client
 - glance-client
 - heat-client
 - ironic-client
 - keystone-client
 - magnum-client
 - manila-client
 - monasca-client
 - neutron-client
 - nova-client
 - ntp-client
 - octavia-client
 - openstack-client
 - swift-client</pre></div></li><li class="step "><p>
    Add the client(s) to the list of <code class="literal">service-components</code> - in
    this example, we add the <code class="literal">openstack-client</code> to the
    standalone deployer:
   </p><div class="verbatim-wrap"><pre class="screen">      clusters:
        - name: cluster0
          cluster-prefix: c0
          server-role: LIFECYCLE-MANAGER-ROLE
          member-count: 1
          allocation-policy: strict
          service-components:
            - ntp-server
            - lifecycle-manager
            <span class="bold"><strong>- openstack-client
            - cinder-client
            - designate-client
            - glance-client
            - heat-client
            - ironic-client
            - keystone-client
            - neutron-client
            - nova-client
            - swift-client
            - monasca-client
            - barbican-client
</strong></span></pre></div></li><li class="step "><p>
    Commit the configuration changes:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "Add explicit client service deployment"</pre></div></li><li class="step "><p>
    Run the configuration processor, followed by the
    <code class="literal">ready-deployment</code> playbook:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml -e encrypt="" \
  -e rekey=""
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
    Add the software for the clients using the following command:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts clients-upgrade.yml</pre></div></li><li class="step "><p>
    Check that the software has been installed correctly. In this instance,
    connect to your standalone deployer and try to use the OpenStack CLI:
   </p><div class="verbatim-wrap"><pre class="screen">source ~/keystone.osrc
openstack project list</pre></div><p>
    You should now see a list of projects returned:
   </p><div class="verbatim-wrap"><pre class="screen">stack@ardana-cp1-c0-m1-mgmt:~$ <span class="bold"><strong>openstack project list</strong></span>

+----------------------------------+------------------+
| ID                               | Name             |
+----------------------------------+------------------+
| 076b6e879f324183bbd28b46a7ee7826 | kronos           |
| 0b81c3a9e59c47cab0e208ea1bb7f827 | backup           |
| 143891c2a6094e2988358afc99043643 | octavia          |
| 1d3972a674434f3c95a1d5ed19e0008f | glance-swift     |
| 2e372dc57cac4915bf06bbee059fc547 | glance-check     |
| 383abda56aa2482b95fb9da0b9dd91f4 | monitor          |
| 606dd3b1fa6146668d468713413fb9a6 | swift-monitor    |
| 87db9d1b30044ea199f0293f63d84652 | admin            |
| 9fbb7494956a483ca731748126f50919 | demo             |
| a59d0c682474434a9ddc240ddfe71871 | services         |
| a69398f0f66a41b2872bcf45d55311a7 | swift-dispersion |
| f5ec48d0328d400992c1c5fb44ec238f | cinderinternal   |
+----------------------------------+------------------+</pre></div></li></ol></div></div></div><div class="chapter " id="tls30"><div class="titlepage"><div><div><h2 class="title"><span class="number">41 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Transport Layer Security (TLS)</span> <a title="Permalink" class="permalink" href="#tls30">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-operations-configuring_tls.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-operations-configuring_tls.xml</li><li><span class="ds-label">ID: </span>tls30</li></ul></div></div><div><div class="abstract"><p>
    TLS is enabled by default during the installation of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 and
    additional configuration options are available to secure your environment,
    as described below.
   </p></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#id-1.3.7.5.11"><span class="number">41.1 </span><span class="name">Configuring TLS in the input model</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.5.12"><span class="number">41.2 </span><span class="name">User-provided certificates and trust chains</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.5.13"><span class="number">41.3 </span><span class="name">Edit the input model to include your certificate files</span></a></span></dt><dt><span class="section"><a href="#sec-generate-certificate"><span class="number">41.4 </span><span class="name">Generate a self-signed CA</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.5.15"><span class="number">41.5 </span><span class="name">Generate a certificate signing request</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.5.16"><span class="number">41.6 </span><span class="name">Generate a server certificate</span></a></span></dt><dt><span class="section"><a href="#sec-upload-toclm"><span class="number">41.7 </span><span class="name">Upload to the Cloud Lifecycle Manager</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.5.18"><span class="number">41.8 </span><span class="name">Configuring the cipher suite</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.5.19"><span class="number">41.9 </span><span class="name">Testing</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.5.20"><span class="number">41.10 </span><span class="name">Verifying that the trust chain is correctly deployed</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.5.21"><span class="number">41.11 </span><span class="name">Turning TLS on or off</span></a></span></dt></dl></div></div><p>
  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9, you can provide your own certificate authority and
  certificates for internal and public virtual IP addresses (VIPs), and you
  should do so for any production cloud. The certificates automatically
  generated by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are useful for testing and setup, but you should always
  install your own for production use. Certificate installation is discussed
  below.
 </p><p>
  Read the following if you are using the default <code class="literal">cert-name:
  my-public-cert</code> in your model.
 </p><p>
  The bundled test certificate for public endpoints, located at
  <code class="filename">~/openstack/my_cloud/config/tls/certs/my-public-cert</code>, is
  now expired but was left in the product in case you changed the content with
  your valid certificate. Please verify if the certificate is expired and
  generate your own, as described in
  <a class="xref" href="#sec-generate-certificate" title="41.4. Generate a self-signed CA">Section 41.4, “Generate a self-signed CA”</a>.
 </p><p>
  You can verify the expiry date by running this command:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openssl x509 -in ~/openstack/my_cloud/config/tls/certs/my-public-cert \
-noout -enddate
notAfter=Oct  8 09:01:58 2016 GMT</pre></div><p>
  Before you begin, the following list of terms will be helpful when generating
  and installing certificates.
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.7.5.9.1"><span class="term "><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>-generated public CA</span></dt><dd><p>
     A <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>-generated public CA
     (<code class="filename">openstack_frontend_cacert.crt</code>) is available for you
     to use in <code class="filename">/etc/pki/trust/anchors/ca-certificates</code>.
    </p></dd><dt id="id-1.3.7.5.9.2"><span class="term ">Fully qualified domain name (FQDN) of the public VIP</span></dt><dd><p>
     The registered domain name. A FQDN is not mandatory. It is perfectly valid
     to have no FQDN and use IP addresses instead. Note that you can use FQDNs
     on public endpoints, and you may change them whenever the need arises.
    </p></dd><dt id="id-1.3.7.5.9.3"><span class="term ">Certificate authority (CA) certificate</span></dt><dd><p>
     Your certificates must be signed by a CA, such as your internal IT
     department or a public certificate authority. For this example we will use
     a self-signed certificate.
    </p></dd><dt id="id-1.3.7.5.9.4"><span class="term ">Server certificate</span></dt><dd><p>
     It is easy to confuse server certificates and CA certificates. Server
     certificates reside on the server and CA certificates reside on the
     client. A server certificate affirms that the server that sent it serves a
     set of IP addresses, domain names, and set of services. A CA certificate
     is used by the client to authenticate this claim.
    </p></dd><dt id="id-1.3.7.5.9.5"><span class="term ">SAN (subject-alt-name)</span></dt><dd><p>
     The set of IP addresses and domain names in a server certificate request:
     A template for a server certificate.
    </p></dd><dt id="id-1.3.7.5.9.6"><span class="term ">Certificate signing request (CSR)</span></dt><dd><p>
     A blob of data generated from a certificate request and sent to a CA,
     which would then sign it, produce a server certificate, and send it back.
    </p></dd><dt id="id-1.3.7.5.9.7"><span class="term ">External VIP</span></dt><dd><p>
     External virtual IP address
    </p></dd><dt id="id-1.3.7.5.9.8"><span class="term ">Internal VIP</span></dt><dd><p>
     Internal virtual IP address
    </p></dd></dl></div><p>
  The major difference between an external VIP certificate and an internal VIP
  certificate is that the internal VIP has approximately 40 domain names in the
  SAN. This is because each service has a different domain name in
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9. So it is unlikely that you can create an internal server
  certificate before running the configuration processor. But after a
  configuration processor run, a certificate request would be created for each
  of your cert-names.
 </p><div class="sect1" id="id-1.3.7.5.11"><div class="titlepage"><div><div><h2 class="title"><span class="number">41.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring TLS in the input model</span> <a title="Permalink" class="permalink" href="#id-1.3.7.5.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-operations-configuring_tls.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-operations-configuring_tls.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   For this example certificate configuration, let us assume there is no FQDN for
   the external VIP and that you are going to use the default IP address
   provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9. Let's also assume that for the internal VIP you
   will use the defaults as well. If you were to call your certificate
   authority "example-CA," the CA certificate would then be called
   "example-CA.crt" and the key would be called "example-CA.key." In the
   following examples, the external VIP certificate will be named
   "example-public-cert" and the internal VIP certificate will be named
   "example-internal-cert."
  </p><div id="id-1.3.7.5.11.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    Cautions:
   </p></div><p>
   Any time you make a cert change when using your own CA:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     You should use a distinct name from those already existing in
     <code class="filename">config/tls/cacerts</code>. This also means that you should
     not <span class="emphasis"><em>reuse</em></span> your CA names (and use unique and
     distinguishable names such as MyCompanyXYZ_PrivateRootCA.crt). A new name
     is what indicates that a file is new or changed, so reusing a name means
     that the file is not considered changed even its contents have changed.
    </p></li><li class="listitem "><p>
     You should not remove any existing CA files from
     <code class="filename">config/tls/cacerts</code>.
    </p></li><li class="listitem "><p>
     If you want to remove an existing CA you must
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       First remove the file.
      </p></li><li class="step "><p>
       Then run:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible -i hosts/verb_hosts FND-STN -a 'sudo keytool -delete -alias \
debian:&lt;filename to remove&gt; \
-keystore /usr/lib/jvm/java-7-openjdk-amd64/jre/lib/security/cacerts \
-storepass changeit'</pre></div></li></ol></div></div></li></ul></div><div id="id-1.3.7.5.11.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    Be sure to install your own certificate for all production clouds after
    installing and testing your cloud. If you ever want to test or troubleshoot
    later, you will be able to revert to the sample certificate to get back to
    a stable state for testing.
   </p></div><div id="id-1.3.7.5.11.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    Unless this is a new deployment, do not update both the certificate and the
    CA together. Add the CA first and then run a site deploy. Then update the
    certificate and run tls-reconfigure, FND-CLU-stop, FND-CLU-start and then
    ardana-reconfigure. If a playbook has failed, rerun it with -vv to get
    detailed error information. The configure, HAproxy restart, and reconfigure
    steps are included below. If this is a new deployment and you are adding
    your own certs/CA before running site.yml this caveat does not apply.
   </p></div><p>
   You can add your own certificate by following the instructions below. All
   changes must go into the file
   <code class="filename">~/openstack/my_cloud/definition/data/network_groups.yml</code>.
  </p><p>
   Below are the entries for TLS for the internal and admin load balancers:
  </p><div class="verbatim-wrap"><pre class="screen">- provider: ip-cluster
        name: lb
        tls-components:
        - default
        components:
        # These services do not currently support TLS so they are not listed
        # under tls-components
        - nova-metadata
        roles:
        - internal
        - admin
        cert-file: openstack-internal-cert
        # The openstack-internal-cert is a reserved name and
        # this certificate will be autogenerated. You
        # can bring in your own certificate with a different name

        # cert-file: customer-provided-internal-cert
        # replace this with name of file in "config/tls/certs/"</pre></div><p>
   The configuration processor will also create a request template for each
   named certificate under <code class="literal">info/cert_reqs/</code> This will be of
   the form:
  </p><div class="verbatim-wrap"><pre class="screen">info/cert_reqs/customer-provided-internal-cert</pre></div><p>
   These request templates contain the subject <code class="literal">Alt-names</code>
   that the certificates need. You can add to this template before generating
   your certificate signing request .
  </p><p>
   You would then send the CSR to your CA to be signed, and once you receive
   the certificate, place it in <code class="filename">config/tls/certs</code>.
  </p><p>
   When you bring in your own certificate, you may want to bring in the trust
   chains (or CA certificate) for this certificate. This is usually not
   required if the CA is a public signer that is typically bundled with the
   operating system. However, we suggest you include it anyway by copying the
   file into the directory <code class="filename">config/cacerts/</code>.
  </p></div><div class="sect1" id="id-1.3.7.5.12"><div class="titlepage"><div><div><h2 class="title"><span class="number">41.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">User-provided certificates and trust chains</span> <a title="Permalink" class="permalink" href="#id-1.3.7.5.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-operations-configuring_tls.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-operations-configuring_tls.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> generates its own internal certificates but is designed to allow
   you to bring in your own certificates for the VIPs. Here is the general
   process.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     You must have a server certificate and a CA certificate to go with it
     (unless the signer is a public CA and it is already bundled with most
     distributions).
    </p></li><li class="step "><p>
     You must decide the names of the server certificates and configure the
     <code class="literal">network_groups.yml</code> file in the input model such that
     each load balancer provider has at least one cert-name associated with it.
    </p></li><li class="step "><p>
     Run the configuration processor. Note that you may or may not have the
     certificate file at this point. The configuration processor would create
     certificate request file artifacts under
     <code class="literal">info/cert_reqs/</code> for each of the cert-name(s) in the
     <code class="literal">network_groups.yml</code> file. While there is no special
     reason to use the request file created for an external endpoint VIP
     certificate, it is important to use the request files created for internal
     certificates since the canonical names for the internal VIP can be many
     and service specific and each of these need to be in the Subject Alt Names
     attribute of the certificate.
    </p></li><li class="step "><p>
     Create a certificate signing request for this request file and send it to
     your internal CA or a public CA to get it certified and issued with a
     certificate. You will now have a server certificate and possibly a trust
     chain or CA certificate.
    </p></li><li class="step "><p>
     Next, upload it to the Cloud Lifecycle Manager. Server certificates should be added to
     <code class="filename">config/tls/certs</code> and CA certificates should be added
     to <code class="filename">config/tls/cacerts</code>. The file extension should be
     CRT file for the CA certificate to be processed by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. Detailed
     steps are next.
    </p></li></ol></div></div></div><div class="sect1" id="id-1.3.7.5.13"><div class="titlepage"><div><div><h2 class="title"><span class="number">41.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Edit the input model to include your certificate files</span> <a title="Permalink" class="permalink" href="#id-1.3.7.5.13">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-operations-configuring_tls.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-operations-configuring_tls.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Edit the load balancer configuration in
   <code class="literal">~/openstack/my_cloud/definition/data/network_groups.yml</code>:
  </p><div class="verbatim-wrap"><pre class="screen">load-balancers:
 - provider: ip-cluster
 name: lb
 tls-components:
 - default
 components:
 - nova-metadata
 roles:
 - internal
 - admin
 cert-file: example-internal-cert #&lt;&lt;&lt;---- Certificate name for the internal VIP

- provider: ip-cluster
 name: extlb
 external-name: myardana.test #&lt;&lt;&lt;--- Use just IP for the external VIP in this example
 tls-components:
 - default
 roles:
 - public
 cert-file: example-public-cert #&lt;&lt;&lt;---- Certificate name for the external VIP</pre></div><p>
   Commit your changes to the local git repository and run the configuration
   processor:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "changed VIP certificates"
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
   Verify that certificate requests have been generated by the configuration
   processor for every certificate file configured in the
   <code class="literal">networks_groups.yml</code> file. In this example, there are two
   files, as shown from the list command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls ~/openstack/my_cloud/info/cert_reqs
example-internal-cert
example-public-cert</pre></div></div><div class="sect1" id="sec-generate-certificate"><div class="titlepage"><div><div><h2 class="title"><span class="number">41.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Generate a self-signed CA</span> <a title="Permalink" class="permalink" href="#sec-generate-certificate">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-operations-configuring_tls.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-operations-configuring_tls.xml</li><li><span class="ds-label">ID: </span>sec-generate-certificate</li></ul></div></div></div></div><div id="id-1.3.7.5.14.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    In a production setting you will not perform this step. You will use your
    company's CA or a valid public CA.
   </p></div><p>
   This section demonstrates to how you can create your own self-signed CA and
   then use this CA to sign server certificates. This CA can be your
   organization's IT internal CA that is self-signed and whose CA certificates
   are deployed on your organization's machines. This way the server
   certificate becomes legitimate.
  </p><div id="id-1.3.7.5.14.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    Please use a unique CN for your example Certificate Authority and do not
    install multiple CA certificates with the same CN into your cloud.
   </p></div><p>
   Copy the commands below to the command line and execute. This will cause the
   two files, <code class="literal">example-CA.key</code> and
   <code class="literal">example-CA.crt</code> to be created:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>export EXAMPLE_CA_KEY_FILE='example-CA.key'
<code class="prompt user">ardana &gt; </code>export EXAMPLE_CA_CERT_FILE='example-CA.crt'
<code class="prompt user">ardana &gt; </code>openssl req -x509 -batch -newkey rsa:2048 -nodes -out "${EXAMPLE_CA_CERT_FILE}" \
-keyout "${EXAMPLE_CA_KEY_FILE}" \
-subj "/C=UK/O=hp/CN=YourOwnUniqueCertAuthorityName" \
-days 365</pre></div><p>
   You can tweak the subj and days settings above to meet your needs, or to
   test. For instance, if you want to test what happens when a CA expires, you
   can set 'days' to a very low value. Grab the configuration
   processor-generated request file from <code class="literal">info/cert_reqs/</code>:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cat ~/openstack/my_cloud/info/cert_reqs/example-internal-cert</pre></div><p>
   Now, copy this file to your working directory and append a
   <code class="literal">.req</code> extension to it.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cp ~/openstack/my_cloud/info/cert_reqs/example-internal-cert \
example-internal-cert.req</pre></div><div class="example" id="sec-tls-private-metadata"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 41.1: </span><span class="name">Certificate request file </span><a title="Permalink" class="permalink" href="#sec-tls-private-metadata">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">[req]
distinguished_name = req_distinguished_name
req_extensions = v3_req
prompt = no

[ req_distinguished_name ]
CN = "openstack-vip"

[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names

[ alt_names ]
DNS.1 = "deployerincloud-ccp-c0-m1-mgmt"
DNS.2 = "deployerincloud-ccp-vip-CEI-API-mgmt"
DNS.3 = "deployerincloud-ccp-vip-CND-API-mgmt"
DNS.4 = "deployerincloud-ccp-vip-DES-API-mgmt"
DNS.5 = "deployerincloud-ccp-vip-FND-MDB-mgmt"
DNS.6 = "deployerincloud-ccp-vip-FND-RMQ-mgmt"
DNS.7 = "deployerincloud-ccp-vip-FND-VDB-mgmt"
DNS.8 = "deployerincloud-ccp-vip-FRE-API-mgmt"
DNS.9 = "deployerincloud-ccp-vip-GLA-API-mgmt"
DNS.10 = "deployerincloud-ccp-vip-GLA-REG-mgmt"
DNS.11 = "deployerincloud-ccp-vip-HEA-ACF-mgmt"
DNS.12 = "deployerincloud-ccp-vip-HEA-ACW-mgmt"
DNS.13 = "deployerincloud-ccp-vip-HEA-API-mgmt"
DNS.14 = "deployerincloud-ccp-vip-HUX-SVC-mgmt"
DNS.15 = "deployerincloud-ccp-vip-HZN-WEB-mgmt"
DNS.16 = "deployerincloud-ccp-vip-KEY-API-mgmt"
DNS.17 = "deployerincloud-ccp-vip-KEYMGR-API-mgmt"
DNS.18 = "deployerincloud-ccp-vip-LOG-API-mgmt"
DNS.19 = "deployerincloud-ccp-vip-LOG-SVR-mgmt"
DNS.20 = "deployerincloud-ccp-vip-MON-API-mgmt"
DNS.21 = "deployerincloud-ccp-vip-NEU-SVR-mgmt"
DNS.22 = "deployerincloud-ccp-vip-NOV-API-mgmt"
DNS.23 = "deployerincloud-ccp-vip-NOV-MTD-mgmt"
DNS.24 = "deployerincloud-ccp-vip-OCT-API-mgmt"
DNS.25 = "deployerincloud-ccp-vip-OPS-WEB-mgmt"
DNS.26 = "deployerincloud-ccp-vip-SHP-API-mgmt"
DNS.27 = "deployerincloud-ccp-vip-SWF-PRX-mgmt"
DNS.28 = "deployerincloud-ccp-vip-admin-CEI-API-mgmt"
DNS.29 = "deployerincloud-ccp-vip-admin-CND-API-mgmt"
DNS.30 = "deployerincloud-ccp-vip-admin-DES-API-mgmt"
DNS.31 = "deployerincloud-ccp-vip-admin-FND-MDB-mgmt"
DNS.32 = "deployerincloud-ccp-vip-admin-FRE-API-mgmt"
DNS.33 = "deployerincloud-ccp-vip-admin-GLA-API-mgmt"
DNS.34 = "deployerincloud-ccp-vip-admin-HEA-ACF-mgmt"
DNS.35 = "deployerincloud-ccp-vip-admin-HEA-ACW-mgmt"
DNS.36 = "deployerincloud-ccp-vip-admin-HEA-API-mgmt"
DNS.37 = "deployerincloud-ccp-vip-admin-HUX-SVC-mgmt"
DNS.38 = "deployerincloud-ccp-vip-admin-HZN-WEB-mgmt"
DNS.39 = "deployerincloud-ccp-vip-admin-KEY-API-mgmt"
DNS.40 = "deployerincloud-ccp-vip-admin-KEYMGR-API-mgmt"
DNS.41 = "deployerincloud-ccp-vip-admin-MON-API-mgmt"
DNS.42 = "deployerincloud-ccp-vip-admin-NEU-SVR-mgmt"
DNS.43 = "deployerincloud-ccp-vip-admin-NOV-API-mgmt"
DNS.44 = "deployerincloud-ccp-vip-admin-OPS-WEB-mgmt"
DNS.45 = "deployerincloud-ccp-vip-admin-SHP-API-mgmt"
DNS.46 = "deployerincloud-ccp-vip-admin-SWF-PRX-mgmt"
DNS.47 = "192.168.245.5"
IP.1 = "192.168.245.5"

=============end of certificate request file.</pre></div></div></div><div id="id-1.3.7.5.14.12" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    In the case of a public VIP certificate, please add all the FQDNs you want
    it to support Currently <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> does not add the hostname for the
    external-name specified in <code class="literal">network_groups.yml</code> to the
    certificate request file . However, you can add it to the certificate
    request file manually. Here we assume that <code class="literal">myardana.test</code>
    is your external-name. In that case you would add this line (to the
    certificate request file that is shown above in
    <a class="xref" href="#sec-tls-private-metadata" title="Certificate request file">Example 41.1, “Certificate request file”</a>):
   </p><div class="verbatim-wrap"><pre class="screen">DNS.48 = "myardana.test"</pre></div></div><div id="id-1.3.7.5.14.13" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    Any attempt to use IP addresses rather than FQDNs in certificates must use
    subject alternate name entries that list both the IP address (needed for
    Google) and DNS with an IP (needed for a Python bug workaround). Failure to
    create the certificates in this manner will cause future installations of
    Go-based tools (such as Cloud Foundry, Stackato and other PaaS components)
    to fail.
   </p></div></div><div class="sect1" id="id-1.3.7.5.15"><div class="titlepage"><div><div><h2 class="title"><span class="number">41.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Generate a certificate signing request</span> <a title="Permalink" class="permalink" href="#id-1.3.7.5.15">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-operations-configuring_tls.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-operations-configuring_tls.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Now that you have a CA and a certificate request file, it is time to generate
   a CSR.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>export EXAMPLE_SERVER_KEY_FILE='example-internal-cert.key'
<code class="prompt user">ardana &gt; </code>export EXAMPLE_SERVER_CSR_FILE='example-internal-cert.csr'
<code class="prompt user">ardana &gt; </code>export EXAMPLE_SERVER_REQ_FILE=example-internal-cert.req
<code class="prompt user">ardana &gt; </code>openssl req -newkey rsa:2048 -nodes -keyout "$EXAMPLE_SERVER_KEY_FILE" \
-out "$EXAMPLE_SERVER_CSR_FILE" -extensions v3_req -config "$EXAMPLE_SERVER_REQ_FILE"</pre></div><p>
   Note that in production you would usually send the generated
   <code class="literal">example-internal-cert.csr</code> file to your IT department. But
   in this example you are your own CA, so sign and generate a server
   certificate.
  </p></div><div class="sect1" id="id-1.3.7.5.16"><div class="titlepage"><div><div><h2 class="title"><span class="number">41.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Generate a server certificate</span> <a title="Permalink" class="permalink" href="#id-1.3.7.5.16">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-operations-configuring_tls.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-operations-configuring_tls.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div id="id-1.3.7.5.16.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    In a production setting you will not perform this step. You will send the
    CSR created in the previous section to your company CA or a to a valid
    public CA and have them sign and send you back the certificate.
   </p></div><p>
   This section demonstrates how you would use your own self-signed CA that
   your created earlier to sign and generate a server certificate. A server
   certificate is essentially a signed public key, the signer being a CA and
   trusted by a client. When you install this the signing CA's certificate
   (called CA certificate or trust chain) on the client machine, you are
   telling the client to trust this CA, and thereby implicitly trusting any
   server certificates that are signed by this CA, thus creating a trust
   anchor.
  </p><p>
   <span class="bold"><strong>CA configuration file</strong></span>
  </p><p>
   When the CA signs the certificate, it uses a configuration file that tells
   it to verify the CSR. Note that in a production scenario the CA takes care
   of this for you.
  </p><p>
   Create a file called <code class="literal">openssl.cnf</code> and add the following
   contents to it.
  </p><div class="verbatim-wrap"><pre class="screen"># Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
#...

# OpenSSL configuration file.
#

# Establish working directory.

dir = .

[ ca ]
default_ca = CA_default

[ CA_default ]
serial = $dir/serial
database = $dir/index.txt
new_certs_dir = $dir/
certificate = $dir/cacert.pem
private_key = $dir/cakey.pem
unique_subject = no
default_crl_days = 365
default_days = 365
default_md = md5
preserve = no
email_in_dn = no
nameopt = default_ca
certopt = default_ca
policy = policy_match
copy_extensions = copy


[ policy_match ]
countryName = optional
stateOrProvinceName = optional
organizationName = optional
organizationalUnitName = optional
commonName = supplied
emailAddress = optional

[ req ]
default_bits = 1024 # Size of keys
default_keyfile = key.pem # name of generated keys
default_md = md5 # message digest algorithm
string_mask = nombstr # permitted characters
distinguished_name = req_distinguished_name
req_extensions = v3_req
x509_extensions = v3_ca

[ req_distinguished_name ]
# Variable name Prompt string
#---------------------- ----------------------------------
0.organizationName = Organization Name (company)
organizationalUnitName = Organizational Unit Name (department, division)
emailAddress = Email Address
emailAddress_max = 40
localityName = Locality Name (city, district)
stateOrProvinceName = State or Province Name (full name)
countryName = Country Name (2 letter code)
countryName_min = 2
countryName_max = 2
commonName = Common Name (hostname, IP, or your name)
commonName_max = 64

# Default values for the above, for consistency and less typing.
# Variable name Value
#------------------------------ ------------------------------
0.organizationName_default = Exampleco PLC
localityName_default = Anytown
stateOrProvinceName_default = Anycounty
countryName_default = UK
commonName_default = my-CA

[ v3_ca ]
basicConstraints = CA:TRUE
subjectKeyIdentifier = hash
authorityKeyIdentifier = keyid:always,issuer:always
subjectAltName = @alt_names

[ v3_req ]
basicConstraints = CA:FALSE
subjectKeyIdentifier = hash

[ alt_names ]

######### end of openssl.cnf #########</pre></div><p>
   <span class="bold"><strong>Sign and create a server certificate</strong></span>
  </p><p>
   Now you can sign the server certificate with your CA. Copy the commands
   below to the command line and execute. This will cause the one file,
   example-internal-cert.crt, to be created:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>export EXAMPLE_SERVER_CERT_FILE='example-internal-cert.crt'
<code class="prompt user">ardana &gt; </code>export EXAMPLE_SERVER_CSR_FILE='example-internal-cert.csr'
<code class="prompt user">ardana &gt; </code>export EXAMPLE_CA_KEY_FILE='example-CA.key'
<code class="prompt user">ardana &gt; </code>export EXAMPLE_CA_CERT_FILE='example-CA.crt'
<code class="prompt user">ardana &gt; </code>touch index.txt
<code class="prompt user">ardana &gt; </code>openssl rand -hex -out serial 6
<code class="prompt user">ardana &gt; </code>openssl ca -batch -notext -md sha256 -in "$EXAMPLE_SERVER_CSR_FILE" \
-cert "$EXAMPLE_CA_CERT_FILE" \
-keyfile "$EXAMPLE_CA_KEY_FILE" \
-out "$EXAMPLE_SERVER_CERT_FILE" \
-config openssl.cnf -extensions v3_req</pre></div><p>
   Finally, concatenate both the server key and certificate in preparation for
   uploading to the Cloud Lifecycle Manager.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cat example-internal-cert.key example-internal-cert.crt &gt; example-internal-cert</pre></div><p>
   Note that you have only created the internal-cert in this example. Repeat
   the above sequence for example-public-cert. Make sure you use the
   appropriate certificate request generated by the configuration processor.
  </p></div><div class="sect1" id="sec-upload-toclm"><div class="titlepage"><div><div><h2 class="title"><span class="number">41.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Upload to the Cloud Lifecycle Manager</span> <a title="Permalink" class="permalink" href="#sec-upload-toclm">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-operations-configuring_tls.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-operations-configuring_tls.xml</li><li><span class="ds-label">ID: </span>sec-upload-toclm</li></ul></div></div></div></div><p>
   The following two files created from the example run above will need to be
   uploaded to the Cloud Lifecycle Manager and copied into <code class="filename">config/tls</code>.
  </p><div class="itemizedlist " id="ul-zcc-v1c-5v"><ul class="itemizedlist"><li class="listitem "><p>
     example-internal-cert
    </p></li><li class="listitem "><p>
     example-CA.crt
    </p></li></ul></div><p>
   Once on the Cloud Lifecycle Manager, execute the following two copy commands to copy to their
   respective directories. Note if you had created an external cert, you can
   copy that in a similar manner, specifying its name using the copy command as
   well.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cp example-internal-cert ~/openstack/my_cloud/config/tls/certs/
<code class="prompt user">ardana &gt; </code>cp example-CA.crt ~/openstack/my_cloud/config/tls/cacerts/</pre></div><p>
   <span class="bold"><strong>Continue with the deployment</strong></span>
  </p><p>
   Next, log into the Cloud Lifecycle Manager node, and save and commit the changes to the local
   git repository:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "updated certificate and CA"</pre></div><p>
   Next, rerun the <code class="literal">config-processor-run</code> playbook, and run
   <code class="literal">ready-deployment.yml</code>:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div><p>
   If you receive any prompts, enter the required information.
  </p><div id="id-1.3.7.5.17.12" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    For automated installation (for example CI) you can specify the required
    passwords on the Ansible command line. For example, the command below will
    disable encryption by the configuration processor:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml -e encrypt="" -e rekey=""</pre></div></div><p>
   Run this series of runbooks to complete the deployment:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts tls-reconfigure.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts FND-CLU-stop.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts FND-CLU-start.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-stop.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml</pre></div></div><div class="sect1" id="id-1.3.7.5.18"><div class="titlepage"><div><div><h2 class="title"><span class="number">41.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the cipher suite</span> <a title="Permalink" class="permalink" href="#id-1.3.7.5.18">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-operations-configuring_tls.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-operations-configuring_tls.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   By default, the cipher suite is set to:
   <code class="literal">HIGH:!aNULL:!eNULL:!DES:!3DES</code>. This setting is
   recommended in the
   <a class="link" href="http://docs.openstack.org/security-guide/secure-communication/introduction-to-ssl-and-tls.html" target="_blank">OpenStack
   documentation site</a>. You may override this. To do so, open
   <code class="filename">config/haproxy/defaults.yml</code> and edit it. The parameters
   can be found under the <code class="literal">haproxy_globals</code> list.
  </p><div class="verbatim-wrap"><pre class="screen">- "ssl-default-bind-ciphers HIGH:!aNULL:!eNULL:!DES:!3DES"
- "ssl-default-server-ciphers HIGH:!aNULL:!eNULL:!DES:!3DES"</pre></div><p>
   Make the changes as needed. It is best to keep the two options identical.
  </p></div><div class="sect1" id="id-1.3.7.5.19"><div class="titlepage"><div><div><h2 class="title"><span class="number">41.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Testing</span> <a title="Permalink" class="permalink" href="#id-1.3.7.5.19">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-operations-configuring_tls.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-operations-configuring_tls.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   You can easily determine if an endpoint is behind TLS. To do so, run the
   following command, which probes a keystone identity service endpoint that is
   behind TLS:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>echo | openssl s_client -connect 192.168.245.5:5000 | openssl x509 -fingerprint -noout
depth=0 CN = openstack-vip
verify error:num=20:unable to get local issuer certificate
verify return:1
depth=0 CN = openstack-vip
verify error:num=27:certificate not trusted
verify return:1
depth=0 CN = openstack-vip
verify error:num=21:unable to verify the first certificate
verify return:1
DONE
SHA1 Fingerprint=C6:46:1E:59:C6:11:BF:72:5E:DD:FC:FF:B0:66:A7:A2:CC:32:1C:B8</pre></div><p>
   The next command probes a MariaDB endpoint that is not behind TLS:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>echo | openssl s_client -connect 192.168.245.5:3306 | openssl x509 -fingerprint -noout
140448358213264:error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol:s23_clnt.c:795:
unable to load certificate
140454148159120:error:0906D06C:PEM routines:PEM_read_bio:no start line:pem_lib.c:703
:Expecting: TRUSTED CERTIFICATE</pre></div></div><div class="sect1" id="id-1.3.7.5.20"><div class="titlepage"><div><div><h2 class="title"><span class="number">41.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Verifying that the trust chain is correctly deployed</span> <a title="Permalink" class="permalink" href="#id-1.3.7.5.20">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-operations-configuring_tls.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-operations-configuring_tls.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   You can determine if the trust chain is correctly deployed by running the
   following commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>echo | openssl s_client -connect 192.168.245.9:5000 2&gt;/dev/null | grep code
Verify return code: 21 (unable to verify the first certificate)
echo | openssl s_client -connect 192.168.245.9:5000 \
-CAfile /etc/pki/trust/anchors/ca-certificates/openstack_frontend_cacert.crt 2&gt;/dev/null | grep code
Verify return code: 0 (ok)</pre></div><p>
   Here, the first command produces error 21, which is then fixed by providing
   the CA certificate file. This verifies that the CA certificate matches the
   server certificate.
  </p></div><div class="sect1" id="id-1.3.7.5.21"><div class="titlepage"><div><div><h2 class="title"><span class="number">41.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Turning TLS on or off</span> <a title="Permalink" class="permalink" href="#id-1.3.7.5.21">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-operations-configuring_tls.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-operations-configuring_tls.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   You should leave TLS enabled in production. However, if you need to disable
   it for any reason, you must change "tls-components" to "components" in
   <code class="literal">network_groups.yml</code> (as shown earlier) and comment out the
   cert-file. Additionally, if you have a <code class="literal">network_groups.yml</code>
   file from a previous installation, you will not have TLS enabled unless you
   change "components" to "tls-components" in that file. By default, horizon is
   configured with TLS in the input model. Note that you should not disable TLS
   in the input model for horizon as that is a public endpoint and is required.
   Additionally, you should keep all services behind TLS, but using the input
   model file <code class="literal">network_groups.yml</code> you may turn TLS off for a
   service for troubleshooting or debugging. TLS should always be enabled for
   production environments.
  </p><p>
   If you are using an example input model on a clean install, all supported
   TLS services will be enabled before deployment of your cloud. If you want to
   change this setting later, for example, when upgrading, you can change the
   input model and reconfigure the system. The process is as follows:
  </p><p>
   Edit the input model <code class="literal">network_groups.yml</code> file
   appropriately, as described above. Then, commit the changes to the git
   repository:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "TLS change"</pre></div><p>
   Change directories again and run the configuration processor and ready
   deployment playbooks:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div><p>
   Change directories again and run the reconfigure playbook:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml</pre></div></div></div><div class="chapter " id="config-availability-zones"><div class="titlepage"><div><div><h2 class="title"><span class="number">42 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Availability Zones</span> <a title="Permalink" class="permalink" href="#config-availability-zones">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-config_availability_zones.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-config_availability_zones.xml</li><li><span class="ds-label">ID: </span>config-availability-zones</li></ul></div></div></div></div><div class="line"></div><p>
  The Cloud Lifecycle Manager only creates a default availability zone during
  installation. If your system has multiple failure/availability zones defined
  in your input model, these zones will not get created automatically.
 </p><p>
  Once the installation has finished, you can run the
  <code class="literal">nova-cloud-configure.yml</code> playbook to configure
  availability zones and assign compute nodes to those zones based on the
  configuration specified in the model.
 </p><p>
  You can run the playbook <code class="literal">nova-cloud-configure.yml</code> any time
  you make changes to the configuration of availability zones in your input
  model. Alternatively, you can use horizon or the command line to perform the
  configuration.
 </p><p>
  For more details, see the OpenStack Availability Zone documentation at
 <a class="link" href="https://docs.openstack.org/nova/rocky/user/aggregates.html" target="_blank">https://docs.openstack.org/nova/rocky/user/aggregates.html</a>.
 </p></div><div class="chapter " id="OctaviaInstall"><div class="titlepage"><div><div><h2 class="title"><span class="number">43 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Load Balancer as a Service</span> <a title="Permalink" class="permalink" href="#OctaviaInstall">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-configure_lbaas.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-configure_lbaas.xml</li><li><span class="ds-label">ID: </span>OctaviaInstall</li></ul></div></div><div><div class="abstract"><p>
    The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> neutron LBaaS service supports several load balancing
    providers. By default, both Octavia and the namespace HAProxy driver are
    configured to be used.
   </p></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#id-1.3.7.7.6"><span class="number">43.1 </span><span class="name">Summary</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.7.7"><span class="number">43.2 </span><span class="name">Prerequisites</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.7.8"><span class="number">43.3 </span><span class="name">Octavia Load Balancing Provider</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.7.9"><span class="number">43.4 </span><span class="name">Prerequisite Setup</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.7.10"><span class="number">43.5 </span><span class="name">Create Load Balancers</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.7.11"><span class="number">43.6 </span><span class="name">Create Floating IPs for Load Balancer</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.7.12"><span class="number">43.7 </span><span class="name">Testing the Octavia Load Balancer</span></a></span></dt></dl></div></div><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> neutron LBaaS service supports several load balancing
  providers. By default, both Octavia and the namespace HAProxy driver are
  configured for use.
 </p><p>
  If you do not specify the <code class="literal">--provider</code> option it will
  default to Octavia. The Octavia driver provides more functionality than the
  HAProxy namespace driver which is deprecated. The HAProxy namespace driver
  will be retired in a future version of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p><p>
  There are additional drivers for third-party hardware load balancers. Please
  refer to the vendor directly. The <code class="command">openstack network service provider
  list</code> command displays the currently installed load balancer
  drivers as well as other installed services such as VPN.
 </p><div class="sect1" id="id-1.3.7.7.6"><div class="titlepage"><div><div><h2 class="title"><span class="number">43.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Summary</span> <a title="Permalink" class="permalink" href="#id-1.3.7.7.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-configure_lbaas.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-configure_lbaas.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
The following procedure demonstrates how to setup a load balancer,
and test that the round robin load balancing is successful between the two 
servers. For demonstration purposes, the security group rules and floating
IPs are applied to the web servers. Adjust these configuration parameters
if needed.
 </p></div><div class="sect1" id="id-1.3.7.7.7"><div class="titlepage"><div><div><h2 class="title"><span class="number">43.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#id-1.3.7.7.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-configure_lbaas.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-configure_lbaas.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   You need have an external network and a registered image to test LBaaS
   functionality.
  </p><p>
   Creating an external network: <a class="xref" href="#create-extnet" title="38.4. Creating an External Network">Section 38.4, “Creating an External Network”</a>.
  </p><p>
   Creating and uploading a glance image: <a class="xref" href="#upload-image" title="38.3. Uploading an Image for Use">Section 38.3, “Uploading an Image for Use”</a>.
  </p></div><div class="sect1" id="id-1.3.7.7.8"><div class="titlepage"><div><div><h2 class="title"><span class="number">43.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Octavia Load Balancing Provider</span> <a title="Permalink" class="permalink" href="#id-1.3.7.7.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-configure_lbaas.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-configure_lbaas.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The Octavia Load balancing provider bundled with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 is an operator
   grade load balancer for <span class="productname">OpenStack</span>. It is based on the <span class="productname">OpenStack</span> Rocky
   version of Octavia. It differs from the namespace driver by starting a new
   nova virtual machine (called an <code class="literal">amphora</code>) to house the
   HAProxy load balancer software that provides the load balancer function. A
   virtual machine for each load balancer requested provides a better
   separation of load balancers between tenants, and simplifies increasing load
   balancing capacity along with compute node growth.  Additionally, if the
   virtual machine fails for any reason, Octavia will replace it with a
   replacement VM from a pool of spare VMs, assuming that the feature is
   configured.
  </p><div id="id-1.3.7.7.8.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    The Health Monitor will not create or replace failed amphoras. If the pool
    of spare VMs is exhausted there will be no additional virtual machines to
    handle load balancing requests.
   </p></div><p>
   Octavia uses two-way SSL encryption to communicate with amphoras. There
   are demo Certificate Authority (CA) certificates included with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 in
   <code class="filename">~/scratch/ansible/next/ardana/ansible/roles/octavia-common/files</code>
   on the Cloud Lifecycle Manager. For additional security in production deployments, replace all
   certificate authorities with ones you generate by running the following
   commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openssl genrsa -passout pass:foobar -des3 -out cakey.pem 2048
<code class="prompt user">ardana &gt; </code>openssl req -x509 -passin pass:foobar -new -nodes -key cakey.pem -out ca_01.pem
<code class="prompt user">ardana &gt; </code>openssl genrsa -passout pass:foobar -des3 -out servercakey.pem 2048
<code class="prompt user">ardana &gt; </code>openssl req -x509 -passin pass:foobar -new -nodes -key cakey.pem -out serverca_01.pem</pre></div><p>
   For more details refer to the
   <a class="link" href="https://www.openssl.org/docs/manmaster/man1/openssl.html" target="_blank">openssl
   man page</a>.
  </p><div id="id-1.3.7.7.8.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    If you change the certificate authority and have amphoras running with an
    old CA, you will not be able to control the amphoras. The amphoras will need
    to be failed over so they can utilize the new certificate. If you change
    the CA password for the server certificate, you need to change that in the
    Octavia configuration files as well. For more information, see
    <span class="intraxref">Book “Operations Guide CLM”, Chapter 10 “Managing Networking”, Section 10.4 “Networking Service Overview”, Section 10.4.9 “Load Balancer: Octavia Driver Administration”, Section 10.4.9.2 “Tuning Octavia Installation”</span>.
   </p></div></div><div class="sect1" id="id-1.3.7.7.9"><div class="titlepage"><div><div><h2 class="title"><span class="number">43.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisite Setup</span> <a title="Permalink" class="permalink" href="#id-1.3.7.7.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-configure_lbaas.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-configure_lbaas.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    <span class="bold"><strong>Octavia Client Installation</strong></span>
  </p><p>
    The Octavia client must be installed in the control plane, see
    <a class="xref" href="#install-openstack-clients" title="Chapter 40. Installing OpenStack Clients">Chapter 40, <em>Installing OpenStack Clients</em></a>.
  </p><p>
   <span class="bold"><strong>Octavia Network and Management Network
   Ports</strong></span>
  </p><p>
   The Octavia management network and Management network must have access to
   each other. If you have a configured firewall between the Octavia management
   network and Management network, you must open up the following ports to
   allow network traffic between the networks.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     From Management network to Octavia network
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       TCP 9443 (amphora API)
      </p></li></ul></div></li><li class="listitem "><p>
     From Octavia network to Management network
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       TCP 9876 (Octavia API)
      </p></li><li class="listitem "><p>
       UDP 5555 (Octavia Health Manager)
      </p></li></ul></div></li></ul></div><p>
   <span class="bold"><strong>Installing the Amphora Image</strong></span>
  </p><p>
   Octavia uses nova VMs for its load balancing function; SUSE
   provides images used to boot those VMs called
   <code class="literal">octavia-amphora</code>.
  </p><div id="id-1.3.7.7.9.9" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
    Without these images the Octavia load balancer will not work.
   </p></div><p>
   <span class="bold"><strong>Register the image.</strong></span> The <span class="productname">OpenStack</span> load
   balancing service (Octavia) does not automatically register the Amphora
   guest image.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     The Amphora image is in an RPM package managed by zypper, display the full path to that file with the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>find /srv -type f -name "*amphora-image*"</pre></div><p>
     For example:
     <code class="filename">/srv/www/suse-12.4/x86_64/repos/Cloud/suse/noarch/openstack-octavia-amphora-image-x86_64-0.1.0-13.157.noarch.rpm</code>.
     The Amphora image may be updated via maintenance updates, which will change the filename.
    </p><p>
     Switch to the <code class="filename">ansible</code> directory and register the
     image by giving the full path and name for the Amphora image as an
     argument to <code class="literal">service_package</code>, replacing the filename
     as needed to reflect an updated amphora image rpm:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts service-guest-image.yml \
-e service_package=\
/srv/www/suse-12.4/x86_64/repos/Cloud/suse/noarch/openstack-octavia-amphora-image-x86_64-0.1.0-13.157.noarch.rpm</pre></div></li><li class="step "><p>
     Source the service user (this can be done on a different computer)
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source service.osrc</pre></div></li><li class="step "><p>
     Verify that the image was registered (this can be done on a computer with
     access to the glance CLI client)
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack image list
+--------------------------------------+------------------------+---------
| ID                                   | Name                   | Status |
+--------------------------------------+------------------------+--------+
...
| 1d4dd309-8670-46b6-801d-3d6af849b6a9 | octavia-amphora-x86_64 | active |
...</pre></div><div id="id-1.3.7.7.9.11.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      In the example above, the status of the
      <code class="literal">octavia-amphora-x86_64</code> image is
      <span class="emphasis"><em>active</em></span> which means the image was successfully
      registered. If a status of the images is <span class="emphasis"><em>queued</em></span>, you
      need to run the image registration again.
     </p><p>
      If you run the registration by accident, the system will only upload a
      new image if the underlying image has been changed.
     </p><p>
      Ensure there are not multiple amphora images listed. Execute following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code> openstack image list --tag amphora</pre></div><p>
      If the above command returns more than one image, unset the old amphora image. To unset the image execute following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code> openstack image unset --tag amphora &lt;oldimageid&gt;</pre></div></div><p>
     If you have already created load balancers, they will not receive the new
     image. Only load balancers created after the image has been successfully
     installed will use the new image. If existing load balancers need to be
     switched to the new image, please follow the instructions in <span class="intraxref">Book “Operations Guide CLM”, Chapter 10 “Managing Networking”, Section 10.4 “Networking Service Overview”, Section 10.4.9 “Load Balancer: Octavia Driver Administration”, Section 10.4.9.2 “Tuning Octavia Installation”</span>.
    </p></li></ol></div></div><p>
   <span class="bold"><strong>Setup network, subnet, router, security and
   IP's</strong></span>
  </p><p>
   If you have already created a network, subnet, router, security settings and
   IPs you can skip the following steps and go directly to creating the load
   balancers. In this example we will boot two VMs running web servers and then 
   use curl scripts to demonstrate load balancing between the two VMs.
   Along the way we will save important parameters to a bash variable and use 
   them later in the process.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Make a tenant network for  load balancer clients (the web servers):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network create lb_net2
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | UP                                   |
| availability_zone_hints   |                                      |
| availability_zones        |                                      |
| created_at                | 2019-10-23T13:27:57Z                 |
| description               |                                      |
| dns_domain                |                                      |
| id                        | 50a66468-084b-457f-88e4-2edb7b81851e |
| ipv4_address_scope        | None                                 |
| ipv6_address_scope        | None                                 |
| is_default                | False                                |
| is_vlan_transparent       | None                                 |
| mtu                       | 1450                                 |
| name                      | lb_net2                              |
| port_security_enabled     | False                                |
| project_id                | de095070f242416cb3dc4cd00e3c79f7     |
| provider:network_type     | vxlan                                |
| provider:physical_network | None                                 |
| provider:segmentation_id  | 1080                                 |
| qos_policy_id             | None                                 |
| revision_number           | 1                                    |
| router:external           | Internal                             |
| segments                  | None                                 |
| shared                    | False                                |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tags                      |                                      |
| updated_at                | 2019-10-23T13:27:57Z                 |
+---------------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     Create a subnet for the load balancing clients on the tenant network:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack subnet create --network lb_net2 --subnet-range 12.12.12.0/24
+-------------------+--------------------------------------+
| Field             | Value                                |
+-------------------+--------------------------------------+
| allocation_pools  | 12.12.12.2-12.12.12.254              |
| cidr              | 12.12.12.0/24                        |
| created_at        | 2019-10-23T13:29:45Z                 |
| description       |                                      |
| dns_nameservers   |                                      |
| enable_dhcp       | True                                 |
| gateway_ip        | 12.12.12.1                           |
| host_routes       |                                      |
| id                | c141858a-a792-4c89-91f0-de4dc4694a7f |
| ip_version        | 4                                    |
| ipv6_address_mode | None                                 |
| ipv6_ra_mode      | None                                 |
| name              | lb_subnet2                           |
| network_id        | 50a66468-084b-457f-88e4-2edb7b81851e |
| project_id        | de095070f242416cb3dc4cd00e3c79f7     |
| revision_number   | 0                                    |
| segment_id        | None                                 |
| service_types     |                                      |
| subnetpool_id     | None                                 |
| tags              |                                      |
| updated_at        | 2019-10-23T13:29:45Z                 |
+-------------------+--------------------------------------+</pre></div><p>
     <span class="bold"><strong>Save the ID of the tenant subnet. 
	     The load balancer will be attached to it later.</strong></span>
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>VIP_SUBNET="c141858a-a792-4c89-91f0-de4dc4694a7f"</pre></div></li><li class="step "><p>
     Create a router for client VMs:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack router create --centralized lb_router2
+-------------------------+--------------------------------------+
| Field                   | Value                                |
+-------------------------+--------------------------------------+
| admin_state_up          | UP                                   |
| availability_zone_hints |                                      |
| availability_zones      |                                      |
| created_at              | 2019-10-23T13:30:21Z                 |
| description             |                                      |
| distributed             | False                                |
| external_gateway_info   | None                                 |
| flavor_id               | None                                 |
| ha                      | False                                |
| id                      | 1c9949fb-a500-475d-8694-346cf66ebf9a |
| name                    | lb_router2                           |
| project_id              | de095070f242416cb3dc4cd00e3c79f7     |
| revision_number         | 0                                    |
| routes                  |                                      |
| status                  | ACTIVE                               |
| tags                    |                                      |
| updated_at              | 2019-10-23T13:30:21Z                 |
+-------------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     Add the subnet to the router and connect it to the external gateway:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack router add subnet lb_router2 lb_subnet2</pre></div></li><li class="step "><p>
      Set gateway for the router. The name of the external network is:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network list
+--------------------------------------+------------------+--------------------------------------+
| ID                                   | Name             | Subnets                              |
+--------------------------------------+------------------+--------------------------------------+
| 2ed7deca-81ed-45ee-87ce-aeb4f565d3ad | ext-net          | 45d31556-08b8-4b0e-98d8-c09dd986296a |
| 50a66468-084b-457f-88e4-2edb7b81851e | lb_net2          | c141858a-a792-4c89-91f0-de4dc4694a7f |
| 898975bd-e3cc-4afd-8605-3c5606fd5c54 | OCTAVIA-MGMT-NET | 68eed774-c07f-45bb-b37a-489515108acb |
+--------------------------------------+------------------+--------------------------------------+

EXT_NET="ext-net"

openstack router set --external-gateway $EXT_NET lb_router2</pre></div></li><li class="step "><p>
     Check the router:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack router show lb_router2 --fit-width
+-------------------------+-------------------------------------------------------------------------------------------------------+
| Field                   | Value                                                                                                 |
+-------------------------+-------------------------------------------------------------------------------------------------------+
| admin_state_up          | UP                                                                                                    |
| availability_zone_hints |                                                                                                       |
| availability_zones      | nova                                                                                                  |
| created_at              | 2019-10-23T13:30:21Z                                                                                  |
| description             |                                                                                                       |
| distributed             | False                                                                                                 |
| external_gateway_info   | {"network_id": "2ed7deca-81ed-45ee-87ce-aeb4f565d3ad", "enable_snat": true, "external_fixed_ips":     |
|                         | [{"subnet_id": "45d31556-08b8-4b0e-98d8-c09dd986296a", "ip_address": "10.84.57.38"}]}                 |
| flavor_id               | None                                                                                                  |
| ha                      | False                                                                                                 |
| id                      | 1c9949fb-a500-475d-8694-346cf66ebf9a                                                                  |
| interfaces_info         | [{"subnet_id": "c141858a-a792-4c89-91f0-de4dc4694a7f", "ip_address": "12.12.12.1", "port_id":         |
|                         | "3d8a8605-dbe5-4fa6-87c3-b351763a9a63"}]                                                              |
| name                    | lb_router2                                                                                            |
| project_id              | de095070f242416cb3dc4cd00e3c79f7                                                                      |
| revision_number         | 3                                                                                                     |
| routes                  |                                                                                                       |
| status                  | ACTIVE                                                                                                |
| tags                    |                                                                                                       |
| updated_at              | 2019-10-23T13:33:22Z                                                                                  |
+-------------------------+-------------------------------------------------------------------------------------------------------+</pre></div></li><li class="step "><p>
     Create a security group for testing with unrestricted inbound 
     and outbound traffic (you will have to restrict the traffic later):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack security group create letmein
+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+
| Field           | Value                                                                                                                                                 |
+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+
| created_at      | 2019-05-13T18:50:28Z                                                                                                                                  |
| description     | letmein                                                                                                                                               |
| id              | 1d136bc8-8186-4c08-88f4-fe83465f3c30                                                                                                                  |
| name            | letmein                                                                                                                                               |
| project_id      | 5bd04b76d1db42988612e5f27170a40a                                                                                                                      |
| revision_number | 2                                                                                                                                                     |
| rules           | created_at='2019-05-13T18:50:28Z', direction='egress', ethertype='IPv6', id='1f73b026-5f7e-4b2c-9a72-07c88d8ea82d', updated_at='2019-05-13T18:50:28Z' |
|                 | created_at='2019-05-13T18:50:28Z', direction='egress', ethertype='IPv4', id='880a04ee-14b4-4d05-b038-6dd42005d3c0', updated_at='2019-05-13T18:50:28Z' |
| updated_at      | 2019-05-13T18:50:28Z                                                                                                                                  |
+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+</pre></div></li><li class="step "><p>
     Create security rules for the VMs:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack security group rule create letmein --ingress --protocol tcp --dst-port 1:1024
<code class="prompt user">ardana &gt; </code>openstack security group rule create letmein --ingress --protocol icmp
<code class="prompt user">ardana &gt; </code>openstack security group rule create letmein --egress --protocol tcp --dst-port 1:1024
<code class="prompt user">ardana &gt; </code>openstack security group rule create letmein --egress --protocol icmp</pre></div></li><li class="step "><p>
      If you have not already created a keypair for the web servers,
      create one now with the <code class="command">openstack keypair create</code>.
      You will use the keypair to boot images.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack keypair create lb_kp1 &gt; lb_kp1.pem
chmod 400 lb_kp1.pem

ls -la lb*
-r-------- 1 ardana ardana 1680 May 13 12:52 lb_kp1.pem</pre></div><p>
      Verify the keypair list:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack keypair list
+---------------+-------------------------------------------------+
| Name          | Fingerprint                                     |
+---------------+-------------------------------------------------+
| id_rsa2       | d3:82:4c:fc:80:79:db:94:b6:31:f1:15:8e:ba:35:0b |
| lb_kp1        | 78:a8:0b:5b:2d:59:f4:68:4c:cb:49:c3:f8:81:3e:d8 |
+---------------+-------------------------------------------------+</pre></div></li><li class="step "><p>
     List the image names available to boot and boot two VMs:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack image list | grep cirros
| faa65f54-e38b-43dd-b0db-ae5f5e3d9b83 | cirros-0.4.0-x86_64             | active |</pre></div><p>
      Boot a VM that will host the pseudo web server:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server create --flavor m1.tiny --image cirros-0.4.0-x86_64 \
--key-name id_rsa2 --security-group letmein --nic net-id=lb_net2 lb2_vm1 --wait
 
+-------------------------------------+------------------------------------------------------------+
| Field                               | Value                                                      |
+-------------------------------------+------------------------------------------------------------+
| OS-DCF:diskConfig                   | MANUAL                                                     |
| OS-EXT-AZ:availability_zone         | nova                                                       |
| OS-EXT-SRV-ATTR:host                | cjd9-cp1-comp0001-mgmt                                     |
| OS-EXT-SRV-ATTR:hypervisor_hostname | cjd9-cp1-comp0001-mgmt                                     |
| OS-EXT-SRV-ATTR:instance_name       | instance-0000001f                                          |
| OS-EXT-STS:power_state              | Running                                                    |
| OS-EXT-STS:task_state               | None                                                       |
| OS-EXT-STS:vm_state                 | active                                                     |
| OS-SRV-USG:launched_at              | 2019-10-23T13:36:32.000000                                 |
| OS-SRV-USG:terminated_at            | None                                                       |
| accessIPv4                          |                                                            |
| accessIPv6                          |                                                            |
| addresses                           | lb_net2=12.12.12.13                                        |
| adminPass                           | 79GcsJQfNt3p                                               |
| config_drive                        |                                                            |
| created                             | 2019-10-23T13:36:17Z                                       |
| flavor                              | m1.tiny (1)                                                |
| hostId                              | ed0511cb20d44f420e707fbc801643e0658eb2efbf980ef986aa45d0   |
| id                                  | 874e675b-727b-44a2-99a9-0ff178590f86                       |
| image                               | cirros-0.4.0-x86_64 (96e0df13-573f-4672-97b9-3fe67ff84d6a) |
| key_name                            | id_rsa2                                                    |
| name                                | lb2_vm1                                                    |
| progress                            | 0                                                          |
| project_id                          | de095070f242416cb3dc4cd00e3c79f7                           |
| properties                          |                                                            |
| security_groups                     | name='letmein'                                             |
| status                              | ACTIVE                                                     |
| updated                             | 2019-10-23T13:36:32Z                                       |
| user_id                             | ec4d4dbf03be4bf09f105c06c7562ba7                           |
| volumes_attached                    |                                                            |
+-------------------------------------+------------------------------------------------------------+</pre></div><p>
     Save the private IP of the first web server:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>IP_LBVM1="12.12.12.13"</pre></div><p>
     Boot a second pseudo web server:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server create --flavor m1.tiny --image cirros-0.4.0-x86_64 \
--key-name id_rsa2 --security-group letmein --nic net-id=lb_net2 lb2_vm2 --wait
 
+-------------------------------------+------------------------------------------------------------+
| Field                               | Value                                                      |
+-------------------------------------+------------------------------------------------------------+
| OS-DCF:diskConfig                   | MANUAL                                                     |
| OS-EXT-AZ:availability_zone         | nova                                                       |
| OS-EXT-SRV-ATTR:host                | cjd9-cp1-comp0002-mgmt                                     |
| OS-EXT-SRV-ATTR:hypervisor_hostname | cjd9-cp1-comp0002-mgmt                                     |
| OS-EXT-SRV-ATTR:instance_name       | instance-00000022                                          |
| OS-EXT-STS:power_state              | Running                                                    |
| OS-EXT-STS:task_state               | None                                                       |
| OS-EXT-STS:vm_state                 | active                                                     |
| OS-SRV-USG:launched_at              | 2019-10-23T13:38:09.000000                                 |
| OS-SRV-USG:terminated_at            | None                                                       |
| accessIPv4                          |                                                            |
| accessIPv6                          |                                                            |
| addresses                           | lb_net2=12.12.12.5                                         |
| adminPass                           | CGDs3mnHcnv8                                               |
| config_drive                        |                                                            |
| created                             | 2019-10-23T13:37:55Z                                       |
| flavor                              | m1.tiny (1)                                                |
| hostId                              | bf4424315eb006ccb91f80e1024b2617cbb86a2cc70cf912b6ca1f95   |
| id                                  | a60dfb93-255b-476d-bf28-2b4f0da285e0                       |
| image                               | cirros-0.4.0-x86_64 (96e0df13-573f-4672-97b9-3fe67ff84d6a) |
| key_name                            | id_rsa2                                                    |
| name                                | lb2_vm2                                                    |
| progress                            | 0                                                          |
| project_id                          | de095070f242416cb3dc4cd00e3c79f7                           |
| properties                          |                                                            |
| security_groups                     | name='letmein'                                             |
| status                              | ACTIVE                                                     |
| updated                             | 2019-10-23T13:38:09Z                                       |
| user_id                             | ec4d4dbf03be4bf09f105c06c7562ba7                           |
| volumes_attached                    |                                                            |
+-------------------------------------+------------------------------------------------------------+</pre></div><p>
     Save the private IP of the second web server:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>IP_LBVM2="12.12.12.5"</pre></div></li><li class="step "><p>
     Verify that all the servers are running:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list
+--------------------------------------+---------+--------+----------------------------------+---------------------+---------+
| ID                                   | Name    | Status | Networks                         | Image               | Flavor  |
+--------------------------------------+---------+--------+----------------------------------+---------------------+---------+
| a60dfb93-255b-476d-bf28-2b4f0da285e0 | lb2_vm2 | ACTIVE | lb_net2=12.12.12.5               | cirros-0.4.0-x86_64 | m1.tiny |
| 874e675b-727b-44a2-99a9-0ff178590f86 | lb2_vm1 | ACTIVE | lb_net2=12.12.12.13              | cirros-0.4.0-x86_64 | m1.tiny |
+--------------------------------------+---------+--------+----------------------------------+---------------------+---------+</pre></div></li><li class="step "><p>
     Check ports for the VMs first (.13 and .5):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port list | grep -e $IP_LBVM2 -e $IP_LBVM1
| 42c356f1-8c49-469a-99c8-12c541378741 || fa:16:3e:2d:6b:be | ip_address='12.12.12.13', subnet_id='c141858a-a792-4c89-91f0-de4dc4694a7f'    | ACTIVE |
| fb7166c1-06f7-4457-bc0d-4692fc2c7fa2 || fa:16:3e:ac:5d:82 | ip_address='12.12.12.5',  subnet_id='c141858a-a792-4c89-91f0-de4dc4694a7f'    | ACTIVE |</pre></div><p>
     Create the floating IPs for the VMs:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack floating ip create $EXT_NET --port fb7166c1-06f7-4457-bc0d-4692fc2c7fa2
openstack floating ip create $EXT_NET --port 42c356f1-8c49-469a-99c8-12c541378741</pre></div><p>
     Verify the floating IP attachment:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list
+--------------------------------------+---------+--------+----------------------------------+---------------------+---------+
| ID                                   | Name    | Status | Networks                         | Image               | Flavor  |
+--------------------------------------+---------+--------+----------------------------------+---------------------+---------+
| a60dfb93-255b-476d-bf28-2b4f0da285e0 | lb2_vm2 | ACTIVE | lb_net2=12.12.12.5, 10.84.57.35  | cirros-0.4.0-x86_64 | m1.tiny |
| 874e675b-727b-44a2-99a9-0ff178590f86 | lb2_vm1 | ACTIVE | lb_net2=12.12.12.13, 10.84.57.39 | cirros-0.4.0-x86_64 | m1.tiny |
+--------------------------------------+---------+--------+----------------------------------+---------------------+---------+</pre></div><p>
     Save the floating IPs of the web server VMs:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>LB_VM2_FIP="10.84.57.35"
<code class="prompt user">ardana &gt; </code>LB_VM1_FIP="10.84.57.39"</pre></div></li><li class="step "><p>
     Use the <code class="literal">ping</code> command on the VMs to verify that 
     they respond on their floating IPs:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ping $LB_VM2_FIP
PING 10.84.57.35 (10.84.57.35) 56(84) bytes of data.
64 bytes from 10.84.57.35: icmp_seq=1 ttl=62 time=2.09 ms
64 bytes from 10.84.57.35: icmp_seq=2 ttl=62 time=0.542 ms
^C
 
ping $LB_VM1_FIP
PING 10.84.57.39 (10.84.57.39) 56(84) bytes of data.
64 bytes from 10.84.57.39: icmp_seq=1 ttl=62 time=1.55 ms
64 bytes from 10.84.57.39: icmp_seq=2 ttl=62 time=0.552 ms
^C</pre></div></li></ol></div></div></div><div class="sect1" id="id-1.3.7.7.10"><div class="titlepage"><div><div><h2 class="title"><span class="number">43.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create Load Balancers</span> <a title="Permalink" class="permalink" href="#id-1.3.7.7.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-configure_lbaas.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-configure_lbaas.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following steps will set up new Octavia Load Balancers.
  </p><div id="id-1.3.7.7.10.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    The following examples assume names and values from the previous section.
   </p></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Create the load balancer on the tenant network. Use the
     bash variable <code class="literal">VIP_SUBNET</code> that holds the ID
     of the subnet that was saved earlier, or the actual subnet ID:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack loadbalancer create --name lb2 --vip-subnet-id $VIP_SUBNET 
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| admin_state_up      | True                                 |
| created_at          | 2019-10-29T16:28:03                  |
| description         |                                      |
| flavor              |                                      |
| id                  | 2b0660e1-2901-41ea-93d8-d1fa590b9cfd |
| listeners           |                                      |
| name                | lb2                                  |
| operating_status    | OFFLINE                              |
| pools               |                                      |
| project_id          | de095070f242416cb3dc4cd00e3c79f7     |
| provider            | amphora                              |
| provisioning_status | PENDING_CREATE                       |
| updated_at          | None                                 |
| vip_address         | 12.12.12.6                           |
| vip_network_id      | 50a66468-084b-457f-88e4-2edb7b81851e |
| vip_port_id         | 5122c1d0-2996-43d5-ad9b-7b3b2c5903d5 |
| vip_qos_policy_id   | None                                 |
| vip_subnet_id       | c141858a-a792-4c89-91f0-de4dc4694a7f |
+---------------------+--------------------------------------+</pre></div><p>
       Save the load balancer <code class="literal">vip_port_id</code>.
       This is used when attaching a floating IP to the load balancer.
     </p><div class="verbatim-wrap"><pre class="screen">LB_PORT="5122c1d0-2996-43d5-ad9b-7b3b2c5903d5"</pre></div></li><li class="step "><p>
     List load balancers. You will need to wait until the load balancer
     <code class="literal">provisioning_status</code>is <code class="literal">ACTIVE</code> before
     proceeding to the next step.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack loadbalancer list
+--------------------------------------+------+----------------------------------+-------------+---------------------+----------+
| id                                   | name | project_id                       | vip_address | provisioning_status | provider |
+--------------------------------------+------+----------------------------------+-------------+---------------------+----------+
| 2b0660e1-2901-41ea-93d8-d1fa590b9cfd | lb2  | de095070f242416cb3dc4cd00e3c79f7 | 12.12.12.6  | PENDING_CREATE      | amphora  |
+--------------------------------------+------+----------------------------------+-------------+---------------------+----------+</pre></div><p>
     After some time has passed, the status will change to <code class="literal">ACTIVE</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack loadbalancer list
+--------------------------------------+------+----------------------------------+-------------+---------------------+----------+
| id                                   | name | project_id                       | vip_address | provisioning_status | provider |
+--------------------------------------+------+----------------------------------+-------------+---------------------+----------+
| 2b0660e1-2901-41ea-93d8-d1fa590b9cfd | lb2  | de095070f242416cb3dc4cd00e3c79f7 | 12.12.12.6  | ACTIVE              | amphora  |
+--------------------------------------+------+----------------------------------+-------------+---------------------+----------+</pre></div></li><li class="step "><p>
     Once the load balancer is created, create the listener. This may take some
     time.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack loadbalancer listener create --protocol HTTP \
--protocol-port=80 --name lb2_listener lb2
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | True                                 |
| connection_limit          | -1                                   |
| created_at                | 2019-10-29T16:32:26                  |
| default_pool_id           | None                                 |
| default_tls_container_ref | None                                 |
| description               |                                      |
| id                        | 65930b35-70bf-47d2-a135-aff49c219222 |
| insert_headers            | None                                 |
| l7policies                |                                      |
| loadbalancers             | 2b0660e1-2901-41ea-93d8-d1fa590b9cfd |
| name                      | lb2_listener                         |
| operating_status          | OFFLINE                              |
| project_id                | de095070f242416cb3dc4cd00e3c79f7     |
| protocol                  | HTTP                                 |
| protocol_port             | 80                                   |
| provisioning_status       | PENDING_CREATE                       |
| sni_container_refs        | []                                   |
| timeout_client_data       | 50000                                |
| timeout_member_connect    | 5000                                 |
| timeout_member_data       | 50000                                |
| timeout_tcp_inspect       | 0                                    |
| updated_at                | None                                 |
+---------------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     Create the load balancing pool. During the creation of the load balancing
     pool, the status for the load balancer goes to
     <code class="literal">PENDING_UPDATE</code>. Use <code class="literal">openstack
     loadbalancer pool show</code> to watch for the change to
     <code class="literal">ACTIVE</code>. Once the load balancer returns to
     <code class="literal">ACTIVE</code>, proceed with the next step.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack loadbalancer pool create --lb-algorithm ROUND_ROBIN \
--protocol HTTP --listener lb2_listener --name lb2_pool
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| admin_state_up      | True                                 |
| created_at          | 2019-10-29T16:35:06                  |
| description         |                                      |
| healthmonitor_id    |                                      |
| id                  | 75cd42fa-0525-421f-afaa-5de996267536 |
| lb_algorithm        | ROUND_ROBIN                          |
| listeners           | 65930b35-70bf-47d2-a135-aff49c219222 |
| loadbalancers       | 2b0660e1-2901-41ea-93d8-d1fa590b9cfd |
| members             |                                      |
| name                | lb2_pool                             |
| operating_status    | OFFLINE                              |
| project_id          | de095070f242416cb3dc4cd00e3c79f7     |
| protocol            | HTTP                                 |
| provisioning_status | PENDING_CREATE                       |
| session_persistence | None                                 |
| updated_at          | None                                 |
+---------------------+--------------------------------------+</pre></div><p>
      Wait for the status to change to <code class="literal">ACTIVE</code>.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack loadbalancer pool show lb2_pool
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| admin_state_up      | True                                 |
| created_at          | 2019-10-29T16:35:06                  |
| description         |                                      |
| healthmonitor_id    |                                      |
| id                  | 75cd42fa-0525-421f-afaa-5de996267536 |
| lb_algorithm        | ROUND_ROBIN                          |
| listeners           | 65930b35-70bf-47d2-a135-aff49c219222 |
| loadbalancers       | 2b0660e1-2901-41ea-93d8-d1fa590b9cfd |
| members             |                                      |
| name                | lb2_pool                             |
| operating_status    | ONLINE                               |
| project_id          | de095070f242416cb3dc4cd00e3c79f7     |
| protocol            | HTTP                                 |
| provisioning_status | ACTIVE                               |
| session_persistence | None                                 |
| updated_at          | 2019-10-29T16:35:10                  |
+---------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     Add the private IPs of the tenant network for the first web server in the pool:
    </p><p>
     In the previous section the addresses for the VMs were saved in the
     <code class="literal">IP_LBVM1</code> and IP_LBVM2 bash variables. Use those
     variables or the or the literal addresses of the VMs.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack loadbalancer member create --subnet $VIP_SUBNET \
--address $IP_LBVM1 --protocol-port 80  lb2_pool
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| address             | 12.12.12.13                          |
| admin_state_up      | True                                 |
| created_at          | 2019-10-29T16:39:02                  |
| id                  | 3012cfa2-359c-48c1-8b6f-c650a47c2b73 |
| name                |                                      |
| operating_status    | NO_MONITOR                           |
| project_id          | de095070f242416cb3dc4cd00e3c79f7     |
| protocol_port       | 80                                   |
| provisioning_status | PENDING_CREATE                       |
| subnet_id           | c141858a-a792-4c89-91f0-de4dc4694a7f |
| updated_at          | None                                 |
| weight              | 1                                    |
| monitor_port        | None                                 |
| monitor_address     | None                                 |
| backup              | False                                |
+---------------------+--------------------------------------+</pre></div></li><li class="step "><p>
      Put the addresses the VMs have on the tenant network into the load balancer pool:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack loadbalancer member create --subnet $VIP_SUBNET \
--address $IP_LBVM2 --protocol-port 80  lb2_pool
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| address             | 12.12.12.5                           |
| admin_state_up      | True                                 |
| created_at          | 2019-10-29T16:39:59                  |
| id                  | 3dab546b-e3ea-46b3-96f2-a7463171c2b9 |
| name                |                                      |
| operating_status    | NO_MONITOR                           |
| project_id          | de095070f242416cb3dc4cd00e3c79f7     |
| protocol_port       | 80                                   |
| provisioning_status | PENDING_CREATE                       |
| subnet_id           | c141858a-a792-4c89-91f0-de4dc4694a7f |
| updated_at          | None                                 |
| weight              | 1                                    |
| monitor_port        | None                                 |
| monitor_address     | None                                 |
| backup              | False                                |
+---------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     Verify the configuration:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack loadbalancer list --fit-width
+---------------------------+------+---------------------------+-------------+---------------------+----------+
| id                        | name | project_id                | vip_address | provisioning_status | provider |
+---------------------------+------+---------------------------+-------------+---------------------+----------+
| 2b0660e1-2901-41ea-       | lb2  | de095070f242416cb3dc4cd00 | 12.12.12.6  | ACTIVE              | amphora  |
| 93d8-d1fa590b9cfd         |      | e3c79f7                   |             |                     |          |
+---------------------------+------+---------------------------+-------------+---------------------+----------+

<code class="prompt user">ardana &gt; </code>openstack loadbalancer show lb2 --fit-width
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| admin_state_up      | True                                 |
| created_at          | 2019-10-29T16:28:03                  |
| description         |                                      |
| flavor              |                                      |
| id                  | 2b0660e1-2901-41ea-93d8-d1fa590b9cfd |
| listeners           | 65930b35-70bf-47d2-a135-aff49c219222 |
| name                | lb2                                  |
| operating_status    | ONLINE                               |
| pools               | 75cd42fa-0525-421f-afaa-5de996267536 |
| project_id          | de095070f242416cb3dc4cd00e3c79f7     |
| provider            | amphora                              |
| provisioning_status | ACTIVE                               |
| updated_at          | 2019-10-29T16:40:01                  |
| vip_address         | 12.12.12.6                           |
| vip_network_id      | 50a66468-084b-457f-88e4-2edb7b81851e |
| vip_port_id         | 5122c1d0-2996-43d5-ad9b-7b3b2c5903d5 |
| vip_qos_policy_id   | None                                 |
| vip_subnet_id       | c141858a-a792-4c89-91f0-de4dc4694a7f |
+---------------------+--------------------------------------+

<code class="prompt user">ardana &gt; </code>openstack loadbalancer listener list --fit-width
+-----------------+-----------------+--------------+-----------------+----------+---------------+----------------+
| id              | default_pool_id | name         | project_id      | protocol | protocol_port | admin_state_up |
+-----------------+-----------------+--------------+-----------------+----------+---------------+----------------+
| 65930b35-70bf-4 | 75cd42fa-0525   | lb2_listener | de095070f242416 | HTTP     |            80 | True           |
| 7d2-a135-aff49c | -421f-afaa-     |              | cb3dc4cd00e3c79 |          |               |                |
| 219222          | 5de996267536    |              | f7              |          |               |                |
+-----------------+-----------------+--------------+-----------------+----------+---------------+----------------+

<code class="prompt user">ardana &gt; </code>openstack loadbalancer listener show lb2_listener  --fit-width
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | True                                 |
| connection_limit          | -1                                   |
| created_at                | 2019-10-29T16:32:26                  |
| default_pool_id           | 75cd42fa-0525-421f-afaa-5de996267536 |
| default_tls_container_ref | None                                 |
| description               |                                      |
| id                        | 65930b35-70bf-47d2-a135-aff49c219222 |
| insert_headers            | None                                 |
| l7policies                |                                      |
| loadbalancers             | 2b0660e1-2901-41ea-93d8-d1fa590b9cfd |
| name                      | lb2_listener                         |
| operating_status          | ONLINE                               |
| project_id                | de095070f242416cb3dc4cd00e3c79f7     |
| protocol                  | HTTP                                 |
| protocol_port             | 80                                   |
| provisioning_status       | ACTIVE                               |
| sni_container_refs        | []                                   |
| timeout_client_data       | 50000                                |
| timeout_member_connect    | 5000                                 |
| timeout_member_data       | 50000                                |
| timeout_tcp_inspect       | 0                                    |
| updated_at                | 2019-10-29T16:40:01                  |
+---------------------------+--------------------------------------+</pre></div></li></ol></div></div></div><div class="sect1" id="id-1.3.7.7.11"><div class="titlepage"><div><div><h2 class="title"><span class="number">43.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create Floating IPs for Load Balancer</span> <a title="Permalink" class="permalink" href="#id-1.3.7.7.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-configure_lbaas.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-configure_lbaas.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To create the floating IP's for the load balancer, you will need to list
   the ports for the load balancer. Once you have the port ID, you can then
   create the floating IP. Notice that the name has changed between the neutron
   lbass-loadbalancer and openstack loadbalancer CLI. In this case, be sure
   to pick the port associated with the name <code class="literal">octavia-lb-</code>
   or use the port ID that was saved when the load balancer was created.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Search for the load balancers port, it will be used when the floating
     IP is created. Alternately, use the bash variable that contains the load
     balancer port from the previous section.

     Verify the port that has the address for the load balancer:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port list --fit-width
+----------------------------+----------------------------+-------------------+-----------------------------+--------+
| ID                         | Name                       | MAC Address       | Fixed IP Addresses          | Status |
+----------------------------+----------------------------+-------------------+-----------------------------+--------+
| 5122c1d0-2996-43d5-ad9b-   | octavia-lb-2b0660e1-2901   | fa:16:3e:6a:59:f3 | ip_address='12.12.12.6', su | DOWN   |
| 7b3b2c5903d5               | -41ea-93d8-d1fa590b9cfd    |                   | bnet_id='c141858a-a792-4c89 |        |
|                            |                            |                   | -91f0-de4dc4694a7f'         |        |</pre></div></li><li class="step "><p>
     Create the floating IP for the load balancers port:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack floating ip create --port $LB_PORT $EXT_NET --fit-width
+---------------------+-------------------------------------------------------------------------------------------------+
| Field               | Value                                                                                           |
+---------------------+-------------------------------------------------------------------------------------------------+
| created_at          | 2019-10-29T16:59:43Z                                                                            |
| description         |                                                                                                 |
| dns_domain          |                                                                                                 |
| dns_name            |                                                                                                 |
| fixed_ip_address    | 12.12.12.6                                                                                      |
| floating_ip_address | 10.84.57.29                                                                                     |
| floating_network_id | 2ed7deca-81ed-45ee-87ce-aeb4f565d3ad                                                            |
| id                  | f4f18854-9d92-4eb4-8c05-96ef059b4a41                                                            |
| name                | 10.84.57.29                                                                                     |
| port_details        | {u'status': u'DOWN', u'name': u'octavia-lb-2b0660e1-2901-41ea-93d8-d1fa590b9cfd',               |
|                     | u'admin_state_up': False, u'network_id': u'50a66468-084b-457f-88e4-2edb7b81851e',               |
|                     | u'device_owner': u'Octavia', u'mac_address': u'fa:16:3e:6a:59:f3', u'device_id': u'lb-          |
|                     | 2b0660e1-2901-41ea-93d8-d1fa590b9cfd'}                                                          |
| port_id             | 5122c1d0-2996-43d5-ad9b-7b3b2c5903d5                                                            |
| project_id          | de095070f242416cb3dc4cd00e3c79f7                                                                |
| qos_policy_id       | None                                                                                            |
| revision_number     | 0                                                                                               |
| router_id           | 1c9949fb-a500-475d-8694-346cf66ebf9a                                                            |
| status              | DOWN                                                                                            |
| subnet_id           | None                                                                                            |
| tags                | []                                                                                              |
| updated_at          | 2019-10-29T16:59:43Z                                                                            |
+---------------------+-------------------------------------------------------------------------------------------------+</pre></div></li><li class="step "><p>
      Save the floating IP for the load balancer for future use.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>LB_FIP="10.84.57.29"</pre></div></li></ol></div></div></div><div class="sect1" id="id-1.3.7.7.12"><div class="titlepage"><div><div><h2 class="title"><span class="number">43.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Testing the Octavia Load Balancer</span> <a title="Permalink" class="permalink" href="#id-1.3.7.7.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-configure_lbaas.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-configure_lbaas.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   This is the web server code that runs on the VMs created earlier. 
   In this example, the web server is on port 80.
  </p><div class="verbatim-wrap"><pre class="screen">echo &lt;&lt;EOF &gt;webserver.sh 
#!/bin/sh
 
MYIP=$(/sbin/ifconfig eth0|grep 'inet addr'|awk -F: '{print $2}'| awk '{print $1}');
while true; do
    echo -e "HTTP/1.0 200 OK
 
Welcome to $MYIP" | sudo nc -l -p 80
done
EOF</pre></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Remove any old floating IPs from <code class="literal">known_hosts</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ssh-keygen -R  $LB_VM1_FIP
<code class="prompt user">ardana &gt; </code>ssh-keygen -R  $LB_VM2_FIP</pre></div></li><li class="step "><p>
     Deploy the web server application:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>
scp -o StrictHostKeyChecking=no -i lb_kp1.pem webserver.sh cirros@$LB_VM1_FIP:webserver.sh 
ssh -o StrictHostKeyChecking=no -i lb_kp1.pem cirros@$LB_VM1_FIP 'chmod +x ./webserver.sh'
ssh -o StrictHostKeyChecking=no -i lb_kp1.pem cirros@$LB_VM1_FIP '(./webserver.sh&amp;)'
 
scp -o StrictHostKeyChecking=no -i lb_kp1.pem webserver.sh cirros@$LB_VM2_FIP:webserver.sh 
ssh -o StrictHostKeyChecking=no -i lb_kp1.pem cirros@$LB_VM2_FIP 'chmod +x ./webserver.sh'
ssh -o StrictHostKeyChecking=no -i lb_kp1.pem cirros@$LB_VM2_FIP '(./webserver.sh&amp;)'</pre></div></li><li class="step "><p>
     Make sure the web servers respond with the correct IPs:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>curl $LB_VM1_FIP
Welcome to 12.12.12.5
curl $LB_VM2_FIP
Welcome to 12.12.12.5</pre></div></li><li class="step "><p>
     Access the web servers through the Octavia load balancer using the floating IP:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>curl $LB_FIP
Welcome to 12.12.12.5
<code class="prompt user">ardana &gt; </code> curl $LB_FIP
Welcome to 12.12.12.13
<code class="prompt user">ardana &gt; </code> curl $LB_FIP
Welcome to 12.12.12.5
<code class="prompt user">ardana &gt; </code> curl $LB_FIP</pre></div></li></ol></div></div></div></div><div class="chapter " id="postinstall-checklist"><div class="titlepage"><div><div><h2 class="title"><span class="number">44 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Other Common Post-Installation Tasks</span> <a title="Permalink" class="permalink" href="#postinstall-checklist">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-postinstall_tasks.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-postinstall_tasks.xml</li><li><span class="ds-label">ID: </span>postinstall-checklist</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#id-1.3.7.8.2"><span class="number">44.1 </span><span class="name">Determining Your User Credentials</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.8.3"><span class="number">44.2 </span><span class="name">Configure your Cloud Lifecycle Manager to use the command-line tools</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.8.4"><span class="number">44.3 </span><span class="name">Protect home directory</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.8.5"><span class="number">44.4 </span><span class="name">Back up Your SSH Keys</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.8.6"><span class="number">44.5 </span><span class="name">Retrieving Service Endpoints</span></a></span></dt><dt><span class="section"><a href="#id-1.3.7.8.7"><span class="number">44.6 </span><span class="name">Other Common Post-Installation Tasks</span></a></span></dt></dl></div></div><div class="sect1" id="id-1.3.7.8.2"><div class="titlepage"><div><div><h2 class="title"><span class="number">44.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Determining Your User Credentials</span> <a title="Permalink" class="permalink" href="#id-1.3.7.8.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-postinstall_tasks.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-postinstall_tasks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   On your Cloud Lifecycle Manager, in the
   <code class="literal">~/scratch/ansible/next/ardana/ansible/group_vars/</code> directory
   you will find several files. In the one labeled as first control plane node
   you can locate the user credentials for both the Administrator user
   (<code class="literal">admin</code>) and your Demo user (<code class="literal">demo</code>)
   which you will use to perform many other actions on your cloud.
  </p><p>
   For example, if you are using the Entry-scale KVM model and used
   the default naming scheme given in the example configuration files, you can
   use these commands on your Cloud Lifecycle Manager to <code class="command">grep</code> for your user
   credentials:
  </p><p>
   <span class="bold"><strong>Administrator</strong></span>
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>grep keystone_admin_pwd entry-scale-kvm-control-plane-1</pre></div><p>
   <span class="bold"><strong>Demo</strong></span>
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>grep keystone_demo_pwd entry-scale-kvm-control-plane-1</pre></div></div><div class="sect1" id="id-1.3.7.8.3"><div class="titlepage"><div><div><h2 class="title"><span class="number">44.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure your Cloud Lifecycle Manager to use the command-line tools</span> <a title="Permalink" class="permalink" href="#id-1.3.7.8.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-postinstall_tasks.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-postinstall_tasks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   This playbook will do a series of steps to update your environment variables
   for your cloud so you can use command-line clients.
  </p><p>
   Run the following command, which will replace <code class="literal">/etc/hosts</code>
   on the Cloud Lifecycle Manager:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts cloud-client-setup.yml</pre></div><p>
   As the <code class="filename">/etc/hosts</code> file no longer has entries for Cloud Lifecycle Manager,
   sudo commands may become a bit slower. To fix this issue, once
   this step is complete, add "ardana" after "127.0.0.1 localhost". The result
   will look like this:
  </p><div class="verbatim-wrap"><pre class="screen">...
# Localhost Information
127.0.0.1 localhost ardana</pre></div></div><div class="sect1" id="id-1.3.7.8.4"><div class="titlepage"><div><div><h2 class="title"><span class="number">44.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Protect home directory</span> <a title="Permalink" class="permalink" href="#id-1.3.7.8.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-postinstall_tasks.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-postinstall_tasks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The home directory of the user that owns the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 scripts should
   not be world readable. Change the permissions so that they are only readable
   by the owner:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>chmod 0700 ~</pre></div></div><div class="sect1" id="id-1.3.7.8.5"><div class="titlepage"><div><div><h2 class="title"><span class="number">44.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Back up Your SSH Keys</span> <a title="Permalink" class="permalink" href="#id-1.3.7.8.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-postinstall_tasks.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-postinstall_tasks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   As part of the cloud deployment setup process, SSH keys to access the
   systems are generated and stored in <code class="literal">~/.ssh</code> on your
   Cloud Lifecycle Manager.
  </p><p>
   These SSH keys allow access to the subsequently deployed systems and should
   be included in the list of content to be archived in any backup strategy.
  </p></div><div class="sect1" id="id-1.3.7.8.6"><div class="titlepage"><div><div><h2 class="title"><span class="number">44.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Retrieving Service Endpoints</span> <a title="Permalink" class="permalink" href="#id-1.3.7.8.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-postinstall_tasks.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-postinstall_tasks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Source the keystone admin credentials:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>unset OS_TENANT_NAME
<code class="prompt user">ardana &gt; </code>source ~/keystone.osrc</pre></div></li><li class="step "><p>
     Using the <span class="productname">OpenStack</span> command-line tool you can then query the keystone
     service for your endpoints:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack endpoint list</pre></div><div id="id-1.3.7.8.6.2.3.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>
      You can use <code class="literal">openstack -h</code> to access the client help
      file and a full list of commands.
     </p></div></li></ol></div></div><p>
   To learn more about keystone, see
   <span class="intraxref">Book “Operations Guide CLM”, Chapter 5 “Managing Identity”, Section 5.1 “The Identity Service”</span>.
  </p></div><div class="sect1" id="id-1.3.7.8.7"><div class="titlepage"><div><div><h2 class="title"><span class="number">44.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Other Common Post-Installation Tasks</span> <a title="Permalink" class="permalink" href="#id-1.3.7.8.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-installation-postinstall_tasks.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-installation-postinstall_tasks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Here are the links to other common post-installation tasks that either the
   Administrator or Demo users can perform:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="intraxref">Book “Operations Guide CLM”, Chapter 6 “Managing Compute”, Section 6.4 “Enabling the Nova Resize and Migrate Features”</span>
    </p></li><li class="listitem "><p>
     <a class="xref" href="#create-extnet" title="38.4. Creating an External Network">Section 38.4, “Creating an External Network”</a>
    </p></li><li class="listitem "><p>
     <a class="xref" href="#upload-image" title="38.3. Uploading an Image for Use">Section 38.3, “Uploading an Image for Use”</a>
    </p></li><li class="listitem "><p>
     <span class="intraxref">Book “Operations Guide CLM”, Chapter 9 “Managing Object Storage”, Section 9.1 “Running the swift Dispersion Report”</span>
    </p></li><li class="listitem "><p>
     <span class="intraxref">Book “Security Guide”, Chapter 4 “Service Admin Role Segregation in the Identity Service”</span>
    </p></li></ul></div></div></div></div><div class="part" id="cha-inst-trouble"><div class="titlepage"><div><div><h1 class="title"><span class="number">Part VI </span><span class="name">Support </span><a title="Permalink" class="permalink" href="#cha-inst-trouble">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-support.xml" title="Edit the source file for this section">Edit source</a></h1></div></div></div><div class="toc"><dl><dt><span class="chapter"><a href="#sec-depl-trouble-faq"><span class="number">45 </span><span class="name">FAQ</span></a></span></dt><dd class="toc-abstract"><p>
   Find solutions for the most common pitfalls and technical details on how
   to create a support request for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> here.
  </p></dd><dt><span class="chapter"><a href="#sec-installation-trouble-support"><span class="number">46 </span><span class="name">Support</span></a></span></dt><dd class="toc-abstract"><p>Before contacting support to help you with a problem on your cloud, it is strongly recommended that you gather as much information about your system and the problem as possible. For this purpose, SUSE OpenStack Cloud ships with a tool called supportconfig. It gathers system information such as the c…</p></dd><dt><span class="chapter"><a href="#inst-support-ptf"><span class="number">47 </span><span class="name">
    Applying PTFs (Program Temporary Fixes) Provided by SUSE L3 Support
   </span></a></span></dt><dd class="toc-abstract"><p>Under certain circumstances, SUSE Support may provide temporary fixes (called PTFs, to customers with an L3 support contract. These PTFs are provided as RPM packages. To make them available on all nodes in SUSE OpenStack Cloud, proceed as follows. If you prefer to test them first on a single node, s…</p></dd><dt><span class="chapter"><a href="#inst-support-ptf-test"><span class="number">48 </span><span class="name">
    Testing PTFs (Program Temporary Fixes) on a Single Node
   </span></a></span></dt><dd class="toc-abstract"><p>
    If you want to test a PTF (Program Temporary Fix) before deploying it on
    all nodes to verify that it fixes a certain issue, you can manually install
    the PTF on a single node.
   </p></dd></dl></div><div class="chapter " id="sec-depl-trouble-faq"><div class="titlepage"><div><div><h2 class="title"><span class="number">45 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">FAQ</span> <a title="Permalink" class="permalink" href="#sec-depl-trouble-faq">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-support.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-support.xml</li><li><span class="ds-label">ID: </span>sec-depl-trouble-faq</li></ul></div></div></div></div><div class="line"></div><p>
   Find solutions for the most common pitfalls and technical details on how
   to create a support request for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> here.
  </p><div class="qandaset" id="id-1.3.8.3.3"><div class="qandadiv-title-wrap"><h3 class="qandadiv-title" id="id-1.3.8.3.3.1">1. Node Deployment</h3></div><div class="qandadiv"><div class="free-id" id="id-1.3.8.3.3.1.2"></div><dl class="qandaentry"><dt class="question" id="id-1.3.8.3.3.1.2.1"><strong>Q:</strong>
       How to Disable the YaST Installer Self-Update when deploying nodes?
      </dt><dd class="answer" id="id-1.3.8.3.3.1.2.2"><p>
       Prior to starting an installation, the YaST installer can update
       itself if respective updates are available. By default this feature is
       enabled. In case of problems with this feature, disable it as follows:
      </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Open
         <code class="filename">~/openstack/ardana/ansible/roles/cobbler/templates/sles.grub.j2</code>
         with an editor and add <code class="literal">self_update=0</code> to the line
         starting with <code class="literal">linuxefi</code>. The results needs to look
         like the following:
        </p><div class="verbatim-wrap"><pre class="screen">linuxefi images/{{ sles_profile_name }}-x86_64/linux ifcfg={{ item[0] }}=dhcp install=http://{{ cobbler_server_ip_addr }}:79/cblr/ks_mirror/{{ sles_profile_name }} self_update=0 AutoYaST2=http://{{ cobbler_server_ip_addr }}:79/cblr/svc/op/ks/system/{{ item[1] }}</pre></div></li><li class="step "><p>
         Commit your changes:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git commit -m "Disable Yast Self Update feature" \
~/openstack/ardana/ansible/roles/cobbler/templates/sles.grub.j2</pre></div></li><li class="step "><p>
         If you need to reenable the installer self-update, remove
         <code class="literal">self_update=0</code> and commit the changes.
        </p></li></ol></div></div></dd></dl></div></div></div><div class="chapter " id="sec-installation-trouble-support"><div class="titlepage"><div><div><h2 class="title"><span class="number">46 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Support</span> <a title="Permalink" class="permalink" href="#sec-installation-trouble-support">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-support.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-support.xml</li><li><span class="ds-label">ID: </span>sec-installation-trouble-support</li></ul></div></div></div></div><div class="line"></div><p>
   
   Before contacting support to help you with a problem on your cloud, it is
   strongly recommended that you gather as much information about your
   system and the problem as possible. For this purpose, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   ships with a tool called <code class="command">supportconfig</code>. It gathers
   system information such as the current kernel version being used, the
   hardware, RPM database, partitions, and other items.
   <code class="command">supportconfig</code> also collects the most important log
   files, making it easier for the supporters to identify and solve your
   problem.
  </p><p>
   It is recommended to always run <code class="command">supportconfig</code> on the
   CLM Server and on the Control Node(s). If a Compute Node or a
   Storage Node is part of the problem, run
   <code class="command">supportconfig</code> on the affected node as well. For
   details on how to run <code class="command">supportconfig</code>, see
   <a class="link" href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-admin/#cha-adm-support" target="_blank">https://documentation.suse.com/sles/15-SP1/single-html/SLES-admin/#cha-adm-support</a>.
  </p></div><div class="chapter " id="inst-support-ptf"><div class="titlepage"><div><div><h2 class="title"><span class="number">47 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">
    Applying PTFs (Program Temporary Fixes) Provided by SUSE L3 Support
   </span> <a title="Permalink" class="permalink" href="#inst-support-ptf">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-support.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-support.xml</li><li><span class="ds-label">ID: </span>inst-support-ptf</li></ul></div></div></div></div><div class="line"></div><p>
    Under certain circumstances, SUSE Support may provide temporary fixes
    (called PTFs, to customers with an L3 support contract. These PTFs are
    provided as RPM packages. To make them available on all nodes in SUSE <span class="productname">OpenStack</span> Cloud,
    proceed as follows. If you prefer to test them first on a single node, see
    <a class="xref" href="#inst-support-ptf-test" title="Chapter 48.  Testing PTFs (Program Temporary Fixes) on a Single Node">Chapter 48, <em>
    Testing PTFs (Program Temporary Fixes) on a Single Node
   </em></a>.
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Download the packages from the location provided by SUSE L3 Support to
      a temporary location on the CLM Server.
     </p></li><li class="step "><p>
      Move the packages from the temporary download location to the
      following directories on the CLM Server:
     </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.3.8.5.3.2.2.1"><span class="term ">
        <span class="quote">“<span class="quote ">noarch</span>”</span> packages (<code class="filename">*.noarch.rpm</code>):
       </span></dt><dd><p>
         <code class="filename">/srv/www/suse-12.4/x86_64/repos/PTF/rpm/noarch/</code>
        </p></dd><dt id="id-1.3.8.5.3.2.2.2"><span class="term ">
        <span class="quote">“<span class="quote ">x86_64</span>”</span> packages (<code class="filename">*.x86_64.rpm</code>)
       </span></dt><dd><p>
         <code class="filename">/srv/www/suse-12.4/x86_64/repos/PTF/rpm/x86_64/</code>
        </p></dd></dl></div></li><li class="step "><p>
      Create or update the repository metadata:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo /usr/local/sbin/createrepo-cloud-ptf</pre></div></li><li class="step "><p>
      To deploy the updates, proceed as described in <span class="intraxref">Book “Operations Guide CLM”, Chapter 15 “System Maintenance”, Section 15.3 “Cloud Lifecycle Manager Maintenance Update Procedure”</span> and refresh the PTF repository before
      installing package updates on a node:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo zypper refresh -fr PTF</pre></div></li></ol></div></div></div><div class="chapter " id="inst-support-ptf-test"><div class="titlepage"><div><div><h2 class="title"><span class="number">48 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">
    Testing PTFs (Program Temporary Fixes) on a Single Node
   </span> <a title="Permalink" class="permalink" href="#inst-support-ptf-test">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/installation-support.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>installation-support.xml</li><li><span class="ds-label">ID: </span>inst-support-ptf-test</li></ul></div></div></div></div><div class="line"></div><p>
    If you want to test a PTF (Program Temporary Fix) before deploying it on
    all nodes to verify that it fixes a certain issue, you can manually install
    the PTF on a single node.
   </p><p>
     In the following procedure, a PTF named
     <code class="filename">venv-openstack-nova-x86_64-ptf.rpm</code>, containing a fix
     for nova, is installed on the Compute Node 01.
    </p><div class="procedure " id="id-1.3.8.6.4"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 48.1: </span><span class="name">Testing a Fix for nova </span><a title="Permalink" class="permalink" href="#id-1.3.8.6.4">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Check the version number of the package(s) that will be upgraded with
       the PTF. Run the following command on the deployer node:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>rpm -q venv-openstack-nova-x86_64</pre></div></li><li class="step "><p>
       Install the PTF on the deployer node:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper up ./venv-openstack-nova-x86_64-ptf.rpm</pre></div><p>
       This will install a new TAR archive in
       <code class="filename">/opt/ardana_packager/ardana-8/sles_venv/x86_64/</code>.
      </p></li><li class="step "><p>
       Register the TAR archive with the indexer:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo  create_index --dir \
      /opt/ardana_packager/ardana-8/sles_venv/x86_64</pre></div><p>
       This will update the indexer
       <code class="filename">/opt/ardana_packager/ardana-8/sles_venv/x86_64/packages</code>.
      </p></li><li class="step "><p>
       Deploy the fix on Compute Node 01:
      </p><ol type="a" class="substeps "><li class="step "><p>
         Check whether the fix can be deployed on a single Compute Node without
         updating the Control Nodes:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-upgrade.yml \
--limit=inputmodel-ccp-compute0001-mgmt --list-hosts</pre></div></li><li class="step "><p>
         If the previous test passes, install the fix:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-upgrade.yml \
--limit=inputmodel-ccp-compute0001-mgmt</pre></div></li></ol></li><li class="step "><p>
       Validate the fix, for example by logging in to the Compute Node to check
       the log files:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ssh ardana@inputmodel-ccp-compute0001-mgmt</pre></div></li><li class="step "><p>
       In case your tests are positive, install the PTF on all nodes as
       described in <a class="xref" href="#inst-support-ptf" title="Chapter 47.  Applying PTFs (Program Temporary Fixes) Provided by SUSE L3 Support">Chapter 47, <em>
    Applying PTFs (Program Temporary Fixes) Provided by SUSE L3 Support
   </em></a>.
      </p><p>
       In case the test are negative uninstall the fix and restore the previous
       state of the Compute Node by running the following commands on the
       deployer node;
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper install --force venv-openstack-nova-x86_64-<em class="replaceable ">OLD-VERSION</em>
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-upgrade.yml \
--limit=inputmodel-ccp-compute0001-mgmt</pre></div><p>
       Make sure to replace <em class="replaceable ">OLD-VERSION</em> with the
       version number you checked in the first step.
      </p></li></ol></div></div></div></div></div></div><div class="page-bottom"><div class="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span class="_share-fb bottom-button">Facebook</span><span class="spacer"> • </span><span class="_share-in bottom-button">LinkedIn</span><span class="spacer"> • </span><span class="_share-tw bottom-button">Twitter</span><span class="spacer"> • </span><span class="_share-mail bottom-button">E-Mail</span></span></div><div class="print"><span class="_print-button bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2020 
        SUSE</p><ul><li><a href="https://jobs.suse.com/" target="_top">Careers</a></li><li><a href="https://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="https://www.suse.com/company/about/" target="_top">About</a></li><li><a href="https://www.suse.com/contact/" target="_top">Contact Us</a></li></ul></div></div></body></html>