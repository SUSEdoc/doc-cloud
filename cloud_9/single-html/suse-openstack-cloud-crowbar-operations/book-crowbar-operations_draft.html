<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Operations Guide Crowbar</title><meta name="generator" content="DocBook XSL Stylesheets V1.79.2" /><style type="text/css">
body { background-image: url('static/images/draft.png');
       background-repeat: no-repeat;
       background-position: top left;
       /* The following properties make the watermark "fixed" on the page. */
       /* I think that's just a bit too distracting for the reader... */
       /* background-attachment: fixed; */
       /* background-position: center center; */
     }</style></head><body onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div xml:lang="en" class="book" lang="en"><div class="titlepage"><div><div><h1 class="title"><a id="book-crowbar-operations"></a><em class="citetitle">Operations Guide Crowbar</em></h1></div><div><div class="legalnotice"><a id="id12981"></a><p>
  Copyright FUJITSU LIMITED 2015 - 2017
 </p><p>
  Copyright ©
2021

  SUSE LLC and contributors. All rights reserved.
 </p><p>
  Licensed under the Apache License, Version 2.0 (the "License"); you may not
  use this file except in compliance with the License. You may obtain a copy of
  the License at <a class="link" href="http://www.apache.org/licenses/LICENSE-2.0" target="_top">http://www.apache.org/licenses/LICENSE-2.0</a>.
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
  WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
  License for the specific language governing permissions and limitations under
  the License.
 </p><p>
  For SUSE trademarks, see <a class="link" href="http://www.suse.com/company/legal/" target="_top">http://www.suse.com/company/legal/</a>.
  All other third-party trademarks are the property of their respective owners.
  Trademark symbols (®, ™ etc.) denote trademarks of SUSE and its
  affiliates. Asterisks (*) denote third-party trademarks.
 </p><p>
  The OpenStack® Word Mark and OpenStack logo are registered
  trademarks/service marks or trademarks/service marks of the OpenStack
  Foundation in the United States and other countries and are used with the
  OpenStack Foundation's permission.
 </p><p>
  All information found in this book has been compiled with utmost attention to
  detail. However, this does not guarantee complete accuracy. Neither SUSE LLC,
  its affiliates, the authors nor the translators shall be held liable for
  possible errors or the consequences thereof.
 </p></div></div></div><hr /></div><div class="toc"><p><strong>Table of Contents</strong></p><dl class="toc"><dt><span class="chapter"><a href="#sec-maintenance">1. Maintenance</a></span></dt><dd><dl><dt><span class="section"><a href="#sec-nodes-update">Keeping the Nodes Up-To-Date</a></span></dt><dt><span class="section"><a href="#sec-service-orders">Service Order on SUSE <span class="productname">OpenStack</span> Cloud Start-up or Shutdown</a></span></dt><dt><span class="section"><a href="#sec-upgrade-8-9">Upgrading from <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8 to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 9</a></span></dt><dd><dl><dt><span class="section"><a href="#sec-upgrade-8-9-requirements">Requirements</a></span></dt><dt><span class="section"><a href="#sec-upgrade-unsupported-configs">Unsupported configurations</a></span></dt><dt><span class="section"><a href="#sec-upgrade-web-ui">Upgrading Using the Web Interface</a></span></dt><dt><span class="section"><a href="#sec-upgrade-command-line">Upgrading from the Command Line</a></span></dt><dt><span class="section"><a href="#sec-depl-maintenance-upgrade-troubleshooting">Troubleshooting Upgrade Issues</a></span></dt></dl></dd><dt><span class="section"><a href="#sec-recover-comp-node-failure">Recovering from Compute Node Failure</a></span></dt><dt><span class="section"><a href="#sec-bootstrap-compute-plane">Bootstrapping the Compute Plane</a></span></dt><dt><span class="section"><a href="#sec-bootstrap-galera-cluster-with-missing-node">Bootstrapping the MariaDB Galera Cluster with Pacemaker when a node is missing</a></span></dt><dt><span class="section"><a href="#id13876">Updating MariaDB with Galera</a></span></dt><dt><span class="section"><a href="#OctaviaMaintenance">Load Balancer: Octavia Administration</a></span></dt><dd><dl><dt><span class="section"><a href="#octavia-admin-delete">Removing load balancers</a></span></dt></dl></dd><dt><span class="section"><a href="#database-maintenance">Periodic OpenStack Maintenance Tasks</a></span></dt><dt><span class="section"><a href="#sec-depl-maintenance-fernet-tokens">Rotating Fernet Tokens</a></span></dt></dl></dd><dt><span class="chapter"><a href="#gpu-passthrough">2. GPU passthrough</a></span></dt><dd><dl><dt><span class="section"><a href="#crow-pci-passthrough">Leveraging PCI Passthrough</a></span></dt><dd><dl><dt><span class="section"><a href="#preparing-comp-passthrough">Preparing nova and glance for passthrough</a></span></dt><dt><span class="section"><a href="#flavor-creation">Flavor Creation</a></span></dt><dt><span class="section"><a href="#additional-examples">Additional examples</a></span></dt></dl></dd></dl></dd><dt><span class="chapter"><a href="#self-assign-certs">3. Generate SUSE <span class="productname">OpenStack</span> Cloud Self Signed Certificate</a></span></dt><dd><dl><dt><span class="section"><a href="#create-root-pair">Create the CA Root Pair</a></span></dt><dt><span class="section"><a href="#sign-server-client-cert">Sign server and client certificates</a></span></dt><dt><span class="section"><a href="#deploy-cert">Deploying the certificate</a></span></dt><dt><span class="section"><a href="#lets-encrypt-cert">Generate Public Certificate using Let’s Encrypt</a></span></dt></dl></dd><dt><span class="chapter"><a href="#soc-monitoring">4. SUSE <span class="productname">OpenStack</span> Cloud Monitoring</a></span></dt><dd><dl><dt><span class="section"><a href="#about-operations-monitoring">About <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring</a></span></dt><dd><dl><dt><span class="section"><a href="#accessing-soc-monitoring">Accessing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring</a></span></dt><dt><span class="section"><a href="#monitoring-overview-data">Overview</a></span></dt></dl></dd><dt><span class="section"><a href="#monitoring-architecture">Architecture</a></span></dt><dd><dl><dt><span class="section"><a href="#id14766">Agents and Services</a></span></dt></dl></dd><dt><span class="section"><a href="#basic-usage-scenario">Basic Usage Scenario</a></span></dt><dd><dl><dt><span class="section"><a href="#metrics-monitoring">Metrics</a></span></dt><dt><span class="section"><a href="#data-visualization-analysis">Data Visualization and Analysis</a></span></dt><dt><span class="section"><a href="#alarms-notifications">Alarms and Notifications</a></span></dt></dl></dd><dt><span class="section"><a href="#key-features">Key Features</a></span></dt><dd><dl><dt><span class="section"><a href="#monitoring">Monitoring</a></span></dt><dt><span class="section"><a href="#metrics">Metrics</a></span></dt><dt><span class="section"><a href="#log-management">Log Management</a></span></dt><dt><span class="section"><a href="#openstack-integration">Integration with <span class="productname">OpenStack</span></a></span></dt></dl></dd><dt><span class="section"><a href="#components">Components</a></span></dt><dd><dl><dt><span class="section"><a href="#monitoring-service">Monitoring Service</a></span></dt><dt><span class="section"><a href="#horizon-plugin">Horizon Plugin</a></span></dt><dt><span class="section"><a href="#metrics-agent">Metrics Agent</a></span></dt><dt><span class="section"><a href="#log-agent">Log Agent</a></span></dt></dl></dd><dt><span class="section"><a href="#users-roles">Users and Roles</a></span></dt><dd><dl><dt><span class="section"><a href="#user-management">User Management</a></span></dt></dl></dd><dt><span class="section"><a href="#ops-maintenance">Operation and Maintenance</a></span></dt><dd><dl><dt><span class="section"><a href="#id15263">Removing Metrics Data</a></span></dt><dt><span class="section"><a href="#id15342">Removing Log Data</a></span></dt><dt><span class="section"><a href="#id15408">Log File Handling</a></span></dt><dt><span class="section"><a href="#backup-recovery">Backup and Recovery</a></span></dt></dl></dd><dt><span class="section"><a href="#data-virtualizations">Working with Data Visualizations</a></span></dt><dd><dl><dt><span class="section"><a href="#metrics-dashboard-openstack">Preconfigured Metrics Dashboard for <span class="productname">OpenStack</span></a></span></dt><dt><span class="section"><a href="#building-dashboards">Building Dashboards</a></span></dt><dt><span class="section"><a href="#creating-dashboards">Creating Dashboards</a></span></dt><dt><span class="section"><a href="#editing-rows">Editing Rows</a></span></dt><dt><span class="section"><a href="#editing-panels">Editing Panels</a></span></dt><dt><span class="section"><a href="#saving-sharing-dashboards">Saving and Sharing Dashboards</a></span></dt></dl></dd><dt><span class="section"><a href="#defining-alarms">Defining Alarms</a></span></dt><dd><dl><dt><span class="section"><a href="#alarm-details">Details</a></span></dt><dt><span class="section"><a href="#alarm-notifications">Notifications</a></span></dt></dl></dd><dt><span class="section"><a href="#defining-notifcations">Defining Notifications</a></span></dt><dt><span class="section"><a href="#status-servers-log-data">Status of Services, Servers, and Log Data</a></span></dt><dt><span class="section"><a href="#supported-metrics">Supported Metrics</a></span></dt><dd><dl><dt><span class="section"><a href="#id15967">Standard Metrics</a></span></dt><dt><span class="section"><a href="#id15986">Additional Metrics</a></span></dt></dl></dd></dl></dd><dt><span class="chapter"><a href="#monitoring-log-management">5. Log Management</a></span></dt><dd><dl><dt><span class="section"><a href="#configure-log-management-window">Working with the Log Management Window</a></span></dt><dd><dl><dt><span class="section"><a href="#query-log-data">Querying Log Data</a></span></dt><dt><span class="section"><a href="#visualize-query-results">Visualizing Query Results</a></span></dt><dt><span class="section"><a href="#combine-visualizations-dashboard">Combining Visualizations in Dashboards</a></span></dt><dt><span class="section"><a href="#filter-query-dashboard">Filtering Query Results in Dashboards</a></span></dt><dt><span class="section"><a href="#sharing-dashboards">Sharing Dashboards</a></span></dt></dl></dd><dt><span class="section"><a href="#configure-index-patterns">Configuring Index Patterns</a></span></dt><dt><span class="section"><a href="#id16347">Monitoring Log Data</a></span></dt><dt><span class="section"><a href="#cha-deploy-logs">Log Files</a></span></dt><dd><dl><dt><span class="section"><a href="#sec-deploy-logs-adminserv">On the Administration Server</a></span></dt><dt><span class="section"><a href="#sec-deploy-logs-crownodes">On All Other Crowbar Nodes</a></span></dt><dt><span class="section"><a href="#sec-deploy-logs-contrnode">On the Control Node(s)</a></span></dt><dt><span class="section"><a href="#sec-deploy-logs-compnode">On Compute Nodes</a></span></dt></dl></dd></dl></dd><dt><span class="chapter"><a href="#troubleshooting">6. Troubleshooting</a></span></dt><dd><dl><dt><span class="section"><a href="#shard-failures">Shard Failures</a></span></dt><dt><span class="section"><a href="#something-went-wrong">Oops! Looks like something went wrong</a></span></dt><dt><span class="section"><a href="#no-results">No results found</a></span></dt></dl></dd><dt><span class="appendix"><a href="#id16537">A. Glossary</a></span></dt></dl></div><div class="list-of-figures"><p><strong>List of Figures</strong></p><dl><dt>3.1. <a href="#id14414">Database Barclamp</a></dt><dt>3.2. <a href="#id14429">RabbitMQ Barclamp</a></dt><dt>3.3. <a href="#id14444">Keystone Barclamp</a></dt><dt>3.4. <a href="#id14460">Glance Barclamp</a></dt><dt>3.5. <a href="#id14476">Cinder Barclamp</a></dt><dt>3.6. <a href="#id14492">Neutron Barclamp</a></dt><dt>3.7. <a href="#id14508">Nova Barclamp</a></dt><dt>4.1. <a href="#fig-socm-monasca">SUSE <span class="productname">OpenStack</span> Cloud horizon Dashboard—Monitoring</a></dt><dt>4.2. <a href="#id15792">Creating an Alarm Definition</a></dt><dt>5.1. <a href="#id16162">The Kibana Dashboard—Discover Page</a></dt></dl></div><div class="chapter"><div class="titlepage"><div><div><h1 class="title"><a id="sec-maintenance"></a>Chapter 1. Maintenance</h1></div></div></div><div class="toc"><p><strong>Table of Contents</strong></p><dl class="toc"><dt><span class="section"><a href="#sec-nodes-update">Keeping the Nodes Up-To-Date</a></span></dt><dt><span class="section"><a href="#sec-service-orders">Service Order on SUSE <span class="productname">OpenStack</span> Cloud Start-up or Shutdown</a></span></dt><dt><span class="section"><a href="#sec-upgrade-8-9">Upgrading from <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8 to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 9</a></span></dt><dd><dl><dt><span class="section"><a href="#sec-upgrade-8-9-requirements">Requirements</a></span></dt><dt><span class="section"><a href="#sec-upgrade-unsupported-configs">Unsupported configurations</a></span></dt><dt><span class="section"><a href="#sec-upgrade-web-ui">Upgrading Using the Web Interface</a></span></dt><dt><span class="section"><a href="#sec-upgrade-command-line">Upgrading from the Command Line</a></span></dt><dt><span class="section"><a href="#sec-depl-maintenance-upgrade-troubleshooting">Troubleshooting Upgrade Issues</a></span></dt></dl></dd><dt><span class="section"><a href="#sec-recover-comp-node-failure">Recovering from Compute Node Failure</a></span></dt><dt><span class="section"><a href="#sec-bootstrap-compute-plane">Bootstrapping the Compute Plane</a></span></dt><dt><span class="section"><a href="#sec-bootstrap-galera-cluster-with-missing-node">Bootstrapping the MariaDB Galera Cluster with Pacemaker when a node is missing</a></span></dt><dt><span class="section"><a href="#id13876">Updating MariaDB with Galera</a></span></dt><dt><span class="section"><a href="#OctaviaMaintenance">Load Balancer: Octavia Administration</a></span></dt><dd><dl><dt><span class="section"><a href="#octavia-admin-delete">Removing load balancers</a></span></dt></dl></dd><dt><span class="section"><a href="#database-maintenance">Periodic OpenStack Maintenance Tasks</a></span></dt><dt><span class="section"><a href="#sec-depl-maintenance-fernet-tokens">Rotating Fernet Tokens</a></span></dt></dl></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="sec-nodes-update"></a>Keeping the Nodes Up-To-Date</h2></div></div></div><p>
      Keeping the nodes in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> up-to-date requires an appropriate
      setup of the update and pool repositories and the deployment of
      either the <span class="guimenu">Updater</span> barclamp or the SUSE Manager
      barclamp. For details, see
      <a class="xref" href="#sec-depl-adm-conf-repos-scc" title="Update and Pool Repositories">the section called “Update and Pool Repositories”</a>, <a class="xref" href="#sec-depl-inst-nodes-post-updater" title="Deploying Node Updates with the Updater Barclamp">the section called “Deploying Node Updates with the Updater Barclamp”</a>, and
      <a class="xref" href="#sec-depl-inst-nodes-post-manager" title="Configuring Node Updates with the SUSE Manager Client Barclamp">the section called “Configuring Node Updates with the <span class="guimenu">SUSE Manager Client</span>
    Barclamp”</a>.
    </p><p>
      If one of those barclamps is deployed, patches are installed on the
      nodes. Patches that do not require a reboot will not cause a service
      interruption. If a patch (for example, a kernel update) requires a reboot
      after the installation, services running on the machine that is rebooted
      will not be available within SUSE <span class="productname">OpenStack</span> Cloud.  Therefore, we strongly recommend
      installing those patches during a maintenance window.
    </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Maintenance Mode</h3><p>
        As of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8, it is not possible to put your entire SUSE <span class="productname">OpenStack</span> Cloud into
        <span class="quote">“<span class="quote">Maintenance Mode</span>”</span> (such as limiting all users to
        read-only operations on the control plane), as <span class="productname">OpenStack</span> does
        not support this. However when Pacemaker is deployed to
        manage HA clusters, it should be used to place services and
        cluster nodes into <span class="quote">“<span class="quote">Maintenance Mode</span>”</span> before
        performing maintenance functions on them. For more
        information,
        see <a class="link" href="https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#cha-conf-hawk2" target="_top">SUSE Linux Enterprise
        High Availability documentation</a>.
      </p></div><div class="variablelist"><p class="title"><strong>Consequences when Rebooting Nodes</strong></p><dl class="variablelist"><dt><span class="term">Administration Server</span></dt><dd><p>
            While the Administration Server is offline, it is not possible to deploy new
            nodes. However, rebooting the Administration Server has no effect on starting
            instances or on instances already running.
          </p></dd><dt><span class="term">Control Nodes</span></dt><dd><p>
            The consequences a reboot of a Control Node depend on the
            services running on that node:
          </p><p><strong>Database, keystone, RabbitMQ, glance, nova: </strong>
              No new instances can be started.
            </p><p><strong>swift: </strong>
              No object storage data is available. If glance uses
              swift, it will not be possible to start new instances.
            </p><p><strong>cinder, Ceph: </strong>
              No block storage data is available.
            </p><p><strong>neutron: </strong>
              No new instances can be started. On running instances the
              network will be unavailable.
            </p><p><strong>horizon. </strong>
              horizon will be unavailable. Starting and managing instances
              can be done with the command line tools.
            </p></dd><dt><span class="term">Compute Nodes</span></dt><dd><p>
            Whenever a Compute Node is rebooted, all instances running on
            that particular node will be shut down and must be manually restarted.
            Therefore it is recommended to <span class="quote">“<span class="quote">evacuate</span>”</span> the node by
            migrating instances to another node, before rebooting it.
          </p></dd></dl></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="sec-service-orders"></a>Service Order on SUSE <span class="productname">OpenStack</span> Cloud Start-up or Shutdown</h2></div></div></div><p>
      In case you need to restart your complete SUSE <span class="productname">OpenStack</span> Cloud (after a complete shut
      down or a power outage), ensure that the external Ceph cluster is started,
      available and healthy.
      Then start nodes and services in the order documented
      below.
    </p><div class="orderedlist"><p class="title"><strong>Service Order on Start-up</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p>
          Control Node/Cluster on which the Database is deployed
        </p></li><li class="listitem"><p>
          Control Node/Cluster on which RabbitMQ is deployed
        </p></li><li class="listitem"><p>
          Control Node/Cluster on which keystone is deployed
        </p></li><li class="listitem"><p>
          For swift:
        </p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>
              Storage Node on which the <code class="literal">swift-storage</code> role is deployed
            </p></li><li class="listitem"><p>
              Storage Node on which the <code class="literal">swift-proxy</code> role is deployed
            </p></li></ol></div></li><li class="listitem"><p>
          Any remaining Control Node/Cluster. The following additional rules apply:
        </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
              The Control Node/Cluster on which the <code class="literal">neutron-server</code>
              role is deployed needs to be started before starting the node/cluster
              on which the <code class="literal">neutron-l3</code> role is deployed.
            </p></li><li class="listitem"><p>
              The Control Node/Cluster on which the <code class="literal">nova-controller</code>
              role is deployed needs to be started before starting the node/cluster
              on which heat is deployed.
            </p></li></ul></div></li><li class="listitem"><p>
          Compute Nodes
        </p></li></ol></div><p>
      If multiple roles are deployed on a single Control Node, the services are
      automatically started in the correct order on that node. If you have more
      than one node with multiple roles, make sure they are
      started as closely as possible to the order listed above.
    </p><p>
      To shut down SUSE <span class="productname">OpenStack</span> Cloud, terminate nodes and services in the order documented
      below (which is the reverse of the start-up order).
    </p><div class="orderedlist"><p class="title"><strong>Service Order on Shutdown</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p>
          Compute Nodes
        </p></li><li class="listitem"><p>
          Control Node/Cluster on which heat is deployed
        </p></li><li class="listitem"><p>
          Control Node/Cluster on which the <code class="literal">nova-controller</code>
          role is deployed
        </p></li><li class="listitem"><p>
          Control Node/Cluster on which the <code class="literal">neutron-l3</code>
          role is deployed
        </p></li><li class="listitem"><p>
          All Control Node(s)/Cluster(s) on which neither of the following services
          is deployed: Database, RabbitMQ, and keystone.
        </p></li><li class="listitem"><p>
          For swift:
        </p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>
              Storage Node on which the <code class="literal">swift-proxy</code> role is
              deployed
            </p></li><li class="listitem"><p>
              Storage Node on which the <code class="literal">swift-storage</code> role is
              deployed
            </p></li></ol></div></li><li class="listitem"><p>
          Control Node/Cluster on which keystone is deployed
        </p></li><li class="listitem"><p>
          Control Node/Cluster on which RabbitMQ is deployed
        </p></li><li class="listitem"><p>
          Control Node/Cluster on which the Database is deployed
        </p></li><li class="listitem"><p>
          If required, gracefully shut down an external Ceph cluster
        </p></li></ol></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="sec-upgrade-8-9"></a>Upgrading from <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8 to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 9</h2></div></div></div><p>
      Upgrading from <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8 to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 9 can be done either via a
      Web interface or from the command line. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> supports a non-disruptive
      upgrade which provides a fully-functional SUSE <span class="productname">OpenStack</span> Cloud operation during most of
      the upgrade procedure, if your installation meets the requirements at <a class="xref" href="#il-upgrade-8-9-non-disruptive" title="Non-Disruptive Upgrade Requirements">Non-Disruptive Upgrade Requirements</a>.
    </p><p>
      If the requirements for a non-disruptive upgrade are not met, the
      upgrade procedure will be done in normal mode. When
      live-migration is set up, instances will be migrated to another node
      before the respective Compute Node is updated to ensure continuous
      operation.
    </p><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">STONITH and Administration Server</h3><p>
        Make sure that the STONITH mechanism in your cloud does not rely on the
        state of the Administration Server (for example, no SBD devices are located there,
        and IPMI is not using the network connection relying on the
        Administration Server). Otherwise, this may affect the clusters when the Administration Server is
        rebooted during the upgrade procedure.
      </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="sec-upgrade-8-9-requirements"></a>Requirements</h3></div></div></div><p>
        When starting the upgrade process, several checks are performed to
        determine whether the SUSE <span class="productname">OpenStack</span> Cloud is in an upgradeable state and whether a
        non-disruptive update would be supported:
      </p><div class="itemizedlist"><p class="title"><strong>General Upgrade Requirements</strong></p><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
            All nodes need to have the latest <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8 updates <span class="bold"><strong>and</strong></span> the latest SLES 12 SP3 updates installed. If
            this is not the case, refer to <a class="xref" href="#sec-depl-inst-nodes-post-updater" title="Deploying Node Updates with the Updater Barclamp">the section called “Deploying Node Updates with the Updater Barclamp”</a> for instructions on how to
            update.
          </p></li><li class="listitem"><p>
            All allocated nodes need to be turned on and have to be in state
            <span class="quote">“<span class="quote">ready</span>”</span>.
          </p></li><li class="listitem"><p>
            All barclamp proposals need to have been successfully deployed. If a
            proposal is in state <span class="quote">“<span class="quote">failed</span>”</span>, the upgrade procedure will
            refuse to start. Fix the issue, or remove the proposal, if necessary.
          </p></li><li class="listitem"><p>
            If the Pacemaker barclamp is deployed, all clusters
            need to be in a healthy state.
          </p></li><li class="listitem"><p> The upgrade will not start when Ceph is deployed via Crowbar. Only
            external Ceph is supported. Documentation for SUSE Enterprise Storage is available at
            <a class="link" href="https://documentation.suse.com/ses/5.5/" target="_top">https://documentation.suse.com/ses/5.5/</a>.
          </p></li><li class="listitem"><p>
            The following repositories need to be available on a server that is
            accessible from the Administration Server. The HA repositories are only needed if you
            have an HA setup. It is recommended to use the same server that also
            hosts the respective repositories of the current version.
          </p><table border="0" summary="Simple list" class="simplelist"><tr><td><code class="literal"><span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-9</span></span>-Pool</code></td></tr><tr><td><code class="literal"><span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-Crowbar-9</span></span>-Update</code></td></tr><tr><td><code class="literal">SLES12-SP4-Pool</code></td></tr><tr><td><code class="literal">SLES12-SP4-Update</code></td></tr><tr><td>
              <code class="literal">SLE-HA12-SP4-Pool</code> (for HA setups only)
            </td></tr><tr><td>
              <code class="literal">SLE-HA12-SP4-Update</code> (for HA setups only)
            </td></tr></table><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Important</h3><p>
              Do not add repositories to the SUSE <span class="productname">OpenStack</span> Cloud repository configuration. This
              needs to be done during the upgrade procedure.
            </p></div></li></ul></div><div class="itemizedlist"><a id="il-upgrade-8-9-non-disruptive"></a><p class="title"><strong>Non-Disruptive Upgrade Requirements</strong></p><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
            All Control Nodes need to be set up highly available.
          </p></li><li class="listitem"><p>
            A non-disruptive upgrade is not supported if the cinder
            has been deployed with the <code class="literal">raw devices</code> or
            <code class="literal">local file</code> back-end. In this case, you have to perform
            a regular upgrade, or change the cinder back-end for a
            non-disruptive upgrade.
          </p></li><li class="listitem"><p>
            A non-disruptive upgrade is prevented if the
            <code class="literal">cinder-volume</code> service is placed on Compute Node. For a
            non-disruptive upgrade, <code class="literal">cinder-volume</code> should either be
            HA-enabled or placed on non-compute nodes.
          </p></li><li class="listitem"><p>
            A non-disruptive upgrade is prevented if <code class="literal">manila-share</code>
            service is placed on a Compute Node. For more information, see <a class="xref" href="#sec-depl-ostack-manila" title="Deploying manila">the section called “Deploying manila”</a>.
          </p></li><li class="listitem"><p>
            Live-migration support needs to be configured and enabled for the
            Compute Nodes. The amount of free resources (CPU and RAM) on the
            Compute Nodes needs to be sufficient to evacuate the nodes one by one.
          </p></li><li class="listitem"><p>
            In case of a non-disruptive upgrade, glance must be configured as a
            shared storage if the <span class="guimenu">Default Storage
              Store</span> value in the glance is set to <code class="literal">File</code>.
          </p></li><li class="listitem"><p>
            For a non-disruptive upgrade, only KVM-based Compute Nodes with
            the <code class="literal">nova-compute-kvm</code> role are allowed in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8.
          </p></li><li class="listitem"><p>
            Non-disruptive upgrade is limited to the following cluster
            configurations:
          </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p>
                Single cluster that has all supported controller roles on it
              </p></li><li class="listitem"><p>
                Two clusters where one only has
                <code class="systemitem">neutron-network</code> and the other one has the
                rest of the controller roles.
              </p></li><li class="listitem"><p>
                Two clusters where one only has
                <code class="systemitem">neutron-server</code> plus
                <code class="systemitem">neutron-network</code> and the other one has the
                rest of the controller roles.
              </p></li><li class="listitem"><p>
                Two clusters, where one cluster runs the database and RabbitMQ
              </p></li><li class="listitem"><p>
                Three clusters, where one cluster runs database and RabbitMQ,
                another cluster has the controller roles, and the third cluster has
                the <code class="systemitem">neutron-network</code> role.
	      </p></li></ul></div><p>
            If your cluster configuration is not supported by the non-disruptive
            upgrade procedure, you can still perform a normal upgrade.
          </p></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="sec-upgrade-unsupported-configs"></a>Unsupported configurations</h3></div></div></div><p>
        In SUSE <span class="productname">OpenStack</span> Cloud 9, certain configurations and barclamp combinations
        are no longer supported. See the <a class="link" href="https://www.suse.com/releasenotes/x86_64/SUSE-OPENSTACK-CLOUD/9/" target="_top">SUSE <span class="productname">OpenStack</span> Cloud
        9 release notes</a> for details. The upgrade prechecks fail
        if they detect an unsupported configuration. Below is a list
        configurations that cause failure and possible solutions.
</p><div class="variablelist"><p class="title"><strong>Unsupported Configurations</strong></p><dl class="variablelist"><dt><span class="term">
            aodh deployed
          </span></dt><dd><p>
              The aodh barclamp has been removed in SUSE <span class="productname">OpenStack</span> Cloud 9. Deactivate and delete
              the aodh proposal before continuing the upgrade.
            </p></dd><dt><span class="term">
            ceilometer deployed without monasca
          </span></dt><dd><p>
              As of SUSE <span class="productname">OpenStack</span> Cloud 9, ceilometer has been reduced to the ceilometer agent,
              with monasca being used as a storage back-end. Consequently, a
              standalone ceilometer without monasca present will fail the upgrade
              prechecks. There are two possible solutions.
            </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
	          Deactivate and delete the ceilometer proposal before the upgrade, and
	          re-enable it after the upgrade.
	        </p></li><li class="listitem"><p>
	          Deploy the monasca barclamp before the upgrade.
	        </p></li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Existing ceilometer Data</h3><p>
		Existing ceilometer data is not migrated to new
		monasca storage. It stays in the MariaDB
		ceilometer database, and it can be accessed directly,
		if needed. The data is also included in the <span class="productname">OpenStack</span>
		database backup created by upgrade process.
	      </p></div></dd><dt><span class="term">
            nova: Xen Compute Nodes present
          </span></dt><dd><p>
              As of SUSE <span class="productname">OpenStack</span> Cloud 9, the Xen hypervisor is no longer supported. On any
              cloud, where Xen based compute nodes are still present, the following
              procedure must be followed before it can be upgraded to SUSE <span class="productname">OpenStack</span> Cloud 9:
            </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
                  Run <span class="command"><strong>nova service-disable</strong></span> for all nova Compute Nodes
                  running the Xen hypervisor. In the Crowbar context, these are the
                  Compute Nodes with the <code class="literal">nova-compute-xen</code> role in the
                  nova barclamp. This prevents any new VMs from being launched on these
                  Compute Nodes. It is safe to leave the Compute Nodes in question in this
                  state for the entire duration of the next step.
                </p></li><li class="listitem"><p>
                  Recreate all nova instances still running on the Xen-based
                  Compute Nodes on non-Xen Compute Nodes or shut them down completely.
                </p></li><li class="listitem"><p>
                  With no other instances on the Xen-based Compute Nodes, remove the nova
                  barclamp's <code class="literal">nova-compute-xen</code> role from these nodes and
                  re-apply the nova barclamp. You may optionally assign a different
                  <code class="literal">nova-compute</code> role, such as
                  <code class="literal">nova-compute-kvm</code>, to configure them as Compute Nodes for
                  one of the remaining hypervisors.
                </p></li></ul></div></dd><dt><span class="term">
            trove deployed
          </span></dt><dd><p>
              The trove barclamp has been removed in SUSE <span class="productname">OpenStack</span> Cloud 9. Deactivate and delete
              the trove proposal before continuing the upgrade.
            </p></dd></dl></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="sec-upgrade-web-ui"></a>Upgrading Using the Web Interface</h3></div></div></div><p>
        The Web interface features a wizard that guides you through the upgrade
        procedure.
      </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Canceling the Upgrade</h3><p>
          You can cancel the upgrade process by clicking <span class="guimenu">Cancel
            Upgrade</span>. The upgrade operation can only be canceled
          before the Administration Server upgrade is started. When the upgrade has been
          canceled, the nodes return to the ready state. However, any user
          modifications must be undone manually. This includes reverting repository
          configuration.
        </p></div><div class="procedure"><ol class="procedure" type="1"><li class="step"><p>
            To start the upgrade procedure, open the Crowbar Web interface on the Administration Server and choose <span class="guimenu">Utilities</span>+<span class="guimenu">Upgrade</span>. Alternatively, point the browser directly to the upgrade
            wizard on the Administration Server, for example
            <code class="literal">http://192.168.124.10/upgrade/</code>.
          </p></li><li class="step"><p>
            On the first screen of the Web interface you will run preliminary checks, get
            information about the upgrade mode and start the upgrade process.
          </p><div class="informalfigure"><div class="mediaobject"><img src="images/depl_upgrade_prepare.png" /></div></div></li><li class="step"><p>
            Perform the preliminary checks to determine whether the upgrade
            requirements are met by clicking <span class="guimenu">Check</span> in
            <code class="literal">Preliminary Checks</code>.
          </p><p>
            The Web interface displays the progress of the checks. You will see green, yellow or
            red indicator next to each check. Yellow means the upgrade can only be
            performed in the normal mode. Red indicates an error, which means that
            you need to fix the problem and run the <span class="guimenu">Check</span> again.
          </p></li><li class="step"><p>
            When all checks in the previous step have passed, <code class="literal">Upgrade
              Mode</code> shows the result of the upgrade analysis. It will indicate
            whether the upgrade procedure will continue in non-disruptive or in
            normal mode.
          </p></li><li class="step"><p>
            To start the upgrade process, click <span class="guimenu">Begin Upgrade</span>.
          </p></li><li class="step"><p>
            While the upgrade of the Administration Server is prepared, the upgrade wizard
            prompts you to <span class="guimenu">Download the Backup of the
              Administration Server</span>. When the backup is done, move it to a safe place. If
            something goes wrong during the upgrade procedure of the Administration Server, you
            can restore the original state from this backup using the
            <span class="command"><strong>crowbarctl backup restore
              <em class="replaceable"><code>NAME</code></em></strong></span> command.
          </p></li><li class="step"><p>
            Check that the repositories required for upgrading the Administration Server are
            available and updated. To do this, click the <span class="guimenu">Check</span>
            button. If the checks fail, add the software repositories as described in
            <a class="xref" href="#cha-depl-repo-conf" title="Chapter 5. Software Repository Setup">Chapter 5, <em>Software Repository Setup</em></a>. Run the
            checks again, and click <span class="guimenu">Next</span>.
          </p><div class="informalfigure"><div class="mediaobject"><img src="images/depl_upgrade_repocheck-admin.png" /></div></div></li><li class="step"><p>
            Click <span class="guimenu">Upgrade Administration Server</span> to upgrade and
            reboot the admin node. Note that this operation may take a while. When
            the Administration Server has been updated, click <span class="guimenu">Next</span>.
          </p><div class="informalfigure"><div class="mediaobject"><img src="images/depl_upgrade_admin.png" /></div></div></li><li class="step"><p>
            Check that the repositories required for upgrading all nodes are
            available and updated.  To do this click the <span class="guimenu">Check</span>
            button. If the check fails, add the software repositories as described in
            <a class="xref" href="#cha-depl-repo-conf" title="Chapter 5. Software Repository Setup">Chapter 5, <em>Software Repository Setup</em></a>. Run the
            checks again, and click <span class="guimenu">Next</span>.
          </p><div class="informalfigure"><div class="mediaobject"><img src="images/depl_upgrade_repocheck-nodes.png" /></div></div></li><li class="step"><p>
            Next, you need to stop <span class="productname">OpenStack</span> services. This makes the <span class="productname">OpenStack</span> API
            unavailable until the upgrade of control plane is
            completed. When you are ready, click <span class="guimenu">Stop
              Services</span>. Wait until the services are stopped and click
            <span class="guimenu">Next</span>.
          </p></li><li class="step"><p>
            Before upgrading the nodes, the wizard prompts you to <span class="guimenu">Back up
              OpenStack Database</span>. The MariaDB database backup will be
            stored on the Administration Server. It can be used to restore the database in case
            something goes wrong during the upgrade. To back up the database, click
            <span class="guimenu">Create Backup</span>. When the backup operation is
            finished, click <span class="guimenu">Next</span>.
          </p></li><li class="step"><p>
            If you prefer to upgrade the controller nodes and postpone upgrading
            Compute Nodes, disable the <span class="guimenu">Upgrade Compute Nodes</span>
            option. In this case, you can use the <span class="guimenu">Go to Dashboard</span>
            button to switch to the Crowbar Web interface to check the current configuration
            and make changes, as long as they do not affect the Compute Nodes. If you
            choose to postpone upgrading Compute Nodes, all <span class="productname">OpenStack</span> services remain up
            and running.
          </p><div class="informalfigure"><div class="mediaobject"><img src="images/depl_compnode_postponed.png" /></div></div></li><li class="step"><p>
            When the upgrade is completed, press <span class="guimenu">Finish</span> to
            return to the Dashboard.
          </p><div class="informalfigure"><div class="mediaobject"><img src="images/depl_upgrade_finished.png" /></div></div></li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Dealing with Errors</h3><p>
          If an error occurs during the upgrade process, the wizard displays a
          message with a description of the error and a possible solution. After
          fixing the error, re-run the step where the error occurred.
        </p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="sec-upgrade-command-line"></a>Upgrading from the Command Line</h3></div></div></div><p>
        The upgrade procedure on the command line is performed by using the program
        <span class="command"><strong>crowbarctl</strong></span>. For general help, run <span class="command"><strong>crowbarctl
          help</strong></span>. To get help on a certain subcommand, run
        <span class="command"><strong>crowbarctl <em class="replaceable"><code>COMMAND</code></em> help</strong></span>.
      </p><p>
        To review the process of the upgrade procedure, you may call
        <span class="command"><strong>crowbarctl upgrade status</strong></span> at any time. Steps may have
        three states: <code class="literal">pending</code>, <code class="literal">running</code>, and
        <code class="literal">passed</code>.
      </p><div class="procedure"><ol class="procedure" type="1"><li class="step"><p>
            To start the upgrade procedure from the command line, log in to the
            Administration Server as <code class="systemitem">root</code>.
          </p></li><li class="step"><p>
            Perform the preliminary checks to determine whether the upgrade
            requirements are met:
          </p><pre class="screen"><code class="prompt">root # </code>crowbarctl upgrade prechecks</pre><p>
            The command's result is shown in a table. Make sure the column
            <span class="guimenu">Errors</span> does not contain any entries. If there are
            errors, fix them and restart the <span class="command"><strong>precheck</strong></span> command
            afterwards. You cannot proceed until the mandatory checks have passed.
          </p><pre class="screen"><code class="prompt">root # </code>crowbarctl upgrade prechecks
            +-------------------------------+--------+----------+--------+------+
            | Check ID                      | Passed | Required | Errors | Help |
            +-------------------------------+--------+----------+--------+------+
            | network_checks                | true   | true     |        |      |
            | cloud_healthy                 | true   | true     |        |      |
            | maintenance_updates_installed | true   | true     |        |      |
            | compute_status                | true   | false    |        |      |
            | ha_configured                 | true   | false    |        |      |
            | clusters_healthy              | true   | true     |        |      |
            +-------------------------------+--------+----------+--------+------+</pre><p>
            Depending on the outcome of the checks, it is automatically decided
            whether the upgrade procedure will continue in non-disruptive or in
            normal mode. For the non-disruptive mode, all the checks must pass,
            including those that are not marked in the table as required.
          </p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Forcing Normal Mode Upgrade</h3><p>
              The non-disruptive update will take longer than an upgrade in normal
              mode, because it performs certain tasks sequentially which are done
              in parallel during the normal upgrade. Live-migrating guests to
              other Compute Nodes during the non-disruptive
              upgrade takes additional time.
            </p><p>
              Therefore, if a non-disruptive upgrade is not a requirement for you, you
              may want to switch to the normal upgrade mode, even if your setup
              supports the non-disruptive method. To force the normal upgrade mode,
              run:
            </p><pre class="screen"><code class="prompt">root # </code>crowbarctl upgrade mode normal</pre><p>
              To query the current upgrade mode run:
            </p><pre class="screen"><code class="prompt">root # </code>crowbarctl upgrade mode</pre><p>
              To switch back to the non-disruptive mode run:
            </p><pre class="screen"><code class="prompt">root # </code>crowbarctl upgrade mode non_disruptive</pre><p>
              It is possible to call this command at any time during the upgrade
              process until the <code class="literal">services</code> step is started. After
              that point the upgrade mode can no longer be changed.
            </p></div></li><li class="step"><p>
            Prepare the nodes by transitioning them into the <span class="quote">“<span class="quote">upgrade</span>”</span>
            state and stopping the chef daemon:
          </p><pre class="screen"><code class="prompt">root # </code>crowbarctl upgrade prepare</pre><p>
            Depending of the size of your SUSE <span class="productname">OpenStack</span> Cloud deployment, this step may take
            some time. Use the command <span class="command"><strong>crowbarctl upgrade status</strong></span>
            to monitor the status of the process named
            <code class="literal">steps.prepare.status</code>. It needs to be in state
            <code class="literal">passed</code> before you proceed:
          </p><pre class="screen"><code class="prompt">root # </code>crowbarctl upgrade status
            +--------------------------------+----------------+
            | Status                         | Value          |
            +--------------------------------+----------------+
            | current_step                   | backup_crowbar |
            | current_substep                |                |
            | current_substep_status         |                |
            | current_nodes                  |                |
            | current_node_action            |                |
            | remaining_nodes                |                |
            | upgraded_nodes                 |                |
            | crowbar_backup                 |                |
            | openstack_backup               |                |
            | suggested_upgrade_mode         | non_disruptive |
            | selected_upgrade_mode          |                |
            | compute_nodes_postponed        | false          |
            | steps.prechecks.status         | passed         |
            | steps.prepare.status           | passed         |
            | steps.backup_crowbar.status    | pending        |
            | steps.repocheck_crowbar.status | pending        |
            | steps.admin.status             | pending        |
            | steps.repocheck_nodes.status   | pending        |
            | steps.services.status          | pending        |
            | steps.backup_openstack.status  | pending        |
            | steps.nodes.status             | pending        |
            +--------------------------------+----------------+</pre></li><li class="step"><p>
            Create a backup of the existing Administration Server installation. In case something
            goes wrong during the upgrade procedure of the Administration Server you can restore
            the original state from this backup with the command <span class="command"><strong>crowbarctl
              backup restore <em class="replaceable"><code>NAME</code></em></strong></span>
          </p><pre class="screen"><code class="prompt">root # </code>crowbarctl upgrade backup crowbar</pre><p>
            To list all existing backups including the one you have just created, run
            the following command:
          </p><pre class="screen"><code class="prompt">root # </code>crowbarctl backup list
            +----------------------------+--------------------------+--------+---------+
            | Name                       | Created                  | Size   | Version |
            +----------------------------+--------------------------+--------+---------+
            | crowbar_upgrade_1534864741 | 2018-08-21T15:19:03.138Z | 219 KB | 4.0     |
            +----------------------------+--------------------------+--------+---------+</pre></li><li class="step"><p>
            This step prepares the upgrade of the Administration Server by checking the
            availability of the update and pool repositories for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>
            9 and SUSE Linux Enterprise Server 12 SP4. Run the following command:
          </p><pre class="screen"><code class="prompt">root # </code>crowbarctl upgrade repocheck crowbar
            +----------------------------------------+------------------+-----------+
            | Repository                             | Status           | Type      |
            +----------------------------------------+------------------+-----------+
            | SLES12-SP4-Pool                        | x86_64 (missing) | os        |
            | SLES12-SP4-Updates                     | x86_64 (missing) | os        |
            | SUSE-OpenStack-Cloud-Crowbar-9-Pool    | available        | openstack |
            | SUSE-OpenStack-Cloud-Crowbar-9-Updates | available        | openstack |
            +----------------------------------------+------------------+-----------+</pre><p>
            Two of the required repositories are reported as missing, because they have
            not yet been added to the Crowbar configuration. To add them to the
            Administration Server proceed as follows.
          </p><p>
            Note that this step is for setting up the repositories for the Administration Server,
            not for the nodes in SUSE <span class="productname">OpenStack</span> Cloud (this will be done in a subsequent step).
          </p><ol type="a" class="substeps"><li class="step"><p>
                Start <span class="command"><strong>yast repositories</strong></span> and proceed with
                <span class="guimenu">Continue</span>. Replace the repositories
                <code class="literal">SLES12-SP3-Pool</code> and
                <code class="literal">SLES12-SP3-Updates</code> with the respective SP4
                repositories.
              </p><p>
                If you prefer to use zypper over YaST, you may alternatively make the
                change using <span class="command"><strong>zypper mr</strong></span>.
              </p></li><li class="step"><p>
                Next, replace the <code class="literal">SUSE-OpenStack-Cloud-Crowbar-8</code>
                update and pool repositories with the respective <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 9 versions.
              </p></li><li class="step"><p>
                Check for other (custom) repositories. All SLES SP3 repositories need
                to be replaced with the respective SLES SP4 version. In case no SP3
                version exists, disable the repository—the respective packages
                from that repository will be deleted during the upgrade.
              </p></li></ol><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Administration Server with external SMT</h3><p>
              If the Administration Server is configured with external SMT, the system
              repositories configuration is managed by the system utilities. In this
              case, skip the above substeps and run the <span class="command"><strong>zypper migration
                --download-only</strong></span> command.
            </p></div><p>
            Once the repository configuration on the Administration Server has been updated, run
            the command to check the repositories again. If the configuration is
            correct, the result should look like the following:
          </p><pre class="screen"><code class="prompt">root # </code>crowbarctl upgrade repocheck crowbar
            +----------------------------------------+-----------+-----------+
            | Repository                             | Status    | Type      |
            +----------------------------------------+-----------+-----------+
            | SLES12-SP4-Pool                        | available | os        |
            | SLES12-SP4-Updates                     | available | os        |
            | SUSE-OpenStack-Cloud-Crowbar-9-Pool    | available | openstack |
            | SUSE-OpenStack-Cloud-Crowbar-9-Updates | available | openstack |
            +----------------------------------------+-----------+-----------+</pre></li><li class="step"><p>
            Now that the repositories are available, the Administration Server itself will be
            upgraded. The update will run in the background using <span class="command"><strong>zypper
              dup</strong></span>. Once all packages have been upgraded, the Administration Server will
            be rebooted and you will be logged out. To start the upgrade run:
          </p><pre class="screen"><code class="prompt">root # </code>crowbarctl upgrade admin</pre></li><li class="step"><p>
            After the Administration Server has been successfully updated, the Control Nodes and
            Compute Nodes will be upgraded. At first the availability of the
            repositories used to provide packages for the SUSE <span class="productname">OpenStack</span> Cloud nodes is tested.
          </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Correct Metadata in the PTF Repository</h3><p>
              When adding new repositories to the nodes, make sure that the new PTF
              repository also contains correct metadata (even if it is empty). To do
              this, run the <span class="command"><strong>createrepo-cloud-ptf</strong></span> command.
            </p></div><p>
            Note that the configuration for these repositories differs from the one
            for the Administration Server that was already done in a previous step. In this step
            the repository locations are made available to Crowbar rather than to
            libzypp on the Administration Server. To check the repository configuration run the
            following command:
          </p><pre class="screen"><code class="prompt">root # </code>crowbarctl upgrade repocheck nodes
            +----------------------------------------+-------------------------------------+-----------+
            | Repository                             | Status                              | Type      |
            +----------------------------------------+-------------------------------------+-----------+
            | SLE12-SP4-HA-Pool                      | missing (x86_64), inactive (x86_64) | ha        |
            | SLE12-SP4-HA-Updates                   | missing (x86_64), inactive (x86_64) | ha        |
            | SLE12-SP4-HA-Updates-test              | missing (x86_64), inactive (x86_64) | ha        |
            | SLES12-SP4-Pool                        | missing (x86_64), inactive (x86_64) | os        |
            | SLES12-SP4-Updates                     | missing (x86_64), inactive (x86_64) | os        |
            | SLES12-SP4-Updates-test                | missing (x86_64), inactive (x86_64) | os        |
            | SUSE-OpenStack-Cloud-Crowbar-9-Pool    | missing (x86_64), inactive (x86_64) | openstack |
            | SUSE-OpenStack-Cloud-Crowbar-9-Updates | missing (x86_64), inactive (x86_64) | openstack |
            +----------------------------------------+-------------------------------------+-----------+</pre><p>
            To update the locations for the listed repositories, start <span class="command"><strong>yast
              crowbar</strong></span> and proceed as described in <a class="xref" href="#sec-depl-adm-inst-crowbar-repos" title="Repositories">the section called “<span class="guimenu">Repositories</span>”</a>.
          </p><p>
            Once the repository configuration for Crowbar has been updated, run the
            command to check the repositories again to determine, whether the current
            configuration is correct.
          </p><pre class="screen"><code class="prompt">root # </code>crowbarctl upgrade repocheck nodes
            +----------------------------------------+-----------+-----------+
            | Repository                             | Status    | Type      |
            +----------------------------------------+-----------+-----------+
            | SLE12-SP4-HA-Pool                      | available | ha        |
            | SLE12-SP4-HA-Updates                   | available | ha        |
            | SLES12-SP4-Pool                        | available | os        |
            | SLES12-SP4-Updates                     | available | os        |
            | SUSE-OpenStack-Cloud-Crowbar-9-Pool    | available | openstack |
            | SUSE-OpenStack-Cloud-Crowbar-9-Updates | available | openstack |
            +----------------------------------------+-----------+-----------+</pre><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Shut Down Running instances in Normal Mode</h3><p>
              If the upgrade is done in the normal mode, you need to shut down or suspend
              all running instances before performing the next step.
            </p></div><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Product Media Repository Copies</h3><p>
              To PXE boot new nodes, an additional SUSE Linux Enterprise Server 12 SP4 repository—a copy
              of the installation system— is required. Although not required
              during the upgrade procedure, it is recommended to set up this directory
              now. Refer to <a class="xref" href="#sec-depl-adm-conf-repos-product" title="Copying the Product Media Repositories">the section called “Copying the Product Media Repositories”</a> for
              details. If you had also copied the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> 8 installation media
              (optional), you may also want to provide the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>
              9 the same way.
            </p><p>
              Once the upgrade procedure has been successfully finished, you may
              delete the previous copies of the installation media in
              <code class="filename">/srv/tftpboot/suse-12.4/x86_64/install</code> and
              <code class="filename">/srv/tftpboot/suse-12.4/x86_64/repos/Cloud</code>.
            </p></div></li><li class="step"><p>
            To ensure the status of the nodes does not change during the upgrade
            process, the majority of the <span class="productname">OpenStack</span> services will be stopped on the
            nodes. As a result, the <span class="productname">OpenStack</span> API will no longer be accessible. In
            case of the non-disruptive mode, the instances will continue to run and
            stay accessible. Run the following command:
          </p><pre class="screen"><code class="prompt">root # </code>crowbarctl upgrade services</pre><p>
            This step takes a while to finish. Monitor the process by running
            <span class="command"><strong>crowbarctl upgrade status</strong></span>. Do not proceed before
            <code class="literal">steps.services.status</code> is set to
            <code class="literal">passed</code>.
          </p></li><li class="step"><p>
            The last step before upgrading the nodes is to make a backup of the
            <span class="productname">OpenStack</span> database. The database dump will be stored on the
            Administration Server and can be used to restore the database in case something goes
            wrong during the upgrade.
          </p><pre class="screen"><code class="prompt">root # </code>crowbarctl upgrade backup openstack</pre><p>
            To find the location of the database dump, run the <span class="command"><strong>crowbarctl
              upgrade status</strong></span>.
          </p></li><li class="step"><p>
            The final step of the upgrade procedure is upgrading the
            nodes.  To start the process, enter:
          </p><pre class="screen"><code class="prompt">root # </code>crowbarctl upgrade nodes all</pre><p>
            The upgrade process runs in the background and can be queried with
            <span class="command"><strong>crowbarctl upgrade status</strong></span>. Depending on the size of
            your SUSE <span class="productname">OpenStack</span> Cloud it may take several hours, especially when performing a
            non-disruptive update. In that case, the Compute Nodes are updated
            one-by-one after instances have been live-migrated to other nodes.
          </p><p>
            Instead of upgrading all nodes you may also upgrade
            the Control Nodes first and individual Compute Nodes afterwards. Refer to
            <span class="command"><strong>crowbarctl upgrade nodes --help</strong></span> for details. If you
            choose this approach, you can use the <span class="command"><strong>crowbarctl upgrade
              status</strong></span> command to monitor the upgrade process. The output of
            this command contains the following entries:
          </p><div class="variablelist"><dl class="variablelist"><dt><span class="term">
                current_node_action
              </span></dt><dd><p>
                  The current action applied to the node.
                </p></dd><dt><span class="term">
                current_substep
              </span></dt><dd><p>
                  Shows the current substep of the node upgrade step. For example,
                  for the <span class="command"><strong>crowbarctl upgrade nodes controllers</strong></span>,
                  the <code class="literal">current_substep</code> entry displays the
                  <code class="literal">controller_nodes</code> status when upgrading controllers.
                </p></dd></dl></div><p>
            After the controllers have been upgraded, the
            <code class="literal">steps.nodes.status</code> entry in the output displays the
            <code class="literal">running</code> status. Check then the status of the
            <code class="literal">current_substep_status</code> entry. If it displays
            <code class="literal">finished</code>, you can move to the next step of upgrading
            the Compute Nodes.
          </p><p>
            <span class="bold"><strong>Postponing the Upgrade</strong></span>
          </p><p>
            It is possible to stop the upgrade of compute nodes and postpone their
            upgrade with the command:
          </p><pre class="screen"><code class="prompt">root # </code>crowbarctl upgrade nodes postpone</pre><p>
            After the upgrade of compute nodes is postponed, you can go to Crowbar
            Web interface, check the configuration. You can also apply some changes, provided
            they do not affect the Compute Nodes. During the postponed upgrade, all
            <span class="productname">OpenStack</span> services should be up and running.
          </p><p>
            To resume the upgrade, issue the command:
          </p><pre class="screen"><code class="prompt">root # </code>crowbarctl upgrade nodes resume</pre><p>
            Finish the upgrade with either <span class="command"><strong>crowbarctl upgrade nodes
              all</strong></span> or upgrade nodes one node by one with <span class="command"><strong>crowbarctl
              upgrade nodes <em class="replaceable"><code>NODE_NAME</code></em></strong></span>.
          </p><p>
            When upgrading individual Compute Nodes using the <span class="command"><strong>crowbarctl
              upgrade nodes</strong></span> <em class="replaceable"><code>NODE_NAME</code></em> command, the
            <code class="literal">current_substep_status</code> entry changes to
            <code class="literal">node_finished</code> when the upgrade of a single node is
            done. After all nodes have been upgraded, the
            <code class="literal">current_substep_status</code> entry displays <code class="literal">finished</code>.
          </p></li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Dealing with Errors</h3><p>
          If an error occurs during the upgrade process, the output of the
          <span class="command"><strong>crowbarctl upgrade status</strong></span> provides a detailed
          description of the failure. In most cases, both the output and the error
          message offer enough information for fixing the issue. When the problem has
          been solved, run the previously-issued upgrade command to resume the
          upgrade process.
        </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="sec-depl-maintenance-parallel-upgrade-cmdl"></a>Simultaneous Upgrade of Multiple Nodes</h4></div></div></div><p>
          It is possible to select more Compute Nodes for selective upgrade instead of
          just one. Upgrading multiple nodes simultaneously significantly reduces the
          time required for the upgrade.
        </p><p>
          To upgrade multiple nodes simultaneously, use the following command:
        </p><pre class="screen"><code class="prompt">root # </code>crowbarctl upgrade nodes <em class="replaceable"><code>NODE_NAME_1</code></em>,<em class="replaceable"><code>NODE_NAME_2</code></em>,<em class="replaceable"><code>NODE_NAME_3</code></em></pre><p>
          Node names can be separated by comma, semicolon, or space. When using
          space as separator, put the part containing node names in quotes.
        </p><p>
          Use the following command to find the names of the nodes that haven't been upgraded:
        </p><pre class="screen"><code class="prompt">root # </code>crowbarctl upgrade status nodes</pre><p>
          Since the simultaneous upgrade is intended to be non-disruptive, all
          Compute Nodes targeted for a simultaneous upgrade must be cleared of any
          running instances.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
            You can check what instances are running on a specific
            node using the following command:
          </p><pre class="screen"><code class="prompt">tux &gt; </code>nova list --all-tenants --host <em class="replaceable"><code>NODE_NAME</code></em></pre></div><p>
          This means that it is not possible to pick an arbitrary number of
          Compute Nodes for the simultaneous upgrade operation: you have to make sure
          that it is possible to live-migrate every instance away from the batch of
          nodes that are supposed to be upgraded in parallel. In case of high load
          on all Compute Nodes, it might not be possible to upgrade more than one node
          at a time. Therefore, it is recommended to perform the following steps for
          each node targeted for the simultaneous upgrade prior to running the
          <span class="command"><strong>crowbarctl upgrade nodes</strong></span> command.
        </p><div class="procedure"><ol class="procedure" type="1"><li class="step"><p>
              Disable the Compute Node so it's not used as a target during
              live-evacuation of any other node:
            </p><pre class="screen"><code class="prompt">tux &gt; </code>openstack compute service set --disable <em class="replaceable"><code>"NODE_NAME"</code></em> nova-compute</pre></li><li class="step"><p>
              Evacuate all running instances from the node:
            </p><pre class="screen"><code class="prompt">tux &gt; </code>nova host-evacuate-live <em class="replaceable"><code>"NODE_NAME"</code></em></pre></li></ol></div><p>
          After completing these steps, you can perform a simultaneous upgrade of
          the selected nodes.
        </p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="sec-depl-maintenance-upgrade-troubleshooting"></a>Troubleshooting Upgrade Issues</h3></div></div></div><div class="qandaset"><a id="id13620"></a><dl><dt>1. <a href="#id13621">
              Upgrade of the admin server has failed.
            </a></dt><dt>2. <a href="#id13629">
              An upgrade step repeatedly fails due to timeout.
            </a></dt><dt>3. <a href="#live-migration-failed">
              Node upgrade has failed during live migration.
            </a></dt><dt>4. <a href="#id13680">
              Node has failed during OS upgrade.
            </a></dt><dt>5. <a href="#id13689">
              Node does not come up after reboot.
            </a></dt><dt>6. <a href="#id13694">
              N number of nodes were provided to compute upgrade using
              crowbarctl upgrade nodes node_1,node_2,...,node_N,
              but less then N were actually upgraded.
            </a></dt><dt>7. <a href="#id13701">
              Node has failed at the initial chef client run stage.
            </a></dt><dt>8. <a href="#id13707">
              I need to change OpenStack configuration during the upgrade but I cannot access Crowbar.
            </a></dt><dt>9. <a href="#id13715">
              Failure occurred when evacuating routers.
            </a></dt><dt>10. <a href="#id13728">
              Some non-controller nodes were upgraded after performing crowbarctl upgrade nodes
                controllers.
            </a></dt></dl><table border="0" style="width: 100%;"><colgroup><col align="left" width="1%" /><col /></colgroup><tbody><tr class="question"><td align="left" valign="top"><a id="id13621"></a><a id="id13622"></a><p><strong>1.</strong></p></td><td align="left" valign="top"><p>
              Upgrade of the admin server has failed.
            </p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
              Check for empty, broken, and not signed repositories in the Administration Server
              upgrade log file <code class="filename">/var/log/crowbar/admin-server-upgrade.log</code>. Fix the
              repository setup. Upgrade then remaining packages manually to SUSE Linux Enterprise Server 12 SP4
              and SUSE <span class="productname">OpenStack</span> Cloud 9 using the command <span class="command"><strong>zypper dup</strong></span>. Reboot the Administration Server.
            </p></td></tr><tr class="question"><td align="left" valign="top"><a id="id13629"></a><a id="id13630"></a><p><strong>2.</strong></p></td><td align="left" valign="top"><p>
              An upgrade step repeatedly fails due to timeout.
            </p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
              Timeouts for most upgrade operations can be adjusted in the
              <code class="filename">/etc/crowbar/upgrade_timeouts.yml</code> file. If the
              file doesn't exist, use the following template, and modify it to your needs:
            </p><pre class="screen">
              :prepare_repositories: 120
              :pre_upgrade: 300
              :upgrade_os: 1500
              :post_upgrade: 600
              :shutdown_services: 600
              :shutdown_remaining_services: 600
              :evacuate_host: 300
              :chef_upgraded: 1200
              :router_migration: 600
              :lbaas_evacuation: 600
              :set_network_agents_state: 300
              :delete_pacemaker_resources: 600
              :delete_cinder_services: 300
              :delete_nova_services: 300
              :wait_until_compute_started: 60
              :reload_nova_services: 120
              :online_migrations: 1800
            </pre><p>
              The following entries may require higher values (all values are
              specified in seconds):
            </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
                  <code class="literal">upgrade_os</code> Time allowed for upgrading all packages of one node.
                </p></li><li class="listitem"><p>
                  <code class="literal">chef_upgraded</code> Time allowed for initial
                  <code class="literal">crowbar_join</code> and <code class="literal">chef-client</code>
                  run on a node that has been upgraded and rebooted.
                </p></li><li class="listitem"><p>
                  <code class="literal">evacuate_host</code> Time allowed for live migrate all VMs from a host.
                </p></li></ul></div></td></tr><tr class="question"><td align="left" valign="top"><a id="live-migration-failed"></a><a id="id13650"></a><p><strong>3.</strong></p></td><td align="left" valign="top"><p>
              Node upgrade has failed during live migration.
            </p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
              The problem may occur when it is not possible to live migrate certain
              VMs anywhere. It may be necessary to shut down or suspend other VMs to
              make room for migration. Note that the Bash shell script that starts
              the live migration for the Compute Node is executed from the
              Control Node. An error message generated by the <span class="command"><strong>crowbarctl
                upgrade status</strong></span> command contains the exact names of both
              nodes. Check the <code class="filename">/var/log/crowbar/node-upgrade.log</code>
              file on the Control Node for the information that can help you with
              troubleshooting. You might also need to check <span class="productname">OpenStack</span> logs in
              <code class="filename">/var/log/nova</code> on the Compute Node as well as on the
              Control Nodes.
            </p><p>
              It is possible that live-migration of a certain VM takes too long. This
              can happen if instances are very large or network connection between
              compute hosts is slow or overloaded. If this case, try to raise the
              global timeout in
              <code class="filename">/etc/crowbar/upgrade_timeouts.yml</code>.
            </p><p>
              We recommend to perform the live migration manually first. After it is
              completed successfully, call the <span class="command"><strong>crowbarctl upgrade</strong></span>
              command again.
            </p><p>
              The following commands can be helpful for analyzing issues with live migrations:
            </p><pre class="screen">
              nova server-migration-list
              nova server-migration-show
              nova instance-action-list
              nova instance-action
            </pre><p>
              Note that these commands require <span class="productname">OpenStack</span> administrator privileges.
            </p><p>
              The following log files may contain useful information:
            </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
                  <code class="filename">/var/log/nova/nova-compute</code> on the Compute Nodes
                  that the migration is performed from and to.
                </p></li><li class="listitem"><p>
                  <code class="filename">/var/log/nova/*.log</code> (especially log files for the
                  conductor, scheduler and placement services) on the Control Nodes.
                </p></li></ul></div><p>
              It can happen that active instances and instances with heavy
              loads cannot be live migrated in a reasonable time. In that case, you
              can abort a running live-migration operation using the <span class="command"><strong>nova
                live-migration-abort <em class="replaceable"><code>MIGRATION-ID</code></em></strong></span>
              command. You can then perform the upgrade of the specific node at a
              later time.
            </p><p>
              Alternatively, it is possible to force the completion of
              the live migration by using the <span class="command"><strong>nova
                live-migration-force-complete
                <em class="replaceable"><code>MIGRATION-ID</code></em></strong></span> command. However,
              this might pause the instances for a prolonged period of time and have
              a negative impact on the workload running inside the instance.
            </p></td></tr><tr class="question"><td align="left" valign="top"><a id="id13680"></a><a id="id13681"></a><p><strong>4.</strong></p></td><td align="left" valign="top"><p>
              Node has failed during OS upgrade.
            </p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
              Possible reasons include an incorrect repository setup or package
              conflicts. Check the <code class="filename">/var/log/crowbar/node-upgrade.log</code> log file on the
              affected node. Check the repositories on node using the <span class="command"><strong>zypper
                lr</strong></span> command. Make sure the required repositories are
              available. To test the setup, install a package manually or run the
              <span class="command"><strong>zypper dup</strong></span> command (this command is executed by the
              upgrade script). Fix the repository setup and run the failed upgrade
              step again. If custom package versions or version locks are in place,
              make sure that they don't interfere with the <span class="command"><strong>zypper dup</strong></span> command.
            </p></td></tr><tr class="question"><td align="left" valign="top"><a id="id13689"></a><a id="id13690"></a><p><strong>5.</strong></p></td><td align="left" valign="top"><p>
              Node does not come up after reboot.
            </p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
              In some cases, a node can take too long to reboot causing a timeout. We
              recommend to check the node manually, make sure it is online, and repeat the step.
            </p></td></tr><tr class="question"><td align="left" valign="top"><a id="id13694"></a><a id="id13695"></a><p><strong>6.</strong></p></td><td align="left" valign="top"><p>
              N number of nodes were provided to compute upgrade using
              <span class="command"><strong>crowbarctl upgrade nodes node_1,node_2,...,node_N</strong></span>,
              but less then N were actually upgraded.
            </p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
              If the live migration cannot be performed for certain nodes due to a timeout,
              Crowbar upgrades only the nodes that it was able to
              live-evacuate in the specified time. Because some nodes have been upgraded, it is possible that
              more resources will be available for live-migration when you try to run this
              step again. See also <a class="xref" href="#live-migration-failed" title="3.">Q: 3</a>.
            </p></td></tr><tr class="question"><td align="left" valign="top"><a id="id13701"></a><a id="id13702"></a><p><strong>7.</strong></p></td><td align="left" valign="top"><p>
              Node has failed at the initial chef client run stage.
            </p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
              An unsupported entry in the configuration file may prevent a service
              from starting. This causes the node to fail at the initial
              chef client run stage. Checking the
              <code class="filename">/var/log/crowbar/crowbar_join/chef.*</code> log files on
              the node is a good starting point.
            </p></td></tr><tr class="question"><td align="left" valign="top"><a id="id13707"></a><a id="id13708"></a><p><strong>8.</strong></p></td><td align="left" valign="top"><p>
              I need to change <span class="productname">OpenStack</span> configuration during the upgrade but I cannot access Crowbar.
            </p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
              Crowbar Web interface is accessible only when an upgrade is completed or
              when it is postponed. Postponing the upgrade can be done only after
              upgrading all Control Nodes using the <span class="command"><strong>crowbarctl upgrade nodes
                postpone</strong></span> command. You can then access Crowbar and
              save your modifications. Before you can continue with the upgrade of
              rest of the nodes, resume the upgrade using the <span class="command"><strong>crowbarctl
                upgrade nodes resume</strong></span> command.
            </p></td></tr><tr class="question"><td align="left" valign="top"><a id="id13715"></a><a id="id13716"></a><p><strong>9.</strong></p></td><td align="left" valign="top"><p>
              Failure occurred when evacuating routers.
            </p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
              Check the <code class="filename">/var/log/crowbar/node-upgrade.log</code> file on
              the node that performs the router evacuation (it should be mentioned in
              the error message). The ID of the router that failed to migrate (or the
              affected network port) is logged to
              <code class="filename">/var/log/crowbar/node-upgrade.log</code>. Use the
              <span class="productname">OpenStack</span> CLI tools to check the state of the affected router and
              its ports. Fix manually, if necessary. This can be done by bringing the
              router or port up and down again. The following
              commands can be useful for solving the issue:
            </p><pre class="screen">
              openstack router show <em class="replaceable"><code>ID</code></em>
              openstack port list --router <em class="replaceable"><code>ROUTER-ID</code></em>
              openstack port show <em class="replaceable"><code>PORT-ID</code></em>
              openstack port set
            </pre><p>
              Resume the upgrade by running the failed upgrade step
              again to continue with the router migration.
            </p></td></tr><tr class="question"><td align="left" valign="top"><a id="id13728"></a><a id="id13729"></a><p><strong>10.</strong></p></td><td align="left" valign="top"><p>
              Some non-controller nodes were upgraded after performing <span class="command"><strong>crowbarctl upgrade nodes
                controllers</strong></span>.
            </p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
              In the current upgrade implementation, <span class="productname">OpenStack</span> nodes are divided
              into Compute Nodes and other nodes. The <span class="command"><strong>crowbarctl upgrade nodes
                controllers</strong></span> command starts the upgrade of all the nodes that
              do not host compute services. This includes the controllers.
            </p></td></tr></tbody></table></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="sec-recover-comp-node-failure"></a>Recovering from Compute Node Failure</h2></div></div></div><p>
      The following procedure assumes that there is at least one Compute Node
      already running. Otherwise, see
      <a class="xref" href="#sec-bootstrap-compute-plane" title="Bootstrapping the Compute Plane">the section called “Bootstrapping the Compute Plane”</a>.
    </p><div class="procedure"><a id="pro-recover-compute-node-failure"></a><p class="title"><strong>Procedure 1.1. Procedure for Recovering from Compute Node Failure</strong></p><ol class="procedure" type="1"><li class="step"><a id="st-compnode-failed-reason"></a><p>
          If the Compute Node failed, it should have been fenced. Verify that this is
          the case. Otherwise, check <code class="filename">/var/log/pacemaker.log</code> on
          the Designated Coordinator to determine why the Compute Node was not fenced.
          The most likely reason is a problem with STONITH devices.
        </p></li><li class="step"><p>
          Determine the cause of the Compute Node's failure.
        </p></li><li class="step"><p>
          Rectify the root cause.
        </p></li><li class="step"><p>
          Boot the Compute Node again.
        </p></li><li class="step"><p>
          Check whether the <code class="systemitem">crowbar_join</code> script ran
          successfully on the Compute Node. If this is not the case, check the log
          files to find out the reason. Refer to
          <a class="xref" href="#sec-deploy-logs-crownodes" title="On All Other Crowbar Nodes">the section called “On All Other Crowbar Nodes”</a> to find the exact
          location of the log file.
        </p></li><li class="step"><p>
          If the <code class="systemitem">chef-client</code> agent triggered by
          <code class="systemitem">crowbar_join</code> succeeded, confirm that the
          <code class="systemitem">pacemaker_remote</code> service is up and running.
        </p></li><li class="step"><p>
          Check whether the remote node is registered and considered healthy by the
          core cluster. If this is not the case check
          <code class="filename">/var/log/pacemaker.log</code> on the Designated Coordinator
          to determine the cause. There should be a remote primitive running on the
          core cluster (active/passive). This primitive is responsible for
          establishing a TCP connection to the
          <code class="systemitem">pacemaker_remote</code> service on port 3121 of the
          Compute Node. Ensure that nothing is preventing this particular TCP
          connection from being established (for example, problems with NICs,
          switches, firewalls etc.). One way to do this is to run the following
          commands:
        </p><pre class="screen"><code class="prompt">tux &gt; </code>lsof -i tcp:3121
          <code class="prompt">tux &gt; </code>tcpdump tcp port 3121
        </pre></li><li class="step"><p>
          If Pacemaker can communicate with the remote node, it should start the
          <code class="systemitem">nova-compute</code> service on it as part of the cloned
          group <code class="literal">cl-g-nova-compute</code> using the NovaCompute OCF
          resource agent. This cloned group will block startup of
          <code class="systemitem">nova-evacuate</code> until at least one clone is
          started.
        </p><p>
          A necessary, related but different procedure is described in
          <a class="xref" href="#sec-bootstrap-compute-plane" title="Bootstrapping the Compute Plane">the section called “Bootstrapping the Compute Plane”</a>.
        </p></li><li class="step"><p>
          It may happen that <code class="systemitem">novaCompute</code> has been launched
          correctly on the Compute Node by <code class="systemitem">lrmd</code>, but the
          <code class="systemitem">openstack-nova-compute</code> service is still not
          running. This usually happens when <code class="systemitem">nova-evacuate</code>
          did not run correctly.
        </p><p>
          If <code class="systemitem">nova-evacuate</code> is not
          running on one of the core cluster nodes, make sure that the service is
          marked as started (<code class="literal">target-role="Started"</code>). If this is
          the case, then your cloud does not have any Compute Nodes already running as
          assumed by this procedure.
        </p><p>
          If <code class="systemitem">nova-evacuate</code> is started but it is
          failing, check the Pacemaker logs to determine the cause.
        </p><p>
          If <code class="systemitem">nova-evacuate</code> is started and
          functioning correctly, it should call nova's
          <code class="literal">evacuate</code> API to release resources used by the
          Compute Node and resurrect elsewhere any VMs that died when it failed.
        </p></li><li class="step"><p>
          If <code class="systemitem">openstack-nova-compute</code> is running, but VMs are
          not booted on the node, check that the service is not disabled or forced
          down using the <span class="command"><strong>openstack compute service list</strong></span>
          command. In case the service is disabled, run the <span class="command"><strong>openstack
            compute service set –enable
            <em class="replaceable"><code>SERVICE_ID</code></em></strong></span> command. If the service is
          forced down, run the following commands:
        </p><pre class="screen"><code class="prompt">tux &gt; </code>fence_nova_param () {
          key="$1"
          cibadmin -Q -A "//primitive[@id="fence-nova"]//nvpair[@name='$key']" | \
          sed -n '/.*value="/{s///;s/".*//;p}'
          }
          <code class="prompt">tux &gt; </code>fence_compute \
          --auth-url=`fence_nova_param auth-url` \
          --endpoint-type=`fence_nova_param endpoint-type` \
          --tenant-name=`fence_nova_param tenant-name` \
          --domain=`fence_nova_param domain` \
          --username=`fence_nova_param login` \
          --password=`fence_nova_param passwd` \
          -n <em class="replaceable"><code>COMPUTE_HOSTNAME</code></em> \
          --action=on
        </pre></li></ol></div><p>
      The above steps should be performed automatically after the node is
      booted. If that does not happen, try the following debugging techniques.
    </p><p>
      Check the <code class="literal">evacuate</code> attribute for the Compute Node in the
      Pacemaker cluster's <code class="systemitem">attrd</code> service using the
      command:
    </p><pre class="screen"><code class="prompt">tux &gt; </code>attrd_updater -p -n evacuate -N <em class="replaceable"><code>NODE</code></em></pre><p>
      Possible results are the following:
    </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
          The attribute is not set. Refer to
          <a class="xref" href="#st-compnode-failed-reason" title="Step 1">Step 1</a> in
          <a class="xref" href="#pro-recover-compute-node-failure" title="Procedure 1.1. Procedure for Recovering from Compute Node Failure">Procedure 1.1, “Procedure for Recovering from Compute Node Failure”</a>.
        </p></li><li class="listitem"><p>
          The attribute is set to <code class="literal">yes</code>. This means that the
          Compute Node was fenced, but <code class="systemitem">nova-evacuate</code> never
          initiated the recovery procedure by calling nova's evacuate API.
        </p></li><li class="listitem"><p>
          The attribute contains a time stamp, in which case the recovery procedure
          was initiated at the time indicated by the time stamp, but has not
          completed yet.
        </p></li><li class="listitem"><p>
          If the attribute is set to <code class="literal">no</code>, the recovery procedure
          recovered successfully and the cloud is ready for the Compute Node to
          rejoin.
        </p></li></ul></div><p>
      If the attribute is stuck with the wrong value, it can be set to
      <code class="literal">no</code> using the command:
    </p><pre class="screen"><code class="prompt">tux &gt; </code>attrd_updater -n evacuate -U no -N <em class="replaceable"><code>NODE</code></em></pre><p>
      After standard fencing has been performed, fence agent
      <code class="systemitem">fence_compute</code> should activate the secondary
      fencing device (<code class="literal">fence-nova</code>). It does this by setting
      the attribute to <code class="literal">yes</code> to mark the node as needing
      recovery. The agent also calls nova's
      <code class="systemitem">force_down</code> API to notify it that the host is down.
      You should be able to see this in
      <code class="filename">/var/log/nova/fence_compute.log</code> on the node in the core
      cluster that was running the <code class="systemitem">fence-nova</code> agent at
      the time of fencing. During the recovery, <code class="literal">fence_compute</code>
      tells nova that the host is up and running again.
    </p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="sec-bootstrap-compute-plane"></a>Bootstrapping the Compute Plane</h2></div></div></div><p>
      If the whole compute plane is down, it is not always obvious how to boot it
      up, because it can be subject to deadlock if evacuate attributes are set on
      every Compute Node. In this case, manual intervention is
      required. Specifically, the operator must manually choose one or more
      Compute Nodes to bootstrap the compute plane, and then run the
      <span class="command"><strong>attrd_updater -n evacuate -U no -N <em class="replaceable"><code>NODE</code></em></strong></span>
      command for each
      of those Compute Nodes to indicate that they do not require the resurrection
      process and can have their <code class="literal">nova-compute</code> start up straight
      away. Once these Compute Nodes are up, this breaks the deadlock allowing
      <code class="literal">nova-evacuate</code> to start. This way, any other nodes that
      require resurrection can be processed automatically. If no resurrection is
      desired anywhere in the cloud, then the attributes should be set to
      <code class="literal">no</code> for all nodes.
    </p><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Important</h3><p>
        If Compute Nodes are started too long after the
        <code class="literal">remote-*</code> resources are started on the control plane,
        they are liable to fencing. This should be avoided.
      </p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="sec-bootstrap-galera-cluster-with-missing-node"></a>Bootstrapping the MariaDB Galera Cluster with Pacemaker when a node is missing</h2></div></div></div><p>
      Pacemaker does not promote a node to master until it received from all
      nodes the latest <a class="glossterm" href="#gloss-galera-sequence-number"><em class="glossterm"><a class="glossterm" href="#gloss-galera-sequence-number" title="Sequence number (seqno)">Sequence number (seqno)</a></em></a>.
      That is a problem when one node of the MariaDB Galera Cluster is down (eg. due
      to hardware or network problems) because the <a class="glossterm" href="#gloss-galera-sequence-number"><em class="glossterm"><a class="glossterm" href="#gloss-galera-sequence-number" title="Sequence number (seqno)">Sequence number</a></em></a>
      can not be received from the unavailable node.
      To recover a MariaDB Galera Cluster manual steps are needed to
      select a bootstrap node for MariaDB Galera Cluster and to promote that node
      with Pacemaker.
    </p><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Important</h3><p>
        Selecting the correct bootstrap node (depending on the highest
        <a class="glossterm" href="#gloss-galera-sequence-number"><em class="glossterm"><a class="glossterm" href="#gloss-galera-sequence-number" title="Sequence number (seqno)">Sequence number (seqno)</a></em></a>)
        is important. If the wrong node is selected data loss is possible.
      </p></div><div class="procedure"><ol class="procedure" type="1"><li class="step"><p>
          To find out which node has the latest Sequence number, call the following
          command on all MariaDB Galera Cluster nodes and select the node with the
          highest Sequence number.
        </p><pre class="screen">mysqld_safe --wsrep-recover
          tail -5 /var/log/mysql/mysqld.log
          ...
          [Note] WSREP: Recovered position: 7a477edc-757d-11e9-a01a-d218e7381711:2490</pre><p>
          At the end of <code class="filename">/var/log/mysql/mysqld.log</code> the Sequence
          number is written (in this example, the sequence number is 2490).
          After all Sequence numbers are collected from all nodes, the node with
          the highest Sequence number is selected for bootstrap node.
          In this example, the node with the highest Sequence number is called
          <code class="literal">node1</code>.
        </p></li><li class="step"><p>
          Temporarily mark the galera Pacemaker resource as unmanaged:
        </p><pre class="screen">
          crm resource unmanage galera
        </pre></li><li class="step"><p>
          Mark the node as bootstrap node (call the following commands from the
          bootstrap node which is <code class="literal">node1</code> in this example):
        </p><pre class="screen">
          crm_attribute -N node1 -l reboot --name galera-bootstrap -v true
          crm_attribute -N node1 -l reboot --name master-galera -v 100
        </pre></li><li class="step"><p>
          Promote the bootstrap node:
        </p><pre class="screen">
          crm_resource --force-promote -r galera -V
        </pre></li><li class="step"><p>
          Redetect the current state of the galera resource:
        </p><pre class="screen">
          crm resource cleanup galera
        </pre></li><li class="step"><p>
          Return the control to Pacemaker:
        </p><pre class="screen">
          crm resource manage galera
          crm resource start galera
        </pre></li></ol></div><p>
      The MariaDB Galera Cluster is now running and Pacemaker is handling the cluster.
    </p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="id13876"></a>Updating MariaDB with Galera</h2></div></div></div><p>
      Updating MariaDB with Galera must be done manually. Crowbar does not
      install updates automatically. Updates can be done with Pacemaker or with
      the CLI. In particular, manual updating applies to upgrades to MariaDB
      10.2.17 or higher from MariaDB 10.2.16 or earlier. See <a class="link" href="https://mariadb.com/kb/en/library/mariadb-10222-release-notes/" target="_top">MariaDB
        10.2.22 Release Notes - Notable Changes</a>.
    </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
        In order to run the following update steps, the database cluster needs to
        be up and healthy.
      </p></div><p>
      Using the Pacemaker GUI, update MariaDB with the following procedure:
    </p><div class="procedure"><ol class="procedure" type="1"><li class="step"><p>
          Put the cluster into maintenance mode. Detailed information about the
          Pacemaker GUI and its operation is available in the <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#cha-conf-hawk2" target="_top">https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#cha-conf-hawk2</a>.
        </p></li><li class="step"><p>
          Perform a rolling upgrade to MariaDB following the instructions at <a class="link" href="https://mariadb.com/kb/en/library/upgrading-between-minor-versions-with-galera-cluster/" target="_top">Upgrading
            Between Minor Versions with Galera Cluster</a>.
        </p><p>
          The process involves the following steps:
        </p><ol type="a" class="substeps"><li class="step"><p>
              Stop MariaDB
            </p></li><li class="step"><p>
              Uninstall the old versions of MariaDB and the Galera wsrep provider
            </p></li><li class="step"><p>
              Install the new versions of MariaDB and the Galera wsrep provider
            </p></li><li class="step"><p>
              Change configuration options if necessary
            </p></li><li class="step"><p>
              Start MariaDB
            </p></li><li class="step"><p>
              Run <span class="command"><strong>mysql_upgrade</strong></span> with the
              <code class="literal">--skip-write-binlog</code> option
            </p></li></ol></li><li class="step"><p>
          Each node must upgraded individually so that the cluster is always
          operational.
        </p></li><li class="step"><p>
          Using the Pacemaker GUI, take the cluster out of maintenance mode.
        </p></li></ol></div><p>
      When updating with the CLI, the database cluster must be up and
      healthy. Update MariaDB with the following procedure:
    </p><div class="procedure"><ol class="procedure" type="1"><li class="step"><p>
          Mark Galera as unmanaged:
        </p><pre class="screen">crm resource unmanage galera</pre><p>
          Or put the whole cluster into maintenance mode:
        </p><pre class="screen">crm configure property maintenance-mode=true</pre></li><li class="step"><p>
          Pick a node other than the one currently targeted by the load balancer and
          stop MariaDB on that node:
        </p><pre class="screen">crm_resource --wait --force-demote -r galera -V</pre></li><li class="step"><p>
          Perform updates with the following steps:
        </p><ol type="a" class="substeps"><li class="step"><p>
              Uninstall the old versions of MariaDB and the Galera wsrep provider.
            </p></li><li class="step"><p>
              Install the new versions of MariaDB and the Galera wsrep
              provider. Select the appropriate instructions at <a class="link" href="https://mariadb.com/kb/en/library/installing-mariadb-with-zypper/" target="_top">Installing
                MariaDB with zypper</a>.
            </p></li><li class="step"><p>
              Change configuration options if necessary.
            </p></li></ol></li><li class="step"><p>
          Start MariaDB on the node.
        </p><pre class="screen">crm_resource --wait --force-promote -r galera -V</pre></li><li class="step"><p>
          Run <span class="command"><strong>mysql_upgrade</strong></span> with the
          <code class="literal">--skip-write-binlog</code> option.
        </p></li><li class="step"><p>
          On the other nodes, repeat the process detailed above: stop MariaDB,
          perform updates, start MariaDB, run <span class="command"><strong>mysql_upgrade</strong></span>.
        </p></li><li class="step"><p>
          Mark Galera as managed:
        </p><pre class="screen">crm resource manage galera</pre><p>
          Or take the cluster out of maintenance mode.
        </p></li></ol></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="OctaviaMaintenance"></a>Load Balancer: Octavia Administration</h2></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="octavia-admin-delete"></a>Removing load balancers</h3></div></div></div><p>
      The following procedures demonstrate how to delete a load
      balancer that is in the <code class="literal">ERROR</code>,
      <code class="literal">PENDING_CREATE</code>, or
      <code class="literal">PENDING_DELETE</code> state.
    </p><div class="procedure"><a id="id13952"></a><p class="title"><strong>Procedure 1.2. 
        Manually deleting load balancers created with neutron lbaasv2
        (in an upgrade/migration scenario)
      </strong></p><ol class="procedure" type="1"><li class="step"><p>
          Query the Neutron service for the loadbalancer ID:
        </p><pre class="screen">
<code class="prompt">tux &gt; </code>neutron lbaas-loadbalancer-list
neutron CLI is deprecated and will be removed in the future. Use openstack CLI instead.
+--------------------------------------+---------+----------------------------------+--------------+---------------------+----------+
| id                                   | name    | tenant_id                        | vip_address  | provisioning_status | provider |
+--------------------------------------+---------+----------------------------------+--------------+---------------------+----------+
| 7be4e4ab-e9c6-4a57-b767-da9af5ba7405 | test-lb | d62a1510b0f54b5693566fb8afeb5e33 | 192.168.1.10 | ERROR               | haproxy  |
+--------------------------------------+---------+----------------------------------+--------------+---------------------+----------+
        </pre></li><li class="step"><p>
          Connect to the neutron database:
        </p><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Important</h3><p>
            The default database name depends on the life cycle
            manager. Ardana uses <code class="literal">ovs_neutron</code> while
            Crowbar uses <code class="literal">neutron</code>.
          </p></div><p>Ardana:</p><pre class="screen">
mysql&gt; use ovs_neutron
        </pre><p>Crowbar:</p><pre class="screen">
mysql&gt; use neutron
        </pre></li><li class="step"><p>
          Get the pools and healthmonitors associated with the loadbalancer:
        </p><pre class="screen">
mysql&gt; select id, healthmonitor_id, loadbalancer_id from lbaas_pools where loadbalancer_id = '7be4e4ab-e9c6-4a57-b767-da9af5ba7405';
+--------------------------------------+--------------------------------------+--------------------------------------+
| id                                   | healthmonitor_id                     | loadbalancer_id                      |
+--------------------------------------+--------------------------------------+--------------------------------------+
| 26c0384b-fc76-4943-83e5-9de40dd1c78c | 323a3c4b-8083-41e1-b1d9-04e1fef1a331 | 7be4e4ab-e9c6-4a57-b767-da9af5ba7405 |
+--------------------------------------+--------------------------------------+--------------------------------------+
        </pre></li><li class="step"><p>
          Get the members associated with the pool:
        </p><pre class="screen">
mysql&gt; select id, pool_id from lbaas_members where pool_id = '26c0384b-fc76-4943-83e5-9de40dd1c78c';
+--------------------------------------+--------------------------------------+
| id                                   | pool_id                              |
+--------------------------------------+--------------------------------------+
| 6730f6c1-634c-4371-9df5-1a880662acc9 | 26c0384b-fc76-4943-83e5-9de40dd1c78c |
| 06f0cfc9-379a-4e3d-ab31-cdba1580afc2 | 26c0384b-fc76-4943-83e5-9de40dd1c78c |
+--------------------------------------+--------------------------------------+
        </pre></li><li class="step"><p>
          Delete the pool members:
        </p><pre class="screen">
mysql&gt; delete from lbaas_members where id = '6730f6c1-634c-4371-9df5-1a880662acc9';
mysql&gt; delete from lbaas_members where id = '06f0cfc9-379a-4e3d-ab31-cdba1580afc2';
        </pre></li><li class="step"><p>
          Find and delete the listener associated with the loadbalancer:
        </p><pre class="screen">
mysql&gt; select id, loadbalancer_id, default_pool_id from lbaas_listeners where loadbalancer_id = '7be4e4ab-e9c6-4a57-b767-da9af5ba7405';
+--------------------------------------+--------------------------------------+--------------------------------------+
| id                                   | loadbalancer_id                      | default_pool_id                      |
+--------------------------------------+--------------------------------------+--------------------------------------+
| 3283f589-8464-43b3-96e0-399377642e0a | 7be4e4ab-e9c6-4a57-b767-da9af5ba7405 | 26c0384b-fc76-4943-83e5-9de40dd1c78c |
+--------------------------------------+--------------------------------------+--------------------------------------+
mysql&gt; delete from lbaas_listeners where id = '3283f589-8464-43b3-96e0-399377642e0a';
        </pre></li><li class="step"><p>
          Delete the pool associated with the loadbalancer:
        </p><pre class="screen">
mysql&gt; delete from lbaas_pools where id = '26c0384b-fc76-4943-83e5-9de40dd1c78c';
        </pre></li><li class="step"><p>
          Delete the healthmonitor associated with the pool:
        </p><pre class="screen">
mysql&gt; delete from lbaas_healthmonitors where id = '323a3c4b-8083-41e1-b1d9-04e1fef1a331';
        </pre></li><li class="step"><p>
          Delete the loadbalancer:
        </p><pre class="screen">
mysql&gt; delete from lbaas_loadbalancer_statistics where loadbalancer_id = '7be4e4ab-e9c6-4a57-b767-da9af5ba7405';
mysql&gt; delete from lbaas_loadbalancers where id = '7be4e4ab-e9c6-4a57-b767-da9af5ba7405';
        </pre></li></ol></div><div class="procedure"><a id="id13989"></a><p class="title"><strong>Procedure 1.3. Manually Deleting Load Balancers Created With Octavia</strong></p><ol class="procedure" type="1"><li class="step"><p>
          Query the Octavia service for the loadbalancer ID:
        </p><pre class="screen">
<code class="prompt">tux &gt; </code>openstack loadbalancer list --column id --column name --column provisioning_status
+--------------------------------------+---------+---------------------+
| id                                   | name    | provisioning_status |
+--------------------------------------+---------+---------------------+
| d8ac085d-e077-4af2-b47a-bdec0c162928 | test-lb | ERROR               |
+--------------------------------------+---------+---------------------+
        </pre></li><li class="step"><p>
          Query the Octavia service for the amphora IDs (in this
          example we use <code class="literal">ACTIVE/STANDBY</code> topology with 1 spare Amphora):
        </p><pre class="screen">
<code class="prompt">tux &gt; </code>openstack loadbalancer amphora list
+--------------------------------------+--------------------------------------+-----------+--------+---------------+-------------+
| id                                   | loadbalancer_id                      | status    | role   | lb_network_ip | ha_ip       |
+--------------------------------------+--------------------------------------+-----------+--------+---------------+-------------+
| 6dc66d41-e4b6-4c33-945d-563f8b26e675 | d8ac085d-e077-4af2-b47a-bdec0c162928 | ALLOCATED | BACKUP | 172.30.1.7    | 192.168.1.8 |
| 1b195602-3b14-4352-b355-5c4a70e200cf | d8ac085d-e077-4af2-b47a-bdec0c162928 | ALLOCATED | MASTER | 172.30.1.6    | 192.168.1.8 |
| b2ee14df-8ac6-4bb0-a8d3-3f378dbc2509 | None                                 | READY     | None   | 172.30.1.20   | None        |
+--------------------------------------+--------------------------------------+-----------+--------+---------------+-------------+
        </pre></li><li class="step"><p>
          Query the Octavia service for the loadbalancer pools:
        </p><pre class="screen">
<code class="prompt">tux &gt; </code>openstack loadbalancer pool list
+--------------------------------------+-----------+----------------------------------+---------------------+----------+--------------+----------------+
| id                                   | name      | project_id                       | provisioning_status | protocol | lb_algorithm | admin_state_up |
+--------------------------------------+-----------+----------------------------------+---------------------+----------+--------------+----------------+
| 39c4c791-6e66-4dd5-9b80-14ea11152bb5 | test-pool | 86fba765e67f430b83437f2f25225b65 | ACTIVE              | TCP      | ROUND_ROBIN  | True           |
+--------------------------------------+-----------+----------------------------------+---------------------+----------+--------------+----------------+
        </pre></li><li class="step"><p>
          Connect to the octavia database:
        </p><pre class="screen">
mysql&gt; use octavia
        </pre></li><li class="step"><p>
          Delete any listeners, pools, health monitors, and members
          from the load balancer:
        </p><pre class="screen">
mysql&gt; delete from listener where load_balancer_id = 'd8ac085d-e077-4af2-b47a-bdec0c162928';
mysql&gt; delete from health_monitor where pool_id = '39c4c791-6e66-4dd5-9b80-14ea11152bb5';
mysql&gt; delete from member where pool_id = '39c4c791-6e66-4dd5-9b80-14ea11152bb5';
mysql&gt; delete from pool where load_balancer_id = 'd8ac085d-e077-4af2-b47a-bdec0c162928';
        </pre></li><li class="step"><p>
          Delete the amphora entries in the database:
        </p><pre class="screen">
mysql&gt; delete from amphora_health where amphora_id = '6dc66d41-e4b6-4c33-945d-563f8b26e675';
mysql&gt; update amphora set status = 'DELETED' where id = '6dc66d41-e4b6-4c33-945d-563f8b26e675';
mysql&gt; delete from amphora_health where amphora_id = '1b195602-3b14-4352-b355-5c4a70e200cf';
mysql&gt; update amphora set status = 'DELETED' where id = '1b195602-3b14-4352-b355-5c4a70e200cf';
        </pre></li><li class="step"><p>
          Delete the load balancer instance:
        </p><pre class="screen">
mysql&gt; update load_balancer set provisioning_status = 'DELETED' where id = 'd8ac085d-e077-4af2-b47a-bdec0c162928';
        </pre></li><li class="step"><p>
          The following script automates the above steps:
        </p><pre class="screen">
#!/bin/bash

if (( $# != 1 )); then
echo "Please specify a loadbalancer ID"
exit 1
fi

LB_ID=$1

set -u -e -x

readarray -t AMPHORAE &lt; &lt;(openstack loadbalancer amphora list \
--format value \
--column id \
--column loadbalancer_id \
| grep ${LB_ID} \
| cut -d ' ' -f 1)

readarray -t POOLS &lt; &lt;(openstack loadbalancer show ${LB_ID} \
--format value \
--column pools)

mysql octavia --execute "delete from listener where load_balancer_id = '${LB_ID}';"
for p in "${POOLS[@]}"; do
mysql octavia --execute "delete from health_monitor where pool_id = '${p}';"
mysql octavia --execute "delete from member where pool_id = '${p}';"
done
mysql octavia --execute "delete from pool where load_balancer_id = '${LB_ID}';"
for a in "${AMPHORAE[@]}"; do
mysql octavia --execute "delete from amphora_health where amphora_id = '${a}';"
mysql octavia --execute "update amphora set status = 'DELETED' where id = '${a}';"
done
mysql octavia --execute "update load_balancer set provisioning_status = 'DELETED' where id = '${LB_ID}';"
        </pre></li></ol></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="database-maintenance"></a>Periodic OpenStack Maintenance Tasks</h2></div></div></div><p>
    Heat-manage helps manage Heat specific database operations. The associated
    database should be periodically purged to save space. The following should
    be setup as a cron job on the servers where the heat service is running at
    <code class="literal">/etc/cron.weekly/local-cleanup-heat</code>
    with the following content:
  </p><pre class="screen">
  #!/bin/bash
  su heat -s /bin/bash -c "/usr/bin/heat-manage purge_deleted -g days 14" || :
  </pre><p>
     nova-manage db archive_deleted_rows command will move deleted rows
     from production tables to shadow tables. Including
     <code class="literal">--until-complete</code> will make the command run continuously
     until all deleted rows are archived. It is recommended to setup this task
     as <code class="literal">/etc/cron.weekly/local-cleanup-nova</code>
     on the servers where the nova service is running, with the
     following content:
  </p><pre class="screen">
  #!/bin/bash
  su nova -s /bin/bash -c "/usr/bin/nova-manage db archive_deleted_rows --until-complete" || :
  </pre></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="sec-depl-maintenance-fernet-tokens"></a>Rotating Fernet Tokens</h2></div></div></div><p>
     Fernet tokens should be rotated frequently for security purposes.
     It is recommended to setup this task as a cron job in
     <code class="literal">/etc/cron.weekly/openstack-keystone-fernet</code>
     on the keystone server designated as a master node in a highly
     available setup with the following content:
   </p><pre class="screen">
  #!/bin/bash
  su keystone -s /bin/bash -c "keystone-manage fernet_rotate"

  /usr/bin/keystone-fernet-keys-push.sh 192.168.81.168; /usr/bin/keystone-fernet-keys-push.sh 192.168.81.169;
   </pre><p>
     The IP addresses in the above example, i.e. 192.168.81.168 and
     192.168.81.169 are the IP addresses of the other two nodes of a
     three-node cluster. Be sure to use the correct IP addresses
     when configuring the cron job. Note that if the master node is offline
     and a new master is elected, the cron job will need to be removed from
     the previous master node and then re-created on the new master node.
     Do not run the fernet_rotate cron job on multiple nodes.
   </p><p>
     For a non-HA setup, the cron job should be configured at
    <code class="literal">/etc/cron.weekly/openstack-keystone-fernet</code>
     on the keystone server as follows:
   </p><pre class="screen">
  #!/bin/bash
  su keystone -s /bin/bash -c "keystone-manage fernet_rotate"
   </pre></div></div><div class="chapter"><div class="titlepage"><div><div><h1 class="title"><a id="gpu-passthrough"></a>Chapter 2. GPU passthrough</h1></div></div></div><div class="toc"><p><strong>Table of Contents</strong></p><dl class="toc"><dt><span class="section"><a href="#crow-pci-passthrough">Leveraging PCI Passthrough</a></span></dt><dd><dl><dt><span class="section"><a href="#preparing-comp-passthrough">Preparing nova and glance for passthrough</a></span></dt><dt><span class="section"><a href="#flavor-creation">Flavor Creation</a></span></dt><dt><span class="section"><a href="#additional-examples">Additional examples</a></span></dt></dl></dd></dl></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> GPU passthrough functionality provides the nova
  instance direct access to the GPU device for better performance.
  This section demonstrates the steps to pass through a NVIDIA GPU
  card supported by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span>.
 </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
   When using PCI Passthrough, resizing the VM to the same host with the same
   PCI card is not supported.
  </p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="crow-pci-passthrough"></a>Leveraging PCI Passthrough</h2></div></div></div><p>
   Two parts are necessary to leverage PCI passthrough on a SUSE <span class="productname">OpenStack</span> Cloud
   9 Compute Node: preparing the Compute Node, preparing nova and
   glance.
  </p><p>To leverage PCI passthrough on a SUSE <span class="productname">OpenStack</span> Cloud
   9 Compute Node, follow the below procedures in sequence:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     <a class="xref" href="#crow-pci-passthrough-prep-node" title="Procedure 2.1. Preparing the Compute Node">Procedure 2.1, “Preparing the Compute Node”</a>
    </p></li><li class="listitem"><p>
     <a class="xref" href="#preparing-comp-passthrough" title="Preparing nova and glance for passthrough">the section called “Preparing nova and glance for passthrough”</a>
    </p></li><li class="listitem"><p>
     <a class="xref" href="#flavor-creation" title="Flavor Creation">the section called “Flavor Creation”</a>
    </p></li></ol></div><div class="procedure"><a id="crow-pci-passthrough-prep-node"></a><p class="title"><strong>Procedure 2.1. Preparing the Compute Node</strong></p><ol class="procedure" type="1"><li class="step"><p>
       There should be no kernel drivers or binaries with direct access to the
       PCI device. If there are kernel modules, they should be blacklisted.
      </p><p>
       For example, it is common to have a <code class="literal">nouveau</code> driver
       from when the node was installed. This driver is a graphics driver for
       NVIDIA-based GPUs. It must be blacklisted as shown in this example.
      </p><pre class="screen"><code class="prompt">root # </code>echo 'blacklist nouveau' &gt;&gt; /etc/modprobe.d/nouveau-default.conf</pre><p>
       The file location and its contents are important, the name of
       the file is your choice. Other drivers can be blacklisted in
       the same manner, possibly including NVIDIA drivers.
      </p></li><li class="step"><p>
       On the host, <code class="literal">iommu_groups</code> is necessary and may
       already be enabled. To check if IOMMU is enabled:
      </p><pre class="screen"><code class="prompt">root # </code>virt-host-validate
.....
QEMU: Checking if IOMMU is enabled by kernel
: WARN (IOMMU appears to be disabled in kernel. Add intel_iommu=on to kernel cmdline arguments)
.....</pre><p>
       To modify the kernel cmdline as suggested in the warning, edit the file
       <code class="filename">/etc/default/grub</code> and append
       <code class="literal">intel_iommu=on</code> to the
       <code class="literal">GRUB_CMDLINE_LINUX_DEFAULT</code> variable. Then run
       <code class="literal">update-bootloader</code>.
      </p><p>
       A reboot is required for <code class="literal">iommu_groups</code> to be
       enabled.
      </p></li><li class="step"><p>
       After the reboot, check that IOMMU is enabled:
      </p><pre class="screen"><code class="prompt">root # </code>virt-host-validate
.....
QEMU: Checking if IOMMU is enabled by kernel
: PASS
.....</pre></li><li class="step"><p>
       Confirm IOMMU groups are available by finding the group associated with
       your PCI device (for example NVIDIA GPU):
      </p><pre class="screen"><code class="prompt">root # </code>lspci -nn | grep -i nvidia
84:00.0 3D controller [0302]: NVIDIA Corporation GV100GL [Tesla V100 PCIe 16GB] [10de:1db4] (rev a1)
</pre><p>
        In this example, <code class="literal">84:00.0</code> is the address of
        the PCI device. The vendorID is <code class="literal">10de</code>. The
        product ID is <code class="literal">1db4</code>.
      </p></li><li class="step"><p>
       Confirm that the devices are available for passthrough:
      </p><pre class="screen"><code class="prompt">root # </code>ls -ld /sys/kernel/iommu_groups/*/devices/*84:00.?/
drwxr-xr-x 3 root root 0 Nov 19 17:00 /sys/kernel/iommu_groups/56/devices/0000:84:00.0/</pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
       With PCI passthrough, only an entire IOMMU group can be passed.
       Parts of the group cannot be passed. In this example, the IOMMU
       group is <code class="literal">56</code>.
       </p></div></li></ol></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="preparing-comp-passthrough"></a>Preparing nova and glance for passthrough</h3></div></div></div><p>
      Information about configuring nova is available in the documentation at
      <a class="link" href="https://docs.openstack.org/nova/rocky/admin/pci-passthrough.html" target="_top">https://docs.openstack.org/nova/rocky/admin/pci-passthrough.html</a>.
      Services like <code class="literal">nova-compute</code>, <code class="literal">nova-scheduler</code>
      and <code class="literal">nova-api</code> must be configured.
     </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
         We suggest user to create new <code class="filename">nova.conf</code>
         file specific to PCI passthrough usage
         under<code class="literal">/etc/nova/nova.conf.d</code> instead of
         modifying the existing <code class="filename">nova.conf</code> to
         avoid any side effects caused from the
         <code class="literal">chef-client.service</code>. For example,
         <code class="filename">100-nova.conf</code> or
         <code class="filename">101-nova-placement.conf</code>. Note that this
         new file, for example,
         <code class="filename">102-nova-pcipassthru.conf</code> will not be
         recorded in any nova proposal and under manipulation by the
         <code class="literal">chef-client.service</code>. Ensure you have
         backups in case of any future incidents.
       </p></div><p>
       Here's an <span class="bold"><strong>example</strong></span> of the
       configuration of a single PCI device being passthrough to the
       nova instance:
     </p><div class="procedure"><a id="id14123"></a><p class="title"><strong>Procedure 2.2. Configure the Compute Host</strong></p><ol class="procedure" type="1"><li class="step"><p>Login to the compute host and change directory to <code class="filename">/etc/nova/nova.conf.d</code> and
    list all the files:
    </p><pre class="screen"><code class="prompt">root # </code>cd /etc/nova/nova.conf.d/
ls -al
</pre></li><li class="step"><p>
      Create a new nova configuration file under this folder and name
      the file appropriately. Note that configuration files are read
      and applied in lexicographic order. For example:
      <code class="filename">102-nova-pcipassthru.conf </code>.
    </p></li><li class="step"><p>
      Add the following configuration entries to the file and the
      values for these entries are specific to your compute node as
      obtained in Preparing the Compute Node step <a class="xref" href="#crow-pci-passthrough-prep-node" title="Procedure 2.1. Preparing the Compute Node">Procedure 2.1, “Preparing the Compute Node”</a>. The configuration
      entries specify the PCI device using whitelisting and the PCI
      alias for the device. Example:
    </p><pre class="screen"><code class="prompt">root # </code>cat /etc/nova/nova.conf.d/102-nova-pcipassthru.conf
[pci]
passthrough_whitelist = [{ "address": "0000:84:00.0" }]
alias = { "vendor_id":"10de", "product_id":"1db4", "device_type":"type-PCI", "name":"a1" }

</pre><p>
    The example above shows how to configure a PCI alias a1 to request a PCI device with a <code class="literal">vendor_id</code>
    of <code class="literal">10de</code> and a
    <code class="literal">product_id</code> of <code class="literal">1db4</code>.
    </p></li><li class="step"><p>Once the file is updated, restart nova compute on the compute node:</p><pre class="screen"><code class="prompt">root # </code>systemctl restart openstack-nova-compute
    </pre></li></ol></div><div class="procedure"><a id="id14147"></a><p class="title"><strong>Procedure 2.3. Configure the Controller nodes</strong></p><ol class="procedure" type="1"><li class="step"><p>
    Follow the following steps for all the controller nodes. Login to the controller host and
    change directory to <code class="filename">/etc/nova/nova.conf.d</code> and list all the files under that folder:
    </p><pre class="screen"><code class="prompt">root # </code>cd /etc/nova/nova.conf.d/
ls -al
</pre></li><li class="step"><p>Create a new nova configuration file under this folder and name the file appropriately in lexicographic
    increasing order. For example: <code class="filename">102-nova-pcipassthru.conf </code>.
    </p></li><li class="step"><p>
    Add the following configuration entries to the file and the configuration entries for controller nodes
    only specify PCI alias for the device from the compute host. For example:
    </p><pre class="screen"><code class="prompt">root # </code>cat /etc/nova/nova.conf.d/102-nova-pcipassthru.conf
[pci]
alias = { "vendor_id":"10de", "product_id":"1db4", "device_type":"type-PCI", "name":"a1" }
</pre><p>
    The example above shows how to configure a PCI alias a1 to request a PCI device with a <code class="literal">vendor_id</code>
    of <code class="literal">10de</code> and a
    <code class="literal">product_id</code> of <code class="literal">1db4</code>.
    </p></li><li class="step"><p>Once the file is updated, restart nova api on the controller node:</p><pre class="screen"><code class="prompt">root # </code>systemctl restart openstack-nova-api
</pre></li></ol></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="flavor-creation"></a>Flavor Creation</h3></div></div></div><p>
    Login into the controller node and create a new flavor or update an existing flavor with the property
    <code class="literal">"pci_passthrough:alias"</code>. For example:
    </p><pre class="screen"><code class="prompt">root # </code>source .openrc
openstack flavor create --ram 8192 --disk 100 --vcpu 8 gpuflavor
openstack flavor set gpuflavor --property "pci_passthrough:alias"="a1:1"
</pre><p>
    In the property, <code class="literal">"pci_passthrough:alias"="a1:1"</code>, <code class="literal">a1</code> before the <code class="literal">:</code>
    references the alias name as provided in the
    configuration entries while the number <code class="literal">1</code> after the <code class="literal">:</code> tells nova that a single GPU
    should be assigned.
    </p><p>
    <span class="bold"><strong>Boot an instance with the flavor created in previous step</strong></span>
    </p><p>
      Make sure the VM becomes <code class="literal">ACTIVE</code>. Login into the virtual instance and verify that GPU is seen
      from the guest
      by running the <span class="command"><strong>lspci</strong></span> command on the guest.
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="additional-examples"></a>Additional examples</h3></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Example 1: Multiple compute hosts</p><p>Compute host x:</p><pre class="screen">
[pci]
passthrough_whitelist = [{"address": "0000:84:00.0"}]
alias = {"vendor_id": "10de", "name": "a1", "device_type": "type-PCI", "product_id": "1db4"}
</pre><p>Compute host y:</p><pre class="screen">
[pci]
passthrough_whitelist = [{"address": "0000:85:00.0"}]
alias = {"vendor_id": "10de", "name": "a1", "device_type": "type-PCI", "product_id": "1db4"}
</pre><p>Controller nodes:</p><pre class="screen">
[pci]
alias = {"vendor_id": "10de", "name": "a1", "device_type": "type-PCI", "product_id": "1db4"}
</pre></li><li class="listitem"><p>Example 2: Multiple PCI devices on the same host </p><p>Compute host z:</p><pre class="screen">
[pci]
passthrough_whitelist = [{"vendor_id": "10de", "product_id": "1db4"}, {"vendor_id": "10de", "product_id":
"1db1"}]
alias = {"vendor_id": "10de", "name": "a2", "device_type": "type-PCI", "product_id": "1db1"}
alias = {"vendor_id": "10de", "name": "a1", "device_type": "type-PCI", "product_id": "1db4"}
</pre><p>Controller nodes:</p><pre class="screen">
[pci]
alias = {"vendor_id": "10de", "name": "a2", "device_type": "type-PCI", "product_id": "1db1"}
alias = {"vendor_id": "10de", "name": "a1", "device_type": "type-PCI", "product_id": "1db4"}
</pre><p>In order to pass both the devices to the instance, you can set the
         <code class="literal">"pci_passthrough:alias"="a1:1,a2:1"</code>
         </p><pre class="screen"><code class="prompt">root # </code>source .openrc
openstack flavor create --ram 8192 --disk 100 --vcpu 8 gpuflavor2
openstack flavor set gpuflavor2 --property "pci_passthrough:alias"="a1:1,a2:1"
</pre></li></ol></div></div></div></div><div class="chapter"><div class="titlepage"><div><div><h1 class="title"><a id="self-assign-certs"></a>Chapter 3. Generate SUSE <span class="productname">OpenStack</span> Cloud Self Signed Certificate</h1></div></div></div><div class="toc"><p><strong>Table of Contents</strong></p><dl class="toc"><dt><span class="section"><a href="#create-root-pair">Create the CA Root Pair</a></span></dt><dt><span class="section"><a href="#sign-server-client-cert">Sign server and client certificates</a></span></dt><dt><span class="section"><a href="#deploy-cert">Deploying the certificate</a></span></dt><dt><span class="section"><a href="#lets-encrypt-cert">Generate Public Certificate using Let’s Encrypt</a></span></dt></dl></div><p>
   The purpose of this document is to help set up SSL Support for several services
   in SUSE OpenStack Cloud. The scope of this document covers all public
   endpoints in your OpenStack cluster. In most cases you want to have a
   Secure CA or External CA where your certificates are signed. You will
   sign with either a public CA or self signed CA, and include x509
   extensions for Subject Alt Names since there might be a highly available
   control plane with alternate names.
 </p><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="create-root-pair"></a>Create the CA Root Pair</h2></div></div></div><p>This section demonstrates how to create the certificate on the
     crowbar or admin node of the SUSE OpenStack Cloud Cluster.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
         To avoid external access to your CA Root Pair, put it on an air-gapped system
         that is permanently isolated from the internet and unplug any cables from the
         ethernet port.
       </p></div><div class="procedure"><a id="id14217"></a><p class="title"><strong>Procedure 3.1. Prepare the directory structure</strong></p><ol class="procedure" type="1"><li class="step"><p>
           Create a directory for your CA Root pair:
         </p><pre class="screen">
           # ssh root@crowbar
           # mkdir -p ~/ssl/root/ca
         </pre></li><li class="step"><p>
           Create a directory structure and add <code class="filename">index.txt</code>
           and serial files to act as flat database of all signed certificates:
         </p><pre class="screen">
           # cd ~/ssl/root/ca
           # mkdir certs crl newcerts private csr
           # chmod 700 private
           # touch index.txt
           # echo 1000 &gt; serial
         </pre></li></ol></div><div class="procedure"><a id="id14226"></a><p class="title"><strong>Procedure 3.2. Prepare the configuration file</strong></p><p>
         This procedure takes you through the full set up. Note that
         when you setup the crowbar server, there is a structure already setup
         under <code class="filename">/etc/ssl</code>. This is where SUSE Linux typically
         contains the CA cert bundle created through YaST when the SMT server
         is set up. However, if you are using an external SMT server
         you will not have this.
       </p><ol class="procedure" type="1"><li class="step"><p>
           Copy <code class="filename">/etc/ssl/openssl.cnf</code> file to your setup.
           We can use this since it is completely annotated.
         </p><pre class="screen">
           # cp /etc/ssl/openssl.cnf ./
         </pre></li><li class="step"><p>
           Edit the file and change the location variable:
         </p><pre class="screen">
           dir = /root/ssl/root/ca
           # Where everything is kept
         </pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
             Make sure <code class="literal">dir</code> is the directory where we created
             <code class="filename">/root/ssl/root/ca</code>.
           </p></div></li></ol></div><div class="procedure"><a id="id14241"></a><p class="title"><strong>Procedure 3.3. Create the root key</strong></p><ol class="procedure" type="1"><li class="step"><p>
          Create the root key encrypted with AES 256-bit encryption
          and a password, using 4096 bits for the creation.
        </p><pre class="screen">
          # cd ~/ssl/root/ca
          # openssl genrsa -aes256 -out private/cakey.pem 4096
        </pre></li><li class="step"><p>
          You will be asked to enter a password here and then verify it.
        </p><pre class="screen">
          # chmod 400 private/cakey.pem
        </pre></li></ol></div><div class="procedure"><a id="id14249"></a><p class="title"><strong>Procedure 3.4. Create the root certificates</strong></p><ul class="procedure"><li class="step"><p>
           Use the root key (<code class="filename">cakey.pem</code>) to create the
           root certificate (cacert.pem). Its important to give it a long
           expiration since all the certificates signed from it will
           become invalid when it expires.
         </p><pre class="screen">
           # cd ~/ssl/root/ca
           # openssl req -config openssl.cnf -key private/cakey.pem -new -x509 -days 10950 -sha256 -extensions v3_ca -out cacert.pem
           Enter pass phrase for cakey.pem: enteryourpassword
           You are about to be asked to enter information that will be incorporated
           into your certificate request.
           -----
           Country Name (2 letter code) [AU]:US
           State or Province Name []:Idaho
           Locality Name []:Meridian
           Organization Name []:SUSEDojo
           Organizational Unit Name []:dojo
           Common Name []:susedojo.com
           Email Address []:admin@susedojo.com

           # chmod 444 cacert.pem
         </pre></li></ul></div><div class="procedure"><a id="id14255"></a><p class="title"><strong>Procedure 3.5. Verify the root certificates</strong></p><ul class="procedure"><li class="step"><p>
          Verify the certificate has the correct dates of validity and the
          algorithm used, Issuer, Subject, and x509v3 extensions. The issuer
          and subject are the same since it is self signed.
        </p><pre class="screen">
          # cd ~/ssl/root/ca
          # openssl x509 -noout -text -in cacert.pem
        </pre></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="sign-server-client-cert"></a>Sign server and client certificates</h2></div></div></div><p>
      This section is if you are the perspective certificate authority (CA).
    </p><div class="procedure"><a id="id14263"></a><p class="title"><strong>Procedure 3.6. Prepare config file</strong></p><ol class="procedure" type="1"><li class="step"><p>
          Modify the <code class="filename">penssl.cnf</code> config file and add a
          line to the <code class="literal">[ v3_req ]</code> section:
        </p><pre class="screen">
          # cd ~/ssl/root/ca
          # vi openssl.cnf
          find v3_req
          Add the following line:
          subjectAltName = DNS:public.your_server_name.your_domain.com, DNS: cluster-control.your_domain.com
          At the bottom of the file create section server_cert with the follwing:
          [ server_cert ]
          subjectAltName = subjectAltName = DNS:public.your_server_name.your_domain.com, DNS: cluster-control.your_domain.com
        </pre></li><li class="step"><p>
          The first DNS name would be used if you only have a single node
          controller as you need the public URL for that server in your cluster.
          For example, <code class="literal">public.db8-ae-ed-77-14-9e.susedojo.com</code>.

          If you have a haproxy setup for your cluster or pacemaker, you have a
          cluster URL. For example, you may have
          <code class="literal">public.cluster.your_domain.com</code> and you need to
          have <code class="literal">cluster.your_domain.com</code> and <code class="literal">public.cluster.your_domain.com</code>
          as Alternative DNS names. This public URL can be used for all
          endpoints unless you have multiple High Availability Clusters for
          your control plane.
        </p></li><li class="step"><p>
          Save and close the file after you have those entered correctly.
        </p></li></ol></div><div class="procedure"><a id="id14278"></a><p class="title"><strong>Procedure 3.7. Create a key</strong></p><ul class="procedure"><li class="step"><p>
           Create a key minus the <code class="literal">-aes256</code> option so that
           you are not presented with a password each time you restart a
           service. (i.e. Apache service) also in 2048 bit so it is quicker to decrypt.
         </p><pre class="screen">
           # cd ~/ssl/root/ca
           # openssl genrsa -out private/susedojo-com.key.pem 2048
           # chmod 400 private/susedojo-com.key.pem
         </pre></li></ul></div><div class="procedure"><a id="id14284"></a><p class="title"><strong>Procedure 3.8. Create a certificate</strong></p><ol class="procedure" type="1"><li class="step"><p>
            Use the private key we just created to create a certificate
            signing request (CSR). The common name must be a fully qualified
            domain name (i.e. www.susedojo.com) The Organization Name must be
            the same as the Organization Name in the CA.
          </p><pre class="screen">
            # cd ~/ssl/root/ca
            # openssl req -config openssl.cnf -key private/susedojo-com.key.pem -new -sha256 -out csr/susedojo-com.csr.pem
            You are about to be asked to enter information that will be incorporated
            into your certificate request.
            -----
            Country Name (2 letter code) [XX]:US
            State or Province Name []:Idaho
            Locality Name []:Meridian
            Organization Name []:SUSEDojo
            Organizational Unit Name []:dojo
            Common Name []:susedojo.com
            Email Address []:admin@susedojo.com
          </pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
              You may be prompted for a challenge password and company name.
              This can be left blank.
            </p></div></li><li class="step"><p>
            Create the certificate using the CA to sign the CSR, using the
            <code class="literal">server_cert</code> extension as this will be used
            on a server. We will give it one year of validity.
          </p><pre class="screen">
            # cd ~/ssl/root/ca
            # openssl ca -config openssl.cnf -extensions server_cert -days 365 -notext -md sha256 -in  csr/susedojo-com.csr.pem -out certs/susedojo-com.cert.pem
              Using configuration from openssl.cnf
              Enter pass phrase for /root/ssl/root/ca/private/cakey.pem:
              Check that the request matches the signature
              Signature ok
                      Serial Number: 4096 (0x1000)
                      Validity
                        Not Before: Aug  8 04:21:08 2018 GMT
                        Not After: Aug  8 04:21:08 2019 GMT
                     Subject:
                          countryName               = US
                          stateOrProvinceName       = Idaho
                          organizationName          = SUSEDojo
                          organizationalUnitName    = dojo
                          commonName                = susedojo.com
                          emailAddress              = admin@susedojo.com
                     X509v3 extensions:
                         X509v3 Basic Constraints:
                            CA:FALSE
                        X509v3 Key Usage:
                              Digital Signature, Non Repudiation, Key Encipherment
                         X509v3 Subject Alternative Name:
                             DNS:public.db8-ae-ed-77-14-9e.susedojo.com
            Certificate is to be certified until Aug  8 04:21:08 2019 GMT (365 days)
            Sign the certificate? [y/n]:y

            1 out of 1 certificate requests certified, commit? [y/n]y
            Write out database with 1 new entries
            Data Base Updated

            # chmod 444 certs/susedojo-com.cert.pem
          </pre></li><li class="step"><p>
            The <code class="filename">index.txt</code> file should now contain a line
            referring to the new certificate that has been created.
            For example, the output should look like the following:
          </p><pre class="screen">
            V       190808042108Z           1000    unknown
            /C=US/ST=Idaho/O=SUSEDojo/OU=dojo/CN=susedojo.com/emailAddress=admin@susedojo.com
          </pre></li></ol></div><div class="procedure"><a id="id14299"></a><p class="title"><strong>Procedure 3.9. Verifying the certificate</strong></p><ol class="procedure" type="1"><li class="step"><p>
             Enter the following in your terminal:
           </p><pre class="screen">
             # openssl x509 -noout -text -in certs/susedojo-com.cert.pem
           </pre></li><li class="step"><p>
             You will notice the Issuer is the CA and you can also see the
             Subject Alternative Name as well in the extensions section.
           </p><pre class="screen">
             # openssl verify -CAfile cacert.pem certs/susedojo-com.cert.pem
             certs/susedojo-com.cert.pem: OK
           </pre></li></ol></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="deploy-cert"></a>Deploying the certificate</h2></div></div></div><div class="procedure"><ol class="procedure" type="1"><li class="step"><p>Now you are ready to copy the newly created certificate and key
         to the control node or controllers in the cluster.</p><pre class="screen">
         # scp newcerts/1000.pem control:/root/
         # scp private/susedojo-com.key control:/root/
       </pre></li><li class="step"><p>
         Copy them into the right location on the controller host:
       </p><pre class="screen">
         # cp susedojo-com.key.pem /etc/keystone/ssl/private
         # cp 1000.pem /etc/keystone/ssl/certs
         # cd /etc/keystone/ssl/certs
         # mv signing_cert.pem signing_cert.pem.todays_date
         # cp 1000.pem signing_cert.pem
         # cd /etc/keystone/ssl/private
         # old signing_key.pem
         # cp susedojo-com.key.pem signing_key.pem
       </pre></li><li class="step"><p>
         Rerun the Barclamp for keystone in order to apply this change to
         the cluster.
       </p></li></ol></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="lets-encrypt-cert"></a>Generate Public Certificate using Let’s Encrypt</h2></div></div></div><p>
     <code class="literal">Let’s Encrypt</code> is a free, automated, and open Certificate Authority.
     Its Root is trusted by all major operating systems now. For SUSE Linux
     Enterprise Server 12 SP3 and higher, the ISRG Root X1 is available in
     <code class="filename">/etc/ssl/certs/ISRG_Root_X1.pem</code>. If not, apply the
     latest updates for your operating system.</p><p><code class="literal">Let’s Encrypt</code> has several clients to choose from depending on your needs.
     For this example, we will be using the <code class="literal">acme.sh</code> client,
     which is written in bash and gives us greater flexibility and ease in our
     solution.</p><p>The next steps walk you through the installation of <code class="literal">acme.sh</code> and the issue
     of a certificate with <code class="literal">Let’s Encrypt</code> followed by the automated load
     of the certificate in OpenStack for the various API endpoints available.</p><div class="procedure"><a id="id14329"></a><p class="title"><strong>Procedure 3.10. Installation of acme.sh letsencrypt client</strong></p><ol class="procedure" type="1"><li class="step"><p>
         Login to your crowbar/admin server change to the root directory.
       </p></li><li class="step"><p>
         Create a new directory for letsencrypt and clone the
         <code class="literal">acme.sh</code> repo:
       </p><pre class="screen">
         # mkdir letsencrypt
         # cd letsencrypt
         # git clone https://github.com/Neilpang/acme.sh.git
         # cd acme.sh
       </pre></li><li class="step"><p>
         The system is prepared for installing <code class="literal">acme.sh</code>.
       </p></li><li class="step"><p>
         Install <code class="literal">socat</code>:
       </p><pre class="screen">
         # export BRANCH=2 #this makes sure you are using the v2 api version of letsencrypt
         # zypper in -y socat
       </pre></li><li class="step"><p>
         Install <code class="literal">acme.sh</code>:
       </p><pre class="screen">
         # ./acme.sh --install
       </pre></li><li class="step"><p>
         After the install of <code class="filename">acme.sh</code> is finished, you
         should see a new directory <code class="filename">/root/.acme.sh/</code> where
         <code class="filename">acme.sh</code> lives and all of its environment,
         account info, and certificates are stored.
         We recommend using this as a backup location if you are using a
         backup tool.
       </p></li></ol></div><div class="procedure"><a id="id14353"></a><p class="title"><strong>Procedure 3.11. Issue a wildcard SSL Certificate</strong></p><p>
      OpenStack and wildcard SSL uses the DNS validation method by
      validating your domain using a TXT record that can either be added
      manually or using the many (over 3 dozen) available DNS API’s.
    </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
        It is important to a wildcard certificate as you have the ability
        to use the same one for all of your public API endpoints in the
        OpenStack Cloud environment. Additional Cloud Native services
        like Kubernetes can also take advantage of it.
      </p></div><ol class="procedure" type="1"><li class="step"><p>
        The manual DNS mode is a method that displays the DNS records that
        have to be created in your DNS servers. It is beneficial to automate
        the injection of DNS records as the maximum days a certificate is
        viable is 60 days.
        In order to issue your wildcard certificate, the command without
        optional settings is:
      </p><pre class="screen">
        # acme.sh --issue -d yourdomain.com -d *.yourdomain.com --dns
      </pre></li><li class="step"><p>
        To debug or test, add the following optional settings:
      </p><pre class="screen">
        # acme.sh --debug --test –issue -d yourdomain.com -d *.yourdomain.com --dns
      </pre></li><li class="step"><p>
        A message returns. For example:
      </p><pre class="screen">
        Add the following TXT record:
        Domain: '_acme-challenge.yourdomain.com'
        TXT value: 'KZvgq3MpOCjUNW7Uzz5nE5kkFdplNk66WGfxE9-H63k'
        Please be aware that you prepend <code class="literal">_acme-challenge.</code> before your domain
        so the resulting subdomain will be: <code class="literal">_acme-challenge.yourdomain.com</code>
      </pre></li><li class="step"><p>
        Using this information, you are ready to insert this TXT record into
        your DNS.
      </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
          When setting this up for SUSE OpenStack Cloud with Crowbar, you need
          to have your external DNS server appended to <code class="filename">/etc/resolv.conf</code>
          in order to resolve as crowbar has its own internal
          DNS. It is not enough to change it in the barclamp as you need the
          DNS server entry to be at the top of the list in <code class="filename">resolv.conf</code>.
          Crowbar returns to the default after a period of time.
          Keep in mind that if you want to automate this step every 90 days
          then you need to ensure the <code class="filename">resolv.conf</code> changes
          every time to bypass the local crowbar DNS Server.
        </p></div></li><li class="step"><p>
        In order to set up TXT record in bind DNS, edit the zone file so
        it looks like the following example:
      </p><pre class="screen">
        yourdomain.com.     IN NS           admin.yourdomain.com.
        _acme-challenge.yourdomain.com. IN TXT "xxxx...your TXT value string here"
      </pre></li><li class="step"><p>
        Restart your named services for <code class="literal">bind</code>.
      </p></li><li class="step"><p>
        Issue the acme-challenge verification of the previous step with the
        following command:
      </p><pre class="screen">
        # acme.sh --renew -d yourdomain.com
      </pre></li><li class="step"><p>
        If the DNS validation is okay, <code class="filename">acme.sh</code> issues a
        wildcard certificate and displays the certificate and private-key path.
        For example:
      </p><pre class="screen">
        Your cert is in:  /root/.acme.sh/susedojo.com/susedojo.com.cer  
        Your cert key is in:  /root/.acme.sh/susedojo.com/susedojo.com.key  
        v2 chain.
        The intermediate CA cert is in:  /root/.acme.sh/susedojo.com/ca.cer  
        And the full chain certs is in:  /root/.acme.sh/susedojo.com/fullchain.cer_on_issue_success
      </pre></li><li class="step"><p>
        Notice the location of your certificate and key. These are now
        ready to be used by OpenStack Cloud.
      </p></li><li class="step"><p>
        To automate the process of setting up the TXT record in your DNS
        servers and prepare it for automated validation, the file
        <code class="filename">account.conf</code> holds account information
        for the DNS Provider. After exporting the authentication variables,
        it stores them automatically after the command is executed for later use.
        To issue your wildcard certificate, the command without optional settings is:
      </p><pre class="screen">
        # export LUA_Key=”your_API_token_from_account”
        # export LUA_Email=”cameron@yourdomain.com”
        # acme.sh -d yourdomain.com -d *.yourdomain.com --dns dns_lua
      </pre></li><li class="step"><p>
       You can now view your DNS records and you will see a new TXT record
       available. When it is finished and the DNS validation is okay,
       <code class="filename">acme.sh</code> issue your wildcard certificate and displays
       your certificate and private-key path just as before.
      </p><pre class="screen">
        Your cert is in:  /root/.acme.sh/susedojo.com/susedojo.com.cer  
        Your cert key is in:  /root/.acme.sh/susedojo.com/susedojo.com.key  
        v2 chain.
        The intermediate CA cert is in:  /root/.acme.sh/susedojo.com/ca.cer  
        And the full chain certs is in:  /root/.acme.sh/susedojo.com/fullchain.cer_on_issue_success
      </pre></li></ol></div><div class="procedure"><a id="id14399"></a><p class="title"><strong>Procedure 3.12. Set Up Certificate Store on Control and Compute Nodes</strong></p><ol class="procedure" type="1"><li class="step"><p>
        Create a shared location for all Certificates on the control nodes.
        Execute these commands on all control nodes and compute nodes:
      </p><pre class="screen">
        mkdir -p /etc/cloud/ssl/certs
        mkdir -p /etc/cloud/ssl/private
      </pre></li><li class="step"><p>
        Copy all certificates to their shared locations on the control nodes
        and compute nodes:
      </p><pre class="screen">
        # scp /root/.acme.sh/susedojo.com/susedojo.com.cer \ root@control:/etc/cloud/ssl/certs
        # scp /root/.acme.sh/susedojo.com/ca.cer root@control:/etc/cloud/ssl/certs
        # scp /root/.acme.sh/susedojo.com/fullchain.cer root@control:/etc/cloud/ssl/certs
        # scp /root/.acme.sh/susedojo.com/susedojo.com.key \ root@control:/etc/cloud/ssl/private
      </pre></li></ol></div><div class="procedure"><a id="id14407"></a><p class="title"><strong>Procedure 3.13. Set Up Issued Certificates in Crowbar Barclamps</strong></p><ol class="procedure" type="1"><li class="step"><p>
        Set your Certificate File and Key File to the proper location, and
        set the CA Certificates File to the <code class="filename">/etc/ssl/ca-bundle.pem</code>
        in the distribution. Make sure <code class="literal">Generate (self-signed) certificates</code>
        is false, and <code class="literal">Certificate is insecure</code> is false.
      </p><div class="figure"><a id="id14414"></a><p class="title"><strong>Figure 3.1. Database Barclamp</strong></p><div class="figure-contents"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="75%"><tr><td><img src="images/database-barclamp.png" width="100%" alt="Database Barclamp" /></td></tr></table></div></div></div><br class="figure-break" /></li><li class="step"><p>
        Click <span class="guimenu">Apply</span>. Your changes will apply in a
        few minutes.
      </p></li><li class="step"><p>
        Set your Certificate File and Key File to the proper location, and
        set the CA Certificates File to the <code class="filename">/etc/ssl/ca-bundle.pem</code>
        in the distribution. Make sure <code class="literal">Generate (self-signed) certificates</code>
        is false, and <code class="literal">Certificate is insecure</code> is false.
      </p><div class="figure"><a id="id14429"></a><p class="title"><strong>Figure 3.2. RabbitMQ Barclamp</strong></p><div class="figure-contents"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="75%"><tr><td><img src="images/rabbitmq-barclamp.png" width="100%" alt="RabbitMQ Barclamp" /></td></tr></table></div></div></div><br class="figure-break" /></li><li class="step"><p>
        Click <span class="guimenu">Apply</span>. Your changes will apply in a
        few minutes.
      </p></li><li class="step"><p>
        Set your Certificate File and Key File to the proper location, and
        set the CA Certificates File to the <code class="filename">/etc/ssl/ca-bundle.pem</code>
        in the distribution. Make sure <code class="literal">Generate (self-signed) certificates</code>
        is false, and <code class="literal">Certificate is insecure</code> is false.
      </p><div class="figure"><a id="id14444"></a><p class="title"><strong>Figure 3.3. Keystone Barclamp</strong></p><div class="figure-contents"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="75%"><tr><td><img src="images/keystone-barclamp.png" width="100%" alt="Keystone Barclamp" /></td></tr></table></div></div></div><br class="figure-break" /></li><li class="step"><p>
        Click <span class="guimenu">Apply</span>. Your changes will apply in a
        few minutes.
      </p></li><li class="step"><p>
        Set your Certificate File and Key File to the proper location, and
        set the CA Certificates File to the <code class="filename">/etc/ssl/ca-bundle.pem</code>
        in the distribution. Make sure <code class="literal">Generate (self-signed) certificates</code>
        is false, and <code class="literal">Certificate is insecure</code> is false, and
        <code class="literal">Require Client Certificates</code> is true.
      </p><div class="figure"><a id="id14460"></a><p class="title"><strong>Figure 3.4. Glance Barclamp</strong></p><div class="figure-contents"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="75%"><tr><td><img src="images/glance-barclamp.png" width="100%" alt="Glance Barclamp" /></td></tr></table></div></div></div><br class="figure-break" /></li><li class="step"><p>
        Click <span class="guimenu">Apply</span>. Your changes will apply in a
        few minutes.
      </p></li><li class="step"><p>
        Set your Certificate File and Key File to the proper location, and
        set the CA Certificates File to the <code class="filename">/etc/ssl/ca-bundle.pem</code>
        in the distribution. Make sure <code class="literal">Generate (self-signed) certificates</code>
        is false, and <code class="literal">Certificate is insecure</code> is false, and
        <code class="literal">Require Client Certificates</code> is false.
      </p><div class="figure"><a id="id14476"></a><p class="title"><strong>Figure 3.5. Cinder Barclamp</strong></p><div class="figure-contents"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="75%"><tr><td><img src="images/cinder-barclamp.png" width="100%" alt="Cinder Barclamp" /></td></tr></table></div></div></div><br class="figure-break" /></li><li class="step"><p>
        Click <span class="guimenu">Apply</span>. Your changes will apply in a
        few minutes.
      </p></li><li class="step"><p>
        Set your Certificate File and Key File to the proper location, and
        set the CA Certificates File to the <code class="filename">/etc/ssl/ca-bundle.pem</code>
        in the distribution. Make sure <code class="literal">Generate (self-signed) certificates</code>
        is false, and <code class="literal">Certificate is insecure</code> is false, and
        <code class="literal">Require Client Certificates</code> is false.
      </p><div class="figure"><a id="id14492"></a><p class="title"><strong>Figure 3.6. Neutron Barclamp</strong></p><div class="figure-contents"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="75%"><tr><td><img src="images/neutron-barclamp.png" width="100%" alt="Neutron Barclamp" /></td></tr></table></div></div></div><br class="figure-break" /></li><li class="step"><p>
        Click <span class="guimenu">Apply</span>. Your changes will apply in a
        few minutes.
      </p></li><li class="step"><p>
        Set your Certificate File and Key File to the proper location, and
        set the CA Certificates File to the <code class="filename">/etc/ssl/ca-bundle.pem</code>
        in the distribution. Make sure <code class="literal">Generate (self-signed) certificates</code>
        is false, and <code class="literal">Certificate is insecure</code> is false, and
        <code class="literal">Require Client Certificates</code> is false.
      </p><div class="figure"><a id="id14508"></a><p class="title"><strong>Figure 3.7. Nova Barclamp</strong></p><div class="figure-contents"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="75%"><tr><td><img src="images/nova-barclamp.png" width="100%" alt="Nova Barclamp" /></td></tr></table></div></div></div><br class="figure-break" /></li><li class="step"><p>
        Click <span class="guimenu">Apply</span>. Your changes will apply in a
        few minutes.
      </p></li></ol></div><p>
    Each Crowbar barclamp that have SSL support are the same. You can change
    the same settings and apply your certificate to the remaining barclamps.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
      Once this is completed, we recommend automating this process as the
      <code class="literal">Let's Encrypt</code> certificates expire after 90 days.
    </p></div></div></div><div class="chapter"><div class="titlepage"><div><div><h1 class="title"><a id="soc-monitoring"></a>Chapter 4. SUSE <span class="productname">OpenStack</span> Cloud Monitoring</h1></div></div></div><div class="toc"><p><strong>Table of Contents</strong></p><dl class="toc"><dt><span class="section"><a href="#about-operations-monitoring">About <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring</a></span></dt><dd><dl><dt><span class="section"><a href="#accessing-soc-monitoring">Accessing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring</a></span></dt><dt><span class="section"><a href="#monitoring-overview-data">Overview</a></span></dt></dl></dd><dt><span class="section"><a href="#monitoring-architecture">Architecture</a></span></dt><dd><dl><dt><span class="section"><a href="#id14766">Agents and Services</a></span></dt></dl></dd><dt><span class="section"><a href="#basic-usage-scenario">Basic Usage Scenario</a></span></dt><dd><dl><dt><span class="section"><a href="#metrics-monitoring">Metrics</a></span></dt><dt><span class="section"><a href="#data-visualization-analysis">Data Visualization and Analysis</a></span></dt><dt><span class="section"><a href="#alarms-notifications">Alarms and Notifications</a></span></dt></dl></dd><dt><span class="section"><a href="#key-features">Key Features</a></span></dt><dd><dl><dt><span class="section"><a href="#monitoring">Monitoring</a></span></dt><dt><span class="section"><a href="#metrics">Metrics</a></span></dt><dt><span class="section"><a href="#log-management">Log Management</a></span></dt><dt><span class="section"><a href="#openstack-integration">Integration with <span class="productname">OpenStack</span></a></span></dt></dl></dd><dt><span class="section"><a href="#components">Components</a></span></dt><dd><dl><dt><span class="section"><a href="#monitoring-service">Monitoring Service</a></span></dt><dt><span class="section"><a href="#horizon-plugin">Horizon Plugin</a></span></dt><dt><span class="section"><a href="#metrics-agent">Metrics Agent</a></span></dt><dt><span class="section"><a href="#log-agent">Log Agent</a></span></dt></dl></dd><dt><span class="section"><a href="#users-roles">Users and Roles</a></span></dt><dd><dl><dt><span class="section"><a href="#user-management">User Management</a></span></dt></dl></dd><dt><span class="section"><a href="#ops-maintenance">Operation and Maintenance</a></span></dt><dd><dl><dt><span class="section"><a href="#id15263">Removing Metrics Data</a></span></dt><dt><span class="section"><a href="#id15342">Removing Log Data</a></span></dt><dt><span class="section"><a href="#id15408">Log File Handling</a></span></dt><dt><span class="section"><a href="#backup-recovery">Backup and Recovery</a></span></dt></dl></dd><dt><span class="section"><a href="#data-virtualizations">Working with Data Visualizations</a></span></dt><dd><dl><dt><span class="section"><a href="#metrics-dashboard-openstack">Preconfigured Metrics Dashboard for <span class="productname">OpenStack</span></a></span></dt><dt><span class="section"><a href="#building-dashboards">Building Dashboards</a></span></dt><dt><span class="section"><a href="#creating-dashboards">Creating Dashboards</a></span></dt><dt><span class="section"><a href="#editing-rows">Editing Rows</a></span></dt><dt><span class="section"><a href="#editing-panels">Editing Panels</a></span></dt><dt><span class="section"><a href="#saving-sharing-dashboards">Saving and Sharing Dashboards</a></span></dt></dl></dd><dt><span class="section"><a href="#defining-alarms">Defining Alarms</a></span></dt><dd><dl><dt><span class="section"><a href="#alarm-details">Details</a></span></dt><dt><span class="section"><a href="#alarm-notifications">Notifications</a></span></dt></dl></dd><dt><span class="section"><a href="#defining-notifcations">Defining Notifications</a></span></dt><dt><span class="section"><a href="#status-servers-log-data">Status of Services, Servers, and Log Data</a></span></dt><dt><span class="section"><a href="#supported-metrics">Supported Metrics</a></span></dt><dd><dl><dt><span class="section"><a href="#id15967">Standard Metrics</a></span></dt><dt><span class="section"><a href="#id15986">Additional Metrics</a></span></dt></dl></dd></dl></div><p>
  As more and more applications are deployed on cloud systems and cloud systems
  are growing in complexity, managing the cloud infrastructure is becoming
  increasingly difficult. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring
  helps mastering this challenge by providing a sophisticated Monitoring as a
  Service solution that is operated on top of <span class="productname">OpenStack</span>-based cloud computing
  platforms.
 </p><p>
  The component architecture of <span class="productname">OpenStack</span> provides for high flexibility, yet it
  increases the burden of system operation because multiple services must be
  handled. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring offers an
  integrated view of all services and assembles and presents related metrics
  and log data in one convenient access point. While being flexible and
  scalable to instantly reflect changes in the <span class="productname">OpenStack</span> platform,
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring provides the ways and means required to
  ensure multi-tenancy, high availability, and data security. The high
  availability architecture of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring
  ensures an optimum level of operational performance eliminating the risk of
  component failures and providing for reliable crossover.
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring covers all aspects of a
  Monitoring as a Service solution:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    Central management of monitoring and log data from medium and large-size
    <span class="productname">OpenStack</span> deployments.
   </p></li><li class="listitem"><p>
    Storage of metrics and log data in a resilient way.
   </p></li><li class="listitem"><p>
    Multi-tenancy architecture to ensure the secure isolation of metrics and
    log data.
   </p></li><li class="listitem"><p>
    Horizontal and vertical scalability to support constantly evolving cloud
    infrastructures. When physical and virtual servers are scaled up or down to
    varying loads, the monitoring and log management solution can be adapted
    accordingly.
   </p></li></ul></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="about-operations-monitoring"></a>About <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring</h2></div></div></div><p>
  The monitoring solution of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring
  addresses the requirements of large-scale public and private clouds where
  high numbers of physical and virtual servers need to be monitored and huge
  amounts of monitoring data need to be managed. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring
  consolidates metrics, alarms, and notifications, as well
  as health and status information from multiple systems, thus reducing the
  complexity and allowing for a higher level analysis of the monitoring data.
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring covers all aspects of a
  Monitoring as a Service solution:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    Storage of monitoring data in a resilient way.
   </p></li><li class="listitem"><p>
    Multi-tenancy architecture for submitting and streaming metrics. The
    architecture ensures the secure isolation of metrics data.
   </p></li><li class="listitem"><p>
    Horizontal and vertical scalability to support constantly evolving cloud
    infrastructures. When physical and virtual servers are scaled up or down to
    varying loads, the monitoring solution can be adapted accordingly.
   </p></li></ul></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring offers various
  features which support you in proactively managing your cloud resources. A
  large number of metrics in combination with early warnings about problems and
  outages assists you in analyzing and troubleshooting any issue you encounter
  in your environment.
 </p><p>
  The monitoring features include:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    A monitoring overview which allows you to access all monitoring
    information.
   </p></li><li class="listitem"><p>
    Metrics dashboards for visualizing your monitoring data.
   </p></li><li class="listitem"><p>
    Alerting features for monitoring.
   </p></li></ul></div><p>
  In the following sections, you will find information on the monitoring
  overview and the metrics dashboards as well as details on how to define and
  handle alarms and notifications.
 </p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="accessing-soc-monitoring"></a>Accessing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring</h3></div></div></div><p>
  For accessing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring and performing
  monitoring tasks, you must have access to the <span class="productname">OpenStack</span> platform as a user with
  the <span class="guimenu">monasca-user</span> or <span class="guimenu">monasca-read-only-user</span>
  role in the <span class="guimenu">monasca</span> tenant.
  </p><p>
  Log in to <span class="productname">OpenStack</span> horizon with your user name and password. The functions
  you can use in <span class="productname">OpenStack</span> horizon depend on your access permissions. To access
  logs and metrics, switch to the <span class="guimenu">monasca</span> tenant in
  horizon. This allows you to access all monitoring data for
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring.
 </p><div class="figure"><a id="fig-socm-monasca"></a><p class="title"><strong>Figure 4.1. SUSE <span class="productname">OpenStack</span> Cloud horizon Dashboard—Monitoring</strong></p><div class="figure-contents"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="90%"><tr><td><img src="images/socm-horizon-monasca.png" width="100%" alt="SUSE OpenStack Cloud horizon Dashboard—Monitoring" /></td></tr></table></div></div></div><br class="figure-break" /></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="monitoring-overview-data"></a>Overview</h3></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring provides one convenient
   access point to your monitoring data. Use <span class="guimenu">Monitoring &gt;
   Overview</span> to keep track of your services and servers and quickly
   check their status. The overview also indicates any irregularities in the log
   data of the system components you are monitoring.
  </p><p>
   On the <span class="guimenu">Overview</span> page, you can:
  </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
     View the status of your services, servers, and log data at a glance. As
     soon as you have defined an alarm for a service, a server, or log data and
     metrics data has been received, there is status information displayed on
     the <span class="guimenu">Overview</span> page. Different colors are used for the
     different statuses.
    </p></li></ul></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="monitoring-architecture"></a>Architecture</h2></div></div></div><p>
  The following illustration provides an overview of the main components of
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring and their interaction:
 </p><div class="informalfigure"><div class="mediaobject"><img src="images/images-cmm-architecture.png" alt="cmm-architecture.png" /></div></div><h5><a id="id14648"></a><span class="productname">OpenStack</span></h5><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring relies on <span class="productname">OpenStack</span> as
  technology for building cloud computing platforms for public and private
  clouds. <span class="productname">OpenStack</span> consists of a series of interrelated projects delivering
  various components for a cloud infrastructure solution and allowing for the
  deployment and management of Infrastructure as a Service (IaaS) platforms.
 </p><h5><a id="id14656"></a>Monitoring Service</h5><p>
  The Monitoring Service is the central <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring
  component. It is responsible for receiving, persisting,
  and processing monitoring and log data, as well as providing the data to the
  users.
 </p><p>
  The Monitoring Service relies on monasca, an open source Monitoring as a
  Service solution. It uses monasca for high-speed metrics querying and
  integrates the Threshold Engine (streaming alarm engine) and the Notification
  Engine of monasca.
 </p><p>
  The Monitoring Service consists of the following components:
 </p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Monitoring API</span></dt><dd><p>
     A RESTful API for monitoring. It is primarily focused on the following
     areas:
    </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
       Metrics: Store and query massive amounts of metrics in real time.
      </p></li><li class="listitem"><p>
       Statistics: Provide statistics for metrics.
      </p></li><li class="listitem"><p>
       Alarm Definitions: Create, update, query, and delete alarm definitions.
      </p></li><li class="listitem"><p>
       Alarms: Query and delete the alarm history.
      </p></li><li class="listitem"><p>
       Notification Methods: Create and delete notification methods and
       associate them with alarms. Users can be notified directly when alarms
       are triggered, for example, via email.
      </p></li></ul></div></dd><dt><span class="term">Message Queue</span></dt><dd><p>
     A component that primarily receives published metrics from the Monitoring
     API, alarm state transition messages from the Threshold Engine, and log
     data from the Log API. The data is consumed by other components, such as
     the Persister, the Notification Engine, and the Log Persister. The Message
     Queue is also used to publish and consume other events in the system. It
     is based on Kafka, a high-performance, distributed, fault-tolerant, and
     scalable message queue with durability built-in. For administrating the
     Message Queue, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring uses
     Zookeeper, a centralized service for maintaining configuration
     information, naming, providing distributed synchronization, and providing
     group services.
    </p></dd><dt><span class="term">Persister</span></dt><dd><p>
     A monasca component that consumes metrics and alarm state transitions from
     the Message Queue and stores them in the Metrics and Alarms Database
     (InfluxDB).
    </p></dd><dt><span class="term">Notification Engine</span></dt><dd><p>
     A monasca component that consumes alarm state transition messages from the
     Message Queue and sends notifications for alarms, such as emails.
    </p></dd><dt><span class="term">Threshold Engine</span></dt><dd><p>
     A monasca component that computes thresholds on metrics and publishes
     alarms to the Message Queue when they are triggered. The Threshold Engine
     is based on Apache Storm, a free and open distributed real-time
     computation system.
    </p></dd><dt><span class="term">Metrics and Alarms Database</span></dt><dd><p>
     An InfluxDB database used for storing metrics and the alarm history.
    </p></dd><dt><span class="term">Config Database</span></dt><dd><p>
     A MariaDB database used for storing configuration information, alarm
     definitions, and notification methods.
    </p></dd><dt><span class="term">Log API</span></dt><dd><p>
     A RESTful API for log management. It gathers log data from the Log Agents
     and forwards it to the Message Queue.
    </p><p>
     The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring log management is
     based on Logstash, a tool for receiving, processing, and publishing all
     kinds of logs. It provides a powerful pipeline for querying and analyzing
     logs. Elasticsearch is used as the back-end datastore, and Kibana as the
     front-end tool for retrieving and visualizing the log data.
    </p></dd><dt><span class="term">Log Transformer</span></dt><dd><p>
     A Logstash component that consumes the log data from the Message Queue,
     performs transformation and aggregation operations on the data, and
     publishes the data that it creates back to the Message Queue.
    </p></dd><dt><span class="term">Log Metrics</span></dt><dd><p>
     A monasca component that consumes log data from the Message Queue, filters
     the data according to severity, and generates metrics for specific
     severities, for example, for errors or warnings. The generated metrics are
     published to the Message Queue and can be further processed by the
     Threshold Engine like any other metrics.
    </p></dd><dt><span class="term">Log Persister</span></dt><dd><p>
     A Logstash component that consumes the transformed and aggregated log data
     from the Message Queue and stores it in the Log Database.
    </p></dd><dt><span class="term">Kibana Server</span></dt><dd><p>
     A Web browser-based analytics and search interface to the Log Database.
    </p></dd><dt><span class="term">Log Database</span></dt><dd><p>
     An Elasticsearch database for storing the log data.
    </p></dd></dl></div><h5><a id="id14734"></a>horizon Plugin</h5><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring comes with a plugin for the
  <span class="productname">OpenStack</span> horizon dashboard. The plugin extends the main dashboard in
  <span class="productname">OpenStack</span> with a view for monitoring. This enables <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring
  users to access the monitoring functions from a
  central Web-based graphical user interface. For details, refer to the
  <a class="link" href="http://docs.openstack.org/developer/horizon/" target="_top"><span class="productname">OpenStack</span> horizon
  documentation</a>.
 </p><p>
  Based on <span class="productname">OpenStack</span> horizon, the monitoring data is visualized on a
  comfortable and easy-to-use dashboard which fully integrates with the
  following applications:
 </p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Grafana (for metrics data)</span></dt><dd><p>
     An open source application for visualizing large-scale measurement data.
    </p></dd><dt><span class="term">Kibana (for log data)</span></dt><dd><p>
     An open source analytics and visualization platform
     designed to work with Elasticsearch.
    </p></dd></dl></div><h5><a id="id14757"></a>Metrics Agent</h5><p>
  A Metrics Agent is required for retrieving metrics data from the host on
  which it runs and sending the metrics data to the Monitoring Service. The
  agent supports metrics from a variety of sources as well as a number of
  built-in system and service checks.
 </p><p>
  A Metrics Agent can be installed on each virtual or physical server to be
  monitored.
 </p><p>
  The agent functionality is fully integrated into the source code base of the
  monasca project. For details, refer to the
  <a class="link" href="https://wiki.openstack.org/wiki/Monasca" target="_top">monasca Wiki</a>.
 </p><h5><a id="id14762"></a>Log Agent</h5><p>
  A Log Agent is needed for collecting log data from the host on which it runs
  and forwarding the log data to the Monitoring Service for further processing.
  It can be installed on each virtual or physical server from which log data is
  to be retrieved.
 </p><p>
  The agent functionality is fully integrated into the source code base of the
  monasca project. For details, refer to the
  <a class="link" href="https://wiki.openstack.org/wiki/Monasca" target="_top">monasca Wiki</a>.
 </p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id14766"></a>Agents and Services</h3></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Service Name</th><th>Description</th></tr></thead><tbody><tr><td><code class="literal">zookeeper</code>
      </td><td>Centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services.</td></tr><tr><td><code class="literal">storm-nimbus</code>
      </td><td>Storm is a distributed real-time computation system for processing large volumes of high-velocity data. The Storm Nimbus daemon is responsible for distributing code around a cluster, assigning tasks to machines, and monitoring for failures. </td></tr><tr><td><code class="literal">storm-supervisor</code>
      </td><td>The Storm supervisor listens for work assigned to its machine and starts and stops worker processes as necessary based on what Nimbus has assigned to it.</td></tr><tr><td><code class="literal">mariadb</code>
      </td><td>MariaDB database service. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring stores configuration information in this database. </td></tr><tr><td><code class="literal">kafka</code>
      </td><td>Message queue service.</td></tr><tr><td><code class="literal">influxdb</code>
      </td><td>InfluxDB database service. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring stores metrics and alarms in this database.</td></tr><tr><td><code class="literal">elasticsearch</code>
      </td><td>Elasticsearch database service. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring stores the log data in this database.</td></tr><tr><td><code class="literal">memcached</code>
      </td><td>Memcached service. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring uses it for caching authentication and authorization information required for the communication between the Log API and <span class="productname">OpenStack</span> keystone.</td></tr><tr><td><code class="literal">openstack-monasca-notification</code>
      </td><td>Notification Engine. </td></tr><tr><td><code class="literal">openstack-monasca-thresh</code>
      </td><td>Threshold Engine. </td></tr><tr><td><code class="literal">openstack-monasca-log-transformer</code>
      </td><td>Log Transformer. </td></tr><tr><td><code class="literal">apache2</code>
      </td><td>Log and monitoring API.</td></tr><tr><td><code class="literal">openstack-monasca-persister </code>
      </td><td>Persister. </td></tr><tr><td><code class="literal">openstack-monasca-agent</code>
      </td><td>Metrics Agent.</td></tr><tr><td><code class="literal">kibana</code>
      </td><td>Kibana server.</td></tr><tr><td><code class="literal">openstack-monasca-log-persister</code>
      </td><td>Log Persister. </td></tr><tr><td><code class="literal">openstack-monasca-log-metrics</code>
      </td><td>Log Metrics.</td></tr><tr><td><code class="literal">openstack-monasca-log-agent</code>
      </td><td>Log Agent.</td></tr></tbody></table></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="basic-usage-scenario"></a>Basic Usage Scenario</h2></div></div></div><p>
  The monitoring solution of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring
  addresses the requirements of large-scale public and private clouds where
  high numbers of physical and virtual servers need to be monitored and huge
  amounts of monitoring data need to be managed. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring
  consolidates metrics, alarms, and notifications, as well
  as health and status information from multiple systems, thus reducing the
  complexity and allowing for a higher level analysis of the monitoring data.
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring covers all aspects of a
  Monitoring as a Service solution:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    Storage of monitoring data in a resilient way.
   </p></li><li class="listitem"><p>
    Multi-tenancy architecture for submitting and streaming metrics. The
    architecture ensures the secure isolation of metrics data.
   </p></li><li class="listitem"><p>
    horizontal and vertical scalability to support constantly evolving cloud
    infrastructures. When physical and virtual servers are scaled up or down to
    varying loads, the monitoring solution can be adapted accordingly.
   </p></li></ul></div><p>
  The basic usage scenario of setting up and using the monitoring features of
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring looks as follows:
 </p><div class="informalfigure"><div class="mediaobject"><img src="images/images-metricslogs.png" alt="MetricsLogs.png" /></div></div><p>
  The <span class="bold"><strong>Monitoring Service operator</strong></span> is
  responsible for providing the monitoring features to the application
  operators and the <span class="productname">OpenStack</span> operator. This enables the application operators
  and the <span class="productname">OpenStack</span> operator to focus on operation and ensure the quality of
  their services without having to carry out the tedious tasks implied by
  setting up and administrating their own system monitoring software. The
  Monitoring Service operator uses the features for monitoring the
  operation of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring.
 </p><p>
  As the Monitoring Service operator, you have the following responsibilities:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    Deploying the Monitoring Service, thus providing the monitoring features to
    the application operators, and the monitoring and log management features
    to the <span class="productname">OpenStack</span> operator.
   </p></li><li class="listitem"><p>
    Regular maintenance of the components and services for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring.
   </p></li><li class="listitem"><p>
    Backup of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring databases,
    configuration files, and customized dashboards.
   </p></li><li class="listitem"><p>
    Monitoring of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring for quality
    assurance.</p></li></ul></div><p>
  <span class="bold"><strong>Application operators</strong></span> monitor the virtual
  machines on which they provide services to <span class="bold"><strong>end
  users</strong></span> or services they need for their development activities. They
  ensure that the physical and virtual servers on which their services are
  provided are up and running as required.
 </p><p>
  The <span class="bold"><strong><span class="productname">OpenStack</span> operator</strong></span> is responsible for
  administrating and maintaining the underlying <span class="productname">OpenStack</span> platform. The
  monitoring and log management services of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring
  enable you to ensure the availability and quality of your platform.
  You use <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring for:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    Monitoring physical and virtual servers, hypervisors, and <span class="productname">OpenStack</span>
    services.
   </p></li><li class="listitem"><p>
    Monitoring middleware components, for example, database services.
   </p></li><li class="listitem"><p>
    Retrieving and analyzing the log data of the <span class="productname">OpenStack</span> services and
    servers, the middleware components, and the operating system.
   </p></li></ul></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="metrics-monitoring"></a>Metrics</h3></div></div></div><p>
   A Metrics Agent can be installed and configured on each physical and virtual
   server where cloud resources are to be monitored. The agent is responsible
   for querying metrics and sending the data to the Monitoring Service for
   further processing.
  </p><p>
   Metrics are self-describing data structures that are uniquely identified by a
   name and a set of dimensions. Each dimension consists of a key/value pair
   that allows for a flexible and concise description of the data to be
   monitored, for example, region, availability zone, service tier, or resource
   ID.
  </p><p>
   The Metrics Agent supports various types of metrics including the following:
  </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
     System metrics, for example, CPU usage, consumed disk space, or network
     traffic.
    </p></li><li class="listitem"><p>
     Host alive checks. The agent can perform active checks on a host to
     determine whether it is alive using ping (ICMP) or SSH.
    </p></li><li class="listitem"><p>
     Process checks. The agent can check and monitor a process, for example, the
     number of instances, memory size, or number of threads.
    </p></li><li class="listitem"><p>
     HTTP endpoint checks. The agent can perform up/down checks on HTTP
     endpoints by sending an HTTP request and reporting success or failure to
     the Monitoring Service.
    </p></li><li class="listitem"><p>
     Service checks. The agent can check middleware services, for example,
     MySQL, Kafka, or RabbitMQ.
    </p></li><li class="listitem"><p>
     <span class="productname">OpenStack</span> services. The agent can perform specific checks on each process
     that is part of an <span class="productname">OpenStack</span> service.
    </p></li><li class="listitem"><p>
     Log metrics. The agent can check and monitor the number of critical log
     entries in the log data retrieved from the cloud resources.
    </p></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="data-visualization-analysis"></a>Data Visualization and Analysis</h3></div></div></div><p>
    All <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring user groups work with a
    graphical user interface that is seamlessly integrated into their cloud
    infrastructure. <span class="phrase">Based on <span class="productname">OpenStack</span> horizon, the user interface
    enables access to all monitoring functionality and the resulting large-scale
    monitoring data.</span>
   </p><p>
    A comfortable dashboard visualizes the health and status of the cloud
    resources. It allows <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring users
    to experiment with many ways of analyzing the performance of their cloud
    resources in real-time. They cannot only view but also share and explore
    visualizations of their monitoring data.
   </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="alarms-notifications"></a>Alarms and Notifications</h3></div></div></div><p>
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring supports GUI-based alarm and
    notification management. <span class="phrase">Template-based alarm definitions allow for
    monitoring a dynamically changing set of resources without the need for
    reconfiguration. While the number of underlying virtual machines is changing,
    for example, this ensures the efficient monitoring of scalable cloud
    services. Alarm definitions allow you to specify expressions that are
    evaluated based on the metrics data that is received. Alarm definitions can
    be combined to form compound alarms. Compound alarms allow you to track and
    process even more complex events. Notifications can be configured in order to
    inform <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring users when an alarm
    is triggered.</span>
   </p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="key-features"></a>Key Features</h2></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring is an out-of-the-box
  solution for monitoring <span class="productname">OpenStack</span>-based cloud environments. It is provided as
  a cloud service to users. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring
  meets different challenges, ranging from small-scale deployments to
  high-availability deployments and deployments with high levels of
  scalability.
 </p><p>
  The core of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring is monasca, an
  open source Monitoring as a Service solution that integrates with <span class="productname">OpenStack</span>.
  The key features of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring form an
  integral part of the monasca project. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring
  extends the source code base of the project through
  active contributions.
 </p><p>
  Compared to the monasca community edition, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring
  provides the following added value:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    Packaging as a commercial enterprise solution
   </p></li><li class="listitem"><p>
    Enterprise-level support
   </p></li></ul></div><p>
  The key features of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring address
  public as well as private cloud service providers. They include:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    Monitoring
   </p></li><li class="listitem"><p>
    Metrics
   </p></li><li class="listitem"><p>
    Log management
   </p></li><li class="listitem"><p>
    Integration with <span class="productname">OpenStack</span>
   </p></li></ul></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="monitoring"></a>Monitoring</h3></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring is a highly scalable and
  fault tolerant monitoring solution for <span class="productname">OpenStack</span>-based cloud infrastructures.
 </p><p>
  The system operator of the cloud infrastructure and the service providers do
  not have to care for system monitoring software any longer. They use
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring to check whether their
  services and servers are working appropriately.
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring provides comprehensive and
  configurable metrics with reasonable defaults for monitoring the status,
  capacity, throughput, and latency of cloud systems. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring
  users can set their own warnings and critical
  thresholds and can combine multiple warnings and thresholds to support the
  processing of complex events. Combined with a notification system, these
  alerting features enable them to quickly analyze and resolve problems in the
  cloud infrastructure.
 </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="metrics"></a>Metrics</h3></div></div></div><p>
  The Metrics agent is responsible for querying metrics and sending them to the
  Monitoring Service for further processing.
 </p><p>
  Metrics are self-describing data structures that are uniquely identified by a
  name and a set of dimensions. Each dimension consists of a key/value pair
  that allows for a flexible and concise description of the data to be
  monitored, for example, region, availability zone, service tier, or resource
  ID.
 </p><p>
  The Metrics Agent supports various types of metrics including the following:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    System metrics, for example, CPU usage, consumed disk space, or network
    traffic.
   </p></li><li class="listitem"><p>
    Host alive checks. The agent can perform active checks on a host to
    determine whether it is alive using ping (ICMP) or SSH.
   </p></li><li class="listitem"><p>
    Process checks. The agent can check and monitor a process, for example, the
    number of instances, memory size, or number of threads.
   </p></li><li class="listitem"><p>
    HTTP endpoint checks. The agent can perform up/down checks on HTTP
    endpoints by sending an HTTP request and reporting success or failure to
    the Monitoring Service.
   </p></li><li class="listitem"><p>
    Service checks. The agent can check middleware services, for example,
    MySQL, Kafka, or RabbitMQ.
   </p></li><li class="listitem"><p>
    <span class="productname">OpenStack</span> services. The agent can perform specific checks on each process
    that is part of an <span class="productname">OpenStack</span> service.
   </p></li><li class="listitem"><p>
    Log metrics. The agent can check and monitor the number of critical log
    entries in the log data retrieved from the cloud resources.
   </p></li></ul></div><p>
  Your individual agent configuration determines which metrics are available
  for monitoring your services and servers. For details on installing and
  configuring a Metrics Agent, see <a class="xref" href="#book-crowbar-deployment" title="Deployment Guide using Crowbar"><em class="citetitle">Deployment Guide using Crowbar</em></a>.
 </p><p>
  As soon as an agent is available, you have access to the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring
  monitoring features. You work with a
  graphical user interface that is seamlessly integrated into your cloud
  infrastructure. Based on <span class="productname">OpenStack</span> horizon, the user interface enables access
  to all monitoring functionality and the resulting large-scale monitoring
  data. A comfortable dashboard visualizes the health and status of your cloud
  resources.
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring provides functions for alarm
  and notification management. <span class="phrase">Template-based alarm definitions allow
  for monitoring a dynamically changing set of resources without the need for
  reconfiguration. While the number of underlying virtual machines is changing,
  for example, this ensures the efficient monitoring of scalable cloud
  services. Alarm definitions allow you to specify expressions that are
  evaluated based on the metrics data that is received. Alarm definitions can
  be combined to form compound alarms. Compound alarms allow you to track and
  process even more complex events. Notifications can be configured in order to
  inform <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring users when an alarm
  is triggered.</span>
 </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="log-management"></a>Log Management</h3></div></div></div><p>
  With the increasing complexity of cloud infrastructures, it is becoming more
  and more difficult and time-consuming for the system operator to gather,
  store, and query the large amounts of log data manually. To cope with these
  problems, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring provides
  centralized log management features.
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring stores the log data in a
  central database. This forms the basis for visualizing the log data for the
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring users. Advanced data
  analysis and visualization of the log data is supported in a variety of
  charts, tables, and maps. Visualizations can easily be combined in dynamic
  dashboards that display changes to search queries in real time.
 </p><p>The log data from a large number of sources can be accessed from a
  single dashboard. Integrated search, filter, and graphics options enable
  system operators to isolate problems and narrow down potential root causes.
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring thus provides valuable
  insights into the log data, even with large amounts of data resulting
  from highly complex environments.
 </p><p>
  Based on <span class="productname">OpenStack</span> horizon, the customizable dashboards are seamlessly
  integrated into your cloud infrastructure. They enable user access to all log
  management functionality.
 </p><p>
  GUI-based alarm and notification management is also supported for log data.
  Based on a template mechanism, you can configure alarms and notifications to
  monitor the number of critical log events over time. Compound alarms can be
  created to analyze more complex log events. This automation of log handling
  guarantees that you can identify problems in your their infrastructure early
  and find the root cause quickly.
 </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="openstack-integration"></a>Integration with <span class="productname">OpenStack</span></h3></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring is integrated with <span class="productname">OpenStack</span>
  core services. These include:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    <span class="productname">OpenStack</span> horizon dashboard for visualizing monitoring metrics and log data
   </p></li><li class="listitem"><p>
    <span class="productname">OpenStack</span> user management
   </p></li><li class="listitem"><p>
    <span class="productname">OpenStack</span> security and access control
   </p></li></ul></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="components"></a>Components</h2></div></div></div><p>
  The following illustration provides an overview of the main components of
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring:
 </p><div class="informalfigure"><div class="mediaobject"><img src="images/images-overview-structure.png" alt="structure_new.png" /></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring relies on <span class="productname">OpenStack</span> as
  technology for building cloud computing platforms for public and private
  clouds. <span class="productname">OpenStack</span> consists of a series of interrelated projects delivering
  various components for a cloud infrastructure solution and allowing for the
  deployment and management of Infrastructure as a Service (IaaS) platforms.
 </p><p>
  For details on <span class="productname">OpenStack</span>, refer to the
  <a class="link" href="http://docs.openstack.org/" target="_top"><span class="productname">OpenStack</span> documentation</a>.
 </p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="monitoring-service"></a>Monitoring Service</h3></div></div></div><p>
  The Monitoring Service is the central <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring
  component. It is responsible for receiving, persisting,
  and processing metrics and log data, as well as providing the data to the
  users.
 </p><p>
  The Monitoring Service relies on monasca. It uses monasca for high-speed
  metrics querying and integrates the streaming alarm engine and the
  notification engine of monasca. For details, refer to the
  <a class="link" href="https://wiki.openstack.org/wiki/monasca" target="_top">monasca Wiki</a>.
 </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="horizon-plugin"></a>Horizon Plugin</h3></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring comes with a plugin for the
  <span class="productname">OpenStack</span> horizon dashboard. The horizon plugin extends the main dashboard in
  <span class="productname">OpenStack</span> with a view for monitoring. This enables <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring
  users to access the monitoring and log management
  functions from a central Web-based graphical user interface. Metrics and log
  data are visualized on a comfortable and easy-to-use dashboard.
 </p><p>
  For details, refer to the
  <a class="link" href="http://docs.openstack.org/developer/horizon/" target="_top"><span class="productname">OpenStack</span> horizon
  documentation</a>.
 </p><p>
  Based on <span class="productname">OpenStack</span> horizon, the monitoring data is visualized on a
  comfortable and easy-to-use dashboard which fully integrates with the
  following applications:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
     Grafana (for metrics data). An open source application for visualizing
     large-scale measurement data.
    </p></li><li class="listitem"><p>
     Kibana (for log data). An open source analytics and visualization platform
     designed to work with Elasticsearch.
    </p></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="metrics-agent"></a>Metrics Agent</h3></div></div></div><p>
  A Metrics Agent is required for retrieving metrics data from the host on
  which it runs and sending the metrics data to the Monitoring Service. The
  agent supports metrics from a variety of sources as well as a number of
  built-in system and service checks.
 </p><p>
  A Metrics Agent can be installed on each virtual or physical server to be
  monitored.
 </p><p>
  The agent functionality is fully integrated into the source code base of the
  monasca project. For details, refer to the
  <a class="link" href="https://wiki.openstack.org/wiki/monasca" target="_top">monasca Wiki</a>.
 </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="log-agent"></a>Log Agent</h3></div></div></div><p>
  A Log Agent is needed for collecting log data from the host on which it runs
  and forwarding the log data to the Monitoring Service for further processing.
  It can be installed on each virtual or physical server from which log data is
  to be retrieved.
 </p><p>
  The agent functionality is fully integrated into the source code base of the
  monasca project. For details, refer to the
  <a class="link" href="https://wiki.openstack.org/wiki/monasca" target="_top">monasca Wiki</a>.
 </p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="users-roles"></a>Users and Roles</h2></div></div></div><p>
  <span class="phrase">CMM</span> users can be grouped by their role. The following user
  roles are distinguished:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    An <span class="bold"><strong>application operator</strong></span> acts as a service
    provider in the <span class="productname">OpenStack</span> environment. He books virtual machines in
    <span class="productname">OpenStack</span> to provide services to end users or to host services that he
    needs for his own development activities. <span class="phrase">CMM</span> helps
    application operators to ensure the quality of their services in the cloud.
   </p><p>
    For details on the tasks of the application operator, refer to the
    <span class="emphasis"><em>Application Operator's Guide</em></span>.
   </p></li><li class="listitem"><p>
    The<span class="bold"><strong> <span class="productname">OpenStack</span> operator</strong></span> is a special
    application operator. He is responsible for administrating and maintaining
    the underlying <span class="productname">OpenStack</span> platform and ensures the availability and quality
    of the <span class="productname">OpenStack</span> services (e.g. heat, nova, cinder, swift, glance, or
    keystone).
   </p><p>
    For details on the tasks of the <span class="productname">OpenStack</span> operator, refer to the
    <span class="emphasis"><em><span class="productname">OpenStack</span> Operator's Guide</em></span>.
   </p></li><li class="listitem"><p>
    The <span class="bold"><strong>Monitoring Service operator</strong></span> is
    responsible for administrating and maintaining <span class="phrase">CMM</span>. He
    provides the cloud monitoring services to the other users and ensures the
    quality of the Monitoring Service.
   </p><p>
    For details on the tasks of the Monitoring Service operator, refer to the
    <span class="emphasis"><em>Monitoring Service Operator's Guide</em></span>.
   </p></li></ul></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="user-management"></a>User Management</h3></div></div></div><p>
  <span class="phrase">CMM</span> is fully integrated with keystone, the identity service
  which serves as the common authentication and authorization system in
  <span class="productname">OpenStack</span>.
 </p><p>
  The <span class="phrase">CMM</span> integration with keystone requires any
  <span class="phrase">CMM</span> user to be registered as an <span class="productname">OpenStack</span> user. All
  authentication and authorization in <span class="phrase">CMM</span> is done through
  keystone. If a user requests monitoring data, for example,
  <span class="phrase">CMM</span> verifies that the user is a valid user in <span class="productname">OpenStack</span> and
  allowed to access the requested metrics.
 </p><p>
  <span class="phrase">CMM</span> users are created and administrated in <span class="productname">OpenStack</span>:
 </p><div class="itemizedlist"><a id="Each-user-assumes-a-role-in-O-concept-conbody-section-2-ul"></a><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    Each user assumes a role in <span class="productname">OpenStack</span> to perform a specific set of
    operations. The <span class="productname">OpenStack</span> role specifies a set of rights and privileges.
   </p></li><li class="listitem"><p>
    Each user is assigned to at least one project in <span class="productname">OpenStack</span>. A project is an
    organizational unit that defines a set of resources which can be accessed
    by the assigned users.
   </p><p>
    Application operators in <span class="phrase">CMM</span> can monitor the set of
    resources that is defined for the projects to which they are assigned.
   </p></li></ul></div><p>
  For details on user management, refer to the
  <a class="link" href="http://docs.openstack.org/" target="_top"><span class="productname">OpenStack</span> documentation</a>.
 </p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ops-maintenance"></a>Operation and Maintenance</h2></div></div></div><p>
  Regular operation and maintenance includes:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    Configuring data retention for the InfluxDB database. This can be
    configured in the monasca barclamp. For details, see
    <a class="xref" href="#book-crowbar-deployment" title="Deployment Guide using Crowbar"><em class="citetitle">Deployment Guide using Crowbar</em></a>.
   </p></li><li class="listitem"><p>
    Configuring data retention for the Elasticsearch database. This can be
    configured in the monasca barclamp. For details, see
    <a class="xref" href="#book-crowbar-deployment" title="Deployment Guide using Crowbar"><em class="citetitle">Deployment Guide using Crowbar</em></a>.
   </p></li><li class="listitem"><p>
    Removing metrics data from the InfluxDB database.
   </p></li><li class="listitem"><p>
    Removing log data from the Elasticsearch database.
   </p></li><li class="listitem"><p>
    Handling log files of agents and services.
   </p></li><li class="listitem"><p>
    Backup and recovery of databases, configuration files, and dashboards.
   </p></li></ul></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id15263"></a>Removing Metrics Data</h3></div></div></div><p>
  Metrics data is stored in the Metrics and Alarms InfluxDB Database. InfluxDB
  features an SQL-like query language for querying data and performing
  aggregations on that data.
 </p><p>
  The Metrics Agent configuration defines the metrics and types of measurement
  for which data is stored. For each measurement, a so-called series is written
  to the InfluxDB database. A series consists of a timestamp, the metrics, and
  the value measured.
 </p><p>
  Every series can be assigned key tags. In the case of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring,
  this is the <code class="literal">_tenant_id</code> tag.
  This tag identifies the <span class="productname">OpenStack</span> project for which the metrics data has been
  collected.
 </p><p>
  From time to time, you may want to delete outdated or unnecessary metrics
  data from the Metrics and Alarms Database, for example, to save space or
  remove data for metrics you are no longer interested in. To delete data, you
  use the InfluxDB command line interface, the interactive shell that is
  provided for the InfluxDB database.
 </p><p>
  Proceed as follows to delete metrics data from the database:
 </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
    Create a backup of the database.
   </p></li><li class="listitem"><p>
    Determine the ID of the <span class="productname">OpenStack</span> project for the data to be deleted:
   </p><p>
    Log in to the <span class="productname">OpenStack</span> dashboard and go to <span class="bold"><strong>Identity
    &gt; Projects</strong></span>!m. The <code class="literal">monasca</code> project initially
    provides all metrics data related to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring.
   </p><p>
    In the course of the productive operation of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring,
    additional projects may be created, for example, for
    application operators.
   </p><p>
    The <span class="bold"><strong>Project ID</strong></span> field shows the relevant
    tenant ID.
   </p></li><li class="listitem"><p>
    Log in to the host where the Monitoring Service is installed.
   </p></li><li class="listitem"><p>
    Go to the directory where InfluxDB is installed:
   </p><pre class="screen">cd /usr/bin</pre></li><li class="listitem"><p>
    Connect to InfluxDB using the InfluxDB command line interface as follows:
   </p><pre class="screen">./influx -host &lt;host_ip&gt;</pre><p>
    Replace <code class="literal">&lt;host_ip&gt;</code> with the IP address of the
    machine on which <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring is
    installed.
   </p><p>
    The output of this command is, for example, as follows:
   </p><pre class="screen">Connected to http://localhost:8086 version 1.1.1
InfluxDB shell version: 1.1.1
</pre></li><li class="listitem"><p>
    Connect to the InfluxDB database of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring
    (<code class="literal">mon</code>):
   </p><pre class="screen">&gt; show databases
name: databases
name
----
mon
_internal

&gt; use mon
Using database mon</pre></li><li class="listitem"><p>
    Check the outdated or unnecessary data to be deleted.
   </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
      You can view all measurements for a specific project as follows:
     </p><pre class="screen">SHOW MEASUREMENTS WHERE _tenant_id = '&lt;project ID&gt;'</pre></li><li class="listitem"><p>
      You can view the series for a specific metrics and project, for example,
      as follows:
     </p><pre class="screen">SHOW SERIES FROM "cpu.user_perc" WHERE _tenant_id = '&lt;project ID&gt;'</pre></li></ul></div></li><li class="listitem"><p>
    Delete the desired data.
   </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
      When a project is no longer relevant or a specific tenant is no longer
      used, delete all series for the project as follows:
     </p><pre class="screen">DROP SERIES WHERE _tenant_id = '&lt;project ID&gt;'</pre><p>
      Example:
     </p><pre class="screen">DROP SERIES WHERE _tenant_id = '27620d7ee6e948e29172f1d0950bd6f4'</pre></li><li class="listitem"><p>
      When a metrics is no longer relevant for a project, delete all series for
      the specific project and metrics as follows:
     </p><pre class="screen">DROP SERIES FROM "&lt;metrics&gt;" WHERE _tenant_id = '&lt;project ID&gt;'</pre><p>
      Example:
     </p><pre class="screen">DROP SERIES FROM "cpu.user_perc" WHERE _tenant_id = '27620d7e'</pre></li></ul></div></li><li class="listitem"><p>
    Restart the <code class="literal">influxdb</code> service, for example, as follows:
   </p><pre class="screen">sudo systemctl restart influxdb</pre></li></ol></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id15342"></a>Removing Log Data</h3></div></div></div><p>
  Log data is stored in the Elasticsearch database. Elasticsearch stores the
  data in indices. One index per day is created for every <span class="productname">OpenStack</span> project.
 </p><p>
  By default, the indices are stored in the following directory on the host
  where the Monitoring Service is installed:
 </p><p>
  <code class="literal">/var/data/elasticsearch/&lt;cluster-name&gt;/nodes/&lt;node-name&gt;</code>
 </p><p>
  Example:
 </p><p>
  <code class="literal">/var/data/elasticsearch/elasticsearch/nodes/0</code>
 </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
   If your system uses a different directory, look up the
   <code class="literal">path.data</code> parameter in the Elasticsearch configuration
   file, <code class="literal">/etc/elasticsearch/elasticsearch.yml</code>.
  </p></div><p>
  If you want to delete outdated or unnecessary log data from the Elasticsearch
  database, proceed as follows:
 </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
    Make sure that <code class="literal">curl</code> is installed. If this is not the
    case, install the package with
   </p><pre class="screen">sudo zypper in curl</pre></li><li class="listitem"><p>
    Create a backup of the Elasticsearch database.
   </p></li><li class="listitem"><p>
    Determine the ID of the <span class="productname">OpenStack</span> project for the data to be deleted:
   </p><p>
    Log in to the <span class="productname">OpenStack</span> dashboard and go to <span class="bold"><strong>Identity
    &gt; Projects</strong></span>.  The <code class="literal">monasca</code> project initially
    provides a ll metrics data related to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring.
   </p><p>
    In the course of the productive operation of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring,
    additional projects may be created.
   </p><p>
    The <span class="bold"><strong>Project ID</strong></span> field shows the relevant
    ID.
   </p></li><li class="listitem"><p>
    Log in to the host where the Monitoring Service is installed.
   </p></li><li class="listitem"><p>
    Make sure that the data you want to delete exists by executing the
    following command:
   </p><pre class="screen">curl -XHEAD -i 'http://localhost:&lt;port&gt;/&lt;projectID-date&gt;'</pre><p>
    For example, if Elasticsearch is listening at port 9200 (default), the ID
    of the <span class="productname">OpenStack</span> project is <code class="literal">abc123</code>, and you want to
    check the index of 2015, July 1st, the command is as follows:
   </p><pre class="screen">curl -XHEAD -i 'http://localhost:9200/abc123-2015-07-01'</pre><p>
    If the HTTP response is <code class="literal">200</code>, the index exists; if the
    response is <code class="literal">404</code>, it does not exist.
   </p></li><li class="listitem"><p>
    Delete the index as follows:
   </p><pre class="screen">curl -XDELETE -i 'http://localhost:&lt;port&gt;/&lt;projectID-date&gt;'</pre><p>
    Example:
   </p><pre class="screen">curl -XDELETE -i 'http://localhost:9200/abc123-2015-07-01'</pre><p>
    This command either returns an error, such as
    <code class="literal">IndexMissingException</code>, or acknowledges the successful
    deletion of the index.
   </p></li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
   Be aware that the <code class="literal">-XDELETE</code> command immediately deletes
   the index file!
  </p></div><p>
  Both, for <code class="literal">-XHEAD</code> and <code class="literal">-XDELETE</code>, you can
  use wildcards for processing several indices. For example, you can delete all
  indices of a specific project for the whole month of July, 2015:
 </p><pre class="screen">curl -XDELETE -i 'http://localhost:9200/abc123-2015-07-*'</pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
   Take extreme care when using wildcards for the deletion of indices. You
   could delete all existing indices with one single command!
  </p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id15408"></a>Log File Handling</h3></div></div></div><p>
  In case of trouble with the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring
  services, you can study their log files to find the reason. The log files are
  also useful if you need to contact your support organization. For storing the
  log files, the default installation uses the <code class="literal">/var/log</code>
  directory on the hosts where the agents or services are installed.
 </p><p>
  You can use <code class="literal">systemd</code>, a system and session manager for
  LINUX, and <code class="literal">journald</code>, a LINUX logging interface, for
  addressing dispersed log files.
 </p><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring installer automatically
  puts all <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring services under the
  control of <code class="literal">systemd</code>. <code class="literal">journald</code> provides a
  centralized management solution for the logging of all processes that are
  controlled by <code class="literal">systemd</code>. The logs are collected and managed
  in a so-called journal controlled by the <code class="literal">journald</code> daemon.
 </p><p>
  For details on the <code class="literal">systemd</code> and <code class="literal">journald</code>
  utilities, refer to the
  <a class="link" href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-admin/#part-system" target="_top">https://documentation.suse.com/sles/15-SP1/single-html/SLES-admin/#part-system</a>.
 </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="backup-recovery"></a>Backup and Recovery</h3></div></div></div><p>
  Typical tasks of the Monitoring Service operator are to make regular backups,
  particularly of the data created during operation.
 </p><p>
  At regular intervals, you should make a backup of all:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    Databases.
   </p></li><li class="listitem"><p>
    Configuration files of the individual agents and services.
   </p></li><li class="listitem"><p>
    Monitoring and log dashboards you have created and saved.
   </p></li></ul></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring does not offer integrated
  backup and recovery mechanisms. Instead, use the mechanisms and
  procedures of the individual components.
 </p><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id15448"></a>Databases</h4></div></div></div><p>
   You need to create regular backups of the following databases on the host
   where the Monitoring Service is installed:
  </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
     Elasticsearch database for historic log data.
    </p></li><li class="listitem"><p>
     InfluxDB database for historic metrics data.
    </p></li><li class="listitem"><p>
     MariaDB database for historic configuration information.
    </p></li></ul></div><p>
   It is recommended that backup and restore operations for databases are
   carried out by experienced operators only.
  </p><h5><a id="id15459"></a>Preparations</h5><p>
 Before backing up and restoring a database, we recommend stopping the Monitoring
 API and the Log API on the <code class="literal">monasca-server</code> node, and check that all data is processed.
 This ensures that no data is written to a database during a backup and restore operation.
 After backing up and restoring a database, restart the APIs.
 </p><p>
 To stop the Monitoring API and the Log API, use the following command:
 </p><pre class="screen">systemctl stop apache2</pre><p>
 To check that all Kafka queues are empty, list the existing consumer groups and
 check the LAG column for each group. It should be 0. For example:
 </p><pre class="screen">
 kafka-consumer-groups.sh --zookeeper 192.168.56.81:2181 --list
 kafka-consumer-groups.sh --zookeeper 192.168.56.81:2181 --describe \
  --group 1_metrics | column -t -s ','
 kafka-consumer-groups.sh --zookeeper 192.168.56.81:2181 --describe \
  --group transformer-logstash-consumer | column -t -s ','
 kafka-consumer-groups.sh --zookeeper 192.168.56.81:2181 --describe \
  --group thresh-metric | column -t -s ','
  </pre><p>
 To restart the Monitoring API and the Log API, use the following command:
 </p><pre class="screen">systemctl start apache2</pre><h5><a id="id15468"></a>Elasticsearch Database</h5><p>
   For backing up and restoring your Elasticsearch database, use the
   Snapshot and Restore module of Elasticsearch.
  </p><p>
   To create a backup of the database, proceed as follows:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Make sure that <span class="command"><strong>curl</strong></span> is installed, <span class="command"><strong>zypper in curl</strong></span>.
    </p></li><li class="listitem"><p>
     Log in to the host where the Monitoring Service is installed.
    </p></li><li class="listitem"><p>
     Create a snapshot repository. You need the Elasticsearch bind address for
     all commands. run <span class="command"><strong>grep network.bind_host /etc/elasticsearch/elasticsearch.yml</strong></span>
     to find the bind address, and replace <em class="replaceable"><code>IP</code></em> in the
     following commands with this address. For example:
    </p><pre class="screen">curl -XPUT http://<em class="replaceable"><code>IP</code></em>:9200/_snapshot/my_backup -d '{
   "type": "fs",
   "settings": {
        "location": "/mount/backup/elasticsearch1/my_backup",
        "compress": true
   }
 }'</pre><p>
     The example registers a shared file system repository (<code class="literal">"type":
     "fs"</code>) that uses the
     <code class="filename">/mount/backup/elasticsearch1</code> directory for storing
     snapshots.
    </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
      The directory for storing snapshots must be configured in the <code class="literal">elasticsearch/repo_dir</code>
      setting in the monasca barclamp (see <a class="xref" href="#sec-depl-ostack-monasca" title="Deploying monasca (Optional)">the section called “Deploying monasca (Optional)”</a>).
      The directory must be manually mounted before creating the snapshot. The
      <code class="literal">elasticsearch</code> user must be specified as the owner of the directory.
     </p></div><p>
     <code class="literal">compress</code> is turned on to compress the metadata files.
    </p></li><li class="listitem"><p>
     Check whether the repository was created successfully:
    </p><pre class="screen">curl -XGET http://<em class="replaceable"><code>IP</code></em>:9200/_snapshot/my_backup</pre><p>
     This example response shows a successfully created repository:
    </p><pre class="screen">{
   "my_backup": {
     "type": "fs",
     "settings": {
       "compress": "true",
       "location": "/mount/backup/elasticsearch1/my_backup"
     }
   }
 }</pre></li><li class="listitem"><p>
     Create a snapshot of your database that contains all indices. A repository
     can contain multiple snapshots of the same database. The name of a snapshot
     must be unique within the snapshots created for your database, for example:
    </p><pre class="screen">curl -XPUT http://<em class="replaceable"><code>IP</code></em>:9200/_snapshot/my_backup/snapshot_1?wait_for_completion=true</pre><p>
     The example creates a snapshot named <code class="literal">snapshot_1</code> for all
     indices in the <code class="literal">my_backup</code> repository.
    </p></li></ol></div><p>
   To restore the database instance, proceed as follows:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Close all indices of your database, for example:
    </p><pre class="screen">curl -XPOST http://<em class="replaceable"><code>IP</code></em>:9200/_all/_close</pre></li><li class="listitem"><p>
     Restore all indices from the snapshot you have created, for example:
 </p><pre class="screen">curl -XPOST http://<em class="replaceable"><code>IP</code></em>:9200/_snapshot/my_backup/snapshot_1/_restore</pre><p>
     The example restores all indices from <code class="literal">snapshot_1</code> that is
     stored in the <code class="literal">my_backup</code> repository.
    </p></li></ol></div><p>
   For additional information on backing up and restoring an Elasticsearch
   database, refer to the
   <a class="link" href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/modules-snapshots.html" target="_top">Elasticsearch documentation</a>.
  </p><h5><a id="id15522"></a>InfluxDB Database</h5><p>
   For backing up and restoring your InfluxDB database, you can use the InfluxDB
   shell. The shell is part of your InfluxDB distribution. If you installed
   InfluxDB via a package manager, the shell is, by default, installed in the
   <code class="filename">/usr/bin</code> directory.
  </p><p>
   To create a backup of the database, proceed as follows:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Log in to the InfluxDB database as a user who is allowed to run the
     <code class="literal">influxdb</code> service, for example:
    </p><pre class="screen">su influxdb -s /bin/bash </pre></li><li class="listitem"><p>
     Back up the database, for example:
    </p><pre class="screen">influxd backup -database mon /mount/backup/mysnapshot</pre><p>
        monasca is using <code class="literal">mon</code> as the name of the database
     The example creates the backup for the database in
     <code class="filename">/mount/backup/mysnapshot</code>.
    </p></li></ol></div><p>
   Before restoring the database, make sure that all database processes are shut
   down. To restore the database, you can then proceed as follows:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     If required, delete all files not included in the backup by dropping the database
     before you carry out the restore operation. A restore operation restores all
     files included in the backup. Files created or merged at a later point in time
     are not affected. For example:
    </p><pre class="screen">influx -host <em class="replaceable"><code>IP</code></em> -execute 'drop database mon;'</pre><p>
        Replace <em class="replaceable"><code>IP</code></em> with the IP address that the database
        is listening to. You can run <span class="command"><strong>influxd config</strong></span> and look up
        the IP address in the <code class="literal">[http]</code> section.
        </p></li><li class="listitem"><p>
        Stop the InfluxDB database service:
    </p><pre class="screen">
 systemctl stop influxdb
 </pre></li><li class="listitem"><p>
     Log in to the InfluxDB database as a user who is allowed to run the
     <code class="literal">influxdb</code> service:
    </p><pre class="screen">su influxdb -s /bin/bash</pre></li><li class="listitem"><p>
    Restore the metastore:
    </p><pre class="screen">influxd restore -metadir /var/opt/influxdb/meta /mount/backup/mysnapshot</pre></li><li class="listitem"><p>
     Restore the database, for example:
    </p><pre class="screen">influxd restore -database mon -datadir /var/opt/influxdb/data /mount/backup/mysnapshot</pre><p>
     The example restores the backup from
     <code class="filename">/mount/backup/mysnapshot</code> to
     <code class="filename">/var/opt/influxdb/influxdb.conf</code>.
    </p></li><li class="listitem"><p>
     Ensure that the file permissions for the restored database are set
     correctly:
    </p><pre class="screen">chown -R influxdb:influxdb /var/opt/influxdb</pre></li><li class="listitem"><p>
     Start the InfluxDB database service:
    </p><pre class="screen">systemctl start influxdb</pre></li></ol></div><p>
   For additional information on backing up and restoring an InfluxDB database,
   refer to the
   <a class="link" href="https://docs.influxdata.com/influxdb/v1.1/administration/backup_and_restore/" target="_top">InfluxDB documentation</a>.
  </p><h5><a id="id15571"></a>MariaDB Database</h5><p>
   For backing up and restoring your MariaDB database, you can use the
   <span class="command"><strong>mysqldump</strong></span> utility program. <span class="command"><strong>mysqldump</strong></span>
   performs a logical backup that produces a set of SQL statements. These
   statements can later be executed to restore the database.
  </p><p>
   To back up your MariaDB database, you must be the owner of the database or a
   user with superuser privileges, for example:
  </p><pre class="screen">mysqldump -u root -p mon &gt; dumpfile.sql</pre><p>
   In addition to the name of the database, you have to specify the name and the
   location where <span class="command"><strong>mysqldump</strong></span> stores its output.
  </p><p>
   To restore your MariaDB database, proceed as follows:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Log in to the host where the Monitoring Service is installed as a user with
     root privileges.
    </p></li><li class="listitem"><p>
        Make sure that the <span class="command"><strong>mariadb</strong></span> service is running:
    </p><pre class="screen">systemctl start mariadb</pre></li><li class="listitem"><p>
     Log in to the database you have backed up as a user with root privileges, for example:
    </p><pre class="screen">mysql -u root -p mon</pre></li><li class="listitem"><p>
     Remove and then re-create the database:
    </p><pre class="screen">
 DROP DATABASE mon;
 CREATE DATABASE mon;</pre></li><li class="listitem"><p>
 Exit mariadb:
    </p><pre class="screen">\q</pre></li><li class="listitem"><p>
     Restore the database, for example:
    </p><pre class="screen">mysql -u root -p mon &lt; dumpfile.sql</pre></li></ol></div><p>
   For additional information on backing up and restoring a MariaDB database
   with <code class="literal">mysqldump</code>, refer to the
   <a class="link" href="https://mariadb.com/kb/en/mariadb/mysqldump/" target="_top">MariaDB
   documentation</a>.
  </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id15602"></a>Configuration Files</h4></div></div></div><p>
   Below you find a list of the configuration files of the agents and the
   individual services included in the Monitoring Service. Back up these files
   at least after you have installed and configured <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring
   and after each change in the configuration.
  </p><pre class="screen">/etc/influxdb/influxdb.conf
 /etc/kafka/server.properties
 /etc/my.cnf
 /etc/my.cnf.d/client.cnf
 /etc/my.cnf.d/mysql-clients.cnf
 /etc/my.cnf.d/server.cnf
 /etc/monasca/agent/agent.yaml
 /etc/monasca/agent/conf.d/*
 /etc/monasca/agent/supervisor.conf
 /etc/monasca/api-config.conf
 /etc/monasca/log-api-config.conf
 /etc/monasca/log-api-config.ini
 /etc/monasca-log-persister/monasca-log-persister.conf
 /etc/monasca-log-transformer/monasca-log-transformer.conf
 /etc/monasca-log-agent/agent.conf
 /etc/monasca-notification/monasca-notification.yaml
 /etc/monasca-persister/monasca-persister.yaml
 /etc/monasca-thresh/thresh.yaml
 /etc/elasticsearch/elasticsearch.yml
 /etc/elasticsearch/logging.yml
 /etc/kibana/kibana.yml</pre><h5><a id="id15609"></a>Recovery</h5><p>
   If you need to recover the configuration of one or more agents or services,
   the recommended procedure is as follows:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     If necessary, uninstall the agents or services, and install them again.
    </p></li><li class="listitem"><p>
     Stop the agents or services.
    </p></li><li class="listitem"><p>
     Copy the backup of your configuration files to the correct location
     according to the table above.
    </p></li><li class="listitem"><p>
     Start the agents or services again.
    </p></li></ol></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="id15620"></a>Dashboards</h4></div></div></div><p>
   Kibana can persist customized log dashboard designs to the Elasticsearch
   database, and allows you to recall them. For details on saving, loading, and
   sharing log management dashboards, refer to the
   <a class="link" href="https://www.elastic.co/guide/en/kibana/4.5/dashboard.html#saving-dashboards" target="_top">Kibana documentation</a>.
  </p><p>
   Grafana allows you to export a monitoring dashboard to a JSON file, and to
   re-import it when necessary. For backing up and restoring the exported
   dashboards, use the standard mechanisms of your file system. For details on
   exporting monitoring dashboards, refer to the
   <a class="link" href="https://grafana.com/docs/guides/getting_started/" target="_top">Getting
   Started</a> tutorial of Grafana.
  </p></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="data-virtualizations"></a>Working with Data Visualizations</h2></div></div></div><p>
  The user interface for monitoring your services, servers, and log data
  integrates with Grafana, an open source application for visualizing
  large-scale monitoring data. Use the options at the top border of the
  <span class="guimenu">Overview</span> page to access Grafana.
 </p><p>
  <span class="phrase">CMM</span> ships with preconfigured metrics dashboards. You can
  instantly use them for monitoring your environment. You can also use them as
  a starting point for building your own dashboards.
 </p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="metrics-dashboard-openstack"></a>Preconfigured Metrics Dashboard for <span class="productname">OpenStack</span></h3></div></div></div><p>
  As an <span class="productname">OpenStack</span> operator, you use the <span class="guimenu">Dashboard</span> option
  on the <span class="guimenu">Overview</span> page to view the metrics data on the
  <span class="productname">OpenStack</span> services. The Monitoring Service operator uses the
  <span class="guimenu">Dashboard</span> option to view the metrics data on the
  Monitoring Service.
 </p><div class="informalfigure"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="435"><tr><td><img src="images/images-grafana-os.png" width="435" alt="grafana-os.png" /></td></tr></table></div></div><p>
  To monitor <span class="productname">OpenStack</span>, the preconfigured dashboard shows the following:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    Status of the main <span class="productname">OpenStack</span> Services (<code class="literal">UP</code> or
    <code class="literal">DOWN</code>). Information on nova, neutron, glance, cinder,
    swift, and keystone is displayed.
   </p></li><li class="listitem"><p>
    Information on system resources.
   </p><p>
    The dashboard shows metrics data on CPU usage: the percentage of time the
    CPU is used in total (<code class="literal">cpu.percent</code>), at user level
    (<code class="literal">cpu.user_perc</code>), and at system level
    (<code class="literal">cpu.system_perc</code>), as well as the percentage of time the
    CPU is idle when no I/O requests are in progress
    (<code class="literal">cpu.wait_perc</code>).
   </p><p>
    The dashboard shows metrics data on memory usage: the number of megabytes
    of total memory (<code class="literal">mem.total_mb</code>), used memory
    (<code class="literal">mem.used_mb</code>), total swap memory
    (<code class="literal">mem.swap_total_mb</code>), and used swap memory
    (<code class="literal">mem.swap_used_mb</code>), as well as the number of megabytes
    used for the page cache (<code class="literal">mem.used_cache</code>).
   </p><p>
    The dashboard shows metrics data on the percentage of disk space that is
    being used on a device (<code class="literal">disk.space_used_perc</code>).
   </p><p>
    The dashboard shows metrics data on the <span class="phrase">CMM</span> system load
    over different periods (<code class="literal">load.avg_1_min</code>,
    <code class="literal">load.avg_5_min</code>, and <code class="literal">load.avg_15_min</code>).
   </p></li><li class="listitem"><p>
    The network usage of <span class="phrase">CMM</span>.
   </p><p>
    The dashboard shows the number of network bytes received and sent per
    second (<code class="literal">net.in_bytes_sec</code> and
    <code class="literal">net.out_bytes_sec</code>).
   </p></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="building-dashboards"></a>Building Dashboards</h3></div></div></div><p>
  Each metrics dashboard is composed of one or more panels that are arranged in
  one or more rows. A row serves as a logical divider within a dashboard. It
  organizes your panels in groups. The panel is the basic building block for
  visualizing your metrics data.
 </p><p>
  For building dashboards, you have two options:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    Start from scratch and create a new dashboard.
   </p></li><li class="listitem"><p>
    Take the dashboard that is shipped with <span class="phrase">CMM</span> as a starting
    point and customize it.
   </p></li></ul></div><p>
  The following sections provide introductory information on dashboards, rows,
  and panels, and make you familiar with the first steps involved in building a
  dashboard. For additional information, you can also refer to the
  <a class="link" href="https://grafana.com/docs/" target="_top">Grafana documentation</a>.
 </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="creating-dashboards"></a>Creating Dashboards</h3></div></div></div><p>
  To create a new dashboard, you use <span class="guimenu">Open Dashboard</span> in the
  top right corner of your dashboard window. The option provides access to
  various features for administrating dashboards. Use <span class="guimenu">New</span>
  to create an empty dashboard that serves as a starting point for adding rows
  and panels.
 </p><div class="informalfigure"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="435"><tr><td><img src="images/images-grafana-emptydb.png" width="435" alt="grafana-cmm.png" /></td></tr></table></div></div><p>
  On the left side of an empty dashboard, there is a green rectangle displayed.
  Hover over this rectangle to access a <span class="guimenu">Row</span> menu. To
  insert your first panel, you can use the options in the <span class="guimenu">Add
  Panel</span> submenu. See below for details on the available panel types.
 </p><p>
  As soon as you have inserted an empty panel, you can add additional rows. For
  this purpose, use the <span class="guimenu">Add Row</span> option on the right side
  of the dashboard.
 </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="editing-rows"></a>Editing Rows</h3></div></div></div><p>
  Features for editing rows can be accessed via the green rectangle that is
  displayed to the left of each row.
 </p><p>
  In addition to adding panels to a row, you can collapse or remove a row, move
  the position of the row within your dashboard, or set the row height. Row
  settings allows you, for example, to insert a row title or to hide the
  <span class="guimenu">Row</span> menu so that the row can no longer be edited.
 </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="editing-panels"></a>Editing Panels</h3></div></div></div><p>
  Grafana distinguishes between three panel types:
 </p><div class="informalfigure"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="435"><tr><td><img src="images/images-grafana-panels.png" width="435" alt="grafana-cmm.png" /></td></tr></table></div></div><p>
  Panels of type <span class="guimenu">Graph</span> are used to visualize metrics data.
  A query editor is provided to define the data to be visualized. The editor
  allows you to combine multiple queries. This means that any number of metrics
  and data series can be visualized in one panel.
 </p><p>
  Panels of type <span class="guimenu">Singlestat</span> are also used to visualize
  metrics data, yet they reduce a single query to a single number. The single
  number can be, for example, the minimum, maximum, average, or sum of values
  of the data series. The single number can be translated into a text value, if
  required.
 </p><p>
  Panels of type <span class="guimenu">Text</span> are used to insert static text. The
  text may, for example, provide information for the dashboard users. Text
  panels are not connected to any metrics data.
 </p><p>
  As soon as you have added a panel to your dashboard, you can access the
  options for editing the panel content. For this purpose, click the panel
  title and use <span class="guimenu">Edit</span>:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    For panels of type <span class="guimenu">Text</span>, a simple text editor is
    displayed for entering text. Plain text, HTML, and markdown format are
    supported.
   </p></li><li class="listitem"><p>
    For panels of type <span class="guimenu">Graph</span> and
    <span class="guimenu">Singlestat</span>, a query editor is displayed to define
    which data it to be shown. You can add multiple metrics, and apply
    functions to the metrics. The query results will be visualized in your
    panel in real time.
   </p></li></ul></div><p>
  A large number of display and formatting features are provided to customize
  how the content is presented in a panel. Click the panel title to access the
  corresponding options. The menu that is displayed also allows you to
  duplicate or remove a panel. To change the size of a panel, click the
  <span class="guimenu">+</span> and <span class="guimenu">-</span> icons.
 </p><p>
  You can move panels on your dashboard by simply dragging and dropping them
  within and between rows.
 </p><p>
  By default, the time range for panels is controlled by dashboard settings.
  Use the time picker in the top right corner of your dashboard window to
  define relative or absolute time ranges. You can also set an auto-refresh
  interval, or manually refresh the data that is displayed.
 </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="saving-sharing-dashboards"></a>Saving and Sharing Dashboards</h3></div></div></div><p>
  <span class="phrase">CMM</span> allows you to save your metrics dashboards locally.
  Saving a dashboard means exporting it to a JSON file. The JSON file can be
  edited, it can be shared with other users, and it can be imported to
  <span class="phrase">CMM</span> again.
 </p><p>
  To save a dashboard, use <span class="guimenu">Save</span> in the top right corner of
  your dashboard window. The option allows you to directly view the JSON syntax
  and export the dashboard to a JSON file. The JSON file can be forwarded to
  other users, if required. To import a JSON file, use <span class="guimenu">Open
  dashboard</span> in the top left corner of the dashboard window.
 </p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="defining-alarms"></a>Defining Alarms</h2></div></div></div><p>
  You have to define alarms to monitor your cloud resources. An alarm
  definition specifies the metrics to be collected and the threshold at which
  an alarm is to be triggered for a cloud resource. If the specified threshold
  is reached or exceeded, the alarm is triggered and notifications can be sent
  to inform users. By default, an alarm definition is evaluated every minute.
 </p><p>
  To handle a large variety of monitoring requirements, you can create either
  simple alarm definitions that refer to one metrics only, or compound alarm
  definitions that combine multiple metrics and allow you to track and process
  more complex events.
 </p><p>
  Example for a simple alarm definition that checks whether the system-level
  load of the CPU exceeds a threshold of 90 percent:
 </p><pre class="screen">cpu.system_perc{hostname=monasca} &gt; 90</pre><p>
  Example for a simple alarm definition that checks the average time of the
  system-level load of the CPU over a period of 480 seconds. The alarm is
  triggered only if this average is greater than 95 percent:
 </p><pre class="screen">avg(cpu.system_perc{hostname=monasca}, 120) &gt; 95 times 4</pre><p>
  Example for a compound alarm definition that evaluates two metrics. The alarm
  is triggered if either the system-level load of the CPU exceeds a threshold
  of 90 percent, or if the disk space that is used by the specified service
  exceeds a threshold of 90 percent:
 </p><pre class="screen">avg(cpu.system_perc{hostname=monasca}) &gt; 90 OR
max(disk.space_used_perc{service=monitoring}) &gt; 90</pre><p>
  To create, edit, and delete alarms, use <span class="guimenu">Monitoring &gt; Alarm
  Definitions</span>.
 </p><p>
  The elements that define an alarm are grouped into
  <span class="guimenu">Details</span>, <span class="guimenu">Expression</span>, and
  <span class="guimenu">Notifications</span>. They are described in the following
  sections.
 </p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="alarm-details"></a>Details</h3></div></div></div><p>
  For an alarm definition, you specify the following details:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    <span class="guimenu">Name</span>. Mandatory identifier of the alarm. The name must
    be unique within the project for which you define the alarm.
   </p></li><li class="listitem"><p>
    <span class="guimenu">Description</span>. Optional. A short description that
    depicts the purpose of the alarm.
   </p></li><li class="listitem"><p>
    <span class="guimenu">Severity</span>. The following severities for an alarm are
    supported: <span class="guimenu">Low</span> (default), <span class="guimenu">Medium</span>,
    <span class="guimenu">High</span>, or <span class="guimenu">Critical</span>.
   </p><p>
    The severity affects the status information on the
    <span class="guimenu">Overview</span> page. If an alarm that is defined as
    <span class="guimenu">Critical</span> is triggered, the corresponding resource is
    displayed in a red box. If an alarm that is defined as
    <span class="guimenu">Low</span>, <span class="guimenu">Medium</span>, or
    <span class="guimenu">High</span> is triggered, the corresponding resource is
    displayed in a yellow box only.
   </p><p>
    The severity level is subjective. Choose a level that is appropriate for
    prioritizing the alarms in your environment.
   </p></li></ul></div><div class="figure"><a id="id15792"></a><p class="title"><strong>Figure 4.2. Creating an Alarm Definition</strong></p><div class="figure-contents"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="90%"><tr><td><img src="images/socm-alarmdef-create.png" width="100%" alt="Creating an Alarm Definition" /></td></tr></table></div></div></div><br class="figure-break" /><h5><a id="id15799"></a>Expression</h5><p>
  The expression defines how to evaluate a metrics. The expression syntax is
  based on a simple expressive grammar. For details, refer to the
  <a class="link" href="https://github.com/openstack/monasca-api/blob/stable/ocata/docs/monasca-api-spec.md" target="_top">monasca API documentation</a>.
 </p><div class="informalfigure"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="90%"><tr><td><img src="images/socm-alarmdef-create-expression.png" width="100%" /></td></tr></table></div></div><p>
  To define an alarm expression, proceed as follows:
 </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Select the metrics to be evaluated.
   </p></li><li class="listitem"><p>
    Select a statistical function for the metrics: <code class="literal">min</code> to
    monitor the minimum values, <code class="literal">max</code> to monitor the maximum
    values, <code class="literal">sum</code> to monitor the sum of the values,
    <code class="literal">count</code> for the monitored number, or
    <code class="literal">avg</code> for the arithmetic average.
   </p></li><li class="listitem"><p>
    Enter one or multiple dimensions in the <span class="guimenu">Add a
    dimension</span> field to further qualify the metrics.
   </p><p>
    Dimensions filter the data to be monitored. They narrow down the evaluation
    to specific entities. Each dimension consists of a key/value pair that
    allows for a flexible and concise description of the data to be monitored,
    for example, region, availability zone, service tier, or resource ID.
   </p><p>
    The dimensions available for the selected metrics are displayed in the
    <span class="guimenu">Matching Metrics</span> section. Type the name of the key you
    want to associate with the metrics in the <span class="guimenu">Add a
    dimension</span> field. You are offered a select list for adding the
    required key/value pair.
   </p></li><li class="listitem"><p>
    Enter the threshold value at which an alarm is to be triggered, and combine
    it with a relational operator <code class="literal">&lt;</code>,
    <code class="literal">&gt;</code>, <code class="literal">&lt;=</code>, or
    <code class="literal">&gt;=</code>.
   </p><p>
    The unit of the threshold value is related to the metrics for which you
    define the threshold, for example, the unit is percentage for
    <code class="literal">cpu.idle_perc</code> or MB for
    <code class="literal">disk.total_used_space_mb</code>.
   </p></li><li class="listitem"><p>
    Switch on the <span class="guimenu">Deterministic</span> option if you evaluate a
    metrics for which data is received only sporadically. The option should be
    switched on, for example, for all log metrics. This ensures that the alarm
    status is <code class="literal">OK</code> and displayed as a green box on the
    <span class="guimenu">Overview</span> page although metrics data has not yet been
    received.
   </p><p>
    Do not switch on the option if you evaluate a metrics for which data is
    received regularly. This ensures that you instantly notice, for example,
    that a host machine is offline and that there is no metrics data for the
    agent to collect. On the <span class="guimenu">Overview</span> page, the alarm
    status therefore changes from <code class="literal">OK</code> to
    <code class="literal">UNDETERMINED</code> and is displayed as a gray box.
   </p></li><li class="listitem"><p>
    Enter one or multiple dimensions in the <span class="guimenu">Match by</span> field
    if you want these dimensions to be taken into account for triggering
    alarms.
   </p><p>
    Example: If you enter <code class="literal">hostname</code> as dimension, individual
    alarms will be created for each host machine on which metrics data is
    collected. The expression you have defined is not evaluated as a whole but
    individually for each host machine in your environment.
   </p><p>
    If <span class="guimenu">Match by</span> is set to a dimension, the number of
    alarms depends on the number of dimension values on which metrics data is
    received. An empty <span class="guimenu">Match by</span> field results in exactly
    one alarm.
   </p><p>
    To enter a dimension, you can simply type the name of the dimension in the
    <span class="guimenu">Match by</span> field. The dimensions you enter cannot be
    changed once the alarm definition is saved.
   </p></li><li class="listitem"><p>
    Build a compound alarm definition to combine multiple metrics in one
    expression. Using the logical operators <code class="literal">AND</code> or
    <code class="literal">OR</code>, any number of sub-expressions can be combined.
   </p><p>
    Use the <span class="guimenu">Add</span> button to create a second expression, and
    choose either <code class="literal">AND</code> or <code class="literal">OR</code> as
    <span class="guimenu">Operator</span> to connect it to the one you have already
    defined. Proceed with the second expression as described in Step 1 to Step
    6 above.
   </p><p>
    The following options are provided for creating and organizing compound
    alarm definitions:
   </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
      Create additional sub-expressions using the <span class="guimenu">Add</span>
      button.
     </p></li><li class="listitem"><p>
      Finish editing a sub-expression using the <span class="guimenu">Submit</span>
      button.
     </p></li><li class="listitem"><p>
      Delete a sub-expression using the <span class="guimenu">Remove</span> button.
     </p></li><li class="listitem"><p>
      Change the position of a sub-expression using the <span class="guimenu">Up</span>
      or <span class="guimenu">Down</span> button.
     </p></li></ul></div></li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
   You can also edit the expression syntax directly. For this purpose, save
   your alarm definition and update it using the <span class="guimenu">Edit Alarm
   Definition</span> option.
  </p><p>
   By default, an alarm definition is evaluated every minute. When updating the
   alarm definition, you can change this interval. For syntax details, refer to
   the monasca API documentation on
   <a class="link" href="https://github.com/openstack/monasca-api/blob/stable/ocata/docs/monasca-api-spec.md#alarm-definition-expressions" target="_top">Alarm Definition Expressions</a>.
  </p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="alarm-notifications"></a>Notifications</h3></div></div></div><p>
  You can enable notifications for an alarm definition. As soon as an alarm is
  triggered, the enabled notifications will be sent.
 </p><div class="informalfigure"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="90%"><tr><td><img src="images/socm-alarmdef-create-notifications.png" width="100%" /></td></tr></table></div></div><p>
  The <span class="guimenu">Notifications</span> tab allows you to select the
  notifications from the ones that are predefined in your environment. For a
  selected notification, you specify whether you want to send it for a status
  transition to <span class="guimenu">Alarm</span>, <span class="guimenu">OK</span>, and/or
  <span class="guimenu">Undetermined</span>.
 </p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="defining-notifcations"></a>Defining Notifications</h2></div></div></div><p>
  Notifications define how users are informed when a threshold value defined
  for an alarm is reached or exceeded. In the alarm definition, you can assign
  one or multiple notifications.
 </p><p>
  For a notification, you specify the following elements:\o/
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    <span class="guimenu">Name</span>. A unique identifier of the notification. The
    name is offered for selection when defining an alarm.
   </p></li><li class="listitem"><p>
    <span class="guimenu">Type</span>. <code class="literal">Email</code> is the notification
    method supported by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring. If
    you want to use <code class="literal">WebHook</code> or <code class="literal">PagerDuty</code>,
    contact your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring support for
    further information.
   </p></li><li class="listitem"><p>
    <span class="guimenu">Address</span>. The email address to be notified when an
    alarm is triggered.
   </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
     Generic top-level domains such as business domain names are not supported
     in email addresses (for example, <code class="literal">user@xyz.company</code>).
    </p></div></li></ul></div><p>
  To create, edit, and delete notifications, use <span class="guimenu">Monitoring &gt;
  Notifications</span>.
 </p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="status-servers-log-data"></a>Status of Services, Servers, and Log Data</h2></div></div></div><p>
  An alarm definition for a service, server, or log data is evaluated over the
  interval specified in the alarm expression. The alarm definition is
  re-evaluated in each subsequent interval. The following alarm statuses are
  distinguished:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    <code class="literal">Alarm</code>. The alarm expression has evaluated to
    <code class="literal">true</code>. An alarm has been triggered for the cloud
    resource.
   </p></li><li class="listitem"><p>
    <code class="literal">OK</code>. The alarm expression has evaluated to
    <code class="literal">false</code>. There is no need to trigger an alarm.
   </p></li><li class="listitem"><p>
    <code class="literal">Undetermined</code>. No metrics data has been received within
    the defined interval.
   </p></li></ul></div><p>
  As soon as you have defined an alarm for a cloud resource, there is status
  information displayed for it on the <span class="guimenu">Overview</span> page:
 </p><p>
  The color of the boxes in the three sections indicates the status:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    A green box for a service or server indicates that it is up and running. A
    green box for a log path indicates that a defined threshold for errors or
    warnings, for example, has not yet been reached or exceeded. There are
    alarms defined for the services, servers, or log paths, but no alarms have
    been triggered.
   </p></li><li class="listitem"><p>
    A red box for a service, server, or log path indicates that there is a
    severe problem that needs to be checked. One or multiple alarms defined for
    a service, a server, or log data have been triggered.
   </p></li><li class="listitem"><p>
    A yellow box indicates a problem. One or multiple alarms have already been
    triggered, yet, the severity of these alarms is low.
   </p></li><li class="listitem"><p>
    A gray box indicates that alarms have been defined. Yet, metrics data has
    not been received.
   </p></li></ul></div><p>
  The status information on the <span class="guimenu">Overview</span> page results from
  one or multiple alarms that have been defined for the corresponding resource.
  If multiple alarms are defined, the severity of the individual alarms
  controls the status color.
 </p><p>
  You can click a resource on the <span class="guimenu">Overview</span> page to display
  details on the related alarms. The details include the status of each alarm
  and the expression that is evaluated. For each alarm, you can drill down on
  the alarm history. To narrow down the problem, the history presents detailed
  information on the status transitions.
 </p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="supported-metrics"></a>Supported Metrics</h2></div></div></div><p>
  The sections below describe the metrics supported by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    Standard metrics for general monitoring of servers and networks.
   </p></li><li class="listitem"><p>
    Additional metrics for monitoring specific servers and services.
   </p></li></ul></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id15967"></a>Standard Metrics</h3></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring supports the following
   standard metrics for monitoring servers and networks. These metrics usually
   do not require specific settings. The metrics are grouped by metrics types.
   Each metrics type references a set of related metrics.
  </p><h5><a id="id15973"></a>cpu.yaml</h5><p>
   Metrics on CPU usage, e.g. the percentage of time the CPU is idle when no I/O
   requests are in progress, or the percentage of time the CPU is used at system
   level or user level.
  </p><h5><a id="id15975"></a>disk.yaml</h5><p>
   Metrics on disk space, e.g. the percentage of disk space that is used on a
   device, or the total amount of disk space aggregated across all the disks on
   a particular node.
  </p><h5><a id="id15977"></a>load.yaml</h5><p>
   Metrics on the average system load over different periods (e.g. 1 minute, 5
   minutes, or 15 minutes).
  </p><h5><a id="id15979"></a>memory.yaml</h5><p>
   Metrics on memory usage, e.g. the number of megabytes of total memory or free
   memory, or the percentage of free swap memory.
  </p><h5><a id="id15981"></a>network.yaml</h5><p>
   Metrics on the network, e.g. the number of network bytes received or sent per
   second, or the number of network errors on incoming or outgoing network
   traffic per second.
  </p><p>
   These metrics are configured automatically on all machines and nodes that have
   the <code class="literal">monasca-agent</code> role assigned. This applies not only to
   <code class="filename">network.yaml</code> but also to all metrics covered in this chapter.
  </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="id15986"></a>Additional Metrics</h3></div></div></div><p>
   In addition to the standard metrics, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring automatically
   adds the following
   additional metrics to the monasca agent configuration on the <span class="productname">OpenStack</span>
   Controller.
   </p><h5><a id="id15993"></a>http_check.yaml</h5><p>
   HTTP endpoint checks perform up/down checks on HTTP endpoints. Based on a
   list of URLs, the agent sends an HTTP request and reports success or failure
   to the Monitoring Service.
  </p><p>
   The following barclamps will automatically create an HTTP check for the API
   services they deploy if the <code class="literal">monasca</code> barclamp is active:
  </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>barbican</p></li><li class="listitem"><p>cinder</p></li><li class="listitem"><p>glance</p></li><li class="listitem"><p>heat</p></li><li class="listitem"><p>keystone</p></li><li class="listitem"><p>Magnum</p></li><li class="listitem"><p>manila</p></li><li class="listitem"><p>neutron</p></li><li class="listitem"><p>nova</p></li><li class="listitem"><p>sahara</p></li><li class="listitem"><p>swift</p></li></ul></div><p>
   By default, the monitoring dashboard is configured to display the service status for the following services:
  </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>cinder</p></li><li class="listitem"><p>glance</p></li><li class="listitem"><p>keystone</p></li><li class="listitem"><p>neutron</p></li><li class="listitem"><p>nova</p></li><li class="listitem"><p>swift</p></li></ul></div><p>
   The status visualization for additional services can be added manually.
  </p><h5><a id="id16035"></a>postgres.yaml</h5><p>
   Postgres checks gather various CRUD and system statistics for a database
   hosted by a PostgreSQL DBMS.
  </p><p>
   The following barclamps will automatically create Postgres checks for their
   service database if the <code class="literal">monasca</code> barclamp is active:
  </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>barbican</p></li><li class="listitem"><p>cinder</p></li><li class="listitem"><p>glance</p></li><li class="listitem"><p>heat</p></li><li class="listitem"><p>keystone</p></li><li class="listitem"><p>Magnum</p></li><li class="listitem"><p>manila</p></li><li class="listitem"><p>neutron</p></li><li class="listitem"><p>nova</p></li><li class="listitem"><p>sahara</p></li></ul></div></div></div></div><div class="chapter"><div class="titlepage"><div><div><h1 class="title"><a id="monitoring-log-management"></a>Chapter 5. Log Management</h1></div></div></div><div class="toc"><p><strong>Table of Contents</strong></p><dl class="toc"><dt><span class="section"><a href="#configure-log-management-window">Working with the Log Management Window</a></span></dt><dd><dl><dt><span class="section"><a href="#query-log-data">Querying Log Data</a></span></dt><dt><span class="section"><a href="#visualize-query-results">Visualizing Query Results</a></span></dt><dt><span class="section"><a href="#combine-visualizations-dashboard">Combining Visualizations in Dashboards</a></span></dt><dt><span class="section"><a href="#filter-query-dashboard">Filtering Query Results in Dashboards</a></span></dt><dt><span class="section"><a href="#sharing-dashboards">Sharing Dashboards</a></span></dt></dl></dd><dt><span class="section"><a href="#configure-index-patterns">Configuring Index Patterns</a></span></dt><dt><span class="section"><a href="#id16347">Monitoring Log Data</a></span></dt><dt><span class="section"><a href="#cha-deploy-logs">Log Files</a></span></dt><dd><dl><dt><span class="section"><a href="#sec-deploy-logs-adminserv">On the Administration Server</a></span></dt><dt><span class="section"><a href="#sec-deploy-logs-crownodes">On All Other Crowbar Nodes</a></span></dt><dt><span class="section"><a href="#sec-deploy-logs-contrnode">On the Control Node(s)</a></span></dt><dt><span class="section"><a href="#sec-deploy-logs-compnode">On Compute Nodes</a></span></dt></dl></dd></dl></div><p>
  Services and servers in a cloud infrastructure generate huge amounts of log
  data, from simply announcing that everything is healthy to detailed
  information on events or processes. The logs are distributed over many
  physical and virtual servers. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring
  collects, stores, and queries these logs and makes them
  accessible to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring users in one
  central place.
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring covers all aspects of an
  enterprise-class log management solution:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    Central management of log data from medium and large-size <span class="productname">OpenStack</span>
    deployments.
   </p></li><li class="listitem"><p>
    Buffered exchange of log data to provide high data throughput.
   </p></li><li class="listitem"><p>
    Logical isolation of log data through multi-tenancy.
   </p></li><li class="listitem"><p>
    Scalable architecture capable of mastering the data load from a large
    number of nodes in <span class="productname">OpenStack</span> environments. The log management solution can
    be horizontally and vertically adapted to constantly changing data loads.
   </p></li></ul></div><p>
  For managing the log data of your services and the virtual and physical
  servers on which they are provisioned, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring
  integrates with Kibana, an open source analytics and
  visualization platform. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring uses
  Kibana as a front-end application to the log data held in the Elasticsearch
  database.
 </p><p>
  Kibana allows you to easily understand large data volumes. Based on the data
  that is stored in Elasticsearch indices, you can perform advanced data
  analysis and visualize your log data in a variety of charts, tables, or maps.
  Changes to the Elasticsearch indices are displayed in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring
  in real time.
 </p><p>
  The log management features of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring
  include:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    Features for searching, visualizing, and analyzing the log data.
   </p></li><li class="listitem"><p>
    Alerting features for monitoring.
   </p></li></ul></div><p>
  In the following sections, you will find information on the Log Management
  Window where you search, visualize, and analyze your log data, as well as
  details on how to use the alerting features.
 </p><h5><a id="id16106"></a>Accessing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring
 </h5><p>
  For accessing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring and performing
  log management tasks, the following prerequisites must be fulfilled:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    You must have access to the <span class="productname">OpenStack</span> platform as a user with the
    <code class="literal">monasca-user</code> role.
   </p></li><li class="listitem"><p>
    You must be assigned to the <span class="productname">OpenStack</span> project you want to monitor.
   </p></li></ul></div><p>
  Log in to <span class="productname">OpenStack</span> horizon with your user name and password. The functions
  you can use in <span class="productname">OpenStack</span> horizon depend on your access permissions. To access
  logs and metrics, switch to the <span class="guimenu">monasca</span> tenant in
  horizon.
 </p><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring functionality is
  available on the <span class="guimenu">Monitoring</span> tab. It provides access to
  the log data of all projects to which you are assigned. The <span class="guimenu">Log
  Management</span> option at the top border of the
  <span class="guimenu">Overview</span> page displays the log management window where
  you can work on the log data.
 </p><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="configure-log-management-window"></a>Working with the Log Management Window</h2></div></div></div><p>
  Index patterns determine which data from the underlying Elasticsearch
  database can be viewed and analyzed in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring's
  log management window. Index patterns are used to
  identify the Elasticsearch indices to run search and analytics against.
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring ships with a preconfigured
  index pattern which allows you to instantly view and analyze your log data
  when accessing the log management window for the first time. You can
  configure additional index patterns to view and analyze different data from
  different indices.
 </p><p>
  Search queries allow you to search the Elasticsearch indices for data that
  match your information requirements. The query results can be graphically
  represented in visualizations, and visualizations can be organized in
  dashboards.
 </p><p>
  The log management window provides features for:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    Querying log data.
   </p></li><li class="listitem"><p>
    Visualizing query results.
   </p></li><li class="listitem"><p>
    Combining visualizations in dashboards.
   </p></li><li class="listitem"><p>
    Filtering query results in dashboards.
   </p></li><li class="listitem"><p>
    Sharing dashboards.
   </p></li></ul></div><p>
  The following sections provide an introduction to queries, visualizations,
  and dashboards. For additional details, refer to the
  <a class="link" href="https://www.elastic.co/guide/en/kibana/4.5/index.html" target="_top">Kibana
  documentation</a>.
 </p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="query-log-data"></a>Querying Log Data</h3></div></div></div><p>
  For querying log data, you use the <span class="guimenu">Discover</span> page in the
  log management window. It is instantly displayed when you access the window.
  It shows the most recently collected log data:
 </p><div class="figure"><a id="id16162"></a><p class="title"><strong>Figure 5.1. The Kibana Dashboard—Discover Page</strong></p><div class="figure-contents"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="100%"><tr><td><img src="images/socm-kibana-dashboard.png" width="100%" alt="The Kibana Dashboard—Discover Page" /></td></tr></table></div></div></div><br class="figure-break" /><p>
  The <span class="guimenu">Discover</span> page allows you to access the log data in
  every index that matches the current index pattern. In addition to submitting
  queries, you can view, filter, and analyze the log data that is returned by
  your queries.
 </p><p>
  On the <span class="guimenu">Discover</span> page the following elements assist you
  in analyzing your log data:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    Below the main navigation bar at the top of the window, there is a
    <span class="bold"><strong>search box</strong></span> for querying your log data. By
    submitting a query, you search all indices that match the current index
    pattern. The name of the current index pattern is displayed directly below
    the search box on the left side. You can select a different index pattern,
    if required.
   </p><p>
    For entering strings in the search box, use the Lucene query syntax. Kibana
    also supports the Elasticsearch Query DSL. For details, refer to the
    <a class="link" href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/query-dsl.html" target="_top">Elasticsearch Reference documentation</a>.
   </p></li><li class="listitem"><p>
    Use the <span class="bold"><strong>clock icon</strong></span> at the top right border
    of the log management window to define a time range for filtering the log
    data. By default, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring displays
    the log data collected during the last 15 minutes. You can deviate from
    this default. Multiple options are provided for defining relative or
    absolute time ranges. The time range you define is instantly applied to all
    log data.
   </p></li><li class="listitem"><p>
    In the bottom right part of the <span class="guimenu">Discover</span> page, you can
    view the <span class="bold"><strong>log data</strong></span> returned by your search
    queries. Depending on whether you have filtered the data by index fields,
    the log data is either restricted to these fields or entire records are
    displayed.
   </p></li><li class="listitem"><p>
    On the left side of the <span class="guimenu">Discover</span> page below the search
    box, you see the <span class="bold"><strong>index fields</strong></span> from the
    indices that match the current index pattern. You can select individual
    fields to modify which log data is displayed on the right side.
   </p><p>
    Select a field from the <span class="guimenu">Available Fields</span> section for
    this purpose and use <span class="guimenu">Add</span>. To remove a field, select it
    in the <span class="guimenu">Selected Fields</span> section and use
    <span class="guimenu">Remove</span>.
   </p><p>
    From the field list, you can expand a field by simply clicking it. This
    shows the most common values for the field. You can also set field values
    as filter, or you can exclude log data with specific field values.
   </p></li><li class="listitem"><p>
    If a time field is configured for the current index pattern, the
    distribution of log entries over time is displayed in a
    <span class="bold"><strong>histogram</strong></span> in the top right part of the
    <span class="guimenu">Discover</span> page.
   </p><p>
    By default, the histogram shows the number of logs entries versus time,
    matched by the underlying query and time filter. You can click the bars in
    the histogram to narrow down the time filter.
   </p></li></ul></div><p>
  Queries can be saved and re-used. They can also be shared with other users.
  For this purpose, use the options to the right of the search box at the top
  border of the log management window:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    To save a query, use <span class="guimenu">Save Search</span>. Saving a query means
    saving both the query syntax and the current index pattern.
   </p></li><li class="listitem"><p>
    To load a query, use <span class="guimenu">Load Saved Search</span>. A saved query
    can be loaded and used by any <span class="productname">OpenStack</span> or Monitoring Service operator.
   </p></li><li class="listitem"><p>
    To share a query with other users, use <span class="guimenu">Share Search</span>.
    The option displays a direct link to the query that you can forward. As a
    prerequisite for using a direct link, a user must have <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring
    access.
   </p></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="visualize-query-results"></a>Visualizing Query Results</h3></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring supports you in building
  graphical representations of your query results. You can choose from
  different visualization types, for example, pie charts, data tables, line
  charts, or vertical bar charts. For visualizing your results, you use the
  <span class="guimenu">Visualize</span> page in the log management window:
 </p><div class="informalfigure"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="90%"><tr><td><img src="images/socm-kibana-visualize.png" width="100%" /></td></tr></table></div></div><p>
  To create a visualization, use <span class="guimenu">New Visualization</span> to the
  right of the search box at the top border of the window. You have to select a
  visualization type and the query to be used. You can either create a new
  query or load a query you have already saved.
 </p><p>
  Based on the visualization type and the query, you can proceed with designing
  the graphical representation in a visualization editor. Multiple design
  options and a preview function are provided for creating, modifying, and
  viewing the graphical representation.
 </p><p>
  You can save and re-use visualizations. You can also share them with other
  users. For this purpose, use the options to the right of the search box at
  the top border of the log management window:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    To save a visualization, use <span class="guimenu">Save Visualization</span>.
   </p></li><li class="listitem"><p>
    To load a visualization, use <span class="guimenu">Load Saved Visualization</span>.
    A saved visualization can be loaded and used by any <span class="productname">OpenStack</span> or Monitoring
    Service operator.
   </p></li><li class="listitem"><p>
    To share a visualization with other users, use <span class="guimenu">Share
    Visualization</span>. The option displays an HTML snippet that can be
    used to embed the visualization in a Web page. It also displays a direct
    link to the visualization that you can forward. As a prerequisite for using
    an embedded visualization or a direct link, a user must have 
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring access.
   </p></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="combine-visualizations-dashboard"></a>Combining Visualizations in Dashboards</h3></div></div></div><p>
  For correlating related information or providing an overview, you can combine
  visualizations in dashboards. Use the <span class="guimenu">Dashboard</span> page in
  the log management window for this purpose:
 </p><p>
  To create a dashboard from scratch, you use <span class="guimenu">New
  Dashboard</span> to the right of the search box at the top border of the
  window. To add a visualization from a list of existing visualizations, use
  <span class="guimenu">Add Visualization</span>. You need at least one saved
  visualization to create a dashboard. In addition to adding visualizations,
  you can also place the tabular output of query results on your dashboards.
  Switch to the <span class="guimenu">Searches</span> tab when adding a visualization,
  and select a saved query. This adds the query result to your dashboard.
 </p><p>
  A visualization or query result is displayed in a container on your
  dashboard. Various options are provided for arranging containers:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    Move a container by clicking and dragging its title bar.
   </p></li><li class="listitem"><p>
    Resize a container by dragging its bottom right corner.
   </p></li><li class="listitem"><p>
    Remove a container using <span class="guimenu">Delete</span> in the top right
    corner of the container.
   </p></li></ul></div><p>
  Using <span class="guimenu">Edit</span> in the top right corner of a container, you
  can switch to the <span class="guimenu">Visualize</span> or
  <span class="guimenu">Discover</span> page. This allows you to design the graphical
  representation or edit the query. To view the raw data behind a
  visualization, you can click the bar at the bottom of the container. This
  replaces your visualization by the underlying raw data. You can export the
  raw data, if required.
 </p><p>
  For each dashboard, you can configure a refresh interval to automatically
  refresh its content with the latest data. The current interval is displayed
  in the top right border of the log management window. Click the interval if
  you want to change it. You can define the interval in absolute or relative
  terms. Use <span class="guimenu">Auto-Refresh</span> next to the interval in the
  border of the log management window to instantly submit the underlying
  queries and refresh the dashboard content.
 </p><p>
  By default, dashboards are displayed with a light background. Using
  <span class="guimenu">Options</span> in the top right border of the log management
  window, you can switch to a dark color scheme.
 </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="filter-query-dashboard"></a>Filtering Query Results in Dashboards</h3></div></div></div><p>
  By submitting a query on the data displayed in a dashboard, you can filter
  out specific sets of data that you want to aggregate while not changing the
  logic of the individual visualizations.
 </p><p>
  Use the search box below the main navigation bar at the top of the log
  management window for entering a query on the whole dashboard. If a
  visualization is already based on a saved query, both queries apply.
 </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="sharing-dashboards"></a>Sharing Dashboards</h3></div></div></div><p>
  Dashboards can be saved and re-used. They can also be shared with other
  users. For this purpose, use the options to the right of the search box at
  the top border of the log management window:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    To save a dashboard, use <span class="guimenu">Save Dashboard</span>. By default,
    saving a dashboard also saves the time filter that is defined at the time
    of saving. You can disable this default by clearing the <span class="guimenu">Store
    time with dashboard</span> option. Disabling the default means that the
    time filter is set to the currently selected time each time the dashboard
    is loaded.
   </p></li><li class="listitem"><p>
    To load a dashboard, use <span class="guimenu">Load Saved Dashboard</span>. A saved
    dashboard can be loaded and used by any <span class="productname">OpenStack</span> or Monitoring Service
    operator.
   </p></li><li class="listitem"><p>
    To share a dashboard with other users, use <span class="guimenu">Share
    Dashboard</span>. The option displays an HTML snippet that can be used
    to embed the dashboard in a Web page. It also displays a direct link to the
    dashboard that you can forward. As a prerequisite for using an embedded
    dashboard or a direct link, a user must have <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring
    access.
   </p></li></ul></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="configure-index-patterns"></a>Configuring Index Patterns</h2></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring enables the dynamic mapping
  of fields. After configuring an index pattern, the indices that match the
  pattern are automatically scanned to display the list of index fields. This
  guarantees that the fields are correctly visualized in the dashboard.
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring ships with a preconfigured
  index pattern that allows you to instantly explore your Elasticsearch indices
  when accessing the dashboard for the first time. You can create additional
  patterns to view and analyze specific sets of data. One or multiple patterns
  can be created per project. When you create additional patterns, you have to
  set one of them as the default.
 </p><p>
  To configure an additional index pattern, use <span class="guimenu">Settings &gt;
  Indices</span>. Click the index pattern that is displayed in the
  <span class="guimenu">Index Patterns</span> field on the left, and use the
  <span class="guimenu">Add New</span> option.
 </p><p>
  Indices that match the pattern you define must exist in the Elasticsearch
  database, and they must contain data. For an index pattern, you specify the
  following elements:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    <span class="guimenu">Index contains time-based events</span>. It is recommended
    that this option is selected. This improves search performance by enabling
    searches only on those indices that contain data on time-based events.
   </p></li><li class="listitem"><p>
    <span class="guimenu">Use event times to create index names</span>. It is
    recommended that this option is selected. This improves search performance
    by enabling searches only on those indices that contain data in the time
    range you specify.
   </p></li><li class="listitem"><p>
    <span class="guimenu">Index pattern interval</span>. Select
    <code class="literal">Daily</code> as index pattern interval. Daily intervals are
    supported by the Monitoring Service.
   </p></li><li class="listitem"><p>
    <span class="guimenu">Index name or pattern</span>. The pattern allows you to
    define dynamic index names. Static text in a pattern is denoted using
    brackets. Replace the predefined pattern (<code class="literal">[logstash-]*</code>
    or <code class="literal">[logstash-]YYYY.MM.DD</code>) as follows:
   </p><p>
    Replace <code class="literal">logstash-</code> by the project ID of the <span class="productname">OpenStack</span>
    project whose log data is to be visualized in the dashboard.
   </p><p>
    Replace <code class="literal">*</code> or <code class="literal">YYYY.MM.DD</code> by
    <code class="literal">YYYY-MM-DD</code> as naming pattern. This naming pattern is
    supported by the Monitoring Service.
   </p><p>
    Example: <code class="literal">[557aff4bf007473d84069aca202a1633-]YYYY-MM-DD</code>
   </p></li><li class="listitem"><p>
    <span class="guimenu">Time-field name</span>. Select <code class="literal">@timestamp</code>
    as time-field name. <code class="literal">@timestamp</code> matches the
    <code class="literal">YYYY-MM-DD</code> naming pattern.
   </p></li></ul></div><p>
  The default index pattern is automatically loaded when you access the log
  management window. It is marked with an asterisk in front of the pattern name
  in the <span class="guimenu">Index Patterns</span> field at the top left corner of
  the <span class="guimenu">Settings</span> page. Select the pattern you want to set as
  the default from the <span class="guimenu">Index Patterns</span> field. The
  content of the log management window is instantly updated.
 </p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="id16347"></a>Monitoring Log Data</h2></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring provides alerting features
  for monitoring your log data. Specific log metrics support you in checking
  the severity of the entries in your log files. Log metrics are handled like
  any other metrics in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring. They
  complete the log management features and support you in analyzing and
  troubleshooting any issue that you encounter in your log data.
 </p><p>
  Using the log metrics for monitoring corresponds to using any other metrics:
 </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
    Use <span class="guimenu">Monitoring &gt; Alarm Definitions</span> to create, edit,
    and delete alarms for log data.
   </p></li><li class="listitem"><p>
    Use <span class="guimenu">Monitoring &gt; Notifications</span> to create, edit, and
    delete notifications for alarms.
   </p></li><li class="listitem"><p>
    Use <span class="guimenu">Monitoring &gt; Overview</span> to check whether there
    are any irregularities in your log data. As soon as you have defined an
    alarm for your log data and metrics data has been received, there is status
    information displayed on the <span class="guimenu">Overview</span> page.
   </p></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="cha-deploy-logs"></a>Log Files</h2></div></div></div><p>
  Find a list of log files below, sorted according to the nodes where they
  can be found.
 </p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="sec-deploy-logs-adminserv"></a>On the Administration Server</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: bullet; "><li class="listitem" style="list-style-type: disc"><p>
     Crowbar Web Interface:
     <code class="filename">/var/log/crowbar/production.log</code>
    </p></li><li class="listitem" style="list-style-type: disc"><p>
     Chef server: <code class="filename">/var/log/chef/server.log</code>
    </p></li><li class="listitem" style="list-style-type: disc"><p>
     Chef expander: <code class="filename">/var/log/chef/expander.log</code>
    </p></li><li class="listitem" style="list-style-type: disc"><p>
     Chef client (for the Administration Server only):
     <code class="filename">/var/log/chef/client.log </code>
    </p></li><li class="listitem" style="list-style-type: disc"><p>
     Upgrade log files (only available if the Administration Server has been upgraded
     from a previous version using <span class="command"><strong>suse-cloud-upgrade</strong></span>):
     <code class="filename">/var/log/crowbar/upgrade/*</code>
    </p></li><li class="listitem" style="list-style-type: disc"><p>
     Apache SOLR (Chef's search server):
     <code class="filename">/var/log/chef/solr.log</code>
    </p></li><li class="listitem" style="list-style-type: disc"><p>
     HTTP (AutoYaST) installation server for provisioner barclamp:
     <code class="filename">/var/log/apache2/provisioner-{access,error}_log</code>
    </p></li><li class="listitem" style="list-style-type: disc"><p>
     Log file from mirroring SMT repositories (optional):
     <code class="filename">/var/log/smt/smt-mirror.log</code>
    </p></li><li class="listitem" style="list-style-type: disc"><p>
     Default SUSE log files: <code class="filename">/var/log/messages</code>,
     <code class="filename">/var/log/zypper.log </code> etc.
    </p></li><li class="listitem" style="list-style-type: disc"><p>
     Syslogs for all nodes: <code class="filename">/var/log/nodes/*.log</code> (these
     are collected via remote syslogging)
    </p></li><li class="listitem" style="list-style-type: disc"><p>
     Other client node log files saved on the Administration Server:
    </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: bullet; "><li class="listitem" style="list-style-type: disc"><p>
       <code class="filename">/var/log/crowbar/sledgehammer/d*.log</code>: Initial
       Chef client run on nodes booted using PXE prior to discovery by
       Crowbar.
      </p></li><li class="listitem" style="list-style-type: disc"><p>
       <code class="filename">/var/log/crowbar/chef-client/d*.log</code>: Output from
       Chef client when proposals are applied to nodes. This is the
       first place to look if a barclamp proposal fails to apply.
      </p></li></ul></div></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="sec-deploy-logs-crownodes"></a>On All Other Crowbar Nodes</h3></div></div></div><p>
   Logs for when the node registers with the Administration Server:
  </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: bullet; "><li class="listitem" style="list-style-type: disc"><p>
     <code class="filename">/var/log/crowbar/crowbar_join/errlog</code>
    </p></li><li class="listitem" style="list-style-type: disc"><p>
     <code class="filename">/var/log/crowbar/crowbar_join/$TOPIC.{log,err}</code>:
     STDOUT/STDERR from running commands associated with $TOPIC when the
     node joins the Crowbar cluster. $TOPIC can be:
    </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: bullet; "><li class="listitem" style="list-style-type: disc"><p>
       <code class="filename">zypper</code>: package management activity
      </p></li><li class="listitem" style="list-style-type: disc"><p>
       <code class="filename">ifup</code>: network configuration activity
      </p></li><li class="listitem" style="list-style-type: disc"><p>
       <code class="filename">Chef</code>: Chef client activity
      </p></li><li class="listitem" style="list-style-type: disc"><p>
       <code class="filename">time</code>: starting of ntp client
      </p></li></ul></div></li><li class="listitem" style="list-style-type: disc"><p>
     Chef client log: <code class="filename">/var/log/chef/client.log</code>
    </p></li><li class="listitem" style="list-style-type: disc"><p>
     Default SUSE log files: <code class="filename">/var/log/messages</code>,
     <code class="filename">/var/log/zypper.log </code> etc.
    </p></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="sec-deploy-logs-contrnode"></a>On the Control Node(s)</h3></div></div></div><p>
   On setups with multiple Control Nodes log files for certain services
   (such as <code class="filename">keystone.log</code>) are only available on the
   nodes where the services are deployed.
  </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: bullet; "><li class="listitem" style="list-style-type: disc"><p>
     <code class="filename">/var/log/apache2/openstack-dashboard-*</code>: Logs for
     the <span class="productname">OpenStack</span> Dashboard
    </p></li><li class="listitem" style="list-style-type: disc"><p>
     <code class="filename">/var/log/ceilometer/*</code>: ceilometer log files.
    </p></li><li class="listitem" style="list-style-type: disc"><p>
     <code class="filename">/var/log/cinder/*</code>: cinder log files.
    </p></li><li class="listitem" style="list-style-type: disc"><p>
     <code class="filename">/var/log/glance/*</code>: glance; log files.
    </p></li><li class="listitem" style="list-style-type: disc"><p>
     <code class="filename">/var/log/heat/*</code>: heat log files.
    </p></li><li class="listitem" style="list-style-type: disc"><p>
     <code class="filename">/var/log/keystone/*</code>: keystone log files.
    </p></li><li class="listitem" style="list-style-type: disc"><p>
     <code class="filename">/var/log/neutron/*</code>: neutron log files.
    </p></li><li class="listitem" style="list-style-type: disc"><p>
     <code class="filename">/var/log/nova/*</code>: various log files relating to
     nova services.
    </p></li><li class="listitem" style="list-style-type: disc"><p>
     <code class="filename">/var/log/rabbitmq/*</code>: RabbitMQ log files.
    </p></li><li class="listitem" style="list-style-type: disc"><p>
     <code class="filename">/var/log/swift/*</code>: swift log files.
    </p></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="sec-deploy-logs-compnode"></a>On Compute Nodes</h3></div></div></div><p>
   <code class="filename">/var/log/nova/nova-compute.log</code>
  </p></div></div></div><div class="chapter"><div class="titlepage"><div><div><h1 class="title"><a id="troubleshooting"></a>Chapter 6. Troubleshooting</h1></div></div></div><div class="toc"><p><strong>Table of Contents</strong></p><dl class="toc"><dt><span class="section"><a href="#shard-failures">Shard Failures</a></span></dt><dt><span class="section"><a href="#something-went-wrong">Oops! Looks like something went wrong</a></span></dt><dt><span class="section"><a href="#no-results">No results found</a></span></dt></dl></div><p>
  Before contacting support to help you with a problem on SUSE <span class="productname">OpenStack</span> Cloud, we
  strongly recommended that you gather as much information about your system
  and the problem as possible. For this purpose, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> ships with a
  tool called <span class="command"><strong>supportconfig</strong></span>. It gathers system information
  such as the current kernel version being used, the hardware, RPM database,
  partitions, and other items. <span class="command"><strong>supportconfig</strong></span> also collects
  the most important log files, making it easier for the supporters to
  identify and solve your problem.
 </p><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="shard-failures"></a>Shard Failures</h2></div></div></div><p>
   This problem may occur the first time the Log Management Window is opened.
   It occurs because the Elasticsearch index for the monasca project does not
   yet exist. A browser reload should fix the problem in most cases.
  </p><div class="informalfigure"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="100%"><tr><td><img src="images/kibana_shard_failure-2.jpg" width="100%" alt="Kibana shard failure" /></td></tr></table></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="something-went-wrong"></a>Oops! Looks like something went wrong</h2></div></div></div><p>
   This problem happens when you were previously logged into horizon and the
   session expired, leaving a stale session state that confuses Kibana. In
   this case, re-login to horizon and then reload Kibana to fix the problem.
  </p><div class="informalfigure"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="100%"><tr><td><img src="images/kibana_oops.png" width="100%" alt="Oops!" /></td></tr></table></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="no-results"></a>No results found</h2></div></div></div><p>
   This may be due to an overly narrow time window (that is, no logs were sent
   in the last 15 minutes, which is the default time window). Try increasing
   the time window with the drop-down box in the screen's top right corner. If
   the problem persists verify that log agents are running properly; make sure
   the <code class="literal">openstack-monasca-log-agent</code> service is running and
   check <code class="filename">/var/log/monasca-log-agent/agent.log</code> for errors
   on the agent nodes.
  </p><div class="informalfigure"><div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" style="cellpadding: 0; cellspacing: 0;" width="100%"><tr><td><img src="images/kibana_noresults.png" width="100%" alt="No results" /></td></tr></table></div></div></div></div><div class="appendix"><div class="titlepage"><div><div><h1 class="title"><a id="id16537"></a>Appendix A. Glossary</h1></div></div></div><h5><a id="id16540"></a>Application Operator</h5><p>
  A person with limited access to cloud resources in <span class="productname">OpenStack</span>. An application
  operator provides services to end users or hosts services for his own
  development activities.
 </p><h5><a id="id16543"></a>Dimension</h5><p>
  A key/value pair that allows for a flexible and concise description of the
  data to be monitored, for example, region, availability zone, service tier,
  or resource ID. Each dimension describes a specific characteristic of the
  metrics to be monitored.
 </p><p>
  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring, metrics are uniquely
  identified by a name and a set of dimensions. Dimensions can serve as a
  filter for the monitoring data.
 </p><h5><a id="id16549"></a>Elasticsearch</h5><p>
  An open source application that provides a highly scalable full-text search
  and analytics engine. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring uses
  Elasticsearch as the underlying technology for storing, searching, and
  analyzing large volumes of log data.
 </p><h5><a id="id16554"></a>Grafana</h5><p>
  An open source application for visualizing large-scale measurement data.
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring integrates with Grafana for
  visualizing the monitoring data.
 </p><h5><a id="id16559"></a>Infrastructure as a Service (IaaS)</h5><p>
  The delivery of computer infrastructure (typically a platform virtualization
  environment) as a service.
 </p><h5><a id="id16561"></a>InfluxDB</h5><p>
  An open source time-series database that supports high write loads and large
  data set storage. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring uses
  InfluxDB as the underlying technology for storing metrics and the alarm
  history.
 </p><h5><a id="id16566"></a>Kibana</h5><p>
  An open source analytics and visualization platform designed to work with
  Elasticsearch. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring integrates
  with Kibana for visualizing the log data.
 </p><h5><a id="id16571"></a>Logstash</h5><p>
  An open source application that provides a data collection engine with
  pipelining capabilities. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring
  integrates with Logstash for collecting, processing, and outputting logs.
 </p><h5><a id="id16576"></a>MariaDB</h5><p>
  An open source relational database that provides an SQL-compliant interface
  for accessing data. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring uses
  MariaDB as the underlying technology for storing configuration information,
  alarm definitions, and notification methods.
 </p><h5><a id="id16581"></a>Metrics</h5><p>
  Self-describing data structures that allow for a flexible and concise
  description of the data to be monitored. Metrics values represent the actual
  monitoring data that is collected and presented in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring.
 </p><h5><a id="id16586"></a>Monasca</h5><p>
  An open source Monitoring as a Service solution that integrates with
  <span class="productname">OpenStack</span>. It forms the core of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring.
 </p><h5><a id="id16592"></a>Monitoring Service Operator</h5><p>
  A person responsible for maintaining and administrating <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring.
 </p><h5><a id="id16597"></a><span class="productname">OpenStack</span> Operator</h5><p>
  A person responsible for maintaining and administrating <span class="productname">OpenStack</span>, the
  underlying platform technology of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud Crowbar</span></span> Monitoring.
 </p><h5><a id="id16604"></a>Platform as a Service (PaaS)</h5><p>
  The delivery of a computing platform and solution stack as a service.
 </p><h5><a id="id16606"></a>Software as a Service (SaaS)</h5><p>
  A model of software deployment where a provider licenses an application to
  customers for use as a service on demand.
 </p></div></div></body></html>