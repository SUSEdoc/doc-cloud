<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Operations Guide CLM | SUSE OpenStack Cloud 9</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DAPS 3.0.0 (https://opensuse.github.io/daps) using SUSE XSL Stylesheets 2.0.17 (based on DocBook XSL Stylesheets 1.79.2)" /><meta name="product-name" content="SUSE OpenStack Cloud" /><meta name="product-number" content="9" /><meta name="book-title" content="Operations Guide CLM" /><meta name="description" content="At the time of the SUSE OpenStack Cloud 9 release, this guide contains information pertaining to the operation, administration, and user functions of SUSE OpenStack Cloud. The audience is the admin-level operator of the cloud." /><meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi" /><meta name="tracker-type" content="bsc" /><meta name="tracker-bsc-component" content="Documentation" /><meta name="tracker-bsc-product" content="SUSE OpenStack Cloud 9" />
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css"></link>');
}
else {
  document.write('<link rel="stylesheet" type="text/css" href="static/css/fonts-onlylocal.css"></link>');
}

</script><noscript><link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft single offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a></div><div id="_outer-wrap"><div id="_white-bg" style="background-color: #FABEBE;"><div id="_header"><div id="_logo"><img src="static/images/logo.png" alt="Logo" /></div><div class="crumbs inactive"><a class="single-crumb" href="#book-operations" accesskey="c"><span class="single-contents-icon"></span>Operations Guide CLM</a><div class="bubble-corner active-contents"></div></div><div class="clearme"></div></div></div><div id="_fixed-header-wrap" style="background-color: #FABEBE;" class="inactive"><div id="_fixed-header"><div class="crumbs inactive"><a class="single-crumb" href="#book-operations" accesskey="c"><span class="single-contents-icon"></span>Show Contents: Operations Guide CLM</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="clearme"></div></div><div class="clearme"></div></div><div class="active-contents bubble"><div class="bubble-container"><div id="_bubble-toc"><ol><li class="inactive"><a href="#gettingstarted-ops"><span class="number">1 </span><span class="name">Operations Overview</span></a></li><li class="inactive"><a href="#tutorials"><span class="number">2 </span><span class="name">Tutorials</span></a></li><li class="inactive"><a href="#clm-admin-ui"><span class="number">3 </span><span class="name">Cloud Lifecycle Manager Admin UI User Guide</span></a></li><li class="inactive"><a href="#third-party-integrations"><span class="number">4 </span><span class="name">Third-Party Integrations</span></a></li><li class="inactive"><a href="#ops-managing-identity"><span class="number">5 </span><span class="name">Managing Identity</span></a></li><li class="inactive"><a href="#ops-managing-compute"><span class="number">6 </span><span class="name">Managing Compute</span></a></li><li class="inactive"><a href="#ops-managing-esx"><span class="number">7 </span><span class="name">Managing ESX</span></a></li><li class="inactive"><a href="#ops-managing-blockstorage"><span class="number">8 </span><span class="name">Managing Block Storage</span></a></li><li class="inactive"><a href="#ops-managing-objectstorage"><span class="number">9 </span><span class="name">Managing Object Storage</span></a></li><li class="inactive"><a href="#ops-managing-networking"><span class="number">10 </span><span class="name">Managing Networking</span></a></li><li class="inactive"><a href="#ops-managing-dashboards"><span class="number">11 </span><span class="name">Managing the Dashboard</span></a></li><li class="inactive"><a href="#ops-managing-orchestration"><span class="number">12 </span><span class="name">Managing Orchestration</span></a></li><li class="inactive"><a href="#topic-ttn-5fg-4v"><span class="number">13 </span><span class="name">Managing Monitoring, Logging, and Usage Reporting</span></a></li><li class="inactive"><a href="#using-container-as-a-service-overview"><span class="number">14 </span><span class="name">Managing Container as a Service (Magnum)</span></a></li><li class="inactive"><a href="#system-maintenance"><span class="number">15 </span><span class="name">System Maintenance</span></a></li><li class="inactive"><a href="#manage-ops-console"><span class="number">16 </span><span class="name">Operations Console</span></a></li><li class="inactive"><a href="#bura-overview"><span class="number">17 </span><span class="name">Backup and Restore</span></a></li><li class="inactive"><a href="#idg-all-operations-troubleshooting-troubleshooting-issues-xml-1"><span class="number">18 </span><span class="name">Troubleshooting Issues</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_toc-bubble-wrap"></div><div id="_content" class="draft "><div class="documentation"><div class="book" id="book-operations"><div class="titlepage"><div><h6 class="version-info"><span class="productname "><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <span class="productnumber ">9</span></h6><div><h1 class="title">Operations Guide CLM <a title="Permalink" class="permalink" href="#book-operations">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/book.operations.xml" title="Edit the source file for this section">Edit source</a></h1></div><div class="abstract "><p>
     At the time of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 release, this guide
     contains information pertaining to the operation,
     administration, and user functions of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
     The audience is the admin-level operator of the cloud.
   </p></div><div class="date"><span class="imprint-label">Publication Date: </span>05/14/2021</div></div></div><div class="toc"><dl><dt><span class="chapter"><a href="#gettingstarted-ops"><span class="number">1 </span><span class="name">Operations Overview</span></a></span></dt><dd><dl><dt><span class="section"><a href="#id-1.5.3.3"><span class="number">1.1 </span><span class="name">What is a cloud operator?</span></a></span></dt><dt><span class="section"><a href="#tools"><span class="number">1.2 </span><span class="name">Tools provided to operate your cloud</span></a></span></dt><dt><span class="section"><a href="#id-1.5.3.5"><span class="number">1.3 </span><span class="name">Daily tasks</span></a></span></dt><dt><span class="section"><a href="#id-1.5.3.6"><span class="number">1.4 </span><span class="name">Weekly or monthly tasks</span></a></span></dt><dt><span class="section"><a href="#id-1.5.3.7"><span class="number">1.5 </span><span class="name">Semi-annual tasks</span></a></span></dt><dt><span class="section"><a href="#id-1.5.3.8"><span class="number">1.6 </span><span class="name">Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#idg-all-operations-operations-overview-xml-11"><span class="number">1.7 </span><span class="name">Common Questions</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#tutorials"><span class="number">2 </span><span class="name">Tutorials</span></a></span></dt><dd><dl><dt><span class="section"><a href="#Quickstart-Guide"><span class="number">2.1 </span><span class="name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Quickstart Guide</span></a></span></dt><dt><span class="section"><a href="#install-cli"><span class="number">2.2 </span><span class="name">Installing the Command-Line Clients</span></a></span></dt><dt><span class="section"><a href="#CreateCloudAdmin"><span class="number">2.3 </span><span class="name">Cloud Admin Actions with the Command Line</span></a></span></dt><dt><span class="section"><a href="#log-management-integration"><span class="number">2.4 </span><span class="name">Log Management and Integration</span></a></span></dt><dt><span class="section"><a href="#Integrating-Kibana-with-Splunk"><span class="number">2.5 </span><span class="name">Integrating Your Logs with Splunk</span></a></span></dt><dt><span class="section"><a href="#LDAP-Integration"><span class="number">2.6 </span><span class="name">Integrating <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> with an LDAP System</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#clm-admin-ui"><span class="number">3 </span><span class="name">Cloud Lifecycle Manager Admin UI User Guide</span></a></span></dt><dd><dl><dt><span class="section"><a href="#accessing-clm-admin-ui"><span class="number">3.1 </span><span class="name">Accessing the Admin UI</span></a></span></dt><dt><span class="section"><a href="#id-1.5.5.4"><span class="number">3.2 </span><span class="name">Admin UI Pages</span></a></span></dt><dt><span class="section"><a href="#clm-admin-topology"><span class="number">3.3 </span><span class="name">Topology</span></a></span></dt><dt><span class="section"><a href="#clm-admin-addserver"><span class="number">3.4 </span><span class="name">Server Management</span></a></span></dt><dt><span class="section"><a href="#clm-server-replacement"><span class="number">3.5 </span><span class="name">Server Replacement</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#third-party-integrations"><span class="number">4 </span><span class="name">Third-Party Integrations</span></a></span></dt><dd><dl><dt><span class="section"><a href="#splunk-integration"><span class="number">4.1 </span><span class="name">Splunk Integration</span></a></span></dt><dt><span class="section"><a href="#topic-kyf-brv-vw"><span class="number">4.2 </span><span class="name">Operations Bridge Integration</span></a></span></dt><dt><span class="section"><a href="#monitoring-3rd-party-components-with-monasca"><span class="number">4.3 </span><span class="name">Monitoring Third-Party Components With Monasca</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#ops-managing-identity"><span class="number">5 </span><span class="name">Managing Identity</span></a></span></dt><dd><dl><dt><span class="section"><a href="#sec-operation-identity-overview"><span class="number">5.1 </span><span class="name">The Identity Service</span></a></span></dt><dt><span class="section"><a href="#supported-upstream-keystone-features"><span class="number">5.2 </span><span class="name">Supported Upstream Keystone Features</span></a></span></dt><dt><span class="section"><a href="#sec-operation-identity"><span class="number">5.3 </span><span class="name">Understanding Domains, Projects, Users, Groups, and Roles</span></a></span></dt><dt><span class="section"><a href="#topic-ffs-dvz-nw"><span class="number">5.4 </span><span class="name">Identity Service Token Validation Example</span></a></span></dt><dt><span class="section"><a href="#topic-qmz-fg3-btx"><span class="number">5.5 </span><span class="name">Configuring the Identity Service</span></a></span></dt><dt><span class="section"><a href="#admin-password"><span class="number">5.6 </span><span class="name">Retrieving the Admin Password</span></a></span></dt><dt><span class="section"><a href="#servicePasswords"><span class="number">5.7 </span><span class="name">Changing Service Passwords</span></a></span></dt><dt><span class="section"><a href="#topic-m43-2j3-bt"><span class="number">5.8 </span><span class="name">Reconfiguring the Identity service</span></a></span></dt><dt><span class="section"><a href="#ldap"><span class="number">5.9 </span><span class="name">Integrating LDAP with the Identity Service</span></a></span></dt><dt><span class="section"><a href="#k2kfed"><span class="number">5.10 </span><span class="name">keystone-to-keystone Federation</span></a></span></dt><dt><span class="section"><a href="#websso"><span class="number">5.11 </span><span class="name">Configuring Web Single Sign-On</span></a></span></dt><dt><span class="section"><a href="#topic-qtp-cn3-bt"><span class="number">5.12 </span><span class="name">Identity Service Notes and Limitations</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#ops-managing-compute"><span class="number">6 </span><span class="name">Managing Compute</span></a></span></dt><dd><dl><dt><span class="section"><a href="#aggregates"><span class="number">6.1 </span><span class="name">Managing Compute Hosts using Aggregates and Scheduler Filters</span></a></span></dt><dt><span class="section"><a href="#topic-vhs-12v-vw"><span class="number">6.2 </span><span class="name">Using Flavor Metadata to Specify CPU Model</span></a></span></dt><dt><span class="section"><a href="#topic-pqr-lyx-yw"><span class="number">6.3 </span><span class="name">Forcing CPU and RAM Overcommit Settings</span></a></span></dt><dt><span class="section"><a href="#enabling-the-nova-resize"><span class="number">6.4 </span><span class="name">Enabling the Nova Resize and Migrate Features</span></a></span></dt><dt><span class="section"><a href="#resize"><span class="number">6.5 </span><span class="name">Enabling ESX Compute Instance(s) Resize Feature</span></a></span></dt><dt><span class="section"><a href="#gpu-passthrough"><span class="number">6.6 </span><span class="name">GPU passthrough</span></a></span></dt><dt><span class="section"><a href="#configure-glance"><span class="number">6.7 </span><span class="name">Configuring the Image Service</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#ops-managing-esx"><span class="number">7 </span><span class="name">Managing ESX</span></a></span></dt><dd><dl><dt><span class="section"><a href="#topic-odg-33x-rt"><span class="number">7.1 </span><span class="name">Networking for ESXi Hypervisor (OVSvApp)</span></a></span></dt><dt><span class="section"><a href="#verify-neutron"><span class="number">7.2 </span><span class="name">Validating the neutron Installation</span></a></span></dt><dt><span class="section"><a href="#sec-esx-remove-cluster"><span class="number">7.3 </span><span class="name">Removing a Cluster from the Compute Resource Pool</span></a></span></dt><dt><span class="section"><a href="#sec-esx-remove-esxi-host"><span class="number">7.4 </span><span class="name">Removing an ESXi Host from a Cluster</span></a></span></dt><dt><span class="section"><a href="#sec-esx-debug"><span class="number">7.5 </span><span class="name">Configuring Debug Logging</span></a></span></dt><dt><span class="section"><a href="#topic-ijt-dyh-rt"><span class="number">7.6 </span><span class="name">Making Scale Configuration Changes</span></a></span></dt><dt><span class="section"><a href="#idg-all-operations-monitoring-vcenter-clusters-xml-1"><span class="number">7.7 </span><span class="name">Monitoring vCenter Clusters</span></a></span></dt><dt><span class="section"><a href="#ovsvapp-monitoring"><span class="number">7.8 </span><span class="name">Monitoring Integration with OVSvApp Appliance</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#ops-managing-blockstorage"><span class="number">8 </span><span class="name">Managing Block Storage</span></a></span></dt><dd><dl><dt><span class="section"><a href="#topic-e5g-z3h-gt"><span class="number">8.1 </span><span class="name">Managing Block Storage using Cinder</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#ops-managing-objectstorage"><span class="number">9 </span><span class="name">Managing Object Storage</span></a></span></dt><dd><dl><dt><span class="section"><a href="#swift-healthcheck"><span class="number">9.1 </span><span class="name">Running the swift Dispersion Report</span></a></span></dt><dt><span class="section"><a href="#swift-recon"><span class="number">9.2 </span><span class="name">Gathering Swift Data</span></a></span></dt><dt><span class="section"><a href="#topic-pcv-fy4-nt"><span class="number">9.3 </span><span class="name">Gathering Swift Monitoring Metrics</span></a></span></dt><dt><span class="section"><a href="#topic-m13-dgp-nt"><span class="number">9.4 </span><span class="name">Using the swift Command-line Client (CLI)</span></a></span></dt><dt><span class="section"><a href="#swift-ring-management"><span class="number">9.5 </span><span class="name">Managing swift Rings</span></a></span></dt><dt><span class="section"><a href="#topic-el2-cqv-mv"><span class="number">9.6 </span><span class="name">Configuring your swift System to Allow Container Sync</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#ops-managing-networking"><span class="number">10 </span><span class="name">Managing Networking</span></a></span></dt><dd><dl><dt><span class="section"><a href="#topic-gll-nsn-15"><span class="number">10.1 </span><span class="name">SUSE <span class="productname">OpenStack</span> Cloud Firewall</span></a></span></dt><dt><span class="section"><a href="#UsingVPNaaS"><span class="number">10.2 </span><span class="name">Using VPN as a Service (VPNaaS)</span></a></span></dt><dt><span class="section"><a href="#designateOverview"><span class="number">10.3 </span><span class="name">DNS Service Overview</span></a></span></dt><dt><span class="section"><a href="#neutron-overview"><span class="number">10.4 </span><span class="name">Networking Service Overview</span></a></span></dt><dt><span class="section"><a href="#CreateHARouter"><span class="number">10.5 </span><span class="name">Creating a Highly Available Router</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#ops-managing-dashboards"><span class="number">11 </span><span class="name">Managing the Dashboard</span></a></span></dt><dd><dl><dt><span class="section"><a href="#topic1564-1"><span class="number">11.1 </span><span class="name">Configuring the Dashboard Service</span></a></span></dt><dt><span class="section"><a href="#horizonTimeout"><span class="number">11.2 </span><span class="name">Changing the Dashboard Timeout Value</span></a></span></dt><dt><span class="section"><a href="#lbaas-dashboard"><span class="number">11.3 </span><span class="name">Creating a Load Balancer with the Dashboard</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#ops-managing-orchestration"><span class="number">12 </span><span class="name">Managing Orchestration</span></a></span></dt><dd><dl><dt><span class="section"><a href="#configure-heat"><span class="number">12.1 </span><span class="name">Configuring the Orchestration Service</span></a></span></dt><dt><span class="section"><a href="#topic-sqg-cvb-dx"><span class="number">12.2 </span><span class="name">Autoscaling using the Orchestration Service</span></a></span></dt><dt><span class="section"><a href="#LBaaSheat"><span class="number">12.3 </span><span class="name">Orchestration Service support for LBaaS v2</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#topic-ttn-5fg-4v"><span class="number">13 </span><span class="name">Managing Monitoring, Logging, and Usage Reporting</span></a></span></dt><dd><dl><dt><span class="section"><a href="#mon"><span class="number">13.1 </span><span class="name">Monitoring</span></a></span></dt><dt><span class="section"><a href="#centralized-logging"><span class="number">13.2 </span><span class="name">Centralized Logging Service</span></a></span></dt><dt><span class="section"><a href="#ceilo-metering-overview"><span class="number">13.3 </span><span class="name">Metering Service (ceilometer) Overview</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#using-container-as-a-service-overview"><span class="number">14 </span><span class="name">Managing Container as a Service (Magnum)</span></a></span></dt><dd><dl><dt><span class="section"><a href="#deploying-kubernetes-fedora-atomic"><span class="number">14.1 </span><span class="name">Deploying a Kubernetes Cluster on Fedora Atomic</span></a></span></dt><dt><span class="section"><a href="#deploying-kubernetes-coreos"><span class="number">14.2 </span><span class="name">Deploying a Kubernetes Cluster on CoreOS</span></a></span></dt><dt><span class="section"><a href="#deploying-docker-fedora-atomic"><span class="number">14.3 </span><span class="name">Deploying a Docker Swarm Cluster on Fedora Atomic</span></a></span></dt><dt><span class="section"><a href="#deploying-apache-mesos-ubuntu"><span class="number">14.4 </span><span class="name">Deploying an Apache Mesos Cluster on Ubuntu</span></a></span></dt><dt><span class="section"><a href="#create-magnum-cluster"><span class="number">14.5 </span><span class="name">Creating a Magnum Cluster with the Dashboard</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#system-maintenance"><span class="number">15 </span><span class="name">System Maintenance</span></a></span></dt><dd><dl><dt><span class="section"><a href="#planned-maintenance"><span class="number">15.1 </span><span class="name">Planned System Maintenance</span></a></span></dt><dt><span class="section"><a href="#unplanned-maintenance"><span class="number">15.2 </span><span class="name">Unplanned System Maintenance</span></a></span></dt><dt><span class="section"><a href="#maintenance-update"><span class="number">15.3 </span><span class="name">Cloud Lifecycle Manager Maintenance Update Procedure</span></a></span></dt><dt><span class="section"><a href="#upgrade-soc"><span class="number">15.4 </span><span class="name">Upgrading Cloud Lifecycle Manager 8 to Cloud Lifecycle Manager 9</span></a></span></dt><dt><span class="section"><a href="#deploy-ptf"><span class="number">15.5 </span><span class="name">Cloud Lifecycle Manager Program Temporary Fix (PTF) Deployment</span></a></span></dt><dt><span class="section"><a href="#database-maintenance"><span class="number">15.6 </span><span class="name">Periodic OpenStack Maintenance Tasks</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#manage-ops-console"><span class="number">16 </span><span class="name">Operations Console</span></a></span></dt><dd><dl><dt><span class="section"><a href="#using-opsconsole"><span class="number">16.1 </span><span class="name">Using the Operations Console</span></a></span></dt><dt><span class="section"><a href="#opsconsole-alarm-definitions"><span class="number">16.2 </span><span class="name">Alarm Definition</span></a></span></dt><dt><span class="section"><a href="#opsconsole-alarm-explorer"><span class="number">16.3 </span><span class="name">Alarm Explorer</span></a></span></dt><dt><span class="section"><a href="#opsconsole-compute-hosts"><span class="number">16.4 </span><span class="name">Compute Hosts</span></a></span></dt><dt><span class="section"><a href="#opsconsole-compute-instances"><span class="number">16.5 </span><span class="name">Compute Instances</span></a></span></dt><dt><span class="section"><a href="#opsconsole-compute-alarm-summary"><span class="number">16.6 </span><span class="name">Compute Summary</span></a></span></dt><dt><span class="section"><a href="#opsconsole-idg-all-operations-opsconsole-en-logging-xml-1"><span class="number">16.7 </span><span class="name">Logging</span></a></span></dt><dt><span class="section"><a href="#opsconsole-my-dashboard-overview"><span class="number">16.8 </span><span class="name">My Dashboard</span></a></span></dt><dt><span class="section"><a href="#opsconsole-networking-alarm-summary"><span class="number">16.9 </span><span class="name">Networking Alarm Summary</span></a></span></dt><dt><span class="section"><a href="#opsconsole-dashboard-overview"><span class="number">16.10 </span><span class="name">Central Dashboard</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#bura-overview"><span class="number">17 </span><span class="name">Backup and Restore</span></a></span></dt><dd><dl><dt><span class="section"><a href="#topic-bxm-gxr-st"><span class="number">17.1 </span><span class="name">Manual Backup Overview</span></a></span></dt><dt><span class="section"><a href="#topic-jsc-qps-qt"><span class="number">17.2 </span><span class="name">Enabling Backups to a Remote Server</span></a></span></dt><dt><span class="section"><a href="#bura-manual-backup"><span class="number">17.3 </span><span class="name">Manual Backup and Restore Procedures</span></a></span></dt><dt><span class="section"><a href="#full-recovery-test"><span class="number">17.4 </span><span class="name">Full Disaster Recovery Test</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#idg-all-operations-troubleshooting-troubleshooting-issues-xml-1"><span class="number">18 </span><span class="name">Troubleshooting Issues</span></a></span></dt><dd><dl><dt><span class="section"><a href="#general-troubleshooting"><span class="number">18.1 </span><span class="name">General Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#troubleshooting-controlplane"><span class="number">18.2 </span><span class="name">Control Plane Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#ts-compute"><span class="number">18.3 </span><span class="name">Troubleshooting Compute service</span></a></span></dt><dt><span class="section"><a href="#neutron-troubleshooting"><span class="number">18.4 </span><span class="name">Network Service Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#troubleshooting-glance"><span class="number">18.5 </span><span class="name">Troubleshooting the Image (glance) Service</span></a></span></dt><dt><span class="section"><a href="#troubleshooting-storage"><span class="number">18.6 </span><span class="name">Storage Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#monitoring-logging-usage-reporting"><span class="number">18.7 </span><span class="name">Monitoring, Logging, and Usage Reporting Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#troubleshooting-orchestration"><span class="number">18.8 </span><span class="name">Orchestration Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#troubleshooting-tools"><span class="number">18.9 </span><span class="name">Troubleshooting Tools</span></a></span></dt></dl></dd></dl></div><div class="list-of-figures"><div class="toc-title">List of Figures</div><dl><dt><span class="figure"><a href="#admin-ui-login-page"><span class="number">3.1 </span><span class="name">Cloud Lifecycle Manager Admin UI Login Page</span></a></span></dt><dt><span class="figure"><a href="#admin-ui-service-info"><span class="number">3.2 </span><span class="name">Cloud Lifecycle Manager Admin UI Service Information</span></a></span></dt><dt><span class="figure"><a href="#admin-ui-cloud-package-versions"><span class="number">3.3 </span><span class="name">Cloud Lifecycle Manager Admin UI SUSE Cloud Package</span></a></span></dt><dt><span class="figure"><a href="#admin-ui-cloud-service-configuration"><span class="number">3.4 </span><span class="name">Cloud Lifecycle Manager Admin UI SUSE Service Configuration</span></a></span></dt><dt><span class="figure"><a href="#admin-ui-cloud-service-configuration-editor"><span class="number">3.5 </span><span class="name">Cloud Lifecycle Manager Admin UI SUSE Service Configuration Editor</span></a></span></dt><dt><span class="figure"><a href="#admin-ui-cloud-service-configuration-update"><span class="number">3.6 </span><span class="name">Cloud Lifecycle Manager Admin UI SUSE Service Configuration Update</span></a></span></dt><dt><span class="figure"><a href="#admin-ui-cloud-services-model"><span class="number">3.7 </span><span class="name">Cloud Lifecycle Manager Admin UI SUSE Service Model</span></a></span></dt><dt><span class="figure"><a href="#admin-ui-cloud-services-model-editor"><span class="number">3.8 </span><span class="name">Cloud Lifecycle Manager Admin UI SUSE Service Model Editor</span></a></span></dt><dt><span class="figure"><a href="#admin-ui-cloud-services-model-confirmation"><span class="number">3.9 </span><span class="name">Cloud Lifecycle Manager Admin UI SUSE Service Model Confirmation</span></a></span></dt><dt><span class="figure"><a href="#admin-ui-cloud-services-model-update"><span class="number">3.10 </span><span class="name">Cloud Lifecycle Manager Admin UI SUSE Service Model Update</span></a></span></dt><dt><span class="figure"><a href="#admin-ui-cloud-pkg-versions"><span class="number">3.11 </span><span class="name">Cloud Lifecycle Manager Admin UI Services Per Role</span></a></span></dt><dt><span class="figure"><a href="#admin-ui-server-summary"><span class="number">3.12 </span><span class="name">Cloud Lifecycle Manager Admin UI Server Summary</span></a></span></dt><dt><span class="figure"><a href="#admin-ui-server-summary-details"><span class="number">3.13 </span><span class="name">Server Details (1/2)</span></a></span></dt><dt><span class="figure"><a href="#admin-ui-server-summary-details-more"><span class="number">3.14 </span><span class="name">Server Details (2/2)</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-topo-control-planes-0"><span class="number">3.15 </span><span class="name">Control Plane Topology</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-topo-control-planes-1"><span class="number">3.16 </span><span class="name">Control Plane Topology - Availability Zones</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-topo-regions-image"><span class="number">3.17 </span><span class="name">Regions Topology</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-topo-services-0"><span class="number">3.18 </span><span class="name">Services Topology</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-topo-service-details"><span class="number">3.19 </span><span class="name">Service Details Topology</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-topo-networks"><span class="number">3.20 </span><span class="name">Networks Topology</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-topo-network-groups"><span class="number">3.21 </span><span class="name">Network Groups Topology</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-topo-server-groups"><span class="number">3.22 </span><span class="name">Server Groups Topology</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-topo-roles"><span class="number">3.23 </span><span class="name">Roles Topology</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-add-server-0"><span class="number">3.24 </span><span class="name">Add Server Overview</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-add-server-image"><span class="number">3.25 </span><span class="name">Manually Add Server</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-add-server-2"><span class="number">3.26 </span><span class="name">Manually Add Server</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-safe-mode"><span class="number">3.27 </span><span class="name">Add Server Settings options</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-add-server-3"><span class="number">3.28 </span><span class="name">Select Servers to Provision OS</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-add-server-4"><span class="number">3.29 </span><span class="name">Confirm Provision OS</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-add-server-5"><span class="number">3.30 </span><span class="name">OS Install Progress</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-add-server-6"><span class="number">3.31 </span><span class="name">OS Install Summary</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-add-server-75"><span class="number">3.32 </span><span class="name">Confirm Deploy Servers</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-add-server-8"><span class="number">3.33 </span><span class="name">Validate Server Changes</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-add-server-9"><span class="number">3.34 </span><span class="name">Prepare Servers</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-add-server-10"><span class="number">3.35 </span><span class="name">Deploy Servers</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-add-server-11"><span class="number">3.36 </span><span class="name">Deploy Summary</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-activate-server-1"><span class="number">3.37 </span><span class="name">Activate Server</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-activate-server-2"><span class="number">3.38 </span><span class="name">Activate Server Progress</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-deactivate-server-1"><span class="number">3.39 </span><span class="name">Deactivate Server</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-deactivate-server-2"><span class="number">3.40 </span><span class="name">Deactivate Server Confirmation</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-deactivate-server-3"><span class="number">3.41 </span><span class="name">Deactivate Server Progress</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-deactivate-with-migration-1"><span class="number">3.42 </span><span class="name">Select Migration Target</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-deactivate-with-migration-2"><span class="number">3.43 </span><span class="name">Deactivate Migration Progress</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-delete-server-1"><span class="number">3.44 </span><span class="name">Delete Server</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-delete-server-2"><span class="number">3.45 </span><span class="name">Delete Server Confirmation</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-delete-server-3"><span class="number">3.46 </span><span class="name">Unreachable Delete Confirmation</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-delete-server-4"><span class="number">3.47 </span><span class="name">Delete Server Progress</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-replaceserver-menu"><span class="number">3.48 </span><span class="name">Replace Server Menu</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-replaceserver-controlplane-replaceform"><span class="number">3.49 </span><span class="name">Replace Controller Form</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-replace-server-controlplane-progress"><span class="number">3.50 </span><span class="name">Replace Controller Progress</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-replacecompute-menu"><span class="number">3.51 </span><span class="name">Replace Compute Menu</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-replacecompute-unreachable-warn"><span class="number">3.52 </span><span class="name">Unreachable Compute Node Warning</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-replacecompute-replaceform"><span class="number">3.53 </span><span class="name">Replace Compute Form</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-replacecompute-installsles"><span class="number">3.54 </span><span class="name">Install SLES on New Compute</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-replacecompute-prepareserver"><span class="number">3.55 </span><span class="name">Prepare Compute Server</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-replacecompute-deploynew"><span class="number">3.56 </span><span class="name">Deploy New Compute Server</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-replacecompute-hostaggwarnings"><span class="number">3.57 </span><span class="name">Host Aggregate Removal Warning</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-replacecompute-migration"><span class="number">3.58 </span><span class="name">Migrate Instances from Existing Compute Server</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-replacecompute-disableexisting"><span class="number">3.59 </span><span class="name">Disable Existing Compute Server</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-replacecompute-shutdownwarning"><span class="number">3.60 </span><span class="name">Existing Server Shutdown Check</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-replacecompute-deleteexisting"><span class="number">3.61 </span><span class="name">Existing Server Delete</span></a></span></dt><dt><span class="figure"><a href="#clm-admin-replacecompute-summary"><span class="number">3.62 </span><span class="name">Compute Replacement Summary</span></a></span></dt><dt><span class="figure"><a href="#fig-keystone-authentication-flow"><span class="number">5.1 </span><span class="name">Keystone Authentication Flow</span></a></span></dt><dt><span class="figure"><a href="#id-1.5.18.6.3.3"><span class="number">16.1 </span><span class="name">Compute Hosts</span></a></span></dt><dt><span class="figure"><a href="#id-1.5.18.8.3.3"><span class="number">16.2 </span><span class="name">Compute Summary</span></a></span></dt></dl></div><div class="list-of-tables"><div class="toc-title">List of Tables</div><dl><dt><span class="table"><a href="#intel-82599-table"><span class="number">10.1 </span><span class="name">Intel 82599 devices supported with SRIOV and PCIPT</span></a></span></dt><dt><span class="table"><a href="#table-ztc-yn5-3y"><span class="number">13.1 </span><span class="name">Aggregated Metrics</span></a></span></dt><dt><span class="table"><a href="#id-1.5.15.3.6.12.3"><span class="number">13.2 </span><span class="name">HTTP Check Metrics</span></a></span></dt><dt><span class="table"><a href="#id-1.5.15.3.6.12.5"><span class="number">13.3 </span><span class="name">HTTP Metric Components</span></a></span></dt><dt><span class="table"><a href="#id-1.5.15.3.6.14.4"><span class="number">13.4 </span><span class="name">Tunable Libvirt Metrics</span></a></span></dt><dt><span class="table"><a href="#id-1.5.15.3.6.14.6"><span class="number">13.5 </span><span class="name">Untunable Libvirt Metrics</span></a></span></dt><dt><span class="table"><a href="#id-1.5.15.3.6.19.4"><span class="number">13.6 </span><span class="name">Per-router metrics</span></a></span></dt><dt><span class="table"><a href="#id-1.5.15.3.6.19.5"><span class="number">13.7 </span><span class="name">Per-DHCP port and rate metrics</span></a></span></dt><dt><span class="table"><a href="#id-1.5.15.3.6.23.3"><span class="number">13.8 </span><span class="name">CPU Metrics</span></a></span></dt><dt><span class="table"><a href="#id-1.5.15.3.6.23.4"><span class="number">13.9 </span><span class="name">Disk Metrics</span></a></span></dt><dt><span class="table"><a href="#id-1.5.15.3.6.23.5"><span class="number">13.10 </span><span class="name">Load Metrics</span></a></span></dt><dt><span class="table"><a href="#id-1.5.15.3.6.23.6"><span class="number">13.11 </span><span class="name">Memory Metrics</span></a></span></dt><dt><span class="table"><a href="#id-1.5.15.3.6.23.7"><span class="number">13.12 </span><span class="name">Network Metrics</span></a></span></dt><dt><span class="table"><a href="#id-1.5.19.9.4.5"><span class="number">17.1 </span><span class="name">Cloud Lifecycle Manager Backup Paths</span></a></span></dt></dl></div><div class="list-of-examples"><div class="toc-title">List of Examples</div><dl><dt><span class="example"><a href="#ex-k2kclient"><span class="number">5.1 </span><span class="name">k2kclient.py</span></a></span></dt></dl></div><div><div class="legalnotice" id="id-1.5.2.1"><p>
  Copyright © 2006–
2021

  SUSE LLC and contributors. All rights reserved.
 </p><p>
  Except where otherwise noted, this document is licensed under
  <span class="bold"><strong>Creative Commons Attribution 3.0 License
  </strong></span>: 
   <a class="link" href="https://creativecommons.org/licenses/by/3.0/legalcode" target="_blank">https://creativecommons.org/licenses/by/3.0/legalcode</a>.
 </p><p>
  For SUSE trademarks, see
  <a class="link" href="https://www.suse.com/company/legal/" target="_blank">https://www.suse.com/company/legal/</a>. All other
  third-party trademarks are the property of their respective owners. Trademark
  symbols (®, ™ etc.) denote trademarks of SUSE and its affiliates.
  Asterisks (*) denote third-party trademarks.
 </p><p>
  All information found in this book has been compiled with utmost attention
  to detail. However, this does not guarantee complete accuracy. Neither
  SUSE LLC, its affiliates, the authors nor the translators shall be held
  liable for possible errors or the consequences thereof.
 </p></div></div><div class="chapter " id="gettingstarted-ops"><div class="titlepage"><div><div><h1 class="title"><span class="number">1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Operations Overview</span> <a title="Permalink" class="permalink" href="#gettingstarted-ops">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-operations_overview.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-operations_overview.xml</li><li><span class="ds-label">ID: </span>gettingstarted-ops</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#id-1.5.3.3"><span class="number">1.1 </span><span class="name">What is a cloud operator?</span></a></span></dt><dt><span class="section"><a href="#tools"><span class="number">1.2 </span><span class="name">Tools provided to operate your cloud</span></a></span></dt><dt><span class="section"><a href="#id-1.5.3.5"><span class="number">1.3 </span><span class="name">Daily tasks</span></a></span></dt><dt><span class="section"><a href="#id-1.5.3.6"><span class="number">1.4 </span><span class="name">Weekly or monthly tasks</span></a></span></dt><dt><span class="section"><a href="#id-1.5.3.7"><span class="number">1.5 </span><span class="name">Semi-annual tasks</span></a></span></dt><dt><span class="section"><a href="#id-1.5.3.8"><span class="number">1.6 </span><span class="name">Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#idg-all-operations-operations-overview-xml-11"><span class="number">1.7 </span><span class="name">Common Questions</span></a></span></dt></dl></div></div><p>
  A high-level overview of the processes related to operating a
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 cloud.
 </p><div class="sect1" id="id-1.5.3.3"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">What is a cloud operator?</span> <a title="Permalink" class="permalink" href="#id-1.5.3.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-operations_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-operations_overview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   When we talk about a cloud operator it is important to understand the scope
   of the tasks and responsibilities we are referring to. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> defines a
   cloud operator as the person or group of people who will be administering
   the cloud infrastructure, which includes:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Monitoring the cloud infrastructure, resolving issues as they arise.
    </p></li><li class="listitem "><p>
     Managing hardware resources, adding/removing hardware due to capacity
     needs.
    </p></li><li class="listitem "><p>
     Repairing, and recovering if needed, any hardware issues.
    </p></li><li class="listitem "><p>
     Performing domain administration tasks, which involves creating and
     managing projects, users, and groups as well as setting and managing
     resource quotas.
    </p></li></ul></div></div><div class="sect1" id="tools"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Tools provided to operate your cloud</span> <a title="Permalink" class="permalink" href="#tools">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-operations_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-operations_overview.xml</li><li><span class="ds-label">ID: </span>tools</li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> provides the following tools which are available to operate your
   cloud:
  </p><p>
   <span class="bold"><strong>Operations Console</strong></span>
  </p><p>
   Often referred to as the Ops Console, you can use this console to view data
   about your cloud infrastructure in a web-based graphical user interface
   (GUI) to make sure your cloud is operating correctly. By logging on to the
   console, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> administrators can manage data in the following ways:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Triage alarm notifications in the central dashboard
    </p></li><li class="listitem "><p>
     Monitor the environment by giving priority to alarms that take precedence
    </p></li><li class="listitem "><p>
     Manage compute nodes and easily use a form to create a new host
    </p></li><li class="listitem "><p>
     Refine the monitoring environment by creating new alarms to specify a
     combination of metrics, services, and hosts that match the triggers unique
     to an environment
    </p></li><li class="listitem "><p>
     Plan for future storage by tracking capacity over time to predict with
     some degree of reliability the amount of additional storage needed
    </p></li></ul></div><p>
   <span class="bold"><strong>Dashboard</strong></span>
  </p><p>
   Often referred to as horizon or the horizon dashboard, you can use this
   console to manage resources on a domain and project level in a web-based
   graphical user interface (GUI). The following are some of the typical
   operational tasks that you may perform using the dashboard:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Creating and managing projects, users, and groups within your domain.
    </p></li><li class="listitem "><p>
     Assigning roles to users and groups to manage access to resources.
    </p></li><li class="listitem "><p>
     Setting and updating resource quotas for the projects.
    </p></li></ul></div><p>
   For more details, see the following page: <a class="xref" href="#sec-operation-identity" title="5.3. Understanding Domains, Projects, Users, Groups, and Roles">Section 5.3, “Understanding Domains, Projects, Users, Groups, and Roles”</a>
  </p><p>
   <span class="bold"><strong>Command-line interface (CLI)</strong></span>
  </p><p>
   The OpenStack community has created a unified client,
   called the openstackclient (OSC), which combines the available commands in the
   various service-specific clients into one tool. Some service-specific
   commands do not have OSC equivalents.
  </p><p>
   You will find processes defined in our documentation that use these
   command-line tools. There is also a list of common cloud administration
   tasks which we have outlined which you can use the command-line tools to do.
  </p><p>
   There are references throughout the SUSE <span class="productname">OpenStack</span> Cloud documentation to the HPE Smart
   Storage Administrator (HPE SSA) CLI. HPE-specific binaries that are not
   based on open source are distributed directly from and supported by HPE. To
   download and install the SSACLI utility, please refer to: <a class="link" href="https://support.hpe.com/hpsc/swd/public/detail?swItemId=MTX_3d16386b418a443388c18da82f" target="_blank">https://support.hpe.com/hpsc/swd/public/detail?swItemId=MTX_3d16386b418a443388c18da82f</a>
  </p></div><div class="sect1" id="id-1.5.3.5"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Daily tasks</span> <a title="Permalink" class="permalink" href="#id-1.5.3.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-operations_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-operations_overview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>Ensure your cloud is running correctly</strong></span>:
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is deployed as a set of highly available services to minimize the
     impact of failures. That said, hardware and software systems can fail.
     Detection of failures early in the process will enable you to address
     issues before they affect the broader system. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> provides a
     monitoring solution, based on OpenStack’s monasca, which provides
     monitoring and metrics for all OpenStack components and much of the
     underlying system, including service status, performance metrics, compute
     node, and virtual machine status. Failures are exposed via the Operations Console
     and/or alarm notifications. In the case where more detailed
     diagnostics are required, you can use a centralized logging system based
     on the Elasticsearch, Logstash, and Kibana (ELK) stack. This provides the
     ability to search service logs to get detailed information on behavior and
     errors.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Perform critical maintenance</strong></span>: To ensure
     your OpenStack installation is running correctly, provides the right
     access and functionality, and is secure, you should make ongoing
     adjustments to the environment. Examples of daily maintenance tasks
     include:
    </p><div class="itemizedlist " id="ul-nx2-z4x-rv"><ul class="itemizedlist"><li class="listitem "><p>
       Add/remove projects and users. The frequency of this task depends on
       your policy.
      </p></li><li class="listitem "><p>
       Apply security patches (if released).
      </p></li><li class="listitem "><p>
       Run daily backups.
      </p></li></ul></div></li></ul></div></div><div class="sect1" id="id-1.5.3.6"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Weekly or monthly tasks</span> <a title="Permalink" class="permalink" href="#id-1.5.3.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-operations_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-operations_overview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist " id="ul-nz4-npx-rv"><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>Do regular capacity planning</strong></span>: Your
     initial deployment will likely reflect the known near to mid-term scale
     requirements, but at some point your needs will outgrow your initial
     deployment’s capacity. You can expand <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> in a variety of ways,
     such as by adding compute and storage capacity.
    </p></li></ul></div><p>
   To manage your cloud’s capacity, begin by determining the load on the
   existing system. OpenStack is a set of relatively independent components and
   services, so there are multiple subsystems that can affect capacity. These
   include control plane nodes, compute nodes, object storage nodes, block
   storage nodes, and an image management system. At the most basic level, you
   should look at the CPU used, RAM used, I/O load, and the disk space used
   relative to the amounts available. For compute nodes, you can also evaluate
   the allocation of resource to hosted virtual machines. This information can
   be viewed in the Operations Console. You can pull historical information
   from the monitoring service (OpenStack’s monasca) by using its client or
   API. Also, OpenStack provides you some ability to manage the hosted resource
   utilization by using quotas for projects. You can track this usage over time
   to get your growth trend so that you can project when you will need to add
   capacity.
  </p></div><div class="sect1" id="id-1.5.3.7"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Semi-annual tasks</span> <a title="Permalink" class="permalink" href="#id-1.5.3.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-operations_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-operations_overview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist " id="ul-y1v-5sx-rv"><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>Perform upgrades</strong></span>: OpenStack releases new
     versions on a six-month cycle. In general, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> will release new
     major versions annually with minor versions and maintenance updates more
     often. Each new release consists of both new functionality and services,
     as well as bug fixes for existing functionality.
    </p></li></ul></div><div id="id-1.5.3.7.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    If you are planning to upgrade, this is also an excellent time to evaluate
    your existing capabilities, especially in terms of capacity (see Capacity
    Planning above).
   </p></div></div><div class="sect1" id="id-1.5.3.8"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting</span> <a title="Permalink" class="permalink" href="#id-1.5.3.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-operations_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-operations_overview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   As part of managing your cloud, you should be ready to troubleshoot issues,
   as needed. The following are some common troubleshooting scenarios and
   solutions:
  </p><p>
   <span class="bold"><strong>How do I determine if my cloud is operating correctly
   now?</strong></span>: <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> provides a monitoring solution based on
   OpenStack’s monasca service. This service provides monitoring and metrics
   for all OpenStack components, as well as much of the underlying system. By
   default, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> comes with a set of alarms that provide coverage of the
   primary systems. In addition, you can define alarms based on threshold
   values for any metrics defined in the system. You can view alarm information
   in the Operations Console. You can also receive or deliver this information
   to others by configuring email or other mechanisms. Alarms provide
   information about whether a component failed and is affecting the system,
   and also what condition triggered the alarm.
  </p><p>
   <span class="bold"><strong>How do I troubleshoot and resolve performance issues
   for my cloud?</strong></span>: There are a variety of factors that can affect the
   performance of a cloud system, such as the following:
  </p><div class="itemizedlist " id="ul-vb3-ttx-rv"><ul class="itemizedlist"><li class="listitem "><p>
     Health of the control plane
    </p></li><li class="listitem "><p>
     Health of the hosting compute node and virtualization layer
    </p></li><li class="listitem "><p>
     Resource allocation on the compute node
    </p></li></ul></div><p>
   If your cloud users are experiencing performance issues on your cloud, use
   the following approach:
  </p><div class="orderedlist " id="ol-wb3-ttx-rv"><ol class="orderedlist" type="1"><li class="listitem "><p>
     View the compute summary page on the Operations Console to determine if
     any alarms have been triggered.
    </p></li><li class="listitem "><p>
     Determine the hosting node of the virtual machine that is having issues.
    </p></li><li class="listitem "><p>
     On the compute hosts page, view the status and resource utilization of the
     compute node to determine if it has errors or is over-allocated.
    </p></li><li class="listitem "><p>
     On the compute instances page you can view the status of the VM along with
     its metrics.
    </p></li></ol></div><p>
   <span class="bold"><strong>How do I troubleshoot and resolve availability issues
   for my cloud?</strong></span>: If your cloud users are experiencing availability
   issues, determine what your users are experiencing that indicates to them
   the cloud is down. For example, can they not access the Dashboard service
   (horizon) console or APIs, indicating a problem with the control plane? Or
   are they having trouble accessing resources? Console/API issues would
   indicate a problem with the control planes. Use the Operations Console to
   view the status of services to see if there is an issue. However, if it is
   an issue of accessing a virtual machine, then also search the consolidated
   logs that are available in the ELK stack or errors related to the virtual
   machine and supporting networking.
  </p></div><div class="sect1" id="idg-all-operations-operations-overview-xml-11"><div class="titlepage"><div><div><h2 class="title"><span class="number">1.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Common Questions</span> <a title="Permalink" class="permalink" href="#idg-all-operations-operations-overview-xml-11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-operations_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-operations_overview.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-operations-overview-xml-11</li></ul></div></div></div></div><p>
   <span class="bold"><strong>What skills do my cloud administrators need?</strong></span>
  </p><p>
   Your administrators should be experienced Linux admins. They should have
   experience in application management, as well as experience with Ansible.
   It is a plus if they have experience with Bash shell scripting and Python
   programming skills.
  </p><p>
   In addition, you will need skilled networking engineering staff to
   administer the cloud network environment.
  </p></div></div><div class="chapter " id="tutorials"><div class="titlepage"><div><div><h1 class="title"><span class="number">2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Tutorials</span> <a title="Permalink" class="permalink" href="#tutorials">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-tutorials.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials.xml</li><li><span class="ds-label">ID: </span>tutorials</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#Quickstart-Guide"><span class="number">2.1 </span><span class="name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Quickstart Guide</span></a></span></dt><dt><span class="section"><a href="#install-cli"><span class="number">2.2 </span><span class="name">Installing the Command-Line Clients</span></a></span></dt><dt><span class="section"><a href="#CreateCloudAdmin"><span class="number">2.3 </span><span class="name">Cloud Admin Actions with the Command Line</span></a></span></dt><dt><span class="section"><a href="#log-management-integration"><span class="number">2.4 </span><span class="name">Log Management and Integration</span></a></span></dt><dt><span class="section"><a href="#Integrating-Kibana-with-Splunk"><span class="number">2.5 </span><span class="name">Integrating Your Logs with Splunk</span></a></span></dt><dt><span class="section"><a href="#LDAP-Integration"><span class="number">2.6 </span><span class="name">Integrating <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> with an LDAP System</span></a></span></dt></dl></div></div><p>
  This section contains tutorials for common tasks for your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9
  cloud.
 </p><div class="sect1" id="Quickstart-Guide"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Quickstart Guide</span> <a title="Permalink" class="permalink" href="#Quickstart-Guide">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-tutorials-quickstart_guide.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-quickstart_guide.xml</li><li><span class="ds-label">ID: </span>Quickstart-Guide</li></ul></div></div></div></div><div class="sect2" id="id-1.5.4.3.2"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Introduction</span> <a title="Permalink" class="permalink" href="#id-1.5.4.3.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-tutorials-quickstart_guide.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-quickstart_guide.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   This document provides simplified instructions for installing and setting up
   a <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. Use this quickstart guide to build testing, demonstration, and
   lab-type environments., rather than production installations. When you
   complete this quickstart process, you will have a fully functioning <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   demo environment.
  </p><div id="id-1.5.4.3.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    These simplified instructions are intended for testing or
    demonstration. Instructions for production installations are in <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”</span>.
   </p></div></div><div class="sect2" id="id-1.5.4.3.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Overview of components</span> <a title="Permalink" class="permalink" href="#id-1.5.4.3.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-tutorials-quickstart_guide.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-quickstart_guide.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following are short descriptions of the components that <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> employs
   when installing and deploying your cloud.
  </p><p><span class="formalpara-title">Ansible. </span>
    Ansible is a powerful configuration management tool used by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to
    manage nearly all aspects of your cloud infrastructure. Most commands in
    this quickstart guide execute Ansible scripts, known as playbooks. You will
    run playbooks that install packages, edit configuration files, manage
    network settings, and take care of the general administration tasks
    required to get your cloud up and running.
   </p><p>
   Get more information on Ansible at
   <a class="link" href="https://www.ansible.com/" target="_blank">https://www.ansible.com/</a>.
  </p><p><span class="formalpara-title">Cobbler. </span>
    Cobbler is another third-party tool used by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to deploy
    operating systems across the physical servers that make up your cloud. Find
    more info at <a class="link" href="http://cobbler.github.io/" target="_blank">http://cobbler.github.io/</a>.
   </p><p><span class="formalpara-title">Git. </span>
    Git is the version control system used to manage the configuration files
    that define your cloud. Any changes made to your cloud configuration files
    must be committed to the locally hosted git repository to take effect. Read
    more information on Git at <a class="link" href="https://git-scm.com/" target="_blank">https://git-scm.com/</a>.
   </p></div><div class="sect2" id="id-1.5.4.3.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Preparation</span> <a title="Permalink" class="permalink" href="#id-1.5.4.3.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-tutorials-quickstart_guide.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-quickstart_guide.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Successfully deploying a <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> environment is a large endeavor, but it is
   not complicated. For a successful deployment, you must put a number of
   components in place before rolling out your cloud. Most importantly, a basic
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> requires the proper network infrastrucure. Because <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   segregates the network traffic of many of its elements, if the necessary
   networks, routes, and firewall access rules are not in place, communication
   required for a successful deployment will not occur.
  </p></div><div class="sect2" id="section-v5g-dvv-xw"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Getting Started</span> <a title="Permalink" class="permalink" href="#section-v5g-dvv-xw">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-tutorials-quickstart_guide.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-quickstart_guide.xml</li><li><span class="ds-label">ID: </span>section-v5g-dvv-xw</li></ul></div></div></div></div><p>
   When your network infrastructure is in place, go ahead and set up the Cloud Lifecycle Manager.
   This is the server that will orchestrate the deployment of the rest of your
   cloud. It is also the server you will run most of your deployment and
   management commands on.
  </p><p>
   <span class="bold"><strong>Set up the Cloud Lifecycle Manager</strong></span>
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     <span class="bold"><strong>Download the installation media</strong></span>
    </p><p>
     Obtain a copy of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installation media, and make sure that it is
     accessible by the server that you are installing it on. Your method of
     doing this may vary. For instance, some may choose to load the
     installation ISO on a USB drive and physically attach it to the server,
     while others may run the IPMI Remote Console and attach the ISO to a
     virtual disc drive.
    </p></li><li class="step "><p>
     <span class="bold"><strong>Install the operating system</strong></span>
    </p><ol type="a" class="substeps "><li class="step "><p>
       Boot your server, using the installation media as the boot source.
      </p></li><li class="step "><p>
       Choose "install" from the list of options and choose your preferred
       keyboard layout, location, language, and other settings.
      </p></li><li class="step "><p>
       Set the address, netmask, and gateway for the primary network interface.
      </p></li><li class="step "><p>
       Create a root user account.
      </p></li></ol><p>
     Proceed with the OS installation. After the installation is complete and
     the server has rebooted into the new OS, log in with the user account you
     created.
    </p></li><li class="step "><p>
     <span class="bold"><strong>Configure the new server</strong></span>
    </p><ol type="a" class="substeps "><li class="step "><p>
       SSH to your new server, and set a valid DNS nameserver in the
       <code class="filename">/etc/resolv.conf</code> file.
      </p></li><li class="step "><p>
       Set the environment variable <code class="literal">LC_ALL</code>:
      </p><div class="verbatim-wrap"><pre class="screen">export LC_ALL=C</pre></div></li></ol><p>
     You now have a server running SUSE Linux Enterprise Server (SLES).
     The next step is to configure this machine as a Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     <span class="bold"><strong>Configure the Cloud Lifecycle Manager</strong></span>
    </p><p>
     The installation media you used to install the OS on the server also has
     the files that will configure your cloud. You need to mount this
     installation media on your new server in order to use these files.
    </p><ol type="a" class="substeps "><li class="step "><p>
       Using the URL that you obtained the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installation media from,
       run <code class="literal">wget</code> to download the ISO file to your server:
      </p><div class="verbatim-wrap"><pre class="screen">wget <em class="replaceable ">INSTALLATION_ISO_URL</em></pre></div></li><li class="step "><p>
       Now mount the ISO in the <code class="filename">/media/cdrom/</code> directory
      </p><div class="verbatim-wrap"><pre class="screen">sudo mount <em class="replaceable ">INSTALLATION_ISO</em> /media/cdrom/</pre></div></li><li class="step "><p>
       Unpack the tar file found in the
       <code class="filename">/media/cdrom/ardana/</code> directory where you just
       mounted the ISO:
      </p><div class="verbatim-wrap"><pre class="screen">tar xvf /media/cdrom/ardana/ardana-x.x.x-x.tar</pre></div></li><li class="step "><p>
       Now you will install and configure all the components needed to turn this
       server into a Cloud Lifecycle Manager. Run the <code class="filename">ardana-init.bash</code>
       script from the uncompressed tar file:
      </p><div class="verbatim-wrap"><pre class="screen">~/ardana-x.x.x/ardana-init.bash</pre></div><p>
       The <code class="filename">ardana-init.bash</code> script prompts you to enter an
       optional SSH passphrase. This passphrase protects the RSA key used to
       SSH to the other cloud nodes. This is an optional passphrase, and you
       can skip it by pressing <span class="keycap">Enter</span> at the prompt.
      </p><p>
       The <code class="filename">ardana-init.bash</code> script automatically installs
       and configures everything needed to set up this server as the lifecycle
       manager for your cloud.
      </p><p>
       When the script has finished running, you can proceed to the next step,
       editing your input files.
      </p></li></ol></li><li class="step "><p>
     <span class="bold"><strong>Edit your input files</strong></span>
    </p><p>
     Your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> input files are where you define your cloud
     infrastructure and how it runs. The input files define options such as
     which servers are included in your cloud, the type of disks the servers
     use, and their network configuration. The input files also define which
     services your cloud will provide and use, the network architecture, and
     the storage backends for your cloud.
    </p><p>
     There are several example configurations, which you can find on your Cloud Lifecycle Manager
     in the <code class="filename">~/openstack/examples/</code> directory.
    </p><ol type="a" class="substeps "><li class="step "><p>
       The simplest way to set up your cloud is to copy the contents of one of
       these example configurations to your
       <code class="filename">~/openstack/mycloud/definition/</code> directory. You can
       then edit the copied files and define your cloud.
      </p><div class="verbatim-wrap"><pre class="screen">cp -r ~/openstack/examples/<em class="replaceable ">CHOSEN_EXAMPLE</em>/* ~/openstack/my_cloud/definition/</pre></div></li><li class="step "><p>
       Edit the files in your
       <code class="filename">~/openstack/my_cloud/definition/</code> directory to
       define your cloud.
      </p></li></ol></li><li class="step "><p>
     <span class="bold"><strong>Commit your changes</strong></span>
    </p><p>
     When you finish editing the necessary input files, stage them, and
     then commit the changes to the local Git repository:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My commit message"</pre></div></li><li class="step "><p>
     <span class="bold"><strong>Image your servers</strong></span>
    </p><p>
     Now that you have finished editing your input files, you can deploy the
     configuration to the servers that will comprise your cloud.
    </p><ol type="a" class="substeps "><li class="step "><p>
       Image the servers. You will install the SLES operating system across
       all the servers in your cloud, using Ansible playbooks to trigger the
       process.
      </p></li><li class="step "><p>
       The following playbook confirms that your servers are accessible over
       their IPMI ports, which is a prerequisite for the imaging process:
      </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost bm-power-status.yml</pre></div></li><li class="step "><p>
       Now validate that your cloud configuration files have proper YAML syntax
       by running the <code class="filename">config-processor-run.yml</code> playbook:
      </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
       If you receive an error when running the preceeding playbook, one or
       more of your configuration files has an issue. Refer to the output of
       the Ansible playbook, and look for clues in the Ansible log file, found
       at <code class="filename">~/.ansible/ansible.log</code>.
      </p></li><li class="step "><p>
       The next step is to prepare your imaging system, Cobbler, to deploy
       operating systems to all your cloud nodes:
      </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step "><p>
       Now you can image your cloud nodes. You will use an Ansible playbook to
       trigger Cobbler to deploy operating systems to all the nodes you
       specified in your input files:
      </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost bm-reimage.yml</pre></div><p>
       The <code class="filename">bm-reimage.yml</code> playbook performs the following
       operations:
      </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
         Powers down the servers.
        </p></li><li class="listitem "><p>
         Sets the servers to boot from a network interface.
        </p></li><li class="listitem "><p>
         Powers on the servers and performs a PXE OS installation.
        </p></li><li class="listitem "><p>
         Waits for the servers to power themselves down as part of a successful
         OS installation. This can take some time.
        </p></li><li class="listitem "><p>
         Sets the servers to boot from their local hard disks and powers on the
         servers.
        </p></li><li class="listitem "><p>
         Waits for the SSH service to start on the servers and verifies that
         they have the expected host-key signature.
        </p></li></ol></div></li></ol></li><li class="step "><p>
     <span class="bold"><strong>Deploy your cloud</strong></span>
    </p><p>
     Now that your servers are running the SLES operating system, it is time
     to configure them for the roles they will play in your new cloud.
    </p><ol type="a" class="substeps "><li class="step "><p>
       Prepare the Cloud Lifecycle Manager to deploy your cloud configuration to all the nodes:
      </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div><p>
       NOTE: The preceding playbook creates a new directory,
       <code class="filename">~/scratch/ansible/next/ardana/ansible/</code>, from which
       you will run many of the following commands.
      </p></li><li class="step "><p><span class="step-optional">(Optional)</span> 
       If you are reusing servers or disks to run your cloud, you can wipe the
       disks of your newly imaged servers by running the
       <code class="filename">wipe_disks.yml</code> playbook:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts wipe_disks.yml</pre></div><p>
       The <code class="filename">wipe_disks.yml</code> playbook removes any existing
       data from the drives on your new servers. This can be helpful if you are
       reusing servers or disks. This action will not affect the OS partitions
       on the servers.
      </p><div id="id-1.5.4.3.5.4.8.3.2.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
        The <code class="filename">wipe_disks.yml</code> playbook is only meant to be
        run on systems immediately after running
        <code class="filename">bm-reimage.yml</code>.  If used for any other case, it
        may not wipe all of the expected partitions. For example, if
        <code class="filename">site.yml</code> fails, you cannot start fresh by running
        <code class="filename">wipe_disks.yml</code>. You must
        <code class="literal">bm-reimage</code> the node first and then run
        <code class="literal">wipe_disks</code>.
       </p></div></li><li class="step "><p>
       Now it is time to deploy your cloud. Do this by running the
       <code class="filename">site.yml</code> playbook, which pushes the configuration
       you defined in the input files out to all the servers that will host
       your cloud.
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts site.yml</pre></div><p>
       The <code class="filename">site.yml</code> playbook installs packages, starts
       services, configures network interface settings, sets iptables firewall
       rules, and more. Upon successful completion of this playbook, your
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> will be in place and in a running state. This playbook can take
       up to six hours to complete.
      </p></li></ol></li><li class="step "><p>
     <span class="bold"><strong>SSH to your nodes</strong></span>
    </p><p>
     Now that you have successfully run <code class="filename">site.yml</code>, your cloud
     will be up and running. You can verify connectivity to your nodes by
     connecting to each one by using SSH. You can find the IP addresses of your
     nodes by viewing the <code class="filename">/etc/hosts</code> file.
    </p><p>
     For security reasons, you can only SSH to your nodes from the Cloud Lifecycle Manager. SSH
     connections from any machine other than the Cloud Lifecycle Manager will be refused by the
     nodes.
    </p><p>
     From the Cloud Lifecycle Manager, SSH to your nodes:
    </p><div class="verbatim-wrap"><pre class="screen">ssh &lt;management IP address of node&gt;</pre></div><p>
     Also note that SSH is limited to your cloud's management network. Each
     node has an address on the management network, and you can find this
     address by reading the <code class="filename">/etc/hosts</code> or
     <code class="filename">server_info.yml</code> file.
    </p></li></ol></div></div></div></div><div class="sect1" id="install-cli"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing the Command-Line Clients</span> <a title="Permalink" class="permalink" href="#install-cli">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-tutorials-installing_cli.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-installing_cli.xml</li><li><span class="ds-label">ID: </span>install-cli</li></ul></div></div></div></div><p>
  During the installation, by default, the suite of OpenStack command-line
  tools are installed on the Cloud Lifecycle Manager and the control plane in your
  environment. You can learn more about these in the OpenStack
  documentation here:
  <a class="link" href="https://docs.openstack.org/python-openstackclient/latest/" target="_blank">OpenStackClient</a>.
 </p><p>
  If you wish to install the command-line interfaces on other nodes in your
  environment, there are two methods you can use to do so that we describe
  below.
 </p><div class="sect2" id="idg-all-userguide-installing-cli-xml-5"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing the CLI tools using the input model</span> <a title="Permalink" class="permalink" href="#idg-all-userguide-installing-cli-xml-5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-tutorials-installing_cli.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-installing_cli.xml</li><li><span class="ds-label">ID: </span>idg-all-userguide-installing-cli-xml-5</li></ul></div></div></div></div><p>
   During the initial install phase of your cloud you can edit your input model
   to request that the command-line clients be installed on any of the node
   clusters in your environment. To do so, follow these steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Edit your <code class="literal">control_plane.yml</code> file. Full path:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/control_plane.yml</pre></div></li><li class="step "><p>
     In this file you will see a list of <code class="literal">service-components</code>
     to be installed on each of your clusters. These clusters will be divided
     per role, with your controller node cluster likely coming at the
     beginning. Here you will see a list of each of the clients that can be
     installed. These include:
    </p><div class="verbatim-wrap"><pre class="screen">keystone-client
glance-client
cinder-client
nova-client
neutron-client
swift-client
heat-client
openstack-client
monasca-client
barbican-client
designate-client</pre></div></li><li class="step "><p>
     For each client you want to install, specify the name under the
     <code class="literal">service-components</code> section for the cluster you want to
     install it on.
    </p><p>
     So, for example, if you would like to install the nova and neutron clients on
     your Compute node cluster, you can do so by adding the
     <code class="literal">nova-client</code> and <code class="literal">neutron-client</code>
     services, like this:
    </p><div class="verbatim-wrap"><pre class="screen">      resources:
        - name: compute
          resource-prefix: comp
          server-role: COMPUTE-ROLE
          allocation-policy: any
          min-count: 0
          service-components:
            - ntp-client
            - nova-compute
            - nova-compute-kvm
            - neutron-l3-agent
            - neutron-metadata-agent
            - neutron-openvswitch-agent
            <span class="bold"><strong>- nova-client
            - neutron-client</strong></span></pre></div><div id="id-1.5.4.4.4.3.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      This example uses the <code class="literal">entry-scale-kvm</code> sample
      file. Your model may be different so use this as a guide but do not
      copy and paste the contents of this example into your input model.
     </p></div></li><li class="step "><p>
     Commit your configuration to the local git repo, as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     Continue with the rest of your installation.
    </p></li></ol></div></div></div><div class="sect2" id="idg-all-userguide-installing-cli-xml-6"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing the CLI tools using Ansible</span> <a title="Permalink" class="permalink" href="#idg-all-userguide-installing-cli-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-tutorials-installing_cli.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-installing_cli.xml</li><li><span class="ds-label">ID: </span>idg-all-userguide-installing-cli-xml-6</li></ul></div></div></div></div><p>
   At any point after your initial installation you can install the
   command-line clients on any of the nodes in your environment. To do so,
   follow these steps:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Obtain the hostname for the nodes you want to install the clients on by
     looking in your hosts file:
    </p><div class="verbatim-wrap"><pre class="screen">cat /etc/hosts</pre></div></li><li class="listitem "><p>
     Install the clients using this playbook, specifying your hostnames using
     commas:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts -e "install_package=&lt;client_name&gt;" client-deploy.yml -e "install_hosts=&lt;hostname&gt;"</pre></div><p>
     So, for example, if you would like to install the novaClient on two of your
     Compute nodes with hostnames <code class="literal">ardana-cp1-comp0001-mgmt</code>
     and <code class="literal">ardana-cp1-comp0002-mgmt</code> you can use this syntax:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts -e "install_package=novaclient" client-deploy.yml -e "install_hosts=ardana-cp1-comp0001-mgmt,ardana-cp1-comp0002-mgmt"</pre></div></li><li class="listitem "><p>
     Once the playbook completes successfully, you should be able to SSH to
     those nodes and, using the proper credentials, authenticate and use the
     command-line interfaces you have installed.
    </p></li></ol></div></div></div><div class="sect1" id="CreateCloudAdmin"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cloud Admin Actions with the Command Line</span> <a title="Permalink" class="permalink" href="#CreateCloudAdmin">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-tutorials-using_cli.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-using_cli.xml</li><li><span class="ds-label">ID: </span>CreateCloudAdmin</li></ul></div></div></div></div><p>
  Cloud admins can use the command line tools to perform domain admin tasks
  such as user and project administration.
 </p><div class="sect2" id="idg-all-operations-cloudadmin-cli-xml-4"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating Additional Cloud Admins</span> <a title="Permalink" class="permalink" href="#idg-all-operations-cloudadmin-cli-xml-4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-tutorials-using_cli.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-using_cli.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-cloudadmin-cli-xml-4</li></ul></div></div></div></div><p>
   You can create additional Cloud Admins to help with the administration of
   your cloud.
  </p><p>
   keystone identity service query and administration tasks can be performed
   using the <span class="productname">OpenStack</span> command line utility. The utility is installed by the
   Cloud Lifecycle Manager onto the Cloud Lifecycle Manager.
  </p><div id="id-1.5.4.5.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    keystone administration tasks should be performed by an
    <span class="emphasis"><em>admin</em></span> user with a token scoped to the
    <span class="emphasis"><em>default</em></span> domain via the keystone v3 identity API.
    These settings are preconfigured in the file
    <code class="filename">~/keystone.osrc</code>. By default,
    <code class="filename">keystone.osrc</code> is configured with the admin endpoint of
    keystone. If the admin endpoint is not accessible from your network, change
    <code class="literal">OS_AUTH_URL</code> to point to the public endpoint.
   </p></div></div><div class="sect2" id="idg-all-operations-cloudadmin-cli-xml-5"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Command Line Examples</span> <a title="Permalink" class="permalink" href="#idg-all-operations-cloudadmin-cli-xml-5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-tutorials-using_cli.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-using_cli.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-cloudadmin-cli-xml-5</li></ul></div></div></div></div><p>
   For a full list of OpenStackClient commands, see
   <a class="link" href="http://docs.openstack.org/developer/python-openstackclient/command-list.html" target="_blank">OpenStackClient
   Command List</a>.
  </p><p>
   <span class="bold"><strong>Sourcing the keystone Administration
   Credentials</strong></span>
  </p><p>
   You can set the environment variables needed for identity administration by
   sourcing the <code class="literal">keystone.osrc</code> file created by the lifecycle
   manager:
  </p><div class="verbatim-wrap"><pre class="screen">source ~/keystone.osrc</pre></div><p>
   <span class="bold"><strong>List users in the default domain</strong></span>
  </p><p>
   These users are created by the Cloud Lifecycle Manager in the MySQL back end:
  </p><div class="verbatim-wrap"><pre class="screen">openstack user list</pre></div><p>
   Example output:
  </p><div class="verbatim-wrap"><pre class="screen">$ openstack user list
+----------------------------------+------------------+
| ID                               | Name             |
+----------------------------------+------------------+
| 155b68eda9634725a1d32c5025b91919 | heat             |
| 303375d5e44d48f298685db7e6a4efce | octavia          |
| 40099e245a394e7f8bb2aa91243168ee | logging          |
| 452596adbf4d49a28cb3768d20a56e38 | admin            |
| 76971c3ad2274820ad5347d46d7560ec | designate        |
| 7b2dc0b5bb8e4ffb92fc338f3fa02bf3 | hlm_backup       |
| 86d345c960e34c9189519548fe13a594 | barbican         |
| 8e7027ab438c4920b5853d52f1e08a22 | nova_monasca     |
| 9c57dfff57e2400190ab04955e7d82a0 | barbican_service |
| a3f99bcc71b242a1bf79dbc9024eec77 | nova             |
| aeeb56fc4c4f40e0a6a938761f7b154a | glance-check     |
| af1ef292a8bb46d9a1167db4da48ac65 | cinder           |
| af3000158c6d4d3d9257462c9cc68dda | demo             |
| b41a7d0cb1264d949614dc66f6449870 | swift            |
| b78a2b17336b43368fb15fea5ed089e9 | cinderinternal   |
| bae1718dee2d47e6a75cd6196fb940bd | monasca          |
| d4b9b32f660943668c9f5963f1ff43f9 | ceilometer       |
| d7bef811fb7e4d8282f19fb3ee5089e9 | swift-monitor    |
| e22bbb2be91342fd9afa20baad4cd490 | neutron          |
| ec0ad2418a644e6b995d8af3eb5ff195 | glance           |
| ef16c37ec7a648338eaf53c029d6e904 | swift-dispersion |
| ef1a6daccb6f4694a27a1c41cc5e7a31 | glance-swift     |
| fed3a599b0864f5b80420c9e387b4901 | monasca-agent    |
+----------------------------------+------------------+</pre></div><p>
   <span class="bold"><strong>List domains created by the installation
   process</strong></span>:
  </p><div class="verbatim-wrap"><pre class="screen">openstack domain list</pre></div><p>
   Example output:
  </p><div class="verbatim-wrap"><pre class="screen">$ openstack domain list
+----------------------------------+---------+---------+----------------------------------------------------------------------+
| ID                               | Name    | Enabled | Description                                                          |
+----------------------------------+---------+---------+----------------------------------------------------------------------+
| 6740dbf7465a4108a36d6476fc967dbd | heat    | True    | Owns users and projects created by heat                              |
| default                          | Default | True    | Owns users and tenants (i.e. projects) available on Identity API v2. |
+----------------------------------+---------+---------+----------------------------------------------------------------------+</pre></div><p>
   <span class="bold"><strong>List the roles</strong></span>:
  </p><div class="verbatim-wrap"><pre class="screen">openstack role list</pre></div><p>
   Example output:
  </p><div class="verbatim-wrap"><pre class="screen">$ openstack role list
+----------------------------------+---------------------------+
| ID                               | Name                      |
+----------------------------------+---------------------------+
| 0be3da26cd3f4cd38d490b4f1a8b0c03 | designate_admin           |
| 13ce16e4e714473285824df8188ee7c0 | monasca-agent             |
| 160f25204add485890bc95a6065b9954 | key-manager:service-admin |
| 27755430b38c411c9ef07f1b78b5ebd7 | monitor                   |
| 2b8eb0a261344fbb8b6b3d5934745fe1 | key-manager:observer      |
| 345f1ec5ab3b4206a7bffdeb5318bd32 | admin                     |
| 49ba3b42696841cea5da8398d0a5d68e | nova_admin                |
| 5129400d4f934d4fbfc2c3dd608b41d9 | ResellerAdmin             |
| 60bc2c44f8c7460a9786232a444b56a5 | neutron_admin             |
| 654bf409c3c94aab8f929e9e82048612 | cinder_admin              |
| 854e542baa144240bfc761cdb5fe0c07 | monitoring-delegate       |
| 8946dbdfa3d346b2aa36fa5941b43643 | key-manager:auditor       |
| 901453d9a4934610ad0d56434d9276b4 | key-manager:admin         |
| 9bc90d1121544e60a39adbfe624a46bc | monasca-user              |
| 9fe2a84a3e7443ae868d1009d6ab4521 | service                   |
| 9fe2ff9ee4384b1894a90878d3e92bab | member                    |
| a24d4e0a5de14bffbe166bfd68b36e6a | swiftoperator             |
| ae088fcbf579425580ee4593bfa680e5 | heat_stack_user           |
| bfba56b2562942e5a2e09b7ed939f01b | keystoneAdmin             |
| c05f54cf4bb34c7cb3a4b2b46c2a448b | glance_admin              |
| fe010be5c57240db8f559e0114a380c1 | key-manager:creator       |
+----------------------------------+---------------------------+</pre></div><p>
   <span class="bold"><strong>List admin user role assignment within default
   domain</strong></span>:
  </p><div class="verbatim-wrap"><pre class="screen">openstack role assignment list --user admin --domain default</pre></div><p>
   Example output:
  </p><div class="verbatim-wrap"><pre class="screen"># This indicates that the admin user is assigned the admin role within the default domain
<code class="prompt user">ardana &gt; </code> openstack role assignment list --user admin --domain default
+----------------------------------+----------------------------------+-------+---------+---------+
| Role                             | User                             | Group | Project | Domain  |
+----------------------------------+----------------------------------+-------+---------+---------+
| b398322103504546a070d607d02618ad | fed1c038d9e64392890b6b44c38f5bbb |       |         | default |
+----------------------------------+----------------------------------+-------+---------+---------+</pre></div><p>
   <span class="bold"><strong>Create a new user in default domain</strong></span>:
  </p><div class="verbatim-wrap"><pre class="screen">openstack user create --domain default --password-prompt --email &lt;email_address&gt; --description &lt;description&gt; --enable &lt;username&gt;</pre></div><p>
   Example output showing the creation of a user named
   <code class="literal">testuser</code> with email address
   <code class="literal">test@example.com</code> and a description of <code class="literal">Test
   User</code>:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code> openstack user create --domain default --password-prompt --email test@example.com --description "Test User" --enable testuser
User Password:
Repeat User Password:
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | Test User                        |
| domain_id   | default                          |
| email       | test@example.com                 |
| enabled     | True                             |
| id          | 8aad69acacf0457e9690abf8c557754b |
| name        | testuser                         |
+-------------+----------------------------------+</pre></div><p>
   <span class="bold"><strong>Assign admin role for testuser within the default
   domain</strong></span>:
  </p><div class="verbatim-wrap"><pre class="screen">openstack role add admin --user &lt;username&gt; --domain default
openstack role assignment list --user &lt;username&gt; --domain default</pre></div><p>
   Example output:
  </p><div class="verbatim-wrap"><pre class="screen"># Just for demonstration purposes - do not do this in a production environment!
<code class="prompt user">ardana &gt; </code> openstack role add admin --user testuser --domain default
<code class="prompt user">ardana &gt; </code> openstack role assignment list --user testuser --domain default
+----------------------------------+----------------------------------+-------+---------+---------+
| Role                             | User                             | Group | Project | Domain  |
+----------------------------------+----------------------------------+-------+---------+---------+
| b398322103504546a070d607d02618ad | 8aad69acacf0457e9690abf8c557754b |       |         | default |
+----------------------------------+----------------------------------+-------+---------+---------+</pre></div></div><div class="sect2" id="default-service-admin-roles"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Assigning the default service admin roles</span> <a title="Permalink" class="permalink" href="#default-service-admin-roles">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-tutorials-using_cli.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-using_cli.xml</li><li><span class="ds-label">ID: </span>default-service-admin-roles</li></ul></div></div></div></div><p>
   The following examples illustrate how you can assign each of the new service
   admin roles to a user.
  </p><p>
   <span class="bold"><strong>Assigning the glance_admin role</strong></span>
  </p><p>
   A user must have the role of admin in order to assign the glance_admin role.
   To assign the role, you will set the environment variables needed for the
   identity service administrator.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     First, source the identity service credentials:
    </p><div class="verbatim-wrap"><pre class="screen">source ~/keystone.osrc</pre></div></li><li class="listitem "><p>
     You can add the glance_admin role to a user on a project with this
     command:
    </p><div class="verbatim-wrap"><pre class="screen">openstack role add --user &lt;username&gt; --project &lt;project_name&gt; glance_admin</pre></div><p>
     Example, showing a user named <code class="literal">testuser</code> being granted
     the <code class="literal">glance_admin</code> role in the
     <code class="literal">test_project</code> project:
    </p><div class="verbatim-wrap"><pre class="screen">openstack role add --user testuser --project test_project glance_admin</pre></div></li><li class="listitem "><p>
     You can confirm the role assignment by listing out the roles:
    </p><div class="verbatim-wrap"><pre class="screen">openstack role assignment list --user &lt;username&gt;</pre></div><p>
     Example output:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code> openstack role assignment list --user testuser
+----------------------------------+----------------------------------+-------+----------------------------------+--------+-----------+
| Role                             | User                             | Group | Project                          | Domain | Inherited |
+----------------------------------+----------------------------------+-------+----------------------------------+--------+-----------+
| 46ba80078bc64853b051c964db918816 | 8bcfe10101964e0c8ebc4de391f3e345 |       | 0ebbf7640d7948d2a17ac08bbbf0ca5b |        | False     |
+----------------------------------+----------------------------------+-------+----------------------------------+--------+-----------+</pre></div></li><li class="listitem "><p>
     Note that only the role ID is displayed. To get the role name, execute the
     following:
    </p><div class="verbatim-wrap"><pre class="screen">openstack role show &lt;role_id&gt;</pre></div><p>
     Example output:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code> openstack role show 46ba80078bc64853b051c964db918816
+-------+----------------------------------+
| Field | Value                            |
+-------+----------------------------------+
| id    | 46ba80078bc64853b051c964db918816 |
| name  | glance_admin                     |
+-------+----------------------------------+</pre></div></li><li class="listitem "><p>
     To demonstrate that the user has glance admin privileges, authenticate
     with those user creds and then upload and publish an image. Only a user
     with an admin role or glance_admin can publish an image.
    </p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       The easiest way to do this will be to make a copy of the
       <code class="literal">service.osrc</code> file and edit it with your user
       credentials. You can do that with this command:
      </p><div class="verbatim-wrap"><pre class="screen">cp ~/service.osrc ~/user.osrc</pre></div></li><li class="listitem "><p>
       Using your preferred editor, edit the <code class="literal">user.osrc</code> file
       and replace the values for the following entries to match your user
       credentials:
      </p><div class="verbatim-wrap"><pre class="screen">export OS_USERNAME=&lt;username&gt;
export OS_PASSWORD=&lt;password&gt;</pre></div></li><li class="listitem "><p>
       You will also need to edit the following lines for your environment:
      </p><div class="verbatim-wrap"><pre class="screen">## Change these values from 'unset' to 'export'
export OS_PROJECT_NAME=&lt;project_name&gt;
export OS_PROJECT_DOMAIN_NAME=Default</pre></div><p>
       Here is an example output:
      </p><div class="verbatim-wrap"><pre class="screen">unset OS_DOMAIN_NAME
export OS_IDENTITY_API_VERSION=3
export OS_AUTH_VERSION=3
export OS_PROJECT_NAME=test_project
export OS_PROJECT_DOMAIN_NAME=Default
export OS_USERNAME=testuser
export OS_USER_DOMAIN_NAME=Default
export OS_PASSWORD=testuser
export OS_AUTH_URL=http://192.168.245.9:35357/v3
export OS_ENDPOINT_TYPE=internalURL
# OpenstackClient uses OS_INTERFACE instead of OS_ENDPOINT
export OS_INTERFACE=internal
export OS_CACERT=/etc/ssl/certs/ca-certificates.crt</pre></div></li></ol></div></li><li class="listitem "><p>
     Source the environment variables for your user:
    </p><div class="verbatim-wrap"><pre class="screen">source ~/user.osrc</pre></div></li><li class="listitem "><p>
     Upload an image and publicize it:
    </p><div class="verbatim-wrap"><pre class="screen">openstack image create --name "upload me" --visibility public --container-format bare --disk-format qcow2 --file uploadme.txt</pre></div><p>
     Example output:
    </p><div class="verbatim-wrap"><pre class="screen">+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | dd75c3b840a16570088ef12f6415dd15     |
| container_format | bare                                 |
| created_at       | 2016-01-06T23:31:27Z                 |
| disk_format      | qcow2                                |
| id               | cf1490f4-1eb1-477c-92e8-15ebbe91da03 |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | upload me                            |
| owner            | bd24897932074780a20b780c4dde34c7     |
| protected        | False                                |
| size             | 10                                   |
| status           | active                               |
| tags             | []                                   |
| updated_at       | 2016-01-06T23:31:31Z                 |
| virtual_size     | None                                 |
| visibility       | public                               |
+------------------+--------------------------------------+</pre></div><div id="id-1.5.4.5.5.5.7.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      You can use the command <code class="command">openstack help image create</code> to
      get the full syntax for this command.
     </p></div></li></ol></div><p>
   <span class="bold"><strong>Assigning the nova_admin role</strong></span>
  </p><p>
   A user must have the role of admin in order to assign the nova_admin role.
   To assign the role, you will set the environment variables needed for the
   identity service administrator.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     First, source the identity service credentials:
    </p><div class="verbatim-wrap"><pre class="screen">source ~/keystone.osrc</pre></div></li><li class="listitem "><p>
     You can add the glance_admin role to a user on a project with this
     command:
    </p><div class="verbatim-wrap"><pre class="screen">openstack role add --user &lt;username&gt; --project &lt;project_name&gt; nova_admin</pre></div><p>
     Example, showing a user named <code class="literal">testuser</code> being granted
     the <code class="literal">glance_admin</code> role in the
     <code class="literal">test_project</code> project:
    </p><div class="verbatim-wrap"><pre class="screen">openstack role add --user testuser --project test_project nova_admin</pre></div></li><li class="listitem "><p>
     You can confirm the role assignment by listing out the roles:
    </p><div class="verbatim-wrap"><pre class="screen">openstack role assignment list --user &lt;username&gt;</pre></div><p>
     Example output:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code> openstack role assignment list --user testuser
+----------------------------------+----------------------------------+-------+----------------------------------+--------+-----------+
| Role                             | User                             | Group | Project                          | Domain | Inherited |
+----------------------------------+----------------------------------+-------+----------------------------------+--------+-----------+
| 8cdb02bab38347f3b65753099f3ab73c | 8bcfe10101964e0c8ebc4de391f3e345 |       | 0ebbf7640d7948d2a17ac08bbbf0ca5b |        | False     |
+----------------------------------+----------------------------------+-------+----------------------------------+--------+-----------+</pre></div></li><li class="listitem "><p>
     Note that only the role ID is displayed. To get the role name, execute the
     following:
    </p><div class="verbatim-wrap"><pre class="screen">openstack role show &lt;role_id&gt;</pre></div><p>
     Example output:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code> openstack role show 8cdb02bab38347f3b65753099f3ab73c
+-------+----------------------------------+
| Field | Value                            |
+-------+----------------------------------+
| id    | 8cdb02bab38347f3b65753099f3ab73c |
| name  | nova_admin                       |
+-------+----------------------------------+</pre></div></li><li class="listitem "><p>
     To demonstrate that the user has nova admin privileges, authenticate with
     those user creds and then upload and publish an image. Only a user with an
     admin role or glance_admin can publish an image.
    </p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       The easiest way to do this will be to make a copy of the
       <code class="literal">service.osrc</code> file and edit it with your user
       credentials. You can do that with this command:
      </p><div class="verbatim-wrap"><pre class="screen">cp ~/service.osrc ~/user.osrc</pre></div></li><li class="listitem "><p>
       Using your preferred editor, edit the <code class="literal">user.osrc</code> file
       and replace the values for the following entries to match your user
       credentials:
      </p><div class="verbatim-wrap"><pre class="screen">export OS_USERNAME=&lt;username&gt;
export OS_PASSWORD=&lt;password&gt;</pre></div></li><li class="listitem "><p>
       You will also need to edit the following lines for your environment:
      </p><div class="verbatim-wrap"><pre class="screen">## Change these values from 'unset' to 'export'
export OS_PROJECT_NAME=&lt;project_name&gt;
export OS_PROJECT_DOMAIN_NAME=Default</pre></div><p>
       Here is an example output:
      </p><div class="verbatim-wrap"><pre class="screen">unset OS_DOMAIN_NAME
export OS_IDENTITY_API_VERSION=3
export OS_AUTH_VERSION=3
export OS_PROJECT_NAME=test_project
export OS_PROJECT_DOMAIN_NAME=Default
export OS_USERNAME=testuser
export OS_USER_DOMAIN_NAME=Default
export OS_PASSWORD=testuser
export OS_AUTH_URL=http://192.168.245.9:35357/v3
export OS_ENDPOINT_TYPE=internalURL
# OpenstackClient uses OS_INTERFACE instead of OS_ENDPOINT
export OS_INTERFACE=internal
export OS_CACERT=/etc/ssl/certs/ca-certificates.crt</pre></div></li></ol></div></li><li class="listitem "><p>
     Source the environment variables for your user:
    </p><div class="verbatim-wrap"><pre class="screen">source ~/user.osrc</pre></div></li><li class="listitem "><p>
     List all of the virtual machines in the project specified in user.osrc:
    </p><div class="verbatim-wrap"><pre class="screen">openstack server list</pre></div><p>
     Example output showing no virtual machines, because there are no virtual
     machines created on the project specified in the user.osrc file:
    </p><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+-------------------------------------------------------+--------+-----------------------------------------------------------------+
| ID                                   | Name                                                  | Status | Networks                                                        |
+--------------------------------------+-------------------------------------------------------+--------+-----------------------------------------------------------------+
+--------------------------------------+-------------------------------------------------------+--------+-----------------------------------------------------------------+</pre></div></li><li class="listitem "><p>
     For this demonstration, we do have a virtual machine associated with a
     different project and because your user has nova_admin permissions, you
     can view those virtual machines using a slightly different command:
    </p><div class="verbatim-wrap"><pre class="screen">openstack server list --all-projects</pre></div><p>
     Example output, now showing a virtual machine:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code> openstack server list --all-projects
+--------------------------------------+-------------------------------------------------------+--------+-----------------------------------------------------------------+
| ID                                   | Name                                                  | Status | Networks                                                        |
+--------------------------------------+-------------------------------------------------------+--------+-----------------------------------------------------------------+
| da4f46e2-4432-411b-82f7-71ab546f91f3 | testvml                                               | ACTIVE |                                                                 |
+--------------------------------------+-------------------------------------------------------+--------+-----------------------------------------------------------------+</pre></div></li><li class="listitem "><p>
     You can also now delete virtual machines in other projects by using the
     <code class="literal">--all-tenants</code> switch:
    </p><div class="verbatim-wrap"><pre class="screen">openstack server delete --all-projects &lt;instance_id&gt;</pre></div><p>
     Example, showing us deleting the instance in the previous step:
    </p><div class="verbatim-wrap"><pre class="screen">openstack server delete --all-projects da4f46e2-4432-411b-82f7-71ab546f91f3</pre></div></li><li class="listitem "><p>
     You can get a full list of available commands by using this:
    </p><div class="verbatim-wrap"><pre class="screen">openstack -h</pre></div></li></ol></div><p>
   You can perform the same steps as above for the neutron and cinder service
   admin roles:
  </p><div class="verbatim-wrap"><pre class="screen">neutron_admin
cinder_admin</pre></div></div><div class="sect2" id="customize-policy"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Customize policy.json on the Cloud Lifecycle Manager</span> <a title="Permalink" class="permalink" href="#customize-policy">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-tutorials-using_cli.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-using_cli.xml</li><li><span class="ds-label">ID: </span>customize-policy</li></ul></div></div></div></div><p>
   One way to deploy <code class="filename">policy.json</code> for a service is by going to each of the
   target nodes and making changes there. This is not necessary anymore. This
   process has been streamlined and policy.json files can be edited on the
   Cloud Lifecycle Manager and then deployed to nodes. Please exercise caution when modifying
   policy.json files. It is best to validate the changes in a non-production
   environment before rolling out policy.json changes into production. It is
   not recommended that you make policy.json changes without a way to validate
   the desired policy behavior. Updated policy.json files can be deployed using
   the appropriate <code class="literal">&lt;service_name&gt;-reconfigure.yml</code>
   playbook.
  </p></div><div class="sect2" id="service-roles"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Roles</span> <a title="Permalink" class="permalink" href="#service-roles">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-tutorials-using_cli.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-using_cli.xml</li><li><span class="ds-label">ID: </span>service-roles</li></ul></div></div></div></div><p>
   Service roles represent the functionality used to implement the <span class="productname">OpenStack</span>
   role based access control (RBAC) model. This is used to manage access to
   each <span class="productname">OpenStack</span> service. Roles are named and assigned per user or group for
   each project by the identity service. Role definition and policy enforcement
   are defined outside of the identity service independently by each <span class="productname">OpenStack</span>
   service.
  </p><p>
   The token generated by the identity service for each user authentication
   contains the role(s) assigned to that user for a particular project. When a
   user attempts to access a specific <span class="productname">OpenStack</span> service, the role is parsed by
   the service, compared to the service-specific policy file, and then granted
   the resource access defined for that role by the service policy file.
  </p><p>
   Each service has its own service policy file with the
   <code class="literal">/etc/[SERVICE_CODENAME]/policy.json</code> file name format
   where <code class="literal">[SERVICE_CODENAME]</code> represents a specific <span class="productname">OpenStack</span>
   service name. For example, the <span class="productname">OpenStack</span> nova service would have a policy
   file called <code class="literal">/etc/nova/policy.json</code>.
  </p><p>
   Service policy files can be modified and deployed to control nodes from the
   Cloud Lifecycle Manager. Administrators are advised to validate policy changes before checking
   in the changes to the site branch of the local git repository before rolling
   the changes into production. Do not make changes to policy files without
   having a way to validate them.
  </p><p>
   The policy files are located at the following site branch directory on the
   Cloud Lifecycle Manager.
  </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/</pre></div><p>
   For test and validation, policy files can be modified in a non-production
   environment from the <code class="literal">~/scratch/</code> directory. For a specific
   policy file, run a search for <code class="literal">policy.json</code>. To deploy
   policy changes for a service, run the service specific reconfiguration
   playbook (for example, <code class="literal">nova-reconfigure.yml</code>). For a
   complete list of reconfiguration playbooks, change directories to
   <code class="literal">~/scratch/ansible/next/ardana/ansible</code> and run this
   command:
  </p><div class="verbatim-wrap"><pre class="screen">ls –l | grep reconfigure</pre></div><div id="note-j2-comments" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    Comments added to any <code class="filename">*.j2</code> files (including templates)
    must follow proper comment syntax. Otherwise you may see errors when
    running the config-processor or any of the service playbooks.
   </p></div></div></div><div class="sect1" id="log-management-integration"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Log Management and Integration</span> <a title="Permalink" class="permalink" href="#log-management-integration">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-tutorials-manage_logs.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-manage_logs.xml</li><li><span class="ds-label">ID: </span>log-management-integration</li></ul></div></div></div></div><div class="sect2" id="id-1.5.4.6.2"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Overview</span> <a title="Permalink" class="permalink" href="#id-1.5.4.6.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-tutorials-manage_logs.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-manage_logs.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses the ELK (Elasticsearch, Logstash, Kibana) stack for log
   management across the entire cloud infrastructure. This configuration
   facilitates simple administration as well as integration with third-party
   tools. This tutorial covers how to forward your logs to a third-party tool
   or service, and how to access and search the Elasticsearch log stores
   through API endpoints.
  </p></div><div class="sect2" id="section-pvm-zkx-1x"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">The ELK stack</span> <a title="Permalink" class="permalink" href="#section-pvm-zkx-1x">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-tutorials-manage_logs.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-manage_logs.xml</li><li><span class="ds-label">ID: </span>section-pvm-zkx-1x</li></ul></div></div></div></div><p>
   The ELK logging stack consists of the Elasticsearch, Logstash, and
   Kibana elements.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="formalpara-title">Elasticsearch. </span>
      Elasticsearch is the storage and indexing component of the ELK stack.
      It stores and indexes the data received from Logstash. Indexing makes
      your log data searchable by tools designed for querying and analyzing
      massive sets of data. You can query the Elasticsearch datasets from the
      built-in Kibana console, a third-party data analysis tool, or through
      the Elasticsearch API (covered later).
     </p></li><li class="listitem "><p><span class="formalpara-title">Logstash. </span>
      Logstash reads the log data from the services running on your servers,
      and then aggregates and ships that data to a storage location. By
      default, Logstash sends the data to the Elasticsearch indexes, but it
      can also be configured to send data to other storage and indexing tools
      such as Splunk.
     </p></li><li class="listitem "><p><span class="formalpara-title">Kibana. </span>
      Kibana provides a simple and easy-to-use method for searching,
      analyzing, and visualizing the log data stored in the Elasticsearch
      indexes. You can customize the Kibana console to provide graphs,
      charts, and other visualizations of your log data.
     </p></li></ul></div></div><div class="sect2" id="section-d3k-gnx-1x"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Elasticsearch API</span> <a title="Permalink" class="permalink" href="#section-d3k-gnx-1x">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-tutorials-manage_logs.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-manage_logs.xml</li><li><span class="ds-label">ID: </span>section-d3k-gnx-1x</li></ul></div></div></div></div><p>
   You can query the Elasticsearch indexes through various language-specific
   APIs, as well as directly over the IP address and port that Elasticsearch
   exposes on your implementation. By default, Elasticsearch presents from
   localhost, port 9200. You can run queries directly from a terminal using
   <code class="literal">curl</code>. For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>curl -XGET 'http://localhost:9200/_search?q=tag:yourSearchTag'</pre></div><p>
   The preceding command searches all indexes for all data with the
   "yourSearchTag" tag.
  </p><p>
   You can also use the Elasticsearch API from outside the logging node. This
   method connects over the Kibana VIP address, port 5601, using basic http
   authentication. For example, you can use the following command to perform
   the same search as the preceding search:
  </p><div class="verbatim-wrap"><pre class="screen">curl -u kibana:&lt;password&gt; kibana_vip:5601/_search?q=tag:yourSearchTag</pre></div><p>
   You can further refine your search to a specific index of data, in this case
   the "elasticsearch" index:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>curl -XGET 'http://localhost:9200/elasticsearch/_search?q=tag:yourSearchTag'</pre></div><p>
   The search API is RESTful, so responses are provided in JSON format. Here's
   a sample (though empty) response:
  </p><div class="verbatim-wrap"><pre class="screen">{
    "took":13,
    "timed_out":false,
    "_shards":{
        "total":45,
        "successful":45,
        "failed":0
    },
    "hits":{
        "total":0,
        "max_score":null,
        "hits":[]
    }
}</pre></div></div><div class="sect2" id="id-1.5.4.6.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="#id-1.5.4.6.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-tutorials-manage_logs.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-manage_logs.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   You can find more detailed Elasticsearch API documentation at
   <a class="link" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search.html" target="_blank">https://www.elastic.co/guide/en/elasticsearch/reference/current/search.html</a>.
  </p><p>
   Review the Elasticsearch Python API documentation at the following sources:
   <a class="link" href="http://elasticsearch-py.readthedocs.io/en/master/api.html" target="_blank">http://elasticsearch-py.readthedocs.io/en/master/api.html</a>
  </p><p>
   Read the Elasticsearch Java API documentation at
   <a class="link" href="https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/index.html" target="_blank">https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/index.html</a>.
  </p></div><div class="sect2" id="section-knd-hcf-bx"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Forwarding your logs</span> <a title="Permalink" class="permalink" href="#section-knd-hcf-bx">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-tutorials-manage_logs.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-manage_logs.xml</li><li><span class="ds-label">ID: </span>section-knd-hcf-bx</li></ul></div></div></div></div><p>
   You can configure Logstash to ship your logs to an outside storage and
   indexing system, such as Splunk. Setting up this configuration is as simple
   as editing a few configuration files, and then running the Ansible playbooks
   that implement the changes. Here are the steps.
  </p><div class="orderedlist " id="ol-mtz-mnr-bx"><ol class="orderedlist" type="1"><li class="listitem "><p>
     Begin by logging in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Verify that the logging system is up and running:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts logging-status.yml</pre></div><p>
     When the preceding playbook completes without error, proceed to the next
     step.
    </p></li><li class="listitem "><p>
     Edit the Logstash configuration file, found at the following location:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/logging-server/templates/logstash.conf.j2</pre></div><p>
     Near the end of the Logstash configuration file, you will find a section for
     configuring Logstash output destinations. The following example
     demonstrates the changes necessary to forward your logs to an outside
     server (changes in bold). The configuration block sets up a TCP connection
     to the destination server's IP address over port 5514.
    </p><div class="verbatim-wrap"><pre class="screen"># Logstash outputs
    output {
      # Configure Elasticsearch output
      # http://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html
      elasticsearch {
        index =&gt; "${[@metadata][es_index]"}
        hosts =&gt; ["{{ elasticsearch_http_host }}:{{ elasticsearch_http_port }}"]
        flush_size =&gt; {{ logstash_flush_size }}
        idle_flush_time =&gt; 5
        workers =&gt; {{ logstash_threads }}
      }
      <span class="bold"><strong>  # Forward Logs to Splunk on TCP port 5514 which matches the one specified in Splunk Web UI.
      tcp {
        mode =&gt; "client"
        host =&gt; "&lt;Enter Destination listener IP address&gt;"
        port =&gt; 5514
      }</strong></span>
    }</pre></div><p>
     Logstash can forward log data to multiple sources, so there is no need to
     remove or alter the Elasticsearch section in the preceding file.
     However, if you choose to stop forwarding your log data to
     Elasticsearch, you can do so by removing the related section in this
     file, and then continue with the following steps.
    </p></li><li class="listitem "><p>
     Commit your changes to the local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Your commit message"</pre></div></li><li class="listitem "><p>
     Run the configuration processor to check the status of all configuration
     files:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Run the ready-deployment playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Implement the changes to the Logstash configuration file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts kronos-server-configure.yml</pre></div></li></ol></div><p>
   Configuring the receiving service will vary from product to product. Consult
   the documentation for your particular product for instructions on how to set
   it up to receive log files from Logstash.
  </p></div></div><div class="sect1" id="Integrating-Kibana-with-Splunk"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Integrating Your Logs with Splunk</span> <a title="Permalink" class="permalink" href="#Integrating-Kibana-with-Splunk">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-tutorials-integrating_logstash_splunk.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-integrating_logstash_splunk.xml</li><li><span class="ds-label">ID: </span>Integrating-Kibana-with-Splunk</li></ul></div></div></div></div><div class="sect2" id="idg-all-operations-tutorials-integrating-logstash-splunk-xml-2"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Integrating with Splunk</span> <a title="Permalink" class="permalink" href="#idg-all-operations-tutorials-integrating-logstash-splunk-xml-2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-tutorials-integrating_logstash_splunk.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-integrating_logstash_splunk.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-tutorials-integrating-logstash-splunk-xml-2</li></ul></div></div></div></div><p>
   The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 logging solution provides a flexible and extensible
   framework to centralize the collection and processing of logs from all nodes
   in your cloud. The logs are shipped to a highly available and fault-tolerant
   cluster where they are transformed and stored for better searching and
   reporting. The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 logging solution uses the ELK stack
   (Elasticsearch, Logstash and Kibana) as a production-grade implementation
   and can support other storage and indexing technologies.
  </p><p>
   You can configure Logstash, the service that aggregates and forwards the
   logs to a searchable index, to send the logs to a third-party target, such
   as Splunk.
  </p><p>
   For how to integrate the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 centralized
   logging solution with Splunk, including the steps to set up and forward
   logs, please refer to <a class="xref" href="#splunk-integration" title="4.1. Splunk Integration">Section 4.1, “Splunk Integration”</a>.
  </p></div></div><div class="sect1" id="LDAP-Integration"><div class="titlepage"><div><div><h2 class="title"><span class="number">2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Integrating <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> with an LDAP System</span> <a title="Permalink" class="permalink" href="#LDAP-Integration">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-tutorials-keystone_ldap_integration.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-keystone_ldap_integration.xml</li><li><span class="ds-label">ID: </span>LDAP-Integration</li></ul></div></div></div></div><p>
  You can configure your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud to work with an outside user
  authentication source such as Active Directory or OpenLDAP. keystone, the
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> identity service, functions as the first stop for any user
  authorization/authentication requests. keystone can also function as a proxy
  for user account authentication, passing along authentication and
  authorization requests to any LDAP-enabled system that has been configured as
  an outside source. This type of integration lets you use an existing
  user-management system such as Active Directory and its powerful group-based
  organization features as a source for permissions in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p><p>
  Upon successful completion of this tutorial, your cloud will refer user
  authentication requests to an outside LDAP-enabled directory system, such as
  Microsoft Active Directory or OpenLDAP.
 </p><div class="sect2" id="id-1.5.4.8.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">2.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure your LDAP source</span> <a title="Permalink" class="permalink" href="#id-1.5.4.8.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-tutorials-keystone_ldap_integration.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-tutorials-keystone_ldap_integration.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To configure your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud to use an outside user-management source,
   perform the following steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Make sure that the LDAP-enabled system you plan to integrate with is up
     and running and accessible over the necessary ports from your cloud
     management network.
    </p></li><li class="step "><p>
     Edit the
     <code class="filename">/var/lib/ardana/openstack/my_cloud/config/keystone/keystone.conf.j2</code>
     file and set the following options:
    </p><div class="verbatim-wrap"><pre class="screen">domain_specific_drivers_enabled = True
domain_configurations_from_database = False</pre></div></li><li class="step " id="st-keystone-ldap-create-yaml"><p>
     Create a YAML file in the
     <code class="filename">/var/lib/ardana/openstack/my_cloud/config/keystone/</code>
     directory that defines your LDAP connection. You can make a copy of the
     sample keystone-LDAP configuration file, and then edit that file with the
     details of your LDAP connection.
    </p><p>
     The following example copies the
     <code class="filename">keystone_configure_ldap_sample.yml</code> file and names the
     new file <code class="filename">keystone_configure_ldap_my.yml</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cp /var/lib/ardana/openstack/my_cloud/config/keystone/keystone_configure_ldap_sample.yml \
  /var/lib/ardana/openstack/my_cloud/config/keystone/keystone_configure_ldap_my.yml</pre></div></li><li class="step "><p>
     Edit the new file to define the connection to your LDAP source. This guide
     does not provide comprehensive information on all aspects of the
     <code class="literal">keystone_configure_ldap.yml</code> file. Find a complete list
     of keystone/LDAP configuration file options at:
     <a class="link" href="https://github.com/openstack/keystone/tree/stable/rocky/etc" target="_blank">https://github.com/openstack/keystone/tree/stable/rocky/etc</a>
    </p><p>
     The following file illustrates an example keystone configuration that is
     customized for an Active Directory connection.
    </p><div class="verbatim-wrap"><pre class="screen">keystone_domainldap_conf:

    # CA certificates file content.
    # Certificates are stored in Base64 PEM format. This may be entire LDAP server
    # certificate (in case of self-signed certificates), certificate of authority
    # which issued LDAP server certificate, or a full certificate chain (Root CA
    # certificate, intermediate CA certificate(s), issuer certificate).
    #
    cert_settings:
      cacert: |
        -----BEGIN CERTIFICATE-----

        certificate appears here

        -----END CERTIFICATE-----

    # A domain will be created in MariaDB with this name, and associated with ldap back end.
    # Installer will also generate a config file named /etc/keystone/domains/keystone.&lt;domain_name&gt;.conf
    #
    domain_settings:
      name: ad
      description: Dedicated domain for ad users

    conf_settings:
      identity:
         driver: ldap


      # For a full list and description of ldap configuration options, please refer to
      # http://docs.openstack.org/liberty/config-reference/content/keystone-configuration-file.html.
      #
      # Please note:
      #  1. LDAP configuration is read-only. Configuration which performs write operations (i.e. creates users, groups, etc)
      #     is not supported at the moment.
      #  2. LDAP is only supported for identity operations (reading users and groups from LDAP). Assignment
      #     operations with LDAP (i.e. managing roles, projects) are not supported.
      #  3. LDAP is configured as non-default domain. Configuring LDAP as a default domain is not supported.
      #

      ldap:
        url: ldap://<em class="replaceable ">YOUR_COMPANY_AD_URL</em>
        suffix: <em class="replaceable ">YOUR_COMPANY_DC</em>
        query_scope: sub
        user_tree_dn: CN=Users,<em class="replaceable ">YOUR_COMPANY_DC</em>
        user : CN=admin,CN=Users,<em class="replaceable ">YOUR_COMPANY_DC</em>
        password: REDACTED
        user_objectclass: user
        user_id_attribute: cn
        user_name_attribute: cn
        group_tree_dn: CN=Users,<em class="replaceable ">YOUR_COMPANY_DC</em>
        group_objectclass: group
        group_id_attribute: cn
        group_name_attribute: cn
        use_pool: True
        user_enabled_attribute: userAccountControl
        user_enabled_mask: 2
        user_enabled_default: 512
        use_tls: True
        tls_req_cert: demand
        # if you are configuring multiple LDAP domains, and LDAP server certificates are issued
        # by different authorities, make sure that you place certs for all the LDAP backend domains in the
        # cacert parameter as seen in this sample yml file so that all the certs are combined in a single CA file
        # and every LDAP domain configuration points to the combined CA file.
        # Note:
        # 1. Please be advised that every time a new ldap domain is configured, the single CA file gets overwritten
        # and hence ensure that you place certs for all the LDAP backend domains in the cacert parameter.
        # 2. There is a known issue on one cert per CA file per domain when the system processes
        # concurrent requests to multiple LDAP domains. Using the single CA file with all certs combined
        # shall get the system working properly.

        tls_cacertfile: /etc/keystone/ssl/certs/all_ldapdomains_ca.pem</pre></div></li><li class="step "><p>
     Add your new file to the local Git repository and commit the changes.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git checkout site
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Adding LDAP server integration config"</pre></div></li><li class="step "><p>
     Run the configuration processor and deployment preparation playbooks to
     validate the YAML files and prepare the environment for configuration.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the keystone reconfiguration playbook to implement your changes,
     passing the newly created YAML file as an argument to the
     <code class="literal">-e@<em class="replaceable ">FILE_PATH</em></code> parameter:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml \
  -e@/var/lib/ardana/openstack/my_cloud/config/keystone/keystone_configure_ldap_my.yml</pre></div><p>
     To integrate your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud with multiple domains, repeat these
     steps starting from <a class="xref" href="#st-keystone-ldap-create-yaml" title="Step 3">Step 3</a>
     for each domain.
    </p></li></ol></div></div></div></div></div><div class="chapter " id="clm-admin-ui"><div class="titlepage"><div><div><h1 class="title"><span class="number">3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cloud Lifecycle Manager Admin UI User Guide</span> <a title="Permalink" class="permalink" href="#clm-admin-ui">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/day2_ui_documentation.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>day2_ui_documentation.xml</li><li><span class="ds-label">ID: </span>clm-admin-ui</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#accessing-clm-admin-ui"><span class="number">3.1 </span><span class="name">Accessing the Admin UI</span></a></span></dt><dt><span class="section"><a href="#id-1.5.5.4"><span class="number">3.2 </span><span class="name">Admin UI Pages</span></a></span></dt><dt><span class="section"><a href="#clm-admin-topology"><span class="number">3.3 </span><span class="name">Topology</span></a></span></dt><dt><span class="section"><a href="#clm-admin-addserver"><span class="number">3.4 </span><span class="name">Server Management</span></a></span></dt><dt><span class="section"><a href="#clm-server-replacement"><span class="number">3.5 </span><span class="name">Server Replacement</span></a></span></dt></dl></div></div><p>
  The Cloud Lifecycle Manager Admin UI is a web-based GUI for viewing and managing the
  configuration of an installed cloud. After successfully deploying the cloud
  with the Install UI, the final screen displays a link to the CLM Admin UI.
  (For example, see <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 21 “Installing with the Install UI”, Section 21.5 “Running the Install UI”, Cloud Deployment Successful</span>). Usually the URL
  associated with this link is
  <code class="literal">https://<em class="replaceable ">DEPLOYER_MGMT_NET_IP</em>:9085</code>,
  although it may be different depending on the cloud configuration and the
  installed version of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p><div class="sect1" id="accessing-clm-admin-ui"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Accessing the Admin UI</span> <a title="Permalink" class="permalink" href="#accessing-clm-admin-ui">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/day2_ui_documentation.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>day2_ui_documentation.xml</li><li><span class="ds-label">ID: </span>accessing-clm-admin-ui</li></ul></div></div></div></div><p>
   In a browser, go to
   <code class="literal">https://<em class="replaceable ">DEPLOYER_MGMT_NET_IP</em>:9085</code>.
  </p><p>
   The
   <code class="literal"><em class="replaceable ">DEPLOYER_MGMT_NET_IP</em>:<em class="replaceable ">PORT_NUMBER</em></code>
   is not necessarily the same for all installations, and can be displayed with
   the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack endpoint list --service ardana --interface admin -c URL</pre></div><p>
   Accessing the Cloud Lifecycle Manager Admin UI requires access to the
   <span class="bold"><strong>MANAGEMENT</strong></span> network that was configured
   when the Cloud was deployed. Access to this network is necessary to be able
   to access the Cloud Lifecycle Manager Admin UI and log in. Depending on the network setup, it
   may be necessary to use an SSH tunnel similar to what is recommended in
   <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 21 “Installing with the Install UI”, Section 21.5 “Running the Install UI”</span>. The Admin UI requires keystone and
   HAProxy to be running and to be accesible. If keystone or HAProxy are
   not running, cloud reconfiguration is limited to the command line.
  </p><p>
   Logging in requires a keystone user. If the user is not an admin on the
   default domain and one or more projects, the Cloud Lifecycle Manager Admin UI will not display
   information about the Cloud and may present errors.
  </p><div class="figure" id="admin-ui-login-page"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_ui_login_page.png" target="_blank"><img src="images/clm_admin_ui_login_page.png" width="" alt="Cloud Lifecycle Manager Admin UI Login Page" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.1: </span><span class="name">Cloud Lifecycle Manager Admin UI Login Page </span><a title="Permalink" class="permalink" href="#admin-ui-login-page">#</a></h6></div></div></div><div class="sect1" id="id-1.5.5.4"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Admin UI Pages</span> <a title="Permalink" class="permalink" href="#id-1.5.5.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/day2_ui_documentation.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>day2_ui_documentation.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="sect2" id="clm-admin-ui-services"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Services</span> <a title="Permalink" class="permalink" href="#clm-admin-ui-services">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/day2_ui_documentation.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>day2_ui_documentation.xml</li><li><span class="ds-label">ID: </span>clm-admin-ui-services</li></ul></div></div></div></div><p>
    Services pages relay information about the various OpenStack and other
    services that have been deployed as part of the cloud. Service information
    displays the list of services registered with keystone and the endpoints
    associated with those services. The information is equivalent to running
    the command <code class="command">openstack endpoint list</code>.
   </p><p>
    The <code class="literal">Service Information</code> table contains the following
    information, based on how the service is registered with keystone:
   </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.5.4.2.4.1"><span class="term ">Name</span></dt><dd><p>
       The name of the service, this may be an OpenStack code name
      </p></dd><dt id="id-1.5.5.4.2.4.2"><span class="term ">Description</span></dt><dd><p>
       Service description, for some services this is a repeat of the name
      </p></dd><dt id="id-1.5.5.4.2.4.3"><span class="term ">Endpoints</span></dt><dd><p>
       Services typically have 1 or more endpoints that are accessible to make
       API calls. The most common configuration is for a service to have
       <code class="literal">Admin</code>, <code class="literal">Public</code>, and
       <code class="literal">Internal</code> endpoints, with each intended for access by
       consumers corresponding to the type of endpoint.
      </p></dd><dt id="id-1.5.5.4.2.4.4"><span class="term ">Region</span></dt><dd><p>
       Service endpoints are part of a region. In multi-region clouds, some
       services will have endpoints in multiple regions.
      </p></dd></dl></div><div class="figure" id="admin-ui-service-info"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/service_information.png" target="_blank"><img src="images/service_information.png" width="" alt="Cloud Lifecycle Manager Admin UI Service Information" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.2: </span><span class="name">Cloud Lifecycle Manager Admin UI Service Information </span><a title="Permalink" class="permalink" href="#admin-ui-service-info">#</a></h6></div></div></div><div class="sect2" id="admin-ui-suse-cloud-packages"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Packages</span> <a title="Permalink" class="permalink" href="#admin-ui-suse-cloud-packages">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/day2_ui_documentation.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>day2_ui_documentation.xml</li><li><span class="ds-label">ID: </span>admin-ui-suse-cloud-packages</li></ul></div></div></div></div><p>
    The <span class="guimenu ">Packages</span> tab displays packages that are part of the
    SUSE OpenStack Cloud product.
   </p><p>
    The <code class="literal">SUSE Cloud Packages</code> table contains the following:
   </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.5.4.3.4.1"><span class="term ">Name</span></dt><dd><p>
       The name of the SUSE Cloud package
      </p></dd><dt id="id-1.5.5.4.3.4.2"><span class="term ">Version</span></dt><dd><p>
       The version of the package which is installed in the Cloud
      </p></dd></dl></div><div class="figure" id="admin-ui-cloud-package-versions"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/cloud_package_versions.png" target="_blank"><img src="images/cloud_package_versions.png" width="" alt="Cloud Lifecycle Manager Admin UI SUSE Cloud Package" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.3: </span><span class="name">Cloud Lifecycle Manager Admin UI SUSE Cloud Package </span><a title="Permalink" class="permalink" href="#admin-ui-cloud-package-versions">#</a></h6></div></div><div id="id-1.5.5.4.3.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     Packages with the <code class="literal">venv-</code> prefix denote the version of
     the specific OpenStack package that is deployed. The release name can be
     determined from the <a class="link" href="https://releases.openstack.org/" target="_blank">OpenStack Releases</a>
     page.
    </p></div></div><div class="sect2" id="admin-ui-suse-cloud-configuration"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuration</span> <a title="Permalink" class="permalink" href="#admin-ui-suse-cloud-configuration">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/day2_ui_documentation.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>day2_ui_documentation.xml</li><li><span class="ds-label">ID: </span>admin-ui-suse-cloud-configuration</li></ul></div></div></div></div><p>
    The <span class="guimenu ">Configuration</span> tab displays services that are
    deployed in the cloud and the configuration files associated with those
    services. Services may be reconfigured by editing the
    <code class="filename">.j2</code> files listed and clicking the
    <span class="guimenu ">Update</span> button.
   </p><p>
    This page also provides the ability to set up SUSE Enterprise Storage Integration after
    initial deployment.
   </p><div class="figure" id="admin-ui-cloud-service-configuration"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_services_configuration_0.png" target="_blank"><img src="images/clm_admin_services_configuration_0.png" width="" alt="Cloud Lifecycle Manager Admin UI SUSE Service Configuration" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.4: </span><span class="name">Cloud Lifecycle Manager Admin UI SUSE Service Configuration </span><a title="Permalink" class="permalink" href="#admin-ui-cloud-service-configuration">#</a></h6></div></div><p>
    Clicking one of the listed configuration files opens the file editor where
    changes can be made. Asterisks identify files that have been edited but
    have not had their updates applied to the cloud.
   </p><div class="figure" id="admin-ui-cloud-service-configuration-editor"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_services_configuration_1.png" target="_blank"><img src="images/clm_admin_services_configuration_1.png" width="" alt="Cloud Lifecycle Manager Admin UI SUSE Service Configuration Editor" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.5: </span><span class="name">Cloud Lifecycle Manager Admin UI SUSE Service Configuration Editor </span><a title="Permalink" class="permalink" href="#admin-ui-cloud-service-configuration-editor">#</a></h6></div></div><p>
    After editing the service configuration, click the
    <span class="guimenu ">Update</span> button to begin deploying configuration changes
    to the cloud. The status of those changes will be streamed to the UI.
   </p><p>
    <span class="bold"><strong>Configure SUSE Enterprise Storage After Initial Deployment</strong></span>
   </p><p>
    A link to the <code class="filename">settings.yml</code> file is available under the
    <code class="literal">ses</code> selection on the <span class="guimenu ">Configuration</span> tab.
   </p><p>
    To set up SUSE Enterprise Storage Integration:
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Click on the link to edit the <code class="filename">settings.yml</code> file.
     </p></li><li class="step "><p>
      Uncomment the <code class="literal">ses_config_path</code> parameter, specify the
      location on the deployer host containing the
      <code class="filename">ses_config.yml</code> file, and save the
      <code class="filename">settings.yml</code> file.
     </p></li><li class="step "><p>
      If the <code class="filename">ses_config.yml</code> file does not yet exist in
      that location on the deployer host, a new link will appear for uploading
      a file from your local workstation.
     </p></li><li class="step "><p>
      When <code class="filename">ses_config.yml</code> is present on the deployer host,
      it will appear in the <code class="literal">ses</code> section of the
      <span class="guimenu ">Configuration</span> tab and can be edited directly
      there.
     </p></li></ol></div></div><div id="id-1.5.5.4.4.12" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     If the cloud is configured using self-signed certificates, the streaming
     status updates (including the log) may be interupted and require a reload
     of the CLM Admin UI. See
     <span class="intraxref">Book “Security Guide”, Chapter 8 “Transport Layer Security (TLS) Overview”, Section 8.2 “TLS Configuration”</span> for details on using
     signed certificates.
    </p></div><div class="figure" id="admin-ui-cloud-service-configuration-update"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_services_configuration_2.png" target="_blank"><img src="images/clm_admin_services_configuration_2.png" width="" alt="Cloud Lifecycle Manager Admin UI SUSE Service Configuration Update" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.6: </span><span class="name">Cloud Lifecycle Manager Admin UI SUSE Service Configuration Update </span><a title="Permalink" class="permalink" href="#admin-ui-cloud-service-configuration-update">#</a></h6></div></div></div><div class="sect2" id="admin-ui-suse-cloud-services-model"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Model</span> <a title="Permalink" class="permalink" href="#admin-ui-suse-cloud-services-model">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/day2_ui_documentation.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>day2_ui_documentation.xml</li><li><span class="ds-label">ID: </span>admin-ui-suse-cloud-services-model</li></ul></div></div></div></div><p>
    The <span class="guimenu ">Model</span> tab displays input models that are deployed in
    the cloud and the associated model files. The model files listed can be
    modified.
   </p><div class="figure" id="admin-ui-cloud-services-model"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_services_model_0.png" target="_blank"><img src="images/clm_admin_services_model_0.png" width="" alt="Cloud Lifecycle Manager Admin UI SUSE Service Model" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.7: </span><span class="name">Cloud Lifecycle Manager Admin UI SUSE Service Model </span><a title="Permalink" class="permalink" href="#admin-ui-cloud-services-model">#</a></h6></div></div><p>
    Clicking one of the listed model files opens the file editor where changes
    can be made. Asterisks identify files that have been edited but have
    not had their updates applied to the cloud.
   </p><div class="figure" id="admin-ui-cloud-services-model-editor"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_services_model_1.png" target="_blank"><img src="images/clm_admin_services_model_1.png" width="" alt="Cloud Lifecycle Manager Admin UI SUSE Service Model Editor" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.8: </span><span class="name">Cloud Lifecycle Manager Admin UI SUSE Service Model Editor </span><a title="Permalink" class="permalink" href="#admin-ui-cloud-services-model-editor">#</a></h6></div></div><p>
    After editing the model file, click the <span class="guimenu ">Validate</span> button
    to validate changes. If validation is successful, <span class="guimenu ">Update</span>
    is enabled. Click the <span class="guimenu ">Update</span> button to deploy the
    changes to the cloud. Before starting deployment, a confirmation dialog
    shows the choices of only running
    <code class="filename">config-processor-run.yml</code> and
    <code class="filename">ready-deployment.yml</code> playbooks or running a full
    deployment. It also indicates the risk of updating the deployed cloud.
   </p><div class="figure" id="admin-ui-cloud-services-model-confirmation"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_services_model_2.png" target="_blank"><img src="images/clm_admin_services_model_2.png" width="" alt="Cloud Lifecycle Manager Admin UI SUSE Service Model Confirmation" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.9: </span><span class="name">Cloud Lifecycle Manager Admin UI SUSE Service Model Confirmation </span><a title="Permalink" class="permalink" href="#admin-ui-cloud-services-model-confirmation">#</a></h6></div></div><p>
    Click <span class="guimenu ">Update</span> to start deployment. The status of the
    changes will be streamed to the UI.
   </p><div id="id-1.5.5.4.5.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     If the cloud is configured using self-signed certificates, the streaming
     status updates (including the log) may be interrupted. The CLM Admin UI
     must be reloaded. See <span class="intraxref">Book “Security Guide”, Chapter 8 “Transport Layer Security (TLS) Overview”, Section 8.2 “TLS Configuration”</span>
     for details on using signed certificates.
    </p></div><div class="figure" id="admin-ui-cloud-services-model-update"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_services_model_3.png" target="_blank"><img src="images/clm_admin_services_model_3.png" width="" alt="Cloud Lifecycle Manager Admin UI SUSE Service Model Update" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.10: </span><span class="name">Cloud Lifecycle Manager Admin UI SUSE Service Model Update </span><a title="Permalink" class="permalink" href="#admin-ui-cloud-services-model-update">#</a></h6></div></div></div><div class="sect2" id="admin-ui-roles"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Roles</span> <a title="Permalink" class="permalink" href="#admin-ui-roles">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/day2_ui_documentation.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>day2_ui_documentation.xml</li><li><span class="ds-label">ID: </span>admin-ui-roles</li></ul></div></div></div></div><p>
    The <span class="guimenu ">Services Per Role</span> tab displays the list of all roles
    that have been defined in the Cloud Lifecycle Manager input model, the list of servers that
    role, and the services installed on those servers.
   </p><p>
    The <code class="literal">Services Per Role</code> table contains the following:
   </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.5.4.6.4.1"><span class="term ">Role</span></dt><dd><p>
       The name of the role in the data model. In the included data model
       templates, these names are descriptive, such as
       <code class="literal">MTRMON-ROLE</code> for a metering and monitoring server.
       There is no strict constraint on role names and they may have been
       altered at install time.
      </p></dd><dt id="id-1.5.5.4.6.4.2"><span class="term ">Servers</span></dt><dd><p>
       The model IDs for the servers that have been assigned this role. This
       does not necessarily correspond to any DNS or other naming labels a host
       has, unless the host ID was set that way during install.
      </p></dd><dt id="id-1.5.5.4.6.4.3"><span class="term ">Services</span></dt><dd><p>
       A list of <span class="productname">OpenStack</span> and other Cloud related services that comprise this
       role. Servers that have been assigned this role will have these services
       installed and enabled.
      </p></dd></dl></div><div class="figure" id="admin-ui-cloud-pkg-versions"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/server_role_information.png" target="_blank"><img src="images/server_role_information.png" width="" alt="Cloud Lifecycle Manager Admin UI Services Per Role" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.11: </span><span class="name">Cloud Lifecycle Manager Admin UI Services Per Role </span><a title="Permalink" class="permalink" href="#admin-ui-cloud-pkg-versions">#</a></h6></div></div></div><div class="sect2" id="admin-ui-servers"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Servers</span> <a title="Permalink" class="permalink" href="#admin-ui-servers">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/day2_ui_documentation.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>day2_ui_documentation.xml</li><li><span class="ds-label">ID: </span>admin-ui-servers</li></ul></div></div></div></div><p>
    The <span class="guimenu ">Servers</span> pages contain information about the hardware
    that comprises the cloud, including the configuration of the servers, and
    the ability to add new compute nodes to the cloud.
   </p><p>
    The <code class="literal">Servers</code> table contains the following information:
   </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.5.4.7.4.1"><span class="term ">ID</span></dt><dd><p>
       This is the ID of the server in the data model. This does not
       necessarily correspond to any DNS or other naming labels a host has,
       unless the host ID was set that way during install.
      </p></dd><dt id="id-1.5.5.4.7.4.2"><span class="term ">IP Address</span></dt><dd><p>
       The management network IP address of the server
      </p></dd><dt id="id-1.5.5.4.7.4.3"><span class="term ">Server Group</span></dt><dd><p>
       The server group which this server is assigned to
      </p></dd><dt id="id-1.5.5.4.7.4.4"><span class="term ">NIC Mapping</span></dt><dd><p>
       The NIC mapping that describes the PCI slot addresses for the servers
       ethernet adapters
      </p></dd><dt id="id-1.5.5.4.7.4.5"><span class="term ">Mac Address</span></dt><dd><p>
       The hardware address of the servers primary physical ethernet adapter
      </p></dd></dl></div><div class="figure" id="admin-ui-server-summary"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/server_summary.png" target="_blank"><img src="images/server_summary.png" width="" alt="Cloud Lifecycle Manager Admin UI Server Summary" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.12: </span><span class="name">Cloud Lifecycle Manager Admin UI Server Summary </span><a title="Permalink" class="permalink" href="#admin-ui-server-summary">#</a></h6></div></div></div><div class="sect2" id="admin-ui-server-details"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.2.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Admin UI Server Details</span> <a title="Permalink" class="permalink" href="#admin-ui-server-details">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/day2_ui_documentation.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>day2_ui_documentation.xml</li><li><span class="ds-label">ID: </span>admin-ui-server-details</li></ul></div></div></div></div><p>
    <code class="literal">Server Details</code> can be viewed by clicking the menu at the
    right side of each row in the <code class="literal">Servers</code> table, the server
    details dialog contains the information from the Servers table and the
    following additional fields:
   </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.5.4.8.3.1"><span class="term ">IPMI IP Address</span></dt><dd><p>
       The IPMI network address, this may be empty if the server was
       provisioned prior to being added to the Cloud
      </p></dd><dt id="id-1.5.5.4.8.3.2"><span class="term ">IPMI Username</span></dt><dd><p>
       The username that was specified for IPMI access
      </p></dd><dt id="id-1.5.5.4.8.3.3"><span class="term ">IPMI Password</span></dt><dd><p>
       This is obscured in the readonly dialog, but is editable when adding a
       new server
      </p></dd><dt id="id-1.5.5.4.8.3.4"><span class="term ">Network Interfaces</span></dt><dd><p>
       The network interfaces configured on the server
      </p></dd><dt id="id-1.5.5.4.8.3.5"><span class="term ">Filesystem Utilization</span></dt><dd><p>
       Filesystem usage (percentage of filesystem in use). Only available if
       monasca is in use
      </p></dd></dl></div><div class="figure" id="admin-ui-server-summary-details"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/server_summary_details.png" target="_blank"><img src="images/server_summary_details.png" width="" alt="Server Details (1/2)" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.13: </span><span class="name">Server Details (1/2) </span><a title="Permalink" class="permalink" href="#admin-ui-server-summary-details">#</a></h6></div></div><div class="figure" id="admin-ui-server-summary-details-more"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/server_summary_details_more.png" target="_blank"><img src="images/server_summary_details_more.png" width="" alt="Server Details (2/2)" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.14: </span><span class="name">Server Details (2/2) </span><a title="Permalink" class="permalink" href="#admin-ui-server-summary-details-more">#</a></h6></div></div></div></div><div class="sect1" id="clm-admin-topology"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Topology</span> <a title="Permalink" class="permalink" href="#clm-admin-topology">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/clm_admin_topology.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>clm_admin_topology.xml</li><li><span class="ds-label">ID: </span>clm-admin-topology</li></ul></div></div></div></div><p>
  The topology section of the Cloud Lifecycle Manager Admin UI displays an overview of how the
  Cloud is configured. Each section of the topology represents some facet of
  the Cloud configuration and provides a visual layout of the way components are
  associated with each other. Many of the components in the topology are linked
  to each other, and can be navigated between by clicking on any component that
  appears as a hyperlink.
 </p><div class="sect2" id="clm-admin-control-planes"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Control Planes</span> <a title="Permalink" class="permalink" href="#clm-admin-control-planes">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/clm_admin_topology.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>clm_admin_topology.xml</li><li><span class="ds-label">ID: </span>clm-admin-control-planes</li></ul></div></div></div></div><p>
   The <span class="guimenu ">Control Planes</span> tab displays control planes and
   availability zones within the Cloud.
  </p><p>
   Each control plane is show as a table of clusters, resources, and load
   balancers (represented by vertical columns in the table).
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.5.5.3.4.1"><span class="term ">Control Plane</span></dt><dd><p>
      A set of servers dedicated to running the infrastructure of the Cloud.
      Many Cloud configurations will have only a single control plane.
     </p></dd><dt id="id-1.5.5.5.3.4.2"><span class="term ">Clusters</span></dt><dd><p>
      A set of one or more servers hosting a particular set of services, tied
      to the <code class="literal">role</code> that has been assigned to that
      server. <code class="literal">Clusters</code> are generally differentiated from
      <code class="literal">Resources</code> in that they are fixed size groups of
      servers that do not grow as the Cloud grows.
     </p></dd><dt id="id-1.5.5.5.3.4.3"><span class="term ">Resources</span></dt><dd><p>
      Servers hosting the scalable parts of the Cloud, such as Compute Hosts
      that host VMs, or swift servers for object storage. These will vary in
      number with the size and scale of the Cloud and can generally be
      increased after the initial Cloud deployment.
     </p></dd><dt id="id-1.5.5.5.3.4.4"><span class="term ">Load Balancers</span></dt><dd><p>
      Servers that distribute API calls across servers hosting the called
      services.
     </p></dd></dl></div><div class="figure" id="clm-admin-topo-control-planes-0"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_topo_control_planes_0.png" target="_blank"><img src="images/clm_admin_topo_control_planes_0.png" width="" alt="Control Plane Topology" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.15: </span><span class="name">Control Plane Topology </span><a title="Permalink" class="permalink" href="#clm-admin-topo-control-planes-0">#</a></h6></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.5.5.3.6.1"><span class="term ">Availability Zones</span></dt><dd><p>
      Listed beneath the running services, groups together in a row the hosts
      in a particular availability zone for a particular cluster or resource
      type (the rows are AZs, the columns are clusters/resources)
     </p></dd></dl></div><div class="figure" id="clm-admin-topo-control-planes-1"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_topo_control_planes_1.png" target="_blank"><img src="images/clm_admin_topo_control_planes_1.png" width="" alt="Control Plane Topology - Availability Zones" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.16: </span><span class="name">Control Plane Topology - Availability Zones </span><a title="Permalink" class="permalink" href="#clm-admin-topo-control-planes-1">#</a></h6></div></div></div><div class="sect2" id="clm-admin-topo-regions"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Regions</span> <a title="Permalink" class="permalink" href="#clm-admin-topo-regions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/clm_admin_topology.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>clm_admin_topology.xml</li><li><span class="ds-label">ID: </span>clm-admin-topo-regions</li></ul></div></div></div></div><p>
   Displays the distribution of control plane services across regions.  Clouds
   that have only a single region will list all services in the same cell.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.5.5.4.3.1"><span class="term ">Control Planes</span></dt><dd><p>
      The group of services that run the Cloud infrastructure
     </p></dd><dt id="id-1.5.5.5.4.3.2"><span class="term ">Region</span></dt><dd><p>
      Each region will be represented by a column with the region name as the
      column header. The list of services that are running in that region will
      be in that column, with each row corresponding to a particular control
      plane.
     </p></dd></dl></div><div class="figure" id="clm-admin-topo-regions-image"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_topo_regions.png" target="_blank"><img src="images/clm_admin_topo_regions.png" width="" alt="Regions Topology" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.17: </span><span class="name">Regions Topology </span><a title="Permalink" class="permalink" href="#clm-admin-topo-regions-image">#</a></h6></div></div></div><div class="sect2" id="clm-admin-topo-services"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Services</span> <a title="Permalink" class="permalink" href="#clm-admin-topo-services">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/clm_admin_topology.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>clm_admin_topology.xml</li><li><span class="ds-label">ID: </span>clm-admin-topo-services</li></ul></div></div></div></div><p>
   A list of services running in the Cloud, organized by the type (class) of
   service. Each service is then listed along with the control planes that the
   service is part of, the other services that each particular service consumes
   (requires), and the endpoints of the service, if the service exposes an API.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.5.5.5.3.1"><span class="term ">Class</span></dt><dd><p>
      A category of like services, such as "security" or "operations".
      Multiple services may belong to the same category.
     </p></dd><dt id="id-1.5.5.5.5.3.2"><span class="term ">Description</span></dt><dd><p>
      A short description of the service, typically sourced from the service
      itself
     </p></dd><dt id="id-1.5.5.5.5.3.3"><span class="term ">Service</span></dt><dd><p>
      The name of the service. For <span class="productname">OpenStack</span> services, this is the project
      codename, such as nova for virtual machine provisioning. Clicking a
      service will navigate to the section of this page with details for that
      particular service.
     </p></dd></dl></div><div class="figure" id="clm-admin-topo-services-0"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_topo_services_0.png" target="_blank"><img src="images/clm_admin_topo_services_0.png" width="" alt="Services Topology" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.18: </span><span class="name">Services Topology </span><a title="Permalink" class="permalink" href="#clm-admin-topo-services-0">#</a></h6></div></div><p>
    The detail data about a service provides additional insight into the
    service, such as what other services are required to run a service, and
    what network protocols can be used to access the service
   </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.5.5.5.6.1"><span class="term ">Components</span></dt><dd><p>
       Each service is made up of one or more components, which are listed
       separately here. The components of a service may represent pieces of the
       service that run on different hosts, provide distinct functionality, or
       modularize business logic.
      </p></dd><dt id="id-1.5.5.5.5.6.2"><span class="term ">Control Planes</span></dt><dd><p>
       A service may be running in multiple control planes. Each control plane
       that a service is running in will be listed here.
      </p></dd><dt id="id-1.5.5.5.5.6.3"><span class="term ">Consumes</span></dt><dd><p>
       Other services required for this service to operate correctly.
      </p></dd><dt id="id-1.5.5.5.5.6.4"><span class="term ">Endpoints</span></dt><dd><p>
       How a service can be accessed, typically a REST API, though other
       network protocols may be listed here. Services that do not expose an API
       or have any sort of external access will not list any entries here.
      </p></dd></dl></div><div class="figure" id="clm-admin-topo-service-details"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_topo_services_1.png" target="_blank"><img src="images/clm_admin_topo_services_1.png" width="" alt="Service Details Topology" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.19: </span><span class="name">Service Details Topology </span><a title="Permalink" class="permalink" href="#clm-admin-topo-service-details">#</a></h6></div></div></div><div class="sect2" id="clm-admin-networks"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networks</span> <a title="Permalink" class="permalink" href="#clm-admin-networks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/clm_admin_topology.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>clm_admin_topology.xml</li><li><span class="ds-label">ID: </span>clm-admin-networks</li></ul></div></div></div></div><p>
   Lists the networks and network groups that comprise the Cloud. Each network
   group is respresented by a row in the table, with columns identifying which
   networks are used by the intersection of the group (row) and
   cluster/resource (column).
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.5.5.6.3.1"><span class="term ">Group</span></dt><dd><p>
      The network group
     </p></dd><dt id="id-1.5.5.5.6.3.2"><span class="term ">Clusters</span></dt><dd><p>
      A set of one or more servers hosting a particular set of services, tied
      to the <code class="literal">role</code> that has been assigned to that
      server. <code class="literal">Clusters</code> are generally differentiated from
      <code class="literal">Resources</code> in that they are fixed size groups of
      servers that do not grow as the Cloud grows.
     </p></dd><dt id="id-1.5.5.5.6.3.3"><span class="term ">Resources</span></dt><dd><p>
      Servers hosting the scalable parts of the Cloud, such as Compute Hosts
      that host VMs, or swift servers for object storage. These will vary in
      number with the size and scale of the Cloud and can generally be
      increased after the initial Cloud deployment.
     </p></dd></dl></div><p>
   Cells in the middle of the table represent the network that is running on
   the resource/cluster represented by that column and is part of the network
   group identified in the leftmost column of the same row.
  </p><div class="figure" id="clm-admin-topo-networks"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_topo_networks_0.png" target="_blank"><img src="images/clm_admin_topo_networks_0.png" width="" alt="Networks Topology" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.20: </span><span class="name">Networks Topology </span><a title="Permalink" class="permalink" href="#clm-admin-topo-networks">#</a></h6></div></div><p>
    Each network group is listed along with the servers and interfaces that
    comprise the network group.
   </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.5.5.6.7.1"><span class="term ">Network Group</span></dt><dd><p>
       The elements that make up the network group, whose name is listed above
       the table
      </p></dd><dt id="id-1.5.5.5.6.7.2"><span class="term ">Networks</span></dt><dd><p>
       Networks that are part of the specified network group
      </p></dd><dt id="id-1.5.5.5.6.7.3"><span class="term ">Address</span></dt><dd><p>
       IP address of the corresponding server
      </p></dd><dt id="id-1.5.5.5.6.7.4"><span class="term ">Server</span></dt><dd><p>
       Server name of the server that is part of this network. Clicking on a
       server will load the server topology details.
      </p></dd><dt id="id-1.5.5.5.6.7.5"><span class="term ">Interface Model</span></dt><dd><p>
       The particular combination of hardware address and bonding that tie this
       server to the specified network group. Clicking on an <code class="literal">Interface
       Model</code> will load the corresponding section of the
       <code class="literal">Roles</code> page.
      </p></dd></dl></div><div class="figure" id="clm-admin-topo-network-groups"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_topo_networks_1.png" target="_blank"><img src="images/clm_admin_topo_networks_1.png" width="" alt="Network Groups Topology" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.21: </span><span class="name">Network Groups Topology </span><a title="Permalink" class="permalink" href="#clm-admin-topo-network-groups">#</a></h6></div></div></div><div class="sect2" id="clm-admin-server-groups"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Servers</span> <a title="Permalink" class="permalink" href="#clm-admin-server-groups">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/clm_admin_topology.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>clm_admin_topology.xml</li><li><span class="ds-label">ID: </span>clm-admin-server-groups</li></ul></div></div></div></div><p>
   A hierarchical display of the tree of Server Groups. Groups will be
   represented by a heading with their name, starting with the first row which
   contains the Cloud-wide server group (often called
   <code class="literal">CLOUD</code>).  Within each Server Group, the Network Groups,
   Networks, Servers, and Server Roles are broken down. Note that server groups
   can be nested, producing a tree-like structure of groups.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.5.5.7.3.1"><span class="term ">Network Groups</span></dt><dd><p>
      The network groups that are part of this server group.
     </p></dd><dt id="id-1.5.5.5.7.3.2"><span class="term ">Networks</span></dt><dd><p>
      The network that is part of the server group and corresponds to the
      network group in the same row.
     </p></dd><dt id="id-1.5.5.5.7.3.3"><span class="term ">Server Roles</span></dt><dd><p>
      The model defined role that was applied to the server, made up of a
      combination of services, and network/storage configurations unique to
      that role within the Cloud
     </p></dd><dt id="id-1.5.5.5.7.3.4"><span class="term ">Servers</span></dt><dd><p>
      The servers that have the role defined in their row and are part of the
      network group represented by the column the server is in.
     </p></dd></dl></div><div class="figure" id="clm-admin-topo-server-groups"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_topo_server_groups.png" target="_blank"><img src="images/clm_admin_topo_server_groups.png" width="" alt="Server Groups Topology" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.22: </span><span class="name">Server Groups Topology </span><a title="Permalink" class="permalink" href="#clm-admin-topo-server-groups">#</a></h6></div></div></div><div class="sect2" id="clm-admin-roles"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.3.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Roles</span> <a title="Permalink" class="permalink" href="#clm-admin-roles">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/clm_admin_topology.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>clm_admin_topology.xml</li><li><span class="ds-label">ID: </span>clm-admin-roles</li></ul></div></div></div></div><p>
   The list of server roles that define the server configurations for the
   Cloud. Each server role consists of several configurations. In this topology
   the focus is on the Disk Models and Network Interface Models that are
   applied to the servers with that role.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.5.5.8.3.1"><span class="term ">Server Role</span></dt><dd><p>
      The name of the role, as it is defined in the model
     </p></dd><dt id="id-1.5.5.5.8.3.2"><span class="term ">Disk Model</span></dt><dd><p>
      The name of the disk model
     </p></dd><dt id="id-1.5.5.5.8.3.3"><span class="term ">Volume Group</span></dt><dd><p>
      Name of the volume group
     </p></dd><dt id="id-1.5.5.5.8.3.4"><span class="term ">Mount</span></dt><dd><p>
      Name of the volume being mounted on the server
     </p></dd><dt id="id-1.5.5.5.8.3.5"><span class="term ">Size</span></dt><dd><p>
      The size of the volume as a percentage of physical disk space
     </p></dd><dt id="id-1.5.5.5.8.3.6"><span class="term ">FS Type</span></dt><dd><p>
      Filesystem type
     </p></dd><dt id="id-1.5.5.5.8.3.7"><span class="term ">Options</span></dt><dd><p>
      Optional flags applied when mounting the volume
     </p></dd><dt id="id-1.5.5.5.8.3.8"><span class="term ">PVol(s)</span></dt><dd><p>
      The physical address to the storage used for this volume group
     </p></dd><dt id="id-1.5.5.5.8.3.9"><span class="term ">Interface Model</span></dt><dd><p>
      The name of the interface model
     </p></dd><dt id="id-1.5.5.5.8.3.10"><span class="term ">Network Group</span></dt><dd><p>
      The name of the network group. Clicking on a Network Group will load the
      details of that group on the Networks page.
     </p></dd><dt id="id-1.5.5.5.8.3.11"><span class="term ">Interface/Options</span></dt><dd><p>
      Includes logical network name, such as <code class="literal">hed1</code>,
      <code class="literal">hed2</code>, and <code class="literal">bond</code> information grouping
      the logical network name together. The Cloud software will map these to
      physical devices.
     </p></dd></dl></div><div class="figure" id="clm-admin-topo-roles"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_topo_roles.png" target="_blank"><img src="images/clm_admin_topo_roles.png" width="" alt="Roles Topology" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.23: </span><span class="name">Roles Topology </span><a title="Permalink" class="permalink" href="#clm-admin-topo-roles">#</a></h6></div></div></div></div><div class="sect1" id="clm-admin-addserver"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Server Management</span> <a title="Permalink" class="permalink" href="#clm-admin-addserver">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/clm_admin_addserver.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>clm_admin_addserver.xml</li><li><span class="ds-label">ID: </span>clm-admin-addserver</li></ul></div></div></div></div><div class="sect2" id="clm-admin-add-server"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding Servers</span> <a title="Permalink" class="permalink" href="#clm-admin-add-server">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/clm_admin_addserver.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>clm_admin_addserver.xml</li><li><span class="ds-label">ID: </span>clm-admin-add-server</li></ul></div></div></div></div><p>
  The Add Server page in the Cloud Lifecycle Manager Admin UI allows for adding additional
  Compute Nodes to the Cloud.
 </p><div class="figure" id="clm-admin-add-server-0"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_add_server_0.png" target="_blank"><img src="images/clm_admin_add_server_0.png" width="" alt="Add Server Overview" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.24: </span><span class="name">Add Server Overview </span><a title="Permalink" class="permalink" href="#clm-admin-add-server-0">#</a></h6></div></div><div class="sect3" id="clm-admin-available-servers"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.4.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Available Servers</span> <a title="Permalink" class="permalink" href="#clm-admin-available-servers">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/clm_admin_addserver.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>clm_admin_addserver.xml</li><li><span class="ds-label">ID: </span>clm-admin-available-servers</li></ul></div></div></div></div><p>
   Servers that can be added to the Cloud are shown on the left side of the
   <code class="literal">Add Server</code> screen. Additional servers can be included in
   this list three different ways:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Discover servers via SUSE Manager or HPE OneView (for details on adding
     servers via autodiscovery, see <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 21 “Installing with the Install UI”, Section 21.4 “Optional: Importing Certificates for SUSE Manager and HPE OneView”</span> and
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 21 “Installing with the Install UI”, Section 21.5 “Running the Install UI”</span>
    </p></li><li class="listitem "><p>
     Manually add servers individually by clicking <span class="guimenu ">Manual
     Entry</span> and filling out the form with the server information
     (instructions below)
    </p></li><li class="listitem "><p>
     Create a CSV file of the servers to be added (see <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 21 “Installing with the Install UI”, Section 21.3 “Optional: Creating a CSV File to Import Server Data”</span>)
    </p></li></ol></div><p>
   Manually adding a server requires the following fields:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.5.6.2.4.5.1"><span class="term ">ID</span></dt><dd><p>
      A unique name for the server
     </p></dd><dt id="id-1.5.5.6.2.4.5.2"><span class="term ">IP Address</span></dt><dd><p>
      The IP address that the server has, or will have, in the Cloud
     </p></dd><dt id="id-1.5.5.6.2.4.5.3"><span class="term ">Server Group</span></dt><dd><p>
      Which server group the server will belong to. The IP address must be
      compatible with the selected Server Group. If the required Server
      Group is not present, it can be created
     </p></dd><dt id="id-1.5.5.6.2.4.5.4"><span class="term ">NIC Mapping</span></dt><dd><p>
      The NIC to PCI address mapping for the server being added to the Cloud.
      If the required NIC mapping is not present, it can be created
     </p></dd><dt id="id-1.5.5.6.2.4.5.5"><span class="term ">Role</span></dt><dd><p>
      Which compute role to add the server to. If this is set, the server will
      be immediately assigned that role on the right side of the page. If it is
      not set, the server will be added to the left side panel of available
      servers
     </p></dd></dl></div><p>
   Some additional fields must be set if the server is not already provisioned
   with an OS, or if a new OS install is desired for the server. These fields
   are not required if an <span class="productname">OpenStack</span> Cloud compatible OS is already installed:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.5.6.2.4.7.1"><span class="term ">MAC Address</span></dt><dd><p>
      The MAC address of the IPMI network card of the server
     </p></dd><dt id="id-1.5.5.6.2.4.7.2"><span class="term ">IPMI IP Address</span></dt><dd><p>
      The IPMI network address (IP address) of the server
     </p></dd><dt id="id-1.5.5.6.2.4.7.3"><span class="term ">IPMI Username</span></dt><dd><p>
      Username to log in to IPMI on the server
     </p></dd><dt id="id-1.5.5.6.2.4.7.4"><span class="term ">IPMI Password</span></dt><dd><p>
      Password to log in to IPMI on the server
     </p></dd></dl></div><div class="figure" id="clm-admin-add-server-image"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_add_server_1.png" target="_blank"><img src="images/clm_admin_add_server_1.png" width="" alt="Manually Add Server" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.25: </span><span class="name">Manually Add Server </span><a title="Permalink" class="permalink" href="#clm-admin-add-server-image">#</a></h6></div></div><p>
   Servers in the available list can be dragged to the desired role on the
   right. Only Compute-related roles will be displayed.
  </p><div class="figure" id="clm-admin-add-server-2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_add_server_2.png" target="_blank"><img src="images/clm_admin_add_server_2.png" width="" alt="Manually Add Server" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.26: </span><span class="name">Manually Add Server </span><a title="Permalink" class="permalink" href="#clm-admin-add-server-2">#</a></h6></div></div></div><div class="sect3" id="clm-admin-addserver-settings"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.4.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Add Server Settings</span> <a title="Permalink" class="permalink" href="#clm-admin-addserver-settings">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/clm_admin_addserver.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>clm_admin_addserver.xml</li><li><span class="ds-label">ID: </span>clm-admin-addserver-settings</li></ul></div></div></div></div><p>
   There are several settings that apply across all Compute Nodes being added to
   the Cloud. Beneath the list of nodes, users will find options to control
   whether existing nodes can be modified, whether the new nodes should have
   their data disks wiped, and whether to activate the new Compute Nodes as part
   of the update process.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.5.6.2.5.3.1"><span class="term ">Safe Mode</span></dt><dd><p>
      Prevents modification of existing Compute Nodes. Can be unchecked to allow
      modifications. Modifying existing Compute Nodes has the potential to disrupt
      the continuous operation of the Cloud and should be done with caution.
     </p></dd><dt id="id-1.5.5.6.2.5.3.2"><span class="term ">Wipe Data Disks</span></dt><dd><p>
      The data disks on the new server will not be wiped by default,
      but users can specify to wipe clean the data disks as part of the
      process of adding the Compute Node(s) to the Cloud.
     </p></dd><dt id="id-1.5.5.6.2.5.3.3"><span class="term ">Activate</span></dt><dd><p>
      Activates the added Compute Node(s) during the process of adding them to
      the Cloud. Activation adds a Compute Node to the pool of nodes that the
      <code class="literal">nova-scheduler</code> uses when instantiating VMs.
     </p></dd></dl></div><div class="figure" id="clm-admin-safe-mode"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_safe_mode.png" target="_blank"><img src="images/clm_admin_safe_mode.png" width="" alt="Add Server Settings options" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.27: </span><span class="name">Add Server Settings options </span><a title="Permalink" class="permalink" href="#clm-admin-safe-mode">#</a></h6></div></div></div><div class="sect3" id="clm-admin-install-os"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.4.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Install OS</span> <a title="Permalink" class="permalink" href="#clm-admin-install-os">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/clm_admin_addserver.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>clm_admin_addserver.xml</li><li><span class="ds-label">ID: </span>clm-admin-install-os</li></ul></div></div></div></div><p>
   Servers that have been assigned a role but not yet deployed can have SLES
   installed as part of the Cloud deployment. This step is necessary for
   servers that are not provisioned with an OS.
  </p><p>
   On the <code class="literal">Install OS</code> page, the <code class="literal">Available
   Servers</code> list will be populated with servers that have been
   assigned to a role but not yet deployed to the Cloud. From here, select
   which servers to install an OS onto and use the arrow controls to move them
   to the Selected Servers box on the right. After all servers that require an
   OS to be provisioned have been added to the Selected Servers list and click
   <span class="guimenu ">Next</span>.
  </p><div class="figure" id="clm-admin-add-server-3"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_add_server_3.png" target="_blank"><img src="images/clm_admin_add_server_3.png" width="" alt="Select Servers to Provision OS" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.28: </span><span class="name">Select Servers to Provision OS </span><a title="Permalink" class="permalink" href="#clm-admin-add-server-3">#</a></h6></div></div><p>
   The UI will prompt for confirmation that the OS should be installed, because
   provisioning an OS will replace any existing operating system on the
   server.
  </p><div class="figure" id="clm-admin-add-server-4"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_add_server_4.png" target="_blank"><img src="images/clm_admin_add_server_4.png" width="" alt="Confirm Provision OS" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.29: </span><span class="name">Confirm Provision OS </span><a title="Permalink" class="permalink" href="#clm-admin-add-server-4">#</a></h6></div></div><p>
   When the OS install begins, progress of the install will be displayed on
   screen
  </p><div class="figure" id="clm-admin-add-server-5"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_add_server_5.png" target="_blank"><img src="images/clm_admin_add_server_5.png" width="" alt="OS Install Progress" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.30: </span><span class="name">OS Install Progress </span><a title="Permalink" class="permalink" href="#clm-admin-add-server-5">#</a></h6></div></div><p>
   After OS provisioning is complete, a summary of the provisioned servers will
   be displayed. Clicking <span class="guimenu ">Close</span> will return the user to the
   role selection page where deployment can continue.
  </p><div class="figure" id="clm-admin-add-server-6"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_add_server_6.png" target="_blank"><img src="images/clm_admin_add_server_6.png" width="" alt="OS Install Summary" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.31: </span><span class="name">OS Install Summary </span><a title="Permalink" class="permalink" href="#clm-admin-add-server-6">#</a></h6></div></div></div><div class="sect3" id="clm-admin-deploy-newservers"><div class="titlepage"><div><div><h4 class="title"><span class="number">3.4.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploy New Servers</span> <a title="Permalink" class="permalink" href="#clm-admin-deploy-newservers">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/clm_admin_addserver.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>clm_admin_addserver.xml</li><li><span class="ds-label">ID: </span>clm-admin-deploy-newservers</li></ul></div></div></div></div><p>
   When all newly added servers have an OS provisioned, either via the Install
   OS process detailed above or having previously been provisioned outside of
   the Cloud Lifecycle Manager Admin UI, deployment can begin.
  </p><p>
   The <span class="guimenu ">Deploy</span> button will be enabled when one or more new
   servers have been assigned roles. Clicking <span class="guimenu ">Deploy</span> prompt
   for confirmation before beginning the deployment process
  </p><div class="figure" id="clm-admin-add-server-75"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_add_server_7.png" target="_blank"><img src="images/clm_admin_add_server_7.png" width="" alt="Confirm Deploy Servers" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.32: </span><span class="name">Confirm Deploy Servers </span><a title="Permalink" class="permalink" href="#clm-admin-add-server-75">#</a></h6></div></div><p>
   The deployment process will begin by running the Configuration Processor in
   basic validation mode to check the values input for the servers being
   added. This will check IP addresses, server groups, and NIC mappings for
   syntax or format errors.
  </p><div class="figure" id="clm-admin-add-server-8"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_add_server_8.png" target="_blank"><img src="images/clm_admin_add_server_8.png" width="" alt="Validate Server Changes" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.33: </span><span class="name">Validate Server Changes </span><a title="Permalink" class="permalink" href="#clm-admin-add-server-8">#</a></h6></div></div><p>
   After validation is successful, the servers will be prepared for
   deployment. The preparation consists of running the full Configuration
   Processor and two additional playbooks to ready servers for deployment.
  </p><div class="figure" id="clm-admin-add-server-9"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_add_server_9.png" target="_blank"><img src="images/clm_admin_add_server_9.png" width="" alt="Prepare Servers" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.34: </span><span class="name">Prepare Servers </span><a title="Permalink" class="permalink" href="#clm-admin-add-server-9">#</a></h6></div></div><p>
   After the servers have been prepared, deployment can begin. This process
   will generate a new <code class="filename">hosts</code> file, run the
   <code class="filename">site.yml</code> playbook, and update monasca (if
   monasca is deployed)
  </p><div class="figure" id="clm-admin-add-server-10"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_add_server_10.png" target="_blank"><img src="images/clm_admin_add_server_10.png" width="" alt="Deploy Servers" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.35: </span><span class="name">Deploy Servers </span><a title="Permalink" class="permalink" href="#clm-admin-add-server-10">#</a></h6></div></div><p>
   When deployment is completed, a summary page will be displayed. Clicking
   <span class="guimenu ">Close</span> will return to the <code class="literal">Add Server</code> page.
  </p><div class="figure" id="clm-admin-add-server-11"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_add_server_11.png" target="_blank"><img src="images/clm_admin_add_server_11.png" width="" alt="Deploy Summary" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.36: </span><span class="name">Deploy Summary </span><a title="Permalink" class="permalink" href="#clm-admin-add-server-11">#</a></h6></div></div></div></div><div class="sect2" id="clm-admin-activate-server"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Activating Servers</span> <a title="Permalink" class="permalink" href="#clm-admin-activate-server">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/clm_admin_addserver.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>clm_admin_addserver.xml</li><li><span class="ds-label">ID: </span>clm-admin-activate-server</li></ul></div></div></div></div><p>
  The Server Summary page in the Cloud Lifecycle Manager Admin UI allows for activating
  Compute Nodes in the Cloud. Compute Nodes may be activated when
  they are added to the Cloud. An activated compute node is available
  for the <code class="literal">nova-scheduler</code> to use for hosting new VMs that are created.
  Only servers that are not currently activated will have the
  activation menu option available.
 </p><div class="figure" id="clm-admin-activate-server-1"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_activate_1.png" target="_blank"><img src="images/clm_admin_activate_1.png" width="" alt="Activate Server" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.37: </span><span class="name">Activate Server </span><a title="Permalink" class="permalink" href="#clm-admin-activate-server-1">#</a></h6></div></div><p>
  Once activation is triggered, the progress of activating the node
  and adding it to the <code class="literal">nova-scheduler</code> is displayed.
 </p><div class="figure" id="clm-admin-activate-server-2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_activate_2.png" target="_blank"><img src="images/clm_admin_activate_2.png" width="" alt="Activate Server Progress" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.38: </span><span class="name">Activate Server Progress </span><a title="Permalink" class="permalink" href="#clm-admin-activate-server-2">#</a></h6></div></div></div><div class="sect2" id="clm-admin-deactivate-server"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deactivating Servers</span> <a title="Permalink" class="permalink" href="#clm-admin-deactivate-server">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/clm_admin_addserver.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>clm_admin_addserver.xml</li><li><span class="ds-label">ID: </span>clm-admin-deactivate-server</li></ul></div></div></div></div><p>
  The Server Summary page in the Cloud Lifecycle Manager Admin UI allows for deactivating
  Compute Nodes in the Cloud. Deactivating a Compute Node removes it from
  the pool of servers that the <code class="literal">nova-scheduler</code> will put VMs on.
  When a Compute Node is deactivated, the UI attempts to migrate any
  currently running VMs from that server to an active node.
 </p><div class="figure" id="clm-admin-deactivate-server-1"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_deactivate_1.png" target="_blank"><img src="images/clm_admin_deactivate_1.png" width="" alt="Deactivate Server" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.39: </span><span class="name">Deactivate Server </span><a title="Permalink" class="permalink" href="#clm-admin-deactivate-server-1">#</a></h6></div></div><p>
  The deactivation process requires confirmation before proceeding.
 </p><div class="figure" id="clm-admin-deactivate-server-2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_deactivate_2.png" target="_blank"><img src="images/clm_admin_deactivate_2.png" width="" alt="Deactivate Server Confirmation" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.40: </span><span class="name">Deactivate Server Confirmation </span><a title="Permalink" class="permalink" href="#clm-admin-deactivate-server-2">#</a></h6></div></div><p>
  Once deactivation is triggered, the progress of deactivating the node
  and removing it from the <code class="literal">nova-scheduler</code> is displayed.
 </p><div class="figure" id="clm-admin-deactivate-server-3"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_deactivate_3.png" target="_blank"><img src="images/clm_admin_deactivate_3.png" width="" alt="Deactivate Server Progress" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.41: </span><span class="name">Deactivate Server Progress </span><a title="Permalink" class="permalink" href="#clm-admin-deactivate-server-3">#</a></h6></div></div><p>
  If a Compute Node selected for deactivation has VMs running on it,
  a prompt will appear to select where to migrate the running VMs
 </p><div class="figure" id="clm-admin-deactivate-with-migration-1"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_deactivate_with_migration_1.png" target="_blank"><img src="images/clm_admin_deactivate_with_migration_1.png" width="" alt="Select Migration Target" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.42: </span><span class="name">Select Migration Target </span><a title="Permalink" class="permalink" href="#clm-admin-deactivate-with-migration-1">#</a></h6></div></div><p>
  A summary of the VMs being migrated will be displayed, along with
  the progress migrating them from the deactivated Compute Node to
  the target host. Once the migration attempt is complete, click
  'Done' to continue the deactivation process.
 </p><div class="figure" id="clm-admin-deactivate-with-migration-2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_deactivate_with_migration_2.png" target="_blank"><img src="images/clm_admin_deactivate_with_migration_2.png" width="" alt="Deactivate Migration Progress" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.43: </span><span class="name">Deactivate Migration Progress </span><a title="Permalink" class="permalink" href="#clm-admin-deactivate-with-migration-2">#</a></h6></div></div></div><div class="sect2" id="clm-admin-delete-server"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deleting Servers</span> <a title="Permalink" class="permalink" href="#clm-admin-delete-server">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/clm_admin_addserver.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>clm_admin_addserver.xml</li><li><span class="ds-label">ID: </span>clm-admin-delete-server</li></ul></div></div></div></div><p>
  The Server Summary page in the Cloud Lifecycle Manager Admin UI allows for deleting
  Compute Nodes from the Cloud. Deleting a Compute Node removes it from
  the cloud. Only Compute Nodes that are deactivated can be deleted.
 </p><div class="figure" id="clm-admin-delete-server-1"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_delete_1.png" target="_blank"><img src="images/clm_admin_delete_1.png" width="" alt="Delete Server" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.44: </span><span class="name">Delete Server </span><a title="Permalink" class="permalink" href="#clm-admin-delete-server-1">#</a></h6></div></div><p>
  The deletion process requires confirmation before proceeding.
 </p><div class="figure" id="clm-admin-delete-server-2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_delete_2.png" target="_blank"><img src="images/clm_admin_delete_2.png" width="" alt="Delete Server Confirmation" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.45: </span><span class="name">Delete Server Confirmation </span><a title="Permalink" class="permalink" href="#clm-admin-delete-server-2">#</a></h6></div></div><p>
  If the Compute Node is not reachable (SSH from the deployer is not
  possible), a warning will appear, requesting confirmation that
  the node is shut down or otherwise removed from the environment.
  Reachable Compute Nodes will be shutdown as part of the deletion
  process.
 </p><div class="figure" id="clm-admin-delete-server-3"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_delete_3.png" target="_blank"><img src="images/clm_admin_delete_3.png" width="" alt="Unreachable Delete Confirmation" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.46: </span><span class="name">Unreachable Delete Confirmation </span><a title="Permalink" class="permalink" href="#clm-admin-delete-server-3">#</a></h6></div></div><p>
  The progress of deleting the Compute Node will be displayed, including
  a streaming log with additional details of the running playbooks.
 </p><div class="figure" id="clm-admin-delete-server-4"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_delete_4.png" target="_blank"><img src="images/clm_admin_delete_4.png" width="" alt="Delete Server Progress" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.47: </span><span class="name">Delete Server Progress </span><a title="Permalink" class="permalink" href="#clm-admin-delete-server-4">#</a></h6></div></div></div></div><div class="sect1" id="clm-server-replacement"><div class="titlepage"><div><div><h2 class="title"><span class="number">3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Server Replacement</span> <a title="Permalink" class="permalink" href="#clm-server-replacement">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/clm_replace_server.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>clm_replace_server.xml</li><li><span class="ds-label">ID: </span>clm-server-replacement</li></ul></div></div></div></div><p>
  The process of replacing a server is initiated from the Server Summary (see <a class="xref" href="#admin-ui-servers" title="3.2.6. Servers">Section 3.2.6, “Servers”</a>). Replacing a server will remove the existing
  server from the Cloud configuration and install the new server in its
  place. The rest of this process varies slightly depending on the type of
  server being replaced.
 </p><div class="sect2" id="control-plane-servers"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Control Plane Servers</span> <a title="Permalink" class="permalink" href="#control-plane-servers">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/clm_replace_server.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>clm_replace_server.xml</li><li><span class="ds-label">ID: </span>control-plane-servers</li></ul></div></div></div></div><p>
   Servers that are part of the Control Plane (generally those that are not
   hosting Compute VMs or ephemeral storage) are replaced "in-place". This
   means the replacement server has the same IP Address and is expected to have the
   same NIC Mapping and Server Group as the server being replaced.
  </p><p>
   To replace a Control Plane server, click the menu to the right of the server
   listing on the <span class="guimenu ">Summary</span> tab of the <a class="xref" href="#admin-ui-servers" title="3.2.6. Servers">Section 3.2.6, “Servers”</a> page.  From the menu options, select
   <span class="guimenu ">Replace</span>.
 </p><div class="figure" id="clm-admin-replaceserver-menu"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_replaceserver_menu.png" target="_blank"><img src="images/clm_admin_replaceserver_menu.png" width="" alt="Replace Server Menu" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.48: </span><span class="name">Replace Server Menu </span><a title="Permalink" class="permalink" href="#clm-admin-replaceserver-menu">#</a></h6></div></div><p>
   Selecting <span class="guimenu ">Replace</span> will open a dialog box that includes
   information about the server being replaced, as well as a form for inputting
   the required information for the new server.
  </p><div class="figure" id="clm-admin-replaceserver-controlplane-replaceform"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_replaceserver_controlplane_replaceform.png" target="_blank"><img src="images/clm_admin_replaceserver_controlplane_replaceform.png" width="" alt="Replace Controller Form" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.49: </span><span class="name">Replace Controller Form </span><a title="Permalink" class="permalink" href="#clm-admin-replaceserver-controlplane-replaceform">#</a></h6></div></div><p>
   The IPMI information for the new server is required to perform the
   replacement process.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.5.7.3.8.1"><span class="term ">MAC Address</span></dt><dd><p>
      The hardware address of the server's primary physical ethernet
      adapter
     </p></dd><dt id="id-1.5.5.7.3.8.2"><span class="term ">IPMI IP Address</span></dt><dd><p>
      The network address for IPMI access to the new server
     </p></dd><dt id="id-1.5.5.7.3.8.3"><span class="term ">IPMI Username</span></dt><dd><p>
      The username credential for IPMI access to the new server
     </p></dd><dt id="id-1.5.5.7.3.8.4"><span class="term ">IPMI Password</span></dt><dd><p>
      The password associated with the <code class="literal">IPMI Username</code> on the
      new server
     </p></dd></dl></div><p>
   To use a server that has already been discovered, check the box for
   <span class="guimenu ">Use available servers</span> and select an existing server
   from the <span class="guimenu ">Available Servers</span> dropdown. This will
   automatically populate the server information fields above with the
   information previously entered/discovered for the specified server.
  </p><p>
   If SLES is not already installed, or to reinstall SLES on the new
   server, check the box for <span class="guimenu ">Install OS</span>. The username will
   be pre-populated with the username from the Cloud install. Installing the OS
   requires specifying the password that was used for deploying the cloud so
   that the replacement process can access the host after the OS is installed.
  </p><p>
   The data disks on the new server will not be wiped by default, but users
   can specify to wipe clean the data disks as part of the replacement process.
  </p><p>
   Once the new server information is set, click the <span class="guimenu ">Replace</span>
   button in the lower right to begin replacement. A list of the replacement
   process steps will be displayed, and there will be a link at the bottom of
   the list to show the log file as the changes are made.
  </p><div class="figure" id="clm-admin-replace-server-controlplane-progress"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_replace_server_controlplane_progress.png" target="_blank"><img src="images/clm_admin_replace_server_controlplane_progress.png" width="" alt="Replace Controller Progress" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.50: </span><span class="name">Replace Controller Progress </span><a title="Permalink" class="permalink" href="#clm-admin-replace-server-controlplane-progress">#</a></h6></div></div><p>
   When all of the steps are complete, click <span class="guimenu ">Close</span> to return
   to the Servers page.
  </p></div><div class="sect2" id="compute-servers"><div class="titlepage"><div><div><h3 class="title"><span class="number">3.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute Servers</span> <a title="Permalink" class="permalink" href="#compute-servers">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/clm_replace_server.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>clm_replace_server.xml</li><li><span class="ds-label">ID: </span>compute-servers</li></ul></div></div></div></div><p>
   When servers that host VMs are replaced, the following actions happen:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     a new server is added
    </p></li><li class="step "><p>
     existing instances are migrated from the existing server to the new server
    </p></li><li class="step "><p>
     the existing server is deleted from the model
    </p></li></ol></div></div><p>
   The new server will not have the same IP Address and may have a different
   NIC Mapping and Server Group than the server being replaced.
  </p><p>
   To replace a Compute server, click the menu to the right of the server
   listing on the <span class="guimenu ">Summary</span> tab of the <a class="xref" href="#admin-ui-servers" title="3.2.6. Servers">Section 3.2.6, “Servers”</a> page.  From the menu options, select
   <span class="guimenu ">Replace</span>.
  </p><div class="figure" id="clm-admin-replacecompute-menu"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_replacecompute_menu.png" target="_blank"><img src="images/clm_admin_replacecompute_menu.png" width="" alt="Replace Compute Menu" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.51: </span><span class="name">Replace Compute Menu </span><a title="Permalink" class="permalink" href="#clm-admin-replacecompute-menu">#</a></h6></div></div><p>
   Selecting <span class="guimenu ">Replace</span> will open a dialog box that includes
   information about the server being replaced, and a form for inputting the
   required information for the new server.
  </p><p>
   If the IP address of the server being replaced cannot be reached
by the deployer, a warning will appear to verify that the replacement
should continue.
  </p><div class="figure" id="clm-admin-replacecompute-unreachable-warn"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_replacecompute_unreachable_warn.png" target="_blank"><img src="images/clm_admin_replacecompute_unreachable_warn.png" width="" alt="Unreachable Compute Node Warning" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.52: </span><span class="name">Unreachable Compute Node Warning </span><a title="Permalink" class="permalink" href="#clm-admin-replacecompute-unreachable-warn">#</a></h6></div></div><div class="figure" id="clm-admin-replacecompute-replaceform"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_replacecompute_replaceform.png" target="_blank"><img src="images/clm_admin_replacecompute_replaceform.png" width="" alt="Replace Compute Form" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.53: </span><span class="name">Replace Compute Form </span><a title="Permalink" class="permalink" href="#clm-admin-replacecompute-replaceform">#</a></h6></div></div><p>
   Replacing a Compute server involves adding the new server and then
   performing migration. This requires some new information:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     an unused IP address
    </p></li><li class="listitem "><p>
     a new ID
    </p></li><li class="listitem "><p>
     selections for Server Group and NIC Mapping, which do not need to match
     the original server.
    </p></li></ul></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.5.7.4.13.1"><span class="term ">ID</span></dt><dd><p>
      This is the ID of the server in the data model. This does not necessarily
      correspond to any DNS or other naming labels of a host, unless the host
      ID was set that way during install.
     </p></dd><dt id="id-1.5.5.7.4.13.2"><span class="term ">IP Address</span></dt><dd><p>
      The management network IP address of the server
     </p></dd><dt id="id-1.5.5.7.4.13.3"><span class="term ">Server Group</span></dt><dd><p>
      The server group which this server is assigned to. If the required
      Server Group does not exist, it can be created
     </p></dd><dt id="id-1.5.5.7.4.13.4"><span class="term ">NIC Mapping</span></dt><dd><p>
      The NIC mapping that describes the PCI slot addresses for the server's
      ethernet adapters. If the required NIC mapping does not exist, it
      can be created
     </p></dd></dl></div><p>
   The IPMI information for the new server is also required to perform the
   replacement process.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.5.7.4.15.1"><span class="term ">Mac Address</span></dt><dd><p>
      The hardware address of the server's primary physical ethernet adapter
     </p></dd><dt id="id-1.5.5.7.4.15.2"><span class="term ">IPMI IP Address</span></dt><dd><p>
      The network address for IPMI access to the new server
     </p></dd><dt id="id-1.5.5.7.4.15.3"><span class="term ">IPMI Username</span></dt><dd><p>
      The username credential for IPMI access to the new server
     </p></dd><dt id="id-1.5.5.7.4.15.4"><span class="term ">IPMI Password</span></dt><dd><p>
      The password associated with the <code class="literal">IPMI Username</code>
     </p></dd></dl></div><p>
   To use a server that has already been discovered, check the box for
   <span class="guimenu ">Use available servers</span> and select an existing server
   from the <span class="guimenu ">Available Servers</span> dropdown. This will
   automatically populate the server information fields above with the
   information previously entered/discovered for the specified server.
  </p><p>
   If SLES is not already installed, or to reinstall SLES on the new
   server, check the box for <span class="guimenu ">Install OS</span>. The username will
   be pre-populated with the username from the Cloud install. Installing the OS
   requires specifying the password that was used for deploying the cloud so
   that the replacement process can access the host after the OS is installed.
  </p><p>
   The data disks on the new server will not be wiped by default, but
   <code class="literal">wipe clean</code> can specified for the data disks as
   part of the replacement process.
  </p><p>
   When the new server information is set, click the <span class="guimenu ">Replace</span>
   button in the lower right to begin replacement. The configuration processor
   will be run to validate that the entered information is compatible with the
   configuration of the Cloud.
  </p><p>
   When validation has completed, the Compute replacement takes place in
   several distinct steps, and each will have its own page with a list of
   process steps displayed. A link at the bottom of the list can show the log
   file as the changes are made.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Install SLES if that option was selected
    </p><div class="figure" id="clm-admin-replacecompute-installsles"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_replacecompute_installsles.png" target="_blank"><img src="images/clm_admin_replacecompute_installsles.png" width="" alt="Install SLES on New Compute" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.54: </span><span class="name">Install SLES on New Compute </span><a title="Permalink" class="permalink" href="#clm-admin-replacecompute-installsles">#</a></h6></div></div></li><li class="step "><p>
     Commit the changes to the data model and run the configuration processor
    </p><div class="figure" id="clm-admin-replacecompute-prepareserver"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_replacecompute_prepareserver.png" target="_blank"><img src="images/clm_admin_replacecompute_prepareserver.png" width="" alt="Prepare Compute Server" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.55: </span><span class="name">Prepare Compute Server </span><a title="Permalink" class="permalink" href="#clm-admin-replacecompute-prepareserver">#</a></h6></div></div></li><li class="step "><p> Deploy the new server, install services on it, update monasca
    (if installed), activate the server with nova so that it can host
    VMs.
    </p><div class="figure" id="clm-admin-replacecompute-deploynew"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_replacecompute_deploynew.png" target="_blank"><img src="images/clm_admin_replacecompute_deploynew.png" width="" alt="Deploy New Compute Server" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.56: </span><span class="name">Deploy New Compute Server </span><a title="Permalink" class="permalink" href="#clm-admin-replacecompute-deploynew">#</a></h6></div></div></li><li class="step "><p>
     Disable the existing server. If the existing server is unreachable, there may
     be warnings about disabling services on that server.
    </p><div class="figure" id="clm-admin-replacecompute-hostaggwarnings"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_replacecompute_hostaggwarnings.png" target="_blank"><img src="images/clm_admin_replacecompute_hostaggwarnings.png" width="" alt="Host Aggregate Removal Warning" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.57: </span><span class="name">Host Aggregate Removal Warning </span><a title="Permalink" class="permalink" href="#clm-admin-replacecompute-hostaggwarnings">#</a></h6></div></div><p>
     If the existing server is reachable, instances on that server will be migrated
     to the new server.
    </p><div class="figure" id="clm-admin-replacecompute-migration"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_replacecompute_migration.png" target="_blank"><img src="images/clm_admin_replacecompute_migration.png" width="" alt="Migrate Instances from Existing Compute Server" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.58: </span><span class="name">Migrate Instances from Existing Compute Server </span><a title="Permalink" class="permalink" href="#clm-admin-replacecompute-migration">#</a></h6></div></div><p>
     If the existing server is not reachable, the migration step will be
     skipped.
    </p><div class="figure" id="clm-admin-replacecompute-disableexisting"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_replacecompute_disableexisting.png" target="_blank"><img src="images/clm_admin_replacecompute_disableexisting.png" width="" alt="Disable Existing Compute Server" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.59: </span><span class="name">Disable Existing Compute Server </span><a title="Permalink" class="permalink" href="#clm-admin-replacecompute-disableexisting">#</a></h6></div></div></li><li class="step "><p>
     Remove the existing server from the model and update the cloud
     configuration. If the server is not reachable, the user is asked to verify
     that the server is shut down. If server is reachable, the cloud services
     running on it will be stopped and the server will be shut down as part of
     the removal from the Cloud.
    </p><div class="figure" id="clm-admin-replacecompute-shutdownwarning"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_replacecompute_shutdownwarning.png" target="_blank"><img src="images/clm_admin_replacecompute_shutdownwarning.png" width="" alt="Existing Server Shutdown Check" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.60: </span><span class="name">Existing Server Shutdown Check </span><a title="Permalink" class="permalink" href="#clm-admin-replacecompute-shutdownwarning">#</a></h6></div></div><p>
     Upon verification that the unreachable host is shut down, it will be removed from the
     data model.
    </p><div class="figure" id="clm-admin-replacecompute-deleteexisting"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_replacecompute_deleteexisting.png" target="_blank"><img src="images/clm_admin_replacecompute_deleteexisting.png" width="" alt="Existing Server Delete" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.61: </span><span class="name">Existing Server Delete </span><a title="Permalink" class="permalink" href="#clm-admin-replacecompute-deleteexisting">#</a></h6></div></div><p>
     After the model has been updated, a summary of the changes will
     appear. Click <span class="guimenu ">Close</span> to return to the server summary
     screen.
    </p><div class="figure" id="clm-admin-replacecompute-summary"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/clm_admin_replacecompute_summary.png" target="_blank"><img src="images/clm_admin_replacecompute_summary.png" width="" alt="Compute Replacement Summary" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3.62: </span><span class="name">Compute Replacement Summary </span><a title="Permalink" class="permalink" href="#clm-admin-replacecompute-summary">#</a></h6></div></div></li></ol></div></div></div></div></div><div class="chapter " id="third-party-integrations"><div class="titlepage"><div><div><h1 class="title"><span class="number">4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Third-Party Integrations</span> <a title="Permalink" class="permalink" href="#third-party-integrations">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-thirdparty_integrations.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-thirdparty_integrations.xml</li><li><span class="ds-label">ID: </span>third-party-integrations</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#splunk-integration"><span class="number">4.1 </span><span class="name">Splunk Integration</span></a></span></dt><dt><span class="section"><a href="#topic-kyf-brv-vw"><span class="number">4.2 </span><span class="name">Operations Bridge Integration</span></a></span></dt><dt><span class="section"><a href="#monitoring-3rd-party-components-with-monasca"><span class="number">4.3 </span><span class="name">Monitoring Third-Party Components With Monasca</span></a></span></dt></dl></div></div><div class="sect1" id="splunk-integration"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Splunk Integration</span> <a title="Permalink" class="permalink" href="#splunk-integration">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-integrating_splunk.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-integrating_splunk.xml</li><li><span class="ds-label">ID: </span>splunk-integration</li></ul></div></div></div></div><p>
  This documentation demonstrates the possible integration between the
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 centralized logging solution and Splunk including the steps
  to set up and forward logs.
 </p><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 logging solution provides a flexible and extensible
  framework to centralize the collection and processing of logs from all of
  the nodes in a cloud. The logs are shipped to a highly available and fault
  tolerant cluster where they are transformed and stored for better searching
  and reporting. The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 logging solution uses the ELK stack
  (Elasticsearch, Logstash and Kibana) as a production grade implementation
  and can support other storage and indexing technologies. The Logstash
  pipeline can be configured to forward the logs to an alternative target if
  you wish.
 </p><p>
  This documentation demonstrates the possible integration between the
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 centralized logging solution and Splunk including the steps
  to set up and forward logs.
 </p><div class="sect2" id="idg-all-operations-integrating-splunk-xml-5"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">What is Splunk?</span> <a title="Permalink" class="permalink" href="#idg-all-operations-integrating-splunk-xml-5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-integrating_splunk.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-integrating_splunk.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-integrating-splunk-xml-5</li></ul></div></div></div></div><p>
   Splunk is software for searching, monitoring, and analyzing
   <a class="link" href="https://en.wikipedia.org/wiki/Machine-generated_data" target="_blank">machine-generated
   big data</a>, via a web-style interface. Splunk captures, indexes and
   correlates real-time data in a searchable repository from which it can
   generate graphs, reports, alerts, dashboards and visualizations. It is
   commercial software (unlike Elasticsearch) and more details about Splunk
   can be found at <a class="link" href="https://www.splunk.com" target="_blank">https://www.splunk.com</a>.
  </p></div><div class="sect2" id="idg-all-operations-integrating-splunk-xml-6"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Splunk to receive log messages from <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9</span> <a title="Permalink" class="permalink" href="#idg-all-operations-integrating-splunk-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-integrating_splunk.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-integrating_splunk.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-integrating-splunk-xml-6</li></ul></div></div></div></div><p>
   This documentation assumes that you already have Splunk set up and running.
   For help with installing and setting up Splunk, refer to
   <a class="link" href="http://docs.splunk.com/Documentation/Splunk/latest/SearchTutorial/Systemrequirements" target="_blank">Splunk
   Tutorial</a>.
  </p><p>
   There are different ways in which a log message (or "event" in Splunk's
   terminology) can be sent to Splunk. These steps will set up a TCP port where
   Splunk will listen for messages.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     On the Splunk web UI, click on the Settings menu in the upper right-hand
     corner.
    </p></li><li class="listitem "><p>
     In the <span class="guimenu ">Data</span> section of the Settings menu, click <span class="guimenu ">Data Inputs</span>.
    </p></li><li class="listitem "><p>
     Choose the <span class="guimenu ">TCP</span> option.
    </p></li><li class="listitem "><p>
     Click the <span class="guimenu ">New</span> button to add an input.
    </p></li><li class="listitem "><p>
     In the <span class="guimenu ">Port</span> field, enter the port number you want to use.
    </p><div id="id-1.5.6.2.6.4.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      If you are on a less secure network and want to restrict connections to
      this port, use the <span class="guimenu ">Only accept connection from</span> field
      to restrict the traffic to a specific IP address.
     </p></div></li><li class="listitem "><p>
     Click the <span class="guimenu ">Next</span> button.
    </p></li><li class="listitem "><p>
     Specify the Source Type by clicking on the <span class="guimenu ">Select</span> button and choosing
     <code class="literal">linux_messages_syslog</code> from the list.
    </p></li><li class="listitem "><p>
     Click the <span class="guimenu ">Review</span> button.
    </p></li><li class="listitem "><p>
     Review the configuration and click the <span class="guimenu ">Submit</span> button.
    </p></li><li class="listitem "><p>
     A success message will be displayed.
    </p></li></ol></div></div><div class="sect2" id="id-1.5.6.2.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Forwarding log messages from <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 Centralized Logging to Splunk</span> <a title="Permalink" class="permalink" href="#id-1.5.6.2.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-integrating_splunk.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-integrating_splunk.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   When you have Splunk set up and configured to receive log messages, you can
   configure <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 to forward the logs to Splunk.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Check the status of the logging service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts logging-status.yml</pre></div><p>
     If everything is up and running, continue to the next step.
    </p></li><li class="step "><p>
     Edit the logstash config file at the location below:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/logging-server/templates/logstash.conf.j2</pre></div><p>
     At the bottom of the file will be a section for the Logstash outputs. Add
     details about your Splunk environment details.
    </p><p>
     Below is an example, showing the placement in bold:
    </p><div class="verbatim-wrap"><pre class="screen"># Logstash outputs
#------------------------------------------------------------------------------
output {
  # Configure Elasticsearch output
  # http://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html
  elasticsearch {
    index =&gt; %{[@metadata][es_index]}
    hosts =&gt; ["{{ elasticsearch_http_host }}:{{ elasticsearch_http_port }}"]
    flush_size =&gt; {{ logstash_flush_size }}
    idle_flush_time =&gt; 5
    workers =&gt; {{ logstash_threads }}
  }
 <span class="bold"><strong>  # Forward Logs to Splunk on the TCP port that matches the one specified in Splunk Web UI.
 tcp {
   mode =&gt; "client"
   host =&gt; "&lt;Enter Splunk listener IP address&gt;"
   port =&gt; <em class="replaceable ">TCP_PORT_NUMBER</em>
 }</strong></span>
}</pre></div><div id="id-1.5.6.2.7.3.3.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      If you are not planning on using the Splunk UI to parse your centralized
      logs, there is no need to forward your logs to Elasticsearch. In this
      situation, comment out the lines in the Logstash outputs pertaining to
      Elasticsearch.
      However, you can continue to forward your centralized logs to multiple
      locations.
     </p></div></li><li class="step "><p>
     Commit your changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Logstash configuration change for Splunk
integration"</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost
ready-deployment.yml</pre></div></li><li class="step "><p>
     Complete this change with a reconfigure of the logging environment:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts kronos-reconfigure.yml</pre></div></li><li class="step "><p>
     In your Splunk UI, confirm that the logs have begun to forward.
    </p></li></ol></div></div></div><div class="sect2" id="id-1.5.6.2.8"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Searching for log messages from the Spunk dashboard</span> <a title="Permalink" class="permalink" href="#id-1.5.6.2.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-integrating_splunk.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-integrating_splunk.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To both verify that your integration worked and to search your log messages
   that have been forwarded you can navigate back to your Splunk dashboard. In
   the search field, use this string:
  </p><div class="verbatim-wrap"><pre class="screen">source="tcp:<em class="replaceable ">TCP_PORT_NUMBER</em>"</pre></div><p>
   Find information on using the Splunk search tool at
   <a class="link" href="http://docs.splunk.com/Documentation/Splunk/6.4.3/SearchTutorial/WelcometotheSearchTutorial" target="_blank">http://docs.splunk.com/Documentation/Splunk/6.4.3/SearchTutorial/WelcometotheSearchTutorial</a>.
  </p></div></div><div class="sect1" id="topic-kyf-brv-vw"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Operations Bridge Integration</span> <a title="Permalink" class="permalink" href="#topic-kyf-brv-vw">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-integrating_opsbridge.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-integrating_opsbridge.xml</li><li><span class="ds-label">ID: </span>topic-kyf-brv-vw</li></ul></div></div></div></div><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 monitoring solution (monasca) can easily be
  integrated with your existing monitoring tools. Integrating <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9
  monasca with Operations Bridge using the Operations Bridge Connector
  simplifies monitoring and managing events and topology information.
 </p><p>
  The integration provides the following functionality:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Forwarding of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> monasca alerts and topology to Operations
    Bridge for event correlation
   </p></li><li class="listitem "><p>
    Customization of forwarded events and topology
   </p></li></ul></div><p>
  For more information about this connector please see
  <a class="link" href="https://software.microfocus.com/en-us/products/operations-bridge-suite/overview" target="_blank">https://software.microfocus.com/en-us/products/operations-bridge-suite/overview</a>.
 </p></div><div class="sect1" id="monitoring-3rd-party-components-with-monasca"><div class="titlepage"><div><div><h2 class="title"><span class="number">4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring Third-Party Components With Monasca</span> <a title="Permalink" class="permalink" href="#monitoring-3rd-party-components-with-monasca">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring_third_party_components.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring_third_party_components.xml</li><li><span class="ds-label">ID: </span>monitoring-3rd-party-components-with-monasca</li></ul></div></div></div></div><div class="sect2" id="monasca-integration-overview"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">monasca Monitoring Integration Overview</span> <a title="Permalink" class="permalink" href="#monasca-integration-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monasca_integration_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monasca_integration_overview.xml</li><li><span class="ds-label">ID: </span>monasca-integration-overview</li></ul></div></div></div></div><p>
  monasca, the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 monitoring service, collects information about
  your cloud's systems, and allows you to create alarm definitions based on
  these measurements. monasca-agent is the component that collects metrics such
  as metric storage and alarm thresholding and forwards them to the monasca-api
  for further processing.
 </p><p>
  With a small amount of configuration, you can use the detection and check
  plugins that are provided with your cloud to monitor integrated third-party
  components. In addition, you can write custom plugins and integrate them with
  the existing monitoring service.
 </p><p>
  Find instructions for customizing existing plugins to monitor third-party
  components in the <a class="xref" href="#configuring-check-plugins" title="4.3.4. Configuring Check Plugins">Section 4.3.4, “Configuring Check Plugins”</a>.
 </p><p>
  Find instructions for installing and configuring new custom plugins in the
 <a class="xref" href="#writing-custom-plugins" title="4.3.3. Writing Custom Plugins">Section 4.3.3, “Writing Custom Plugins”</a>.
 </p><p>
  You can also use existing alarm definitions, as well as create new alarm
  definitions that relate to a custom plugin or metric. Instructions for
  defining new alarm definitions are in the <a class="xref" href="#configuring-alarm-definitions" title="4.3.6. Configuring Alarm Definitions">Section 4.3.6, “Configuring Alarm Definitions”</a>.
 </p><p>
  You can use the Operations Console and monasca CLI to list all of the alarms,
  alarm-definitions, and metrics that exist on your cloud.
 </p></div><div class="sect2" id="monasca-agent"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">monasca Agent</span> <a title="Permalink" class="permalink" href="#monasca-agent">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monasca_agent.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monasca_agent.xml</li><li><span class="ds-label">ID: </span>monasca-agent</li></ul></div></div></div></div><p>
  The monasca agent (monasca-agent) collects information about your cloud using
  the installed plugins. The plugins are written in Python, and determine the
  monitoring metrics for your system, as well as the interval for collection.
  The default collection interval is 30 seconds, and we strongly recommend
  not changing this default value.
 </p><p>
  The following two types of custom plugins can be added to your cloud.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <span class="bold"><strong>Detection Plugin</strong></span>. Determines whether the
    monasca-agent has the ability to monitor the specified component or service
    on a host. If successful, this type of plugin configures an associated
    <span class="bold"><strong>check plugin</strong></span> by creating a YAML
    configuration file.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Check Plugin</strong></span>. Specifies the metrics to be
    monitored, using the configuration file created by the detection plugin.
   </p></li></ul></div><p>
  monasca-agent is installed on every server in your cloud, and provides
  plugins that monitor the following.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    System metrics relating to CPU, memory, disks, host availability, etc.
   </p></li><li class="listitem "><p>
    Process health metrics (process, http_check)
   </p></li><li class="listitem "><p>
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9-specific component metrics, such as apache
    rabbitmq, kafka, cassandra, etc.
   </p></li></ul></div><p>
  monasca is pre-configured with default check plugins and associated detection
  plugins. The default plugins can be reconfigured to monitor third-party
  components, and often only require small adjustments to adapt them to this
  purpose. Find a list of the default plugins here:
  <a class="link" href="https://github.com/openstack/monasca-agent/blob/master/docs/Plugins.md#detection-plugins" target="_blank">https://github.com/openstack/monasca-agent/blob/master/docs/Plugins.md#detection-plugins</a>
 </p><p>
  Often, a single check plugin will be used to monitor multiple services. For
  example, many services use the <code class="literal">http_check.py</code> detection
  plugin to detect the up/down status of a service endpoint. Often the
  <code class="literal">process.py</code> check plugin, which provides process monitoring
  metrics, is used as a basis for a custom process detection plugin.
 </p><p>
  More information about the monasca agent can be found in the following
  locations
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    monasca agent overview:
    <a class="link" href="https://github.com/openstack/monasca-agent/blob/master/docs/Agent.md" target="_blank">https://github.com/openstack/monasca-agent/blob/master/docs/Agent.md</a>
   </p></li><li class="listitem "><p>
    Information on existing plugins:
    <a class="link" href="https://github.com/openstack/monasca-agent/blob/master/docs/Plugins.md" target="_blank">https://github.com/openstack/monasca-agent/blob/master/docs/Plugins.md</a>
   </p></li><li class="listitem "><p>
    Information on plugin customizations:
    <a class="link" href="https://github.com/openstack/monasca-agent/blob/master/docs/Customizations.md" target="_blank">https://github.com/openstack/monasca-agent/blob/master/docs/Customizations.md</a>
   </p></li></ul></div></div><div class="sect2" id="writing-custom-plugins"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Writing Custom Plugins</span> <a title="Permalink" class="permalink" href="#writing-custom-plugins">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-writing_custom_plugins.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-writing_custom_plugins.xml</li><li><span class="ds-label">ID: </span>writing-custom-plugins</li></ul></div></div></div></div><p>
  When the pre-built monasca plugins do not meet your monitoring needs, you can
  write custom plugins to monitor your cloud. After you have written a plugin,
  you must install and configure it.
 </p><p>
  When your needs dictate a very specific custom monitoring check, you must
  provide both a detection and check plugin.
 </p><p>
  The steps involved in configuring a custom plugin include running a detection
  plugin and passing any necesssary parameters to the detection plugin so the
  resulting check configuration file is created with all necessary data.
 </p><p>
  When using an existing check plugin to monitor a third-party component, a
  custom detection plugin is needed only if there is not an associated default
  detection plugin.
 </p><p>
  <span class="bold"><strong>Check plugin configuration files</strong></span>
 </p><p>
  Each plugin needs a corresponding YAML configuration file with the same stem
  name as the plugin check file. For example, the plugin file
  <code class="filename">http_check.py</code> (in
  <code class="filename">/usr/lib/python2.7/site-packages/monasca_agent/collector/checks_d/</code>)
  should have a corresponding configuration file,
  <code class="filename">http_check.yaml</code> (in
  <code class="filename">/etc/monasca/agent/conf.d/http_check.yaml</code>).  The stem
  name <code class="literal">http_check</code> must be the same for both files.
 </p><p>
  Permissions for the YAML configuration file must be <span class="bold"><strong>read+write</strong></span> for <code class="literal">mon-agent</code> user (the
  user that must also own the file), and <span class="bold"><strong>read</strong></span>
  for the <code class="literal">mon-agent</code> group. Permissions for the file must be
  restricted to the <span class="bold"><strong>mon-agent</strong></span> user and
  <span class="bold"><strong>monasca</strong></span> group. The following example shows
  correct permissions settings for the file
  <code class="filename">http_check.yaml</code>.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls -alt /etc/monasca/agent/conf.d/http_check.yaml
-rw-r----- 1 monasca-agent monasca 10590 Jul 26 05:44 http_check.yaml</pre></div><p>
  A check plugin YAML configuration file has the following structure.
 </p><div class="verbatim-wrap"><pre class="screen">init_config:
    key1: value1
    key2: value2

instances:
    - name: john_smith
      username: john_smith
      password: 123456
    - name: jane_smith
      username: jane_smith
      password: 789012</pre></div><p>
  In the above file structure, the <code class="literal">init_config</code> section
  allows you to specify any number of global
  <span class="bold"><strong>key:value</strong></span> pairs. Each pair will be available
  on every run of the check that relates to the YAML configuration file.
 </p><p>
  The <code class="literal">instances</code> section allows you to list the instances
  that the related check will be run on. The check will be run once on each
  instance listed in the <code class="literal">instances</code> section. Ensure that each
  instance listed in the <code class="literal">instances</code> section has a unique
  name.
 </p><p>
  <span class="bold"><strong>Custom detection plugins</strong></span>
 </p><p>
  Detection plugins should be written to perform checks that ensure that a
  component can be monitored on a host. Any arguments needed by the associated
  check plugin are passed into the detection plugin at setup (configuration)
  time. The detection plugin will write to the associated check configuration
  file.
 </p><p>
  When a detection plugin is successfully run in the configuration step, it
  will write to the check configuration YAML file. The configuration file for
  the check is written to the following directory.
 </p><div class="verbatim-wrap"><pre class="screen">/etc/monasca/agent/conf.d/</pre></div><p>
  <span class="bold"><strong>Writing process detection plugin using the
  ServicePlugin class</strong></span>
 </p><p>
  The monasca-agent provides a <code class="literal">ServicePlugin</code>
  class that makes process detection monitoring easy.
 </p><p>
  <span class="bold"><strong>Process check</strong></span>
 </p><p>
  The process check plugin generates metrics based on the process status for
  specified process names. It generates
  <code class="literal">process.pid_count</code> metrics for the specified
  dimensions, and a set of detailed process metrics for the specified
  dimensions by default.
 </p><p>
  The ServicePlugin class allows you to specify a list of process name(s) to
  detect, and uses <span class="bold"><strong>psutil</strong></span> to see if the
  process exists on the host. It then appends the <code class="filename">process.yml</code> configuration
  file with the process name(s), if they do not already exist.
 </p><p>
  The following is an example of a <code class="literal">process.py</code>
  check <code class="literal">ServicePlugin</code>.
 </p><div class="verbatim-wrap"><pre class="screen">import monasca_setup.detection

class monascaTransformDetect(monasca_setup.detection.ServicePlugin):
    """Detect monasca Transform daemons and setup configuration to monitor them."""
    def __init__(self, template_dir, overwrite=False, args=None):
        log.info("      Watching the monasca transform processes.")
        service_params = {
            'args': {},
            'template_dir': template_dir,
            'overwrite': overwrite,
            'service_name': 'monasca-transform',
            'process_names': ['monasca-transform','pyspark',
                              'transform/lib/driver']
        }
        super(monascaTransformDetect, self).__init__(service_params)</pre></div><p>
  <span class="bold"><strong>Writing a Custom Detection Plugin using Plugin or
  ArgsPlugin classes</strong></span>
 </p><p>
  A custom detection plugin class should derive from either the Plugin or
  ArgsPlugin classes provided in the
  <code class="filename">/usr/lib/python2.7/site-packages/monasca_setup/detection</code> directory.
 </p><p>
  If the plugin parses command line arguments, the <code class="literal">ArgsPlugin</code> class is useful.
  The ArgsPlugin class derives from the Plugin class. The ArgsPlugin class has
  a method to check for required arguments, and a method to return the instance
  that will be used for writing to the configuration file with the dimensions
  from the command line parsed and included.
 </p><p>
  If the ArgsPlugin methods do not seem to apply, then derive directly from the
  Plugin class.
 </p><p>
  When deriving from these classes, the following methods should be
  implemented.
 </p><div class="itemizedlist " id="ul-dfd-kvs-px"><ul class="itemizedlist"><li class="listitem "><p>
    _<span class="emphasis"><em>detect - set self.available=True when conditions are met that
    the thing to monitor exists on a host.</em></span>
   </p></li><li class="listitem "><p>
    <span class="emphasis"><em>build</em></span>_config - writes the instance information to the
    configuration and return the configuration.
   </p></li><li class="listitem "><p>
    dependencies_installed (default implementation is in ArgsPlugin, but not
    Plugin) - return true when python dependent libraries are installed.
   </p></li></ul></div><p>
  The following is an example custom detection plugin.
 </p><div class="verbatim-wrap"><pre class="screen">import ast
import logging

import monasca_setup.agent_config
import monasca_setup.detection

log = logging.getLogger(__name__)


class HttpCheck(monasca_setup.detection.ArgsPlugin):
    """Setup an http_check according to the passed in args.
       Despite being a detection plugin this plugin does no detection and will be a noop without   arguments.
       Expects space separated arguments, the required argument is url. Optional parameters include:
       disable_ssl_validation and match_pattern.
    """

    def _detect(self):
        """Run detection, set self.available True if the service is detected.
        """
        self.available = self._check_required_args(['url'])

    def build_config(self):
        """Build the config as a Plugins object and return.
        """
        config = monasca_setup.agent_config.Plugins()
        # No support for setting headers at this time
        instance = self._build_instance(['url', 'timeout', 'username', 'password',
                                         'match_pattern', 'disable_ssl_validation',
                                         'name', 'use_keystone', 'collect_response_time'])

        # Normalize any boolean parameters
        for param in ['use_keystone', 'collect_response_time']:
            if param in self.args:
                instance[param] = ast.literal_eval(self.args[param].capitalize())
        # Set some defaults
        if 'collect_response_time' not in instance:
            instance['collect_response_time'] = True
        if 'name' not in instance:
            instance['name'] = self.args['url']

        config['http_check'] = {'init_config': None, 'instances': [instance]}

        return config</pre></div><p>
  <span class="bold"><strong>Installing a detection plugin in the <span class="productname">OpenStack</span> version delivered with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></strong></span>
 </p><p>
  Install a plugin by copying it to the plugin directory
  (<code class="filename">/usr/lib/python2.7/site-packages/monasca_agent/collector/checks_d/</code>).
 </p><p>
  The plugin should have file permissions of
  <span class="bold"><strong>read+write</strong></span> for the root user (the user that
  should also own the file) and <span class="bold"><strong>read</strong></span> for the
  root group and all other users.
 </p><p>
  The following is an example of correct file permissions for the
  <span class="bold"><strong>http_check.py</strong></span> file.
 </p><div class="verbatim-wrap"><pre class="screen">-rw-r--r-- 1 root root 1769 Sep 19 20:14 http_check.py</pre></div><p>
  Detection plugins should be placed in the following directory.
 </p><div class="verbatim-wrap"><pre class="screen">/usr/lib/monasca/agent/custom_detect.d/</pre></div><p>
  The detection plugin directory name should be accessed using the
  <code class="literal">monasca_agent_detection_plugin_dir</code> Ansible variable. This
  variable is defined in the
  <code class="literal">roles/monasca-agent/vars/main.yml</code> file.
 </p><div class="verbatim-wrap"><pre class="screen">monasca_agent_detection_plugin_dir: /usr/lib/monasca/agent/custom_detect.d/</pre></div><p>
  Example: Add Ansible <code class="literal">monasca_configure</code> task to install the
  plugin. (The <code class="literal">monasca_configure</code> task can be added to any
  service playbook.) In this example, it is added to
  <code class="filename">~/openstack/ardana/ansible/roles/_CEI-CMN/tasks/monasca_configure.yml</code>.
 </p><div class="verbatim-wrap"><pre class="screen">---
- name: _CEI-CMN | monasca_configure |
    Copy ceilometer Custom plugin
  become: yes
  copy:
    src: ardanaceilometer_mon_plugin.py
    dest: "{{ monasca_agent_detection_plugin_dir }}"
    owner: root
    group: root
    mode: 0440</pre></div><p>
  <span class="bold"><strong>Custom check plugins</strong></span>
 </p><p>
  Custom check plugins generate metrics. Scalability should be taken into
  consideration on systems that will have hundreds of servers, as a large
  number of metrics can affect performance by impacting disk performance, RAM
  and CPU usage.
 </p><p>
  You may want to tune your configuration parameters so that less-important
  metrics are not monitored as frequently. When check plugins are configured
  (when they have an associated YAML configuration file) the agent will attempt
  to run them.
 </p><p>
  Checks should be able to run within the 30-second metric collection window.
  If your check runs a command, you should provide a timeout to prevent the
  check from running longer than the default 30-second window. You can use the
  <code class="literal">monasca_agent.common.util.timeout_command</code> to set a timeout
  for in your custom check plugin python code.
 </p><p>
  Find a description of how to write custom check plugins at
  <a class="link" href="https://github.com/openstack/monasca-agent/blob/master/docs/Customizations.md#creating-a-custom-check-plugin" target="_blank">https://github.com/openstack/monasca-agent/blob/master/docs/Customizations.md#creating-a-custom-check-plugin</a>
 </p><p>
  Custom checks derive from the AgentCheck class located in the
  <code class="literal">monasca_agent/collector/checks/check.py</code> file. A check
  method is required.
 </p><p>
  Metrics should contain dimensions that make each item that you are monitoring
  unique (such as service, component, hostname). The hostname dimension is
  defined by default within the AgentCheck class, so every metric has this
  dimension.
 </p><p>
  A custom check will do the following.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Read the configuration instance passed into the check method.
   </p></li><li class="listitem "><p>
    Set dimensions that will be included in the metric.
   </p></li><li class="listitem "><p>
    Create the metric with gauge, rate, or counter types.
   </p></li></ul></div><p>
  Metric Types:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    gauge: Instantaneous reading of a particular value (for example,
    mem.free_mb).
   </p></li><li class="listitem "><p>
    rate: Measurement over a time period. The following equation can be used to
    define rate.
   </p><div class="verbatim-wrap"><pre class="screen">rate=delta_v/float(delta_t)</pre></div></li><li class="listitem "><p>
    counter: The number of events, increment and decrement methods, for
    example, zookeeper.timeouts
   </p></li></ul></div><p>
  The following is an example component check named SimpleCassandraExample.
 </p><div class="verbatim-wrap"><pre class="screen">import monasca_agent.collector.checks as checks
from monasca_agent.common.util import timeout_command

CASSANDRA_VERSION_QUERY = "SELECT version();"


class SimpleCassandraExample(checks.AgentCheck):

    def __init__(self, name, init_config, agent_config):
        super(SimpleCassandraExample, self).__init__(name, init_config, agent_config)

    @staticmethod
    def _get_config(instance):
        user = instance.get('user')
        password = instance.get('password')
        service = instance.get('service')
        timeout = int(instance.get('timeout'))

        return user, password, service, timeout

    def check(self, instance):
        user, password, service, node_name, timeout = self._get_config(instance)

        dimensions = self._set_dimensions({'component': 'cassandra', 'service': service}, instance)

        results, connection_status = self._query_database(user, password, timeout, CASSANDRA_VERSION_QUERY)

        if connection_status != 0:
            self.gauge('cassandra.connection_status', 1, dimensions=dimensions)
        else:
            # successful connection status
            self.gauge('cassandra.connection_status', 0, dimensions=dimensions)

    def _query_database(self, user, password, timeout, query):
        stdout, stderr, return_code = timeout_command(["/opt/cassandra/bin/vsql", "-U", user, "-w", password, "-A", "-R",
                                                       "|", "-t", "-F", ",", "-x"], timeout, command_input=query)
        if return_code == 0:
            # remove trailing newline
            stdout = stdout.rstrip()
            return stdout, 0
        else:
            self.log.error("Error querying cassandra with return code of {0} and error {1}".format(return_code, stderr))
            return stderr, 1</pre></div><p>
  <span class="bold"><strong>Installing check plugin</strong></span>
 </p><p>
  The check plugin needs to have the same file permissions as the detection
  plugin. File permissions must be <span class="bold"><strong>read+write</strong></span>
  for the root user (the user that should own the file), and
  <span class="bold"><strong>read</strong></span> for the root group and all other users.
 </p><p>
  Check plugins should be placed in the following directory.
 </p><div class="verbatim-wrap"><pre class="screen">/usr/lib/monasca/agent/custom_checks.d/</pre></div><p>
  The check plugin directory should be accessed using the
  <code class="literal">monasca_agent_check_plugin_dir</code> Ansible variable. This
  variable is defined in the
  <code class="literal">roles/monasca-agent/vars/main.yml</code> file.
 </p><div class="verbatim-wrap"><pre class="screen">monasca_agent_check_plugin_dir: /usr/lib/monasca/agent/custom_checks.d/</pre></div></div><div class="sect2" id="configuring-check-plugins"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Check Plugins</span> <a title="Permalink" class="permalink" href="#configuring-check-plugins">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring_check_plugins.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring_check_plugins.xml</li><li><span class="ds-label">ID: </span>configuring-check-plugins</li></ul></div></div></div></div><p>
  <span class="bold"><strong>Manually configure a plugin when unit-testing using the
  monasca-setup script installed with the monasca-agent</strong></span>
 </p><p>
  Find a good explanation of configuring plugins here:
  <a class="link" href="https://github.com/openstack/monasca-agent/blob/master/docs/Agent.md#configuring" target="_blank">https://github.com/openstack/monasca-agent/blob/master/docs/Agent.md#configuring</a>
 </p><p>
  SSH to a node that has both the monasca-agent installed as well as the
  component you wish to monitor.
 </p><p>
  The following is an example command that configures a plugin that has no
  parameters (uses the detection plugin class name).
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>/usr/bin/monasca-setup -d ARDANACeilometer</pre></div><p>
  The following is an example command that configures the apache plugin and
  includes related parameters.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>/usr/bin/monasca-setup -d apache -a 'url=http://192.168.245.3:9095/server-status?auto'</pre></div><p>
  If there is a change in the configuration it will restart the monasca-agent
  on the host so the configuration is loaded.
 </p><p>
  After the plugin is configured, you can verify that the configuration file
  has your changes (see the next <span class="bold"><strong>Verify that your check plugin
  is configured</strong></span> section).
 </p><p>
  Use the monasca CLI to see if your metric exists (see the
  <span class="bold"><strong>Verify that metrics exist</strong></span> section).
 </p><p>
  <span class="bold"><strong>Using Ansible modules to configure plugins in
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9</strong></span>
 </p><p>
  The <code class="literal">monasca_agent_plugin</code> module is installed as part of
  the monasca-agent role.
 </p><p>
  The following Ansible example configures the process.py plugin for the
  ceilometer detection plugin. The following example only passes in the name of
  the detection class.
 </p><div class="verbatim-wrap"><pre class="screen">- name: _CEI-CMN | monasca_configure |
    Run monasca agent Cloud Lifecycle Manager specific ceilometer detection plugin
  become: yes
  monasca_agent_plugin:
    name: "ARDANACeilometer"</pre></div><p>
  If a password or other sensitive data are passed to the detection plugin, the
  <code class="literal">no_log</code> option should be set to
  <span class="bold"><strong>True</strong></span>. If the <code class="literal">no_log</code>
  option is not set to <span class="bold"><strong>True</strong></span>, the data passed
  to the plugin will be logged to syslog.
 </p><p>
  The following Ansible example configures the Cassandra plugin and passes in
  related arguments.
 </p><div class="verbatim-wrap"><pre class="screen"> - name: Run monasca Agent detection plugin for Cassandra
   monasca_agent_plugin:
     name: "Cassandra"
     args="directory_names={{ FND_CDB.vars.cassandra_data_dir }},{{ FND_CDB.vars.cassandra_commit_log_dir }} process_username={{ FND_CDB.vars.cassandra_user }}"
   when: database_type == 'cassandra'</pre></div><p>
  The following Ansible example configures the keystone endpoint using the
  http_check.py detection plugin. The class name <code class="literal">httpcheck</code>
  of the http_check.py detection plugin is the name.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>- name:  keystone-monitor | local_monitor |
    Setup active check on keystone internal endpoint locally
  become: yes
  monasca_agent_plugin:
    name: "httpcheck"
    args: "use_keystone=False \
           url=http://{{ keystone_internal_listen_ip }}:{{
               keystone_internal_port }}/v3 \
           dimensions=service:identity-service,\
                       component:keystone-api,\
                       api_endpoint:internal,\
                       monitored_host_type:instance"
  tags:
    - keystone
    - keystone_monitor</pre></div><p>
  <span class="bold"><strong>Verify that your check plugin is configured</strong></span>
 </p><p>
  All check configuration files are located in the following directory. You can
  see the plugins that are running by looking at the plugin configuration
  directory.
 </p><div class="verbatim-wrap"><pre class="screen">/etc/monasca/agent/conf.d/</pre></div><p>
  When the monasca-agent starts up, all of the check plugins that have a
  matching configuration file in the
  <code class="literal">/etc/monasca/agent/conf.d/</code> directory will be loaded.
 </p><p>
  If there are errors running the check plugin they will be written to the
  following error log file.
 </p><div class="verbatim-wrap"><pre class="screen">/var/log/monasca/agent/collector.log</pre></div><p>
  You can change the monasca-agent log level by modifying the
  <code class="literal">log_level</code> option in the
  <code class="literal">/etc/monasca/agent/agent.yaml</code> configuration file, and then
  restarting the monasca-agent, using the following command.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>service openstack-monasca-agent restart</pre></div><p>
  You can debug a check plugin by running <code class="literal">monasca-collector</code>
  with the check option. The following is an example of the
  <code class="literal">monasca-collector</code> command.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo /usr/bin/monasca-collector check <em class="replaceable ">CHECK_NAME</em></pre></div><p>
  <span class="bold"><strong>Verify that metrics exist</strong></span>
 </p><p>
  Begin by logging in to your deployer or controller node.
 </p><p>
  Run the following set of commands, including the <code class="literal">monasca
  metric-list</code> command. If the metric exists, it will be displayed in
  the output.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>monasca metric-list --name <em class="replaceable ">METRIC_NAME</em></pre></div></div><div class="sect2" id="metric-performance-considerations"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Metric Performance Considerations</span> <a title="Permalink" class="permalink" href="#metric-performance-considerations">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-metric_performance_considerations.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-metric_performance_considerations.xml</li><li><span class="ds-label">ID: </span>metric-performance-considerations</li></ul></div></div></div></div><p>
  Collecting metrics on your virtual machines can greatly affect performance.
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 supports 200 compute nodes, with up to 40 VMs each. If your
  environment is managing maximum number of VMs, adding a single metric for all
  VMs is the equivalent of adding 8000 metrics.
 </p><p>
  Because of the potential impact that new metrics have on system performance,
  consider adding only new metrics that are useful for alarm-definition,
  capacity planning, or debugging process failure.
 </p></div><div class="sect2" id="configuring-alarm-definitions"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.3.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Alarm Definitions</span> <a title="Permalink" class="permalink" href="#configuring-alarm-definitions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring_alarm_definitions.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring_alarm_definitions.xml</li><li><span class="ds-label">ID: </span>configuring-alarm-definitions</li></ul></div></div></div></div><p>
  The monasca-api-spec, found here
  <a class="link" href="https://github.com/openstack/monasca-api/blob/master/docs/monasca-api-spec.md" target="_blank">https://github.com/openstack/monasca-api/blob/master/docs/monasca-api-spec.md</a>
  provides an explanation of Alarm Definitions and Alarms. You can find more
  information on alarm definition expressions at the following page:
  <a class="link" href="https://github.com/openstack/monasca-api/blob/master/docs/monasca-api-spec.md#alarm-definition-expressions" target="_blank">https://github.com/openstack/monasca-api/blob/master/docs/monasca-api-spec.md#alarm-definition-expressions</a>.
 </p><p>
  When an alarm definition is defined, the monasca-threshold engine will
  generate an alarm for each unique instance of the match_by metric dimensions
  found in the metric. This allows a single alarm definition that can
  dynamically handle the addition of new hosts.
 </p><p>
  There are default alarm definitions configured for all "process check"
  (process.py check) and "HTTP Status" (http_check.py check) metrics in the
  monasca-default-alarms role. The monasca-default-alarms role is installed as
  part of the monasca deployment phase of your cloud's deployment. You do not
  need to create alarm definitions for these existing checks.
 </p><p>
  Third parties should create an alarm definition when they wish to alarm on a
  custom plugin metric. The alarm definition should only be defined once.
  Setting a notification method for the alarm definition is recommended but not
  required.
 </p><p>
  The following Ansible modules used for alarm definitions are installed as
  part of the monasca-alarm-definition role. This process takes place during
  the monasca set up phase of your cloud's deployment.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    monasca_alarm_definition
   </p></li><li class="listitem "><p>
    monasca_notification_method
   </p></li></ul></div><p>
  The following examples, found in the
  <code class="literal">~/openstack/ardana/ansible/roles/monasca-default-alarms</code>
  directory, illustrate how monasca sets up the default alarm definitions.
 </p><p>
  <span class="bold"><strong>monasca Notification Methods</strong></span>
 </p><p>
  The monasca-api-spec, found in the following link, provides details about
  creating a notification
  <a class="link" href="https://github.com/openstack/monasca-api/blob/master/docs/monasca-api-spec.md#create-notification-method" target="_blank">https://github.com/openstack/monasca-api/blob/master/docs/monasca-api-spec.md#create-notification-method</a>
 </p><p>
  The following are supported notification types.
 </p><div class="itemizedlist " id="ul-t4b-pt4-qx"><ul class="itemizedlist"><li class="listitem "><p>
    EMAIL
   </p></li><li class="listitem "><p>
    WEBHOOK
   </p></li><li class="listitem "><p>
    PAGERDUTY
   </p></li></ul></div><p>
  The <code class="literal">keystone_admin_tenant</code> project is used so that the
  alarms will show up on the Operations Console UI.
 </p><p>
  The following file snippet shows variables from the
  <code class="literal">~/openstack/ardana/ansible/roles/monasca-default-alarms/defaults/main.yml</code>
  file.
 </p><div class="verbatim-wrap"><pre class="screen">---
notification_address: root@localhost
notification_name: 'Default Email'
notification_type: EMAIL

monasca_keystone_url: "{{ KEY_API.advertises.vips.private[0].url }}/v3"
monasca_api_url: "{{ MON_AGN.consumes_MON_API.vips.private[0].url }}/v2.0"
monasca_keystone_user: "{{ MON_API.consumes_KEY_API.vars.keystone_monasca_user }}"
monasca_keystone_password: "{{ MON_API.consumes_KEY_API.vars.keystone_monasca_password | quote }}"
monasca_keystone_project: "{{ KEY_API.vars.keystone_admin_tenant }}"

monasca_client_retries: 3
monasca_client_retry_delay: 2</pre></div><p>
  You can specify a single default notification method in the
  <code class="literal">~/openstack/ardana/ansible/roles/monasca-default-alarms/tasks/main.yml</code>
  file. You can also add or modify the notification type and related details
  using the Operations Console UI or monasca CLI.
 </p><p>
  The following is a code snippet from the
  <code class="literal">~/openstack/ardana/ansible/roles/monasca-default-alarms/tasks/main.yml</code>
  file.
 </p><div class="verbatim-wrap"><pre class="screen">---
- name: monasca-default-alarms | main | Setup default notification method
  monasca_notification_method:
    name: "{{ notification_name }}"
    type: "{{ notification_type }}"
    address: "{{ notification_address }}"
    keystone_url: "{{ monasca_keystone_url }}"
    keystone_user: "{{ monasca_keystone_user }}"
    keystone_password: "{{ monasca_keystone_password }}"
    keystone_project: "{{ monasca_keystone_project }}"
    monasca_api_url: "{{ monasca_api_url }}"
  no_log: True
  tags:
    - system_alarms
    - monasca_alarms
    - openstack_alarms
  register: default_notification_result
  until: not default_notification_result | failed
  retries: "{{ monasca_client_retries }}"
  delay: "{{ monasca_client_retry_delay }}"</pre></div><p>
  <span class="bold"><strong>monasca Alarm Definition</strong></span>
 </p><p>
  In the alarm definition "expression" field, you can specify the metric name
  and threshold. The "match_by" field is used to create a new alarm for every
  unique combination of the match_by metric dimensions.
 </p><p>
  Find more details on alarm definitions at the monasca API documentation:
  (<a class="link" href="https://github.com/stackforge/monasca-api/blob/master/docs/monasca-api-spec.md#alarm-definitions-and-alarms" target="_blank">https://github.com/stackforge/monasca-api/blob/master/docs/monasca-api-spec.md#alarm-definitions-and-alarms</a>).
 </p><p>
  The following is a code snippet from the
  <code class="literal">~/openstack/ardana/ansible/roles/monasca-default-alarms/tasks/main.yml</code>
  file.
 </p><div class="verbatim-wrap"><pre class="screen">- name: monasca-default-alarms | main | Create Alarm Definitions
  monasca_alarm_definition:
    name: "{{ item.name }}"
    description: "{{ item.description | default('') }}"
    expression: "{{ item.expression }}"
    keystone_token: "{{ default_notification_result.keystone_token }}"
    match_by: "{{ item.match_by | default(['hostname']) }}"
    monasca_api_url: "{{ default_notification_result.monasca_api_url }}"
    severity: "{{ item.severity | default('LOW') }}"
    alarm_actions:
      - "{{ default_notification_result.notification_method_id }}"
    ok_actions:
      - "{{ default_notification_result.notification_method_id }}"
    undetermined_actions:
      - "{{ default_notification_result.notification_method_id }}"
  register: monasca_system_alarms_result
  until: not monasca_system_alarms_result | failed
  retries: "{{ monasca_client_retries }}"
  delay: "{{ monasca_client_retry_delay }}"
  with_flattened:
    - monasca_alarm_definitions_system
    - monasca_alarm_definitions_monasca
    - monasca_alarm_definitions_openstack
    - monasca_alarm_definitions_misc_services
  when: monasca_create_definitions</pre></div><p>
  In the following example
  <code class="literal">~/openstack/ardana/ansible/roles/monasca-default-alarms/vars/main.yml</code>
  Ansible variables file, the alarm definition named
  <span class="bold"><strong>Process Check</strong></span> sets the
  <span class="bold"><strong>match_by</strong></span> variable with the following
  parameters.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    process_name
   </p></li><li class="listitem "><p>
    hostname
   </p></li></ul></div><div class="verbatim-wrap"><pre class="screen">monasca_alarm_definitions_system:
  - name: "Host Status"
    description: "Alarms when the specified host is down or not reachable"
    severity: "HIGH"
    expression: "host_alive_status &gt; 0"
    match_by:
      - "target_host"
      - "hostname"
  - name: "HTTP Status"
    description: &gt;
      "Alarms when the specified HTTP endpoint is down or not reachable"
    severity: "HIGH"
    expression: "http_status &gt; 0"
    match_by:
      - "service"
      - "component"
      - "hostname"
      - "url"
  - name: "CPU Usage"
    description: "Alarms when CPU usage is high"
    expression: "avg(cpu.idle_perc) &lt; 10 times 3"
  - name: "High CPU IOWait"
    description: "Alarms when CPU IOWait is high, possible slow disk issue"
    expression: "avg(cpu.wait_perc) &gt; 40 times 3"
    match_by:
      - "hostname"
  - name: "Disk Inode Usage"
    description: "Alarms when disk inode usage is high"
    expression: "disk.inode_used_perc &gt; 90"
    match_by:
      - "hostname"
      - "device"
    severity: "HIGH"
  - name: "Disk Usage"
    description: "Alarms when disk usage is high"
    expression: "disk.space_used_perc &gt; 90"
    match_by:
      - "hostname"
      - "device"
    severity: "HIGH"
  - name: "Memory Usage"
    description: "Alarms when memory usage is high"
    severity: "HIGH"
    expression: "avg(mem.usable_perc) &lt; 10 times 3"
  - name: "Network Errors"
    description: &gt;
      "Alarms when either incoming or outgoing network errors are high"
    severity: "MEDIUM"
    expression: "net.in_errors_sec &gt; 5 or net.out_errors_sec &gt; 5"
  - name: "Process Check"
    description: "Alarms when the specified process is not running"
    severity: "HIGH"
    expression: "process.pid_count &lt; 1"
    match_by:
      - "process_name"
      - "hostname"
  - name: "Crash Dump Count"
    description: "Alarms when a crash directory is found"
    severity: "MEDIUM"
    expression: "crash.dump_count &gt; 0"
    match_by:
      - "hostname"</pre></div><p>
  The preceding configuration would result in the creation of an alarm for each
  unique metric that matched the following criteria.
 </p><div class="verbatim-wrap"><pre class="screen">process.pid_count + process_name + hostname</pre></div><p>
  <span class="bold"><strong>Check that the alarms exist</strong></span>
 </p><p>
  Begin by using the following commands, including <code class="literal">monasca
  alarm-definition-list</code>, to check that the alarm definition exists.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>monasca alarm-definition-list --name <em class="replaceable ">ALARM_DEFINITION_NAME</em></pre></div><p>
  Then use either of the following commands to check that the alarm has been
  generated. A status of "OK" indicates a healthy alarm.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>monasca alarm-list --metric-name <em class="replaceable ">metric name</em></pre></div><p>
  Or
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>monasca alarm-list --alarm-definition-id <em class="replaceable ">ID_FROM_ALARM-DEFINITION-LIST</em></pre></div><div id="id-1.5.6.4.7.36" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   To see CLI options use the <code class="literal">monasca help</code> command.
  </p></div><p>
  <span class="bold"><strong>Alarm state upgrade considerations</strong></span>
 </p><p>
  If the name of a monitoring metric changes or is no longer being sent,
  existing alarms will show the alarm state as
  <code class="literal">UNDETERMINED</code>. You can update an alarm definition as long
  as you do not change the <span class="bold"><strong>metric name</strong></span> or
  <span class="bold"><strong>dimension name</strong></span> values in the <span class="bold"><strong>expression</strong></span> or <span class="bold"><strong>match_by</strong></span> fields. If you find that you need to alter
  either of these values, you must delete the old alarm definitions and create
  new definitions with the updated values.
 </p><p>
  If a metric is never sent, but has a related alarm definition, then no alarms
  would exist. If you find that metrics are never sent, then you should
  remove the related alarm definitions.
 </p><p>
  When removing an alarm definition, the Ansible module
  <span class="bold"><strong>monasca_alarm_definition</strong></span> supports the state
  <code class="literal">absent</code>.
 </p><p>
  The following file snippet shows an example of how to remove an alarm
  definition by setting the state to <code class="literal">absent</code>.
 </p><div class="verbatim-wrap"><pre class="screen">- name: monasca-pre-upgrade | Remove alarm definitions
   monasca_alarm_definition:
     name: "{{ item.name }}"
     state: "absent"
     keystone_url: "{{ monasca_keystone_url }}"
     keystone_user: "{{ monasca_keystone_user }}"
     keystone_password: "{{ monasca_keystone_password }}"
     keystone_project: "{{ monasca_keystone_project }}"
     monasca_api_url: "{{ monasca_api_url }}"
   with_items:
     - { name: "Kafka Consumer Lag" }</pre></div><p>
  An alarm exists in the OK state when the monasca threshold engine has seen at
  least one metric associated with the alarm definition and has not exceeded
  the alarm definition threshold.
 </p></div><div class="sect2" id="integration-of-plugins-with-monasca-agent"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.3.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Openstack Integration of Custom Plugins into monasca-Agent (if applicable)</span> <a title="Permalink" class="permalink" href="#integration-of-plugins-with-monasca-agent">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-integration_of_plugins_with_monasca-agent.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-integration_of_plugins_with_monasca-agent.xml</li><li><span class="ds-label">ID: </span>integration-of-plugins-with-monasca-agent</li></ul></div></div></div></div><p>
  monasca-agent is an OpenStack open-source project. monasca can also monitor
  non-openstack services. Third parties should install custom plugins into
  their <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 system using the steps outlined in the
  <a class="xref" href="#writing-custom-plugins" title="4.3.3. Writing Custom Plugins">Section 4.3.3, “Writing Custom Plugins”</a>. If the OpenStack community
  determines that the custom plugins are of general benefit, the plugin may be
  added to the openstack/monasca-agent so that they are installed with the
  monasca-agent. During the review process for openstack/monasca-agent there
  are no guarantees that code will be approved or merged by a deadline.
  Open-source contributors are expected to help with codereviews in order to get
  their code accepted. Once changes are approved and integrated into the
  openstack/monasca-agent and that version of the monasca-agent is integrated
  with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9, the third party can remove the custom plugin
  installation steps since they would be installed in the default monasca-agent
  venv.
 </p><p>
  Find the open source repository for the monaca-agent here:
  <a class="link" href="https://github.com/openstack/monasca-agent" target="_blank">https://github.com/openstack/monasca-agent</a>
 </p></div></div></div><div class="chapter " id="ops-managing-identity"><div class="titlepage"><div><div><h1 class="title"><span class="number">5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Identity</span> <a title="Permalink" class="permalink" href="#ops-managing-identity">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-managing_identity.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_identity.xml</li><li><span class="ds-label">ID: </span>ops-managing-identity</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#sec-operation-identity-overview"><span class="number">5.1 </span><span class="name">The Identity Service</span></a></span></dt><dt><span class="section"><a href="#supported-upstream-keystone-features"><span class="number">5.2 </span><span class="name">Supported Upstream Keystone Features</span></a></span></dt><dt><span class="section"><a href="#sec-operation-identity"><span class="number">5.3 </span><span class="name">Understanding Domains, Projects, Users, Groups, and Roles</span></a></span></dt><dt><span class="section"><a href="#topic-ffs-dvz-nw"><span class="number">5.4 </span><span class="name">Identity Service Token Validation Example</span></a></span></dt><dt><span class="section"><a href="#topic-qmz-fg3-btx"><span class="number">5.5 </span><span class="name">Configuring the Identity Service</span></a></span></dt><dt><span class="section"><a href="#admin-password"><span class="number">5.6 </span><span class="name">Retrieving the Admin Password</span></a></span></dt><dt><span class="section"><a href="#servicePasswords"><span class="number">5.7 </span><span class="name">Changing Service Passwords</span></a></span></dt><dt><span class="section"><a href="#topic-m43-2j3-bt"><span class="number">5.8 </span><span class="name">Reconfiguring the Identity service</span></a></span></dt><dt><span class="section"><a href="#ldap"><span class="number">5.9 </span><span class="name">Integrating LDAP with the Identity Service</span></a></span></dt><dt><span class="section"><a href="#k2kfed"><span class="number">5.10 </span><span class="name">keystone-to-keystone Federation</span></a></span></dt><dt><span class="section"><a href="#websso"><span class="number">5.11 </span><span class="name">Configuring Web Single Sign-On</span></a></span></dt><dt><span class="section"><a href="#topic-qtp-cn3-bt"><span class="number">5.12 </span><span class="name">Identity Service Notes and Limitations</span></a></span></dt></dl></div></div><p>
  The Identity service provides the structure for user authentication to your
  cloud.
 </p><div class="sect1" id="sec-operation-identity-overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">The Identity Service</span> <a title="Permalink" class="permalink" href="#sec-operation-identity-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-identity_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_overview.xml</li><li><span class="ds-label">ID: </span>sec-operation-identity-overview</li></ul></div></div></div></div><p>
  This topic explains the purpose and mechanisms of the identity service.
 </p><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Identity service, based on the OpenStack keystone API, is
  responsible for providing UserID authentication and access authorization to
  enable organizations to achieve their access security and compliance
  objectives and successfully deploy OpenStack. In short, the Identity service
  is the gateway to the rest of the OpenStack services.
 </p><div class="sect2" id="idg-all-operations-configuring-identity-identity-overview-xml-7"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Which version of the Identity service should you use?</span> <a title="Permalink" class="permalink" href="#idg-all-operations-configuring-identity-identity-overview-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-identity_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_overview.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-configuring-identity-identity-overview-xml-7</li></ul></div></div></div></div><p>
   Use Identity API version 3.0, as previous versions no longer exist as
   endpoints for Identity API queries.
  </p><p>
   Similarly, when performing queries, you must use the OpenStack CLI (the
   <code class="command">openstack</code> command), and not the keystone CLI
   (<code class="command">keystone</code>) as the latter is only compatible with API
   versions prior to 3.0.
  </p></div><div class="sect2" id="idg-all-operations-configuring-identity-identity-overview-xml-8"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Authentication</span> <a title="Permalink" class="permalink" href="#idg-all-operations-configuring-identity-identity-overview-xml-8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-identity_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_overview.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-configuring-identity-identity-overview-xml-8</li></ul></div></div></div></div><p>
   The authentication function provides the initial login function to
   OpenStack. keystone supports multiple sources of authentication, including a
   native or built-in authentication system. The keystone native system can be
   used for all user management functions for proof of concept deployments or
   small deployments not requiring integration with a corporate authentication
   system, but it lacks some of the advanced functions usually found in user
   management systems such as forcing password changes. The focus of the
   keystone native authentication system is to be the source of authentication
   for OpenStack-specific users required for the operation of the various
   OpenStack services. These users are stored by keystone in a default domain;
   the addition of these IDs to an external authentication system is not
   required.
  </p><p>
   keystone is more commonly integrated with external authentication systems
   such as OpenLDAP or Microsoft Active Directory. These systems are usually
   centrally deployed by organizations to serve as the single source of user
   management and authentication for all in-house deployed applications and
   systems requiring user authentication. In addition to LDAP and Microsoft
   Active Directory, support for integration with Security Assertion Markup
   Language (SAML)-based identity providers from companies such as Ping, CA,
   IBM, Oracle, and others is also nearly "production-ready".
  </p><p>
   keystone also provides architectural support via the underlying Apache
   deployment for other types of authentication systems such as Multi-Factor
   Authentication. These types of systems typically require driver support and
   integration from the respective provider vendors.
  </p><div id="id-1.5.7.3.5.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    While support for Identity Providers and Multi-factor authentication is
    available in keystone, it has not yet been certified by the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
    engineering team and is an experimental feature in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
   </p></div><p>
   LDAP-compatible directories such as OpenLDAP and Microsoft Active Directory
   are recommended alternatives to using the keystone local authentication.
   Both methods are widely used by organizations and are integrated with a
   variety of other enterprise applications. These directories act as the
   single source of user information within an organization. keystone can be
   configured to authenticate against an LDAP-compatible directory on a
   per-domain basis.
  </p><p>
   Domains, as explained in <a class="xref" href="#sec-operation-identity" title="5.3. Understanding Domains, Projects, Users, Groups, and Roles">Section 5.3, “Understanding Domains, Projects, Users, Groups, and Roles”</a>,
   can be configured so that based on the user ID, a incoming user is
   automatically mapped to a specific domain. This domain can then be
   configured to authenticate against a specific LDAP directory. The user
   credentials provided by the user to keystone are passed along to the
   designated LDAP source for authentication. This communication can be
   optionally configured to be secure via SSL encryption. No special LDAP
   administrative access is required, and only read-only access is needed for
   this configuration. keystone will not add any LDAP information. All user
   additions, deletions, and modifications are performed by the application's
   front end in the LDAP directories. After a user has been successfully
   authenticated, they are then assigned to the groups, roles, and projects
   defined by the keystone domain or project administrators. This information
   is stored within the keystone service database.
  </p><p>
   Another form of external authentication provided by the keystone service is
   via integration with SAML-based Identity Providers (IdP) such as Ping
   Identity, IBM Tivoli, and Microsoft Active Directory Federation Server. A
   SAML-based identity provider provides authentication that is often called
   "single sign-on". The IdP server is configured to authenticate against
   identity sources such as Active Directory and provides a single
   authentication API against multiple types of downstream identity sources.
   This means that an organization could have multiple identity storage sources
   but a single authentication source. In addition, if a user has logged into
   one such source during a defined session time frame, they do not need to
   re-authenticate within the defined session. Instead, the IdP will
   automatically validate the user to requesting applications and services.
  </p><p>
   A SAML-based IdP authentication source is configured with keystone on a
   per-domain basis similar to the manner in which native LDAP directories are
   configured. Extra mapping rules are required in the configuration that
   define which keystone group an incoming UID is automatically assigned to.
   This means that groups need to be defined in keystone first, but it also
   removes the requirement that a domain or project admin assign user roles and
   project membership on a per-user basis. Instead, groups are used to define
   project membership and roles and incoming users are automatically mapped to
   keystone groups based on their upstream group membership. This provides a
   very consistent role-based access control (RBAC) model based on the upstream
   identity source. The configuration of this option is fairly straightforward.
   IdP vendors such as Ping and IBM are contributing to the maintenance of this
   function and have also produced their own integration documentation.
   Microsoft Active Directory Federation Services (ADFS) is used for functional
   testing and future documentation.
  </p><p>
   In addition to SAML-based IdP, keystone also supports external authentication
   with a third party IdP using OpenID Connect protocol by leveraging the
   capabilities provided by the Apache2 <a class="link" href="https://github.com/zmartzone/mod_auth_openidc" target="_blank">auth_mod_openidc</a> module. The configuration
   of OpenID Connect is similar to SAML.
  </p><p>
   The third keystone-supported authentication source is known as Multi-Factor
   Authentication (MFA). MFA typically requires an external source of
   authentication beyond a login name and password, and can include options
   such as SMS text, a temporal token generator, a fingerprint scanner, etc.
   Each of these types of MFA are usually specific to a particular MFA vendor.
   The keystone architecture supports an MFA-based authentication system, but
   this has not yet been certified or documented for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  </p></div><div class="sect2" id="idg-all-operations-configuring-identity-identity-overview-xml-9"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Authorization</span> <a title="Permalink" class="permalink" href="#idg-all-operations-configuring-identity-identity-overview-xml-9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-identity_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_overview.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-configuring-identity-identity-overview-xml-9</li></ul></div></div></div></div><p>
   The second major function provided by the keystone service is access
   authorization that determines what resources and actions are available based
   on the UserID, the role of the user, and the projects that a user is
   provided access to. All of this information is created, managed, and stored
   by keystone. These functions are applied via the horizon web interface, the
   OpenStack command-line interface, or the direct keystone API.
  </p><p>
   keystone provides support for organizing users via three entities including:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.7.3.6.4.1"><span class="term ">Domains</span></dt><dd><p>
      Domains provide the highest level of organization. Domains are intended
      to be used as high-level containers for multiple projects. A domain can
      represent different tenants, companies or organizations for an OpenStack
      cloud deployed for public cloud deployments or represent major business
      units, functions, or any other type of top-level organization unit in an
      OpenStack private cloud deployment. Each domain has at least one Domain
      Admin assigned to it. This Domain Admin can then create multiple projects
      within the domain and assign the project admin role to specific project
      owners. Each domain created in an OpenStack deployment is unique and the
      projects assigned to a domain cannot exist in another domain.
     </p></dd><dt id="id-1.5.7.3.6.4.2"><span class="term ">Projects</span></dt><dd><p>
      Projects are entities within a domain that represent groups of users,
      each user role within that project, and how many underlying
      infrastructure resources can be consumed by members of the project.
     </p></dd><dt id="id-1.5.7.3.6.4.3"><span class="term ">Groups</span></dt><dd><p>
      Groups are an optional function and provide the means of assigning
      project roles to multiple users at once.
     </p></dd></dl></div><p>
   keystone also provides the means to create and assign roles to groups of
   users or individual users. The role names are created and user assignments
   are made within keystone. The actual function of a role is defined currently
   per each OpenStack service via scripts. When a user requests access to an
   OpenStack service, his access token contains information about his assigned
   project membership and role for that project. This role is then matched to
   the service-specific script and the user is allowed to perform functions
   within that service defined by the role mapping.
  </p></div></div><div class="sect1" id="supported-upstream-keystone-features"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Supported Upstream Keystone Features</span> <a title="Permalink" class="permalink" href="#supported-upstream-keystone-features">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-keystone_features.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-keystone_features.xml</li><li><span class="ds-label">ID: </span>supported-upstream-keystone-features</li></ul></div></div></div></div><div class="sect2" id="section-azv-113-jx"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">OpenStack upstream features that are enabled by default in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9</span> <a title="Permalink" class="permalink" href="#section-azv-113-jx">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-keystone_features.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-keystone_features.xml</li><li><span class="ds-label">ID: </span>section-azv-113-jx</li></ul></div></div></div></div><p>
   The following supported keystone features are enabled by default in the
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 release.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /><col /></colgroup><thead><tr><th>Name</th><th>User/Admin</th><th>Note: API support only. No CLI/UI support</th></tr></thead><tbody><tr><td>Implied Roles</td><td>Admin</td><td>https://blueprints.launchpad.net/keystone/+spec/implied-roles</td></tr><tr><td>Domain-Specific Roles</td><td>Admin</td><td>https://blueprints.launchpad.net/keystone/+spec/domain-specific-roles</td></tr><tr><td>Fernet Token Provider</td><td>User and Admin</td><td>https://docs.openstack.org/keystone/rocky/admin/identity-fernet-token-faq.html</td></tr></tbody></table></div><p>
   <span class="bold"><strong>Implied rules</strong></span>
  </p><p>
   To allow for the practice of hierarchical permissions in user roles, this
   feature enables roles to be linked in such a way that they function as a
   hierarchy with role inheritance.
  </p><p>
   When a user is assigned a superior role, the user will also be assigned all
   roles implied by any subordinate roles. The hierarchy of the assigned roles
   will be expanded when issuing the user a token.
  </p><p>
   <span class="bold"><strong>Domain-specific roles</strong></span>
  </p><p>
   This feature extends the principle of <span class="bold"><strong>implied
   roles</strong></span> to include a set of roles that are specific to a domain. At
   the time a token is issued, the domain-specific roles are not included in
   the token, however, the roles that they map to are.
  </p><p>
   <span class="bold"><strong>Fernet token provider</strong></span>
  </p><p>
   Provides tokens in the Fernet format. This feature is automatically configured
   and is enabled by default. Fernet tokens are preferred and used by default
   instead of the older UUID token format.
  </p></div><div class="sect2" id="section-rpw-21h-jx"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">OpenStack upstream features that are disabled by default in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9</span> <a title="Permalink" class="permalink" href="#section-rpw-21h-jx">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-keystone_features.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-keystone_features.xml</li><li><span class="ds-label">ID: </span>section-rpw-21h-jx</li></ul></div></div></div></div><p>
   The following is a list of features which are fully supported in the
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 release, but are disabled by default. Customers can run a
   playbook to enable the features.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /><col /></colgroup><thead><tr><th>Name</th><th>User/Admin</th><th>Reason Disabled</th></tr></thead><tbody><tr><td>Support multiple LDAP backends via per-domain configuration</td><td>Admin</td><td>Needs explicit configuration.</td></tr><tr><td>WebSSO</td><td>User and Admin</td><td>Needs explicit configuration.</td></tr><tr><td>keystone-to-keystone (K2K) federation</td><td>User and Admin</td><td>Needs explicit configuration.</td></tr><tr><td>Domain-specific config in SQL</td><td>Admin</td><td>Domain specific configuration options can be stored in SQL instead of
                        configuration files, using the new REST APIs.</td></tr></tbody></table></div><p>
   <span class="bold"><strong>Multiple LDAP backends for each domain</strong></span>
  </p><p>
   This feature allows identity backends to be configured on a domain-by-domain
   basis. Domains will be capable of having their own exclusive LDAP service
   (or multiple services). A single LDAP service can also serve multiple
   domains, with each domain in a separate subtree.
  </p><p>
   To implement this feature, individual domains will require domain-specific
   configuration files. Domains that do not implement this feature will
   continue to share a common backend driver.
  </p><p>
   <span class="bold"><strong>WebSSO</strong></span>
  </p><p>
   This feature enables the keystone service to provide federated identity
   services through a token-based single sign-on page. This feature is disabled
   by default, as it requires explicit configuration.
  </p><p>
   <span class="bold"><strong>keystone-to-keystone (K2K) federation</strong></span>
  </p><p>
   This feature enables separate keystone instances to federate identities
   among the instances, offering inter-cloud authorization. This feature is
   disabled by default, as it requires explicit configuration.
  </p><p>
   <span class="bold"><strong>Domain-specific config in SQL</strong></span>
  </p><p>
   Using the new REST APIs, domain-specific configuration options can be stored
   in a SQL database instead of in configuration files.
  </p></div><div class="sect2" id="section-fm3-mch-jx"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Stack upstream features that have been specifically disabled in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9</span> <a title="Permalink" class="permalink" href="#section-fm3-mch-jx">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-keystone_features.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-keystone_features.xml</li><li><span class="ds-label">ID: </span>section-fm3-mch-jx</li></ul></div></div></div></div><p>
   The following is a list of extensions which are disabled by default in
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9, according to keystone policy.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /><col /><col /></colgroup><thead><tr><th>Target Release</th><th>Name</th><th>User/Admin</th><th>Reason Disabled</th></tr></thead><tbody><tr><td>TBD</td><td>Endpoint Filtering</td><td>Admin</td><td>
       <p>
        This extension was implemented to facilitate service activation.
        However, due to lack of enforcement at the service side, this feature
        is only half effective right now.
       </p>
      </td></tr><tr><td>TBD</td><td>Endpoint Policy</td><td>Admin</td><td>
       <p>
        This extension was intended to facilitate policy (policy.json)
        management and enforcement. This feature is useless right now due to
        lack of the needed middleware to utilize the policy files stored in
        keystone.
       </p>
      </td></tr><tr><td>TBD</td><td>OATH 1.0a</td><td>User and Admin</td><td>
       <p>
        Complexity in workflow. Lack of adoption. Its alternative, keystone
        Trust, is enabled by default. HEAT is using keystone Trust.
       </p>
      </td></tr><tr><td>TBD</td><td>Revocation Events</td><td>Admin</td><td>
       <p>
        For PKI token only and PKI token is disabled by default due to
        usability concerns.
       </p>
      </td></tr><tr><td>TBD</td><td>OS CERT</td><td>Admin</td><td>
       <p>
        For PKI token only and PKI token is disabled by default due to
        usability concerns.
       </p>
      </td></tr><tr><td>TBD</td><td>PKI Token</td><td>Admin</td><td>
       <p>
        PKI token is disabled by default due to usability concerns.
       </p>
      </td></tr><tr><td>TBD</td><td>Driver level caching</td><td>Admin</td><td>
       <p>
        Driver level caching is disabled by default due to complexity in setup.
       </p>
      </td></tr><tr><td>TBD</td><td>Tokenless Authz</td><td>Admin</td><td>
       <p>
        Tokenless authorization with X.509 SSL client certificate.
       </p>
      </td></tr><tr><td>TBD</td><td>TOTP Authentication</td><td>User</td><td>
       <p>
        Not fully baked. Has not been battle-tested.
       </p>
      </td></tr><tr><td>TBD</td><td>is_admin_project</td><td>Admin</td><td>
       <p>
        No integration with the services.
       </p>
      </td></tr></tbody></table></div></div></div><div class="sect1" id="sec-operation-identity"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Understanding Domains, Projects, Users, Groups, and Roles</span> <a title="Permalink" class="permalink" href="#sec-operation-identity">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-understanding_identity.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-understanding_identity.xml</li><li><span class="ds-label">ID: </span>sec-operation-identity</li></ul></div></div></div></div><p>
  The identity service uses these concepts for authentication within your cloud
  and these are descriptions of each of them.
 </p><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 identity service uses OpenStack keystone and the concepts
  of domains, projects, users, groups, and roles to manage authentication. This
  page describes how these work together.
 </p><div class="sect2" id="domains-projects-roles"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Domains, Projects, Users, Groups, and Roles</span> <a title="Permalink" class="permalink" href="#domains-projects-roles">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-understanding_identity.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-understanding_identity.xml</li><li><span class="ds-label">ID: </span>domains-projects-roles</li></ul></div></div></div></div><p>
   Most large business organizations use an identity system such as Microsoft
   Active Directory to store and manage their internal user information. A
   variety of applications such as HR systems are, in turn, used to manage the
   data inside of Active Directory. These same organizations often deploy a
   separate user management system for external users such as contractors,
   partners, and customers. Multiple authentication systems are then deployed
   to support multiple types of users.
  </p><p>
   An LDAP-compatible directory such as Active Directory provides a top-level
   organization or domain component. In this example, the organization is
   called Acme. The domain component (DC) is defined as acme.com. Underneath
   the top level domain component are entities referred to as organizational
   units (OU). Organizational units are typically designed to reflect the
   entity structure of the organization. For example, this particular schema
   has 3 different organizational units for the Marketing, IT, and Contractors
   units or departments of the Acme organization. Users (and other types of
   entities like printers) are then defined appropriately underneath each
   organizational entity. The keystone domain entity can be used to match the
   LDAP OU entity; each LDAP OU can have a corresponding keystone domain
   created. In this example, both the Marketing and IT domains represent
   internal employees of Acme and use the same authentication source. The
   Contractors domain contains all external people associated with Acme.
   UserIDs associated with the Contractor domain are maintained in a separate
   user directory and thus have a different authentication source assigned to
   the corresponding keystone-defined Contractors domain.
  </p><p>
   A public cloud deployment usually supports multiple, separate organizations.
   keystone domains can be created to provide a domain per organization with
   each domain configured to the underlying organization's authentication
   source. For example, the ABC company would have a keystone domain created
   called "abc". All users authenticating to the "abc" domain would be
   authenticated against the authentication system provided by the ABC
   organization; in this case ldap://ad.abc.com
  </p></div><div class="sect2" id="domains"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Domains</span> <a title="Permalink" class="permalink" href="#domains">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-understanding_identity.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-understanding_identity.xml</li><li><span class="ds-label">ID: </span>domains</li></ul></div></div></div></div><p>
   A domain is a top-level container targeted at defining major organizational
   entities.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Domains can be used in a multi-tenant OpenStack deployment to segregate
     projects and users from different companies in a public cloud deployment
     or different organizational units in a private cloud setting.
    </p></li><li class="listitem "><p>
     Domains provide the means to identify multiple authentication sources.
    </p></li><li class="listitem "><p>
     Each domain is unique within an OpenStack implementation.
    </p></li><li class="listitem "><p>
     Multiple projects can be assigned to a domain but each project can only
     belong to a single domain.
    </p></li><li class="listitem "><p>
     Each domain has an assigned "admin".
    </p></li><li class="listitem "><p>
     Each project has an assigned "admin".
    </p></li><li class="listitem "><p>
     Domains are created by the "admin" service account and domain admins are
     assigned by the "admin" user.
    </p></li><li class="listitem "><p>
     The "admin" UserID (UID) is created during the keystone installation, has
     the "admin" role assigned to it, and is defined as the "Cloud Admin". This
     UID is created using the "magic" or "secret" admin token found in the
     default 'keystone.conf' file installed during <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> keystone
     installation after the keystone service has been installed. This secret
     token should be removed after installation and the "admin" password
     changed.
    </p></li><li class="listitem "><p>
     The "default" domain is created automatically during the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> keystone
     installation.
    </p></li><li class="listitem "><p>
     The "default" domain contains all OpenStack service accounts that are
     installed during the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> keystone installation process.
    </p></li><li class="listitem "><p>
     No users but the OpenStack service accounts should be assigned to the
     "default" domain.
    </p></li><li class="listitem "><p>
     Domain admins can be any UserID inside or outside of the domain.
    </p></li></ul></div></div><div class="sect2" id="domain-admin"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Domain Administrator</span> <a title="Permalink" class="permalink" href="#domain-admin">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-understanding_identity.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-understanding_identity.xml</li><li><span class="ds-label">ID: </span>domain-admin</li></ul></div></div></div></div><p>
   A UUID is a domain administrator for a given domain if that UID has a
   domain-scoped token scoped for the given domain. This means that the UID has
   the "admin" role assigned to it for the selected domain.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The Cloud Admin UID assigns the domain administrator role for a domain to
     a selected UID.
    </p></li><li class="listitem "><p>
     A domain administrator can create and delete local users who have
     authenticated against keystone. These users will be assigned to the domain
     belonging to the domain administrator who creates the UserID.
    </p></li><li class="listitem "><p>
     A domain administrator can only create users and projects within her
     assigned domains.
    </p></li><li class="listitem "><p>
     A domain administrator can assign the "admin" role of their domains to
     another UID or revoke it; each UID with the "admin" role for a specified
     domain will be a co-administrator for that domain.
    </p></li><li class="listitem "><p>
     A UID can be assigned to be the domain admin of multiple domains.
    </p></li><li class="listitem "><p>
     A domain administrator can assign non-admin roles to any users and groups
     within their assigned domain, including projects owned by their assigned
     domain.
    </p></li><li class="listitem "><p>
     A domain admin UID can belong to projects within their administered
     domains.
    </p></li><li class="listitem "><p>
     Each domain can have a different authentication source.
    </p></li><li class="listitem "><p>
     The domain field is used during the initial login to define the source of
     authentication.
    </p></li><li class="listitem "><p>
     The "List Users" function can only be executed by a UID with the domain
     admin role.
    </p></li><li class="listitem "><p>
     A domain administrator can assign a UID from outside of their domain the
     "domain admin" role, but it is assumed that the domain admin would know the
     specific UID and would not need to list users from an external domain.
    </p></li><li class="listitem "><p>
     A domain administrator can assign a UID from outside of their domain the
     "project admin" role for a specific project within their domain, but it is
     assumed that the domain admin would know the specific UID and would not
     need to list users from an external domain.
    </p></li><li class="listitem "><p>
     Any user that needs the ability to create a user in a project should be
     granted the "admin" role for the domain where the user and the project
     reside.
    </p></li><li class="listitem "><p>
     In order for the horizon <span class="guimenu ">Compute</span> › <span class="guimenu ">Images</span> panel to properly fill the "Owner"
     column, any user that is granted the admin role on a project must also be
     granted the "member" or "admin" role in the domain.
    </p></li></ul></div></div><div class="sect2" id="projects"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Projects</span> <a title="Permalink" class="permalink" href="#projects">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-understanding_identity.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-understanding_identity.xml</li><li><span class="ds-label">ID: </span>projects</li></ul></div></div></div></div><p>
   The domain administrator creates projects within his assigned domain and
   assigns the project admin role to each project to a selected UID. A UID is a
   project administrator for a given project if that UID has a project-scoped
   token scoped for the given project. There can be multiple projects per
   domain. The project admin sets the project quota settings, adds/deletes
   users and groups to and from the project, and defines the user/group roles
   for the assigned project. Users can be belong to multiple projects and have
   different roles on each project. Users are assigned to a specific domain and
   a default project. Roles are assigned per project.
  </p></div><div class="sect2" id="users-groups"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Users and Groups</span> <a title="Permalink" class="permalink" href="#users-groups">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-understanding_identity.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-understanding_identity.xml</li><li><span class="ds-label">ID: </span>users-groups</li></ul></div></div></div></div><p>
   Each user belongs to one domain only. Domain assignments are defined either
   by the domain configuration files or by a domain administrator when creating
   a new, local (user authenticated against keystone) user. There is no current
   method for "moving" a user from one domain to another. A user can belong to
   multiple projects within a domain with a different role assignment per
   project. A group is a collection of users. Users can be assigned to groups
   either by the project admin or automatically via mappings if an external
   authentication source is defined for the assigned domain. Groups can be
   assigned to multiple projects within a domain and have different roles
   assigned to the group per project. A group can be assigned the "admin" role
   for a domain or project. All members of the group will be an "admin" for the
   selected domain or project.
  </p></div><div class="sect2" id="roles"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.3.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Roles</span> <a title="Permalink" class="permalink" href="#roles">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-understanding_identity.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-understanding_identity.xml</li><li><span class="ds-label">ID: </span>roles</li></ul></div></div></div></div><p>
   Service roles represent the functionality used to implement the OpenStack
   role based access control (RBAC), model used to manage access to each
   OpenStack service. Roles are named and assigned per user or group for each
   project by the identity service. Role definition and policy
   enforcement are defined outside of the identity service independently by
   each OpenStack service. The token generated by the identity service for each
   user authentication contains the role assigned to that user for a particular
   project. When a user attempts to access a specific OpenStack service, the
   role is parsed by the service, compared to the service-specific policy file,
   and then granted the resource access defined for that role by the service
   policy file.
  </p><p>
   Each service has its own service policy file with the
   /etc/[SERVICE_CODENAME]/policy.json file name format where
   [SERVICE_CODENAME] represents a specific OpenStack service name. For
   example, the OpenStack nova service would have a policy file called
   /etc/nova/policy.json. With Service policy files can be modified and
   deployed to control nodes from the Cloud Lifecycle Manager. Administrators are
   advised to validate policy changes before checking in the changes to the
   site branch of the local git repository before rolling the changes into
   production. Do not make changes to policy files without having a way to
   validate them.
  </p><p>
   The policy files are located at the following site branch locations on the
   Cloud Lifecycle Manager.
  </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/GLA-API/templates/policy.json.j2
~/openstack/ardana/ansible/roles/ironic-common/files/policy.json
~/openstack/ardana/ansible/roles/KEYMGR-API/templates/policy.json
~/openstack/ardana/ansible/roles/heat-common/files/policy.json
~/openstack/ardana/ansible/roles/CND-API/templates/policy.json
~/openstack/ardana/ansible/roles/nova-common/files/policy.json
~/openstack/ardana/ansible/roles/CEI-API/templates/policy.json.j2
~/openstack/ardana/ansible/roles/neutron-common/templates/policy.json.j2</pre></div><p>
   For test and validation, policy files can be modified in a non-production
   environment from the <code class="filename">~/scratch/</code> directory. For a specific
   policy file, run a search for policy.json. To deploy policy changes for a
   service, run the service specific reconfiguration playbook (for example,
   nova-reconfigure.yml). For a complete list of reconfiguration playbooks,
   change directories to <code class="filename">~/scratch/ansible/next/ardana/ansible</code>
   and run this command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls | grep reconfigure</pre></div><p>
   A read-only role named <code class="literal">project_observer</code> is explicitly
   created in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9. Any user who is granted this role can use
   <code class="literal">list_project</code>.
  </p></div></div><div class="sect1" id="topic-ffs-dvz-nw"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Identity Service Token Validation Example</span> <a title="Permalink" class="permalink" href="#topic-ffs-dvz-nw">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-keystone_token_validation.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-keystone_token_validation.xml</li><li><span class="ds-label">ID: </span>topic-ffs-dvz-nw</li></ul></div></div></div></div><p>
  The following diagram illustrates the flow of typical Identity service
  (keystone) requests/responses between <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services and the Identity
  service. It shows how keystone issues and validates tokens to ensure the
  identity of the caller of each service.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-keystone-KeystoneTokenValidationExample.png" target="_blank"><img src="images/media-keystone-KeystoneTokenValidationExample.png" width="" /></a></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     
     horizon sends an HTTP authentication request to keystone for user
     credentials.
    </p></li><li class="listitem "><p>
     
     keystone validates the credentials and replies with token.
    </p></li><li class="listitem "><p>
     
     horizon sends a POST request, with token to nova to start
     provisioning a virtual machine.
    </p></li><li class="listitem "><p>
     
     nova sends token to keystone for validation.
    </p></li><li class="listitem "><p>
     
     keystone validates the token.
    </p></li><li class="listitem "><p>
     
     nova forwards a request for an image with the attached token.
    </p></li><li class="listitem "><p>
     
     glance sends token to keystone for validation.
    </p></li><li class="listitem "><p>
     
     keystone validates the token.
    </p></li><li class="listitem "><p>
     
     glance provides image-related information to nova.
    </p></li><li class="listitem "><p>
     
     nova sends request for networks to neutron with token.
    </p></li><li class="listitem "><p>
     
     neutron sends token to keystone for validation.
    </p></li><li class="listitem "><p>
     
     keystone validates the token.
    </p></li><li class="listitem "><p>
     
     neutron provides network-related information to nova.
    </p></li><li class="listitem "><p>
     
     nova reports the status of the virtual machine provisioning request.
    </p></li></ol></div></div><div class="sect1" id="topic-qmz-fg3-btx"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Identity Service</span> <a title="Permalink" class="permalink" href="#topic-qmz-fg3-btx">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-configure_identity.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-configure_identity.xml</li><li><span class="ds-label">ID: </span>topic-qmz-fg3-btx</li></ul></div></div></div></div><div class="sect2" id="id-1.5.7.7.2"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">What is the Identity service?</span> <a title="Permalink" class="permalink" href="#id-1.5.7.7.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-configure_identity.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-configure_identity.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Identity service, based on the OpenStack keystone API, provides
   UserID authentication and access authorization to help organizations achieve
   their access security and compliance objectives and successfully deploy
   OpenStack. In short, the Identity service is the gateway to the rest of the
   OpenStack services.
  </p><p>
   The identity service is installed automatically by the Cloud Lifecycle Manager
   (just after MySQL and RabbitMQ). When your cloud is up and running, you can
   customize keystone in a number of ways, including integrating with LDAP
   servers. This topic describes the default configuration. See
   <a class="xref" href="#topic-m43-2j3-bt" title="5.8. Reconfiguring the Identity service">Section 5.8, “Reconfiguring the Identity service”</a> for changes you can
   implement. Also see <a class="xref" href="#ldap" title="5.9. Integrating LDAP with the Identity Service">Section 5.9, “Integrating LDAP with the Identity Service”</a> for information
   on integrating with an LDAP provider.
  </p></div><div class="sect2" id="idg-all-operations-configuring-identity-configure-identity-xml-6"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Which version of the Identity service should you use?</span> <a title="Permalink" class="permalink" href="#idg-all-operations-configuring-identity-configure-identity-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-configure_identity.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-configure_identity.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-configuring-identity-configure-identity-xml-6</li></ul></div></div></div></div><p>
   Note that you should use identity API version 3.0. Identity API v2.0 was has
   been deprecated. Many features such as LDAP integration and fine-grained
   access control will not work with v2.0. The following are a few questions you
   may have regarding versions.
  </p><p>
   <span class="bold"><strong>Why does the keystone identity catalog still show
   version 2.0?</strong></span>
  </p><p>
   Tempest tests still use the v2.0 API. They are in the process of migrating
   to v3.0. We will remove the v2.0 version once tempest has migrated the
   tests. The Identity catalog has version 2.0 just to support tempest
   migration.
  </p><p>
   <span class="bold"><strong>Will the keystone identity v3.0 API work if the
   identity catalog has only the v2.0 endpoint?</strong></span>
  </p><p>
   Identity v3.0 does not rely on the content of the catalog. It will continue
   to work regardless of the version of the API in the catalog.
  </p><p>
   <span class="bold"><strong>Which CLI client should you use?</strong></span>
  </p><p>
   You should use the OpenStack CLI, not the keystone CLI, because it is
   deprecated. The keystone CLI does not support the v3.0 API; only the
   OpenStack CLI supports the v3.0 API.
  </p></div><div class="sect2" id="idg-all-operations-configuring-identity-configure-identity-xml-7"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Authentication</span> <a title="Permalink" class="permalink" href="#idg-all-operations-configuring-identity-configure-identity-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-configure_identity.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-configure_identity.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-configuring-identity-configure-identity-xml-7</li></ul></div></div></div></div><p>
   The authentication function provides the initial login function to
   OpenStack. keystone supports multiple sources of authentication, including a
   native or built-in authentication system. You can use the keystone native
   system for all user management functions for proof-of-concept deployments or
   small deployments not requiring integration with a corporate authentication
   system, but it lacks some of the advanced functions usually found in user
   management systems such as forcing password changes. The focus of the
   keystone native authentication system is to be the source of authentication
   for OpenStack-specific users required to operate various OpenStack services.
   These users are stored by keystone in a default domain; the addition of
   these IDs to an external authentication system is not required.
  </p><p>
   keystone is more commonly integrated with external authentication systems
   such as OpenLDAP or Microsoft Active Directory. These systems are usually
   centrally deployed by organizations to serve as the single source of user
   management and authentication for all in-house deployed applications and
   systems requiring user authentication. In addition to LDAP and Microsoft
   Active Directory, support for integration with Security Assertion Markup
   Language (SAML)-based identity providers from companies such as Ping, CA,
   IBM, Oracle, and others is also nearly "production-ready."
  </p><p>
   keystone also provides architectural support through the underlying Apache
   deployment for other types of authentication systems, such as multi-factor
   authentication. These types of systems typically require driver support and
   integration from the respective providers.
  </p><div id="id-1.5.7.7.4.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    While support for Identity providers and multi-factor authentication is
    available in keystone, it has not yet been certified by the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
    engineering team and is an experimental feature in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
   </p></div><p>
   LDAP-compatible directories such as OpenLDAP and Microsoft Active Directory
   are recommended alternatives to using keystone local authentication. Both
   methods are widely used by organizations and are integrated with a variety
   of other enterprise applications. These directories act as the single source
   of user information within an organization. You can configure keystone to
   authenticate against an LDAP-compatible directory on a per-domain basis.
  </p><p>
   Domains, as explained in <a class="xref" href="#sec-operation-identity" title="5.3. Understanding Domains, Projects, Users, Groups, and Roles">Section 5.3, “Understanding Domains, Projects, Users, Groups, and Roles”</a>,
   can be configured so that, based on the user ID, an incoming user is
   automatically mapped to a specific domain. You can then configure this
   domain to authenticate against a specific LDAP directory. User credentials
   provided by the user to keystone are passed along to the designated LDAP
   source for authentication. You can optionally configure this communication
   to be secure through SSL encryption. No special LDAP administrative access
   is required, and only read-only access is needed for this configuration.
   keystone will not add any LDAP information. All user additions, deletions,
   and modifications are performed by the application's front end in the LDAP
   directories. After a user has been successfully authenticated, that user
   is then assigned to the groups, roles, and projects defined by the
   keystone domain or project administrators. This information is stored in
   the keystone service database.
  </p><p>
   Another form of external authentication provided by the keystone service is
   through integration with SAML-based identity providers (IdP) such as Ping
   Identity, IBM Tivoli, and Microsoft Active Directory Federation Server. A
   SAML-based identity provider provides authentication that is often called
   "single sign-on." The IdP server is configured to authenticate against
   identity sources such as Active Directory and provides a single
   authentication API against multiple types of downstream identity sources.
   This means that an organization could have multiple identity storage sources
   but a single authentication source. In addition, if a user has logged into
   one such source during a defined session time frame, that user does not need
   to reauthenticate within the defined session. Instead, the IdP automatically
   validates the user to requesting applications and services.
  </p><p>
   A SAML-based IdP authentication source is configured with keystone on a
   per-domain basis similar to the manner in which native LDAP directories are
   configured. Extra mapping rules are required in the configuration that
   define which keystone group an incoming UID is automatically assigned to.
   
   
   This means that groups need to be defined in keystone first, but it also
   removes the requirement that a domain or project administrator assign user
   roles and project membership on a per-user basis. Instead, groups are used
   to define project membership and roles and incoming users are automatically
   mapped to keystone groups based on their upstream group membership. This
   strategy provides a consistent role-based access control (RBAC) model based
   on the upstream identity source. The configuration of this option is fairly
   straightforward. IdP vendors such as Ping and IBM are contributing to the
   maintenance of this function and have also produced their own integration
   documentation. HPE is using the Microsoft Active Directory Federation
   Services (AD FS) for functional testing and future documentation.
  </p><p>
   The third keystone-supported authentication source is known as multi-factor
   authentication (MFA). MFA typically requires an external source of
   authentication beyond a login name and password, and can include options
   such as SMS text, a temporal token generator, or a fingerprint scanner. Each
   of these types of MFAs are usually specific to a particular MFA vendor. The
   keystone architecture supports an MFA-based authentication system, but this
   has not yet been certified or documented for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  </p></div><div class="sect2" id="idg-all-operations-configuring-identity-configure-identity-xml-8"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Authorization</span> <a title="Permalink" class="permalink" href="#idg-all-operations-configuring-identity-configure-identity-xml-8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-configure_identity.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-configure_identity.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-configuring-identity-configure-identity-xml-8</li></ul></div></div></div></div><p>
   Another major function provided by the keystone service is access
   authorization that determines which resources and actions are available
   based on the UserID, the role of the user, and the projects that a user is
   provided access to. All of this information is created, managed, and stored
   by keystone. These functions are applied through the horizon web interface,
   the OpenStack command-line interface, or the direct keystone API.
  </p><p>
   keystone provides support for organizing users by using three entities:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.7.7.5.4.1"><span class="term ">Domains</span></dt><dd><p>
      Domains provide the highest level of organization. Domains are intended
      to be used as high-level containers for multiple projects. A domain can
      represent different tenants, companies, or organizations for an OpenStack
      cloud deployed for public cloud deployments or it can represent major
      business units, functions, or any other type of top-level organization
      unit in an OpenStack private cloud deployment. Each domain has at least
      one Domain Admin assigned to it. This Domain Admin can then create
      multiple projects within the domain and assign the project administrator
      role to specific project owners. Each domain created in an OpenStack
      deployment is unique and the projects assigned to a domain cannot exist
      in another domain.
     </p></dd><dt id="id-1.5.7.7.5.4.2"><span class="term ">Projects</span></dt><dd><p>
      Projects are entities within a domain that represent groups of users,
      each user role within that project, and how many underlying
      infrastructure resources can be consumed by members of the project.
     </p></dd><dt id="id-1.5.7.7.5.4.3"><span class="term ">Groups</span></dt><dd><p>
      Groups are an optional function and provide the means of assigning
      project roles to multiple users at once.
     </p></dd></dl></div><p>
   keystone also makes it possible to create and assign roles to groups of
   users or individual users. Role names are created and user assignments are
   made within keystone. The actual function of a role is defined currently for
   each OpenStack service via scripts. When users request access to an
   OpenStack service, their access tokens contain information about their
   assigned project membership and role for that project. This role is then
   matched to the service-specific script and users are allowed to perform
   functions within that service defined by the role mapping.
  </p></div><div class="sect2" id="idg-all-operations-configuring-identity-configure-identity-xml-9"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.5.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Default settings</span> <a title="Permalink" class="permalink" href="#idg-all-operations-configuring-identity-configure-identity-xml-9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-configure_identity.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-configure_identity.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-configuring-identity-configure-identity-xml-9</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Identity service configuration settings</strong></span>
  </p><p>
   The identity service configuration options are described in the OpenStack
   documentation at
   <a class="link" href="https://docs.openstack.org/keystone/rocky/configuration.html" target="_blank">keystone Configuration Options</a>
   on the OpenStack site.
  </p><p>
   <span class="bold"><strong>Default domain and service accounts</strong></span>
  </p><p>
   The "default" domain is automatically created during the installation to
   contain the various required OpenStack service accounts, including the
   following:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="1" /><col class="2" /><col class="3" /></colgroup><thead><tr><th></th><th></th><th></th></tr></thead><tbody><tr><td><p>admin</p></td><td><p>heat</p></td><td><p>monasca-agent </p></td></tr><tr><td><p>barbican</p></td><td><p>logging</p></td><td><p>neutron</p></td></tr><tr><td><p>barbican_service</p></td><td><p>logging_api</p></td><td><p>nova </p></td></tr><tr><td><p>ceilometer</p></td><td><p>logging_beaver</p></td><td><p>nova_monasca</p></td></tr><tr><td><p>cinder</p></td><td><p>logging_monitor</p></td><td><p>octavia</p></td></tr><tr><td><p>cinderinternal</p></td><td><p>magnum</p></td><td><p>placement</p></td></tr><tr><td><p>demo</p></td><td><p>manila</p></td><td><p>swift</p></td></tr><tr><td><p>designate</p></td><td><p>manilainternal</p></td><td><p>swift-demo</p></td></tr><tr><td><p>glance</p></td><td><p>monasca</p></td><td><p>swift-dispersion</p></td></tr><tr><td><p>glance-check</p></td><td><p>monasca_read_only</p></td><td><p>swift-monitor</p></td></tr><tr><td><p>glance-swift</p></td><td></td><td></td></tr></tbody></table></div><p>
   These are required accounts and are used by the underlying OpenStack
   services. These accounts should not be removed or reassigned to a different
   domain. These "default" domain should be used only for these service
   accounts.
  </p></div><div class="sect2" id="id-1.5.7.7.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.5.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Preinstalled roles</span> <a title="Permalink" class="permalink" href="#id-1.5.7.7.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-configure_identity.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-configure_identity.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following are the preinstalled roles. You can create additional roles by
   UIDs with the "admin" role. Roles are defined on a per-service basis (more
   information is available at
   <a class="link" href="http://docs.openstack.org/user-guide-admin/manage_projects_users_and_roles.html" target="_blank">Manage
   projects, users, and roles</a> on the OpenStack website).
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Role</th><th>Description</th></tr></thead><tbody><tr><td>admin</td><td>
       <p>
        The "superuser" role. Provides full access to all <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services
        across all domains and projects. This role should be given only to a
        cloud administrator.
       </p>
      </td></tr><tr><td>member</td><td>
       <p>
        A general role that enables a user to access resources within an
        assigned project including creating, modifying, and deleting compute,
        storage, and network resources.
       </p>
      </td></tr></tbody></table></div><p>
   You can find additional information on these roles in each service policy
   stored in the <code class="literal">/etc/PROJECT/policy.json</code> files where
   PROJECT is a placeholder for an OpenStack service. For example, the Compute
   (nova) service roles are stored in the
   <code class="literal">/etc/nova/policy.json</code> file. Each service policy file
   defines the specific API functions available to a role label.
  </p></div></div><div class="sect1" id="admin-password"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Retrieving the Admin Password</span> <a title="Permalink" class="permalink" href="#admin-password">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-retrieve_adminpassword.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-retrieve_adminpassword.xml</li><li><span class="ds-label">ID: </span>admin-password</li></ul></div></div></div></div><p>
  The admin password will be used to access the dashboard and Operations Console as
  well as allow you to authenticate to use the command-line tools and API.
 </p><p>
  In a default <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 installation there is a randomly generated
  password for the Admin user created. These steps will show you how to
  retrieve this password.
 </p><div class="sect2" id="retrieve"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Retrieving the Admin Password</span> <a title="Permalink" class="permalink" href="#retrieve">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-retrieve_adminpassword.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-retrieve_adminpassword.xml</li><li><span class="ds-label">ID: </span>retrieve</li></ul></div></div></div></div><p>
   You can retrieve the randomly generated Admin password by using this command
   on the Cloud Lifecycle Manager:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cat ~/service.osrc</pre></div><p>
   In this example output, the value for <code class="literal">OS_PASSWORD</code> is the
   Admin password:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cat ~/service.osrc
unset OS_DOMAIN_NAME
export OS_IDENTITY_API_VERSION=3
export OS_AUTH_VERSION=3
export OS_PROJECT_NAME=admin
export OS_PROJECT_DOMAIN_NAME=Default
export OS_USERNAME=admin
export OS_USER_DOMAIN_NAME=Default
export OS_PASSWORD=SlWSfwxuJY0
export OS_AUTH_URL=https://10.13.111.145:5000/v3
export OS_ENDPOINT_TYPE=internalURL
# OpenstackClient uses OS_INTERFACE instead of OS_ENDPOINT
export OS_INTERFACE=internal
export OS_CACERT=/etc/ssl/certs/ca-certificates.crt
export OS_COMPUTE_API_VERSION=2</pre></div></div></div><div class="sect1" id="servicePasswords"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Changing Service Passwords</span> <a title="Permalink" class="permalink" href="#servicePasswords">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-change_service_passwords.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-change_service_passwords.xml</li><li><span class="ds-label">ID: </span>servicePasswords</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> provides a process for changing the default service
  passwords, including your admin user password, which you may want to do for
  security or other purposes.
 </p><p>
  You can easily change the inter-service passwords used for authenticating
  communications between services in your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> deployment,
  promoting better compliance with your organization’s security policies.
  The inter-service passwords that can be changed include (but are not limited
  to) keystone, MariaDB, RabbitMQ, Cloud Lifecycle Manager cluster, monasca and barbican.
 </p><p>
  The general process for changing the passwords is to:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Indicate to the configuration processor which password(s) you want to
    change, and optionally include the value of that password
   </p></li><li class="listitem "><p>
    Run the configuration processor to generate the new passwords (you do not
    need to run <code class="command">git add</code> before this)
   </p></li><li class="listitem "><p>
    Run ready-deployment
   </p></li><li class="listitem "><p>
    Check your password name(s) against the tables included below to see which
    high-level credentials-change playbook(s) you need to run
   </p></li><li class="listitem "><p>
    Run the appropriate high-level credentials-change playbook(s)
   </p></li></ul></div><div class="sect2" id="password-strength"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Password Strength</span> <a title="Permalink" class="permalink" href="#password-strength">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-change_service_passwords.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-change_service_passwords.xml</li><li><span class="ds-label">ID: </span>password-strength</li></ul></div></div></div></div><p>
   Encryption passwords supplied to the configuration processor for use with
   Ansible Vault and for encrypting the configuration processor’s persistent
   state must have a minimum length of 12 characters and a maximum of 128
   characters. Passwords must contain characters from each of the following
   three categories:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Uppercase characters (A-Z)
    </p></li><li class="listitem "><p>
     Lowercase characters (a-z)
    </p></li><li class="listitem "><p>
     Base 10 digits (0-9)
    </p></li></ul></div><p>
   Service Passwords that are automatically generated by the configuration
   processor are chosen from the 62 characters made up of the 26 uppercase,
   the 26 lowercase, and the 10 numeric characters, with no preference given
   to any character or set of characters, with the minimum and maximum lengths
   being determined by the specific requirements of individual services.
  </p><div id="id-1.5.7.9.6.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    Currently, you can not use any special characters with Ansible Vault,
    Service Passwords, or vCenter configuration.
   </p></div></div><div class="sect2" id="id-1.5.7.9.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Telling the configuration processor which password(s) you want to change</span> <a title="Permalink" class="permalink" href="#id-1.5.7.9.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-change_service_passwords.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-change_service_passwords.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9, the configuration processor will produce metadata about
   each of the passwords (and other variables) that it generates in the file
   <code class="literal">~/openstack/my_cloud/info/private_data_metadata_ccp.yml</code>. A
   snippet of this file follows. Expand the header to see the file:
  </p></div><div class="sect2" id="idg-all-operations-change-service-passwords-xml-7"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.7.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">private_data_metadata_ccp.yml</span> <a title="Permalink" class="permalink" href="#idg-all-operations-change-service-passwords-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-change_service_passwords.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-change_service_passwords.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-change-service-passwords-xml-7</li></ul></div></div></div></div><div class="verbatim-wrap"><pre class="screen">metadata_proxy_shared_secret:
  metadata:
  - clusters:
    - cluster1
    component: nova-metadata
    consuming-cp: ccp
    cp: ccp
  version: '2.0'
mysql_admin_password:
  metadata:
  - clusters:
    - cluster1
    component: ceilometer
    consumes: mysql
    consuming-cp: ccp
    cp: ccp
  - clusters:
    - cluster1
    component: heat
    consumes: mysql
    consuming-cp: ccp
    cp: ccp
  - clusters:
    - cluster1
    component: keystone
    consumes: mysql
    consuming-cp: ccp
    cp: ccp
  - clusters:
    - cluster1
    - compute
    component: nova
    consumes: mysql
    consuming-cp: ccp
    cp: ccp
  - clusters:
    - cluster1
    component: cinder
    consumes: mysql
    consuming-cp: ccp
    cp: ccp
  - clusters:
    - cluster1
    component: glance
    consumes: mysql
    consuming-cp: ccp
    cp: ccp
  - clusters:
    - cluster1
    - compute
    component: neutron
    consumes: mysql
    consuming-cp: ccp
    cp: ccp
  - clusters:
    - cluster1
    component: horizon
    consumes: mysql
    consuming-cp: ccp
    cp: ccp
  version: '2.0'
mysql_barbican_password:
  metadata:
  - clusters:
    - cluster1
    component: barbican
    consumes: mysql
    consuming-cp: ccp
    cp: ccp
  version: '2.0'</pre></div><p>
   For each variable, there is a metadata entry for each pair of services that
   use the variable including a list of the clusters on which the service
   component that consumes the variable (defined as "component:" in
   <code class="literal">private_data_metadata_ccp.yml</code> above) runs.
  </p><p>
   Note above that the variable <code class="literal">mysql_admin_password</code> is used by a number of
   service components, and the service that is consumed in each case is <code class="literal">mysql</code>,
   which in this context refers to the MariaDB instance that is part of the
   product.
  </p></div><div class="sect2" id="steps-to-change-password"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.7.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Steps to change a password</span> <a title="Permalink" class="permalink" href="#steps-to-change-password">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-change_service_passwords.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-change_service_passwords.xml</li><li><span class="ds-label">ID: </span>steps-to-change-password</li></ul></div></div></div></div><p>
   First, make sure that you have a copy of
   <code class="filename">private_data_metadata_ccp.yml</code>. If you
   do not, generate one to run the configuration processor:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
   Make a copy of the <code class="literal">private_data_metadata_ccp.yml</code> file and
   place it into the <code class="literal">~/openstack/change_credentials</code> directory:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cp ~/openstack/my_cloud/info/private_data_metadata_control-plane-1.yml \
 ~/openstack/change_credentials/</pre></div><p>
   Edit the copied file in <code class="literal">~/openstack/change_credentials</code>
   leaving only those passwords you intend to change. All entries in this
   template file should be deleted <span class="emphasis"><em>except for those
   passwords</em></span>.
  </p><div id="id-1.5.7.9.9.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    If you leave other passwords in that file that you do
    <span class="emphasis"><em>not</em></span> want to change, they will be regenerated and no
    longer match those in use which could disrupt operations.
   </p></div><div id="id-1.5.7.9.9.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    It is required that you change passwords in batches of each category
    listed below.
   </p></div><p>
   For example, the snippet below would result in the configuration processor
   generating new random values for keystone_backup_password,
   keystone_ceilometer_password, and keystone_cinder_password:
  </p><div class="verbatim-wrap"><pre class="screen">keystone_backup_password:
  metadata:
  - clusters:
    - cluster0
    - cluster1
    - compute
    consumes: keystone-api
    consuming-cp: ccp
    cp: ccp
  version: '2.0'
keystone_ceilometer_password:
  metadata:
  - clusters:
    - cluster1
    component: ceilometer-common
    consumes: keystone-api
    consuming-cp: ccp
    cp: ccp
  version: '2.0'
keystone_cinder_password:
  metadata:
  - clusters:
    - cluster1
    component: cinder-api
    consumes: keystone-api
    consuming-cp: ccp
    cp: ccp
  version: '2.0'</pre></div></div><div class="sect2" id="specifying-password-value"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.7.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Specifying password value</span> <a title="Permalink" class="permalink" href="#specifying-password-value">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-change_service_passwords.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-change_service_passwords.xml</li><li><span class="ds-label">ID: </span>specifying-password-value</li></ul></div></div></div></div><p>
   Optionally, you can specify a value for the password by including a "value:"
   key and value at the same level as metadata:
  </p><div class="verbatim-wrap"><pre class="screen">keystone_backup_password:
    value: 'new_password'
    metadata:
    - clusters:
        - cluster0
        - cluster1
        - compute
        consumes: keystone-api
        consuming-cp: ccp
        cp: ccp
      version: '2.0'</pre></div><p>
   Note that you can have multiple files in openstack/change_credentials. The
   configuration processor will only read files that end in .yml or .yaml.
  </p><div id="id-1.5.7.9.10.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    If you have specified a password value in your credential change file, you
    may want to encrypt it using ansible-vault. If you decide to encrypt with
    ansible-vault, make sure that you use the encryption key you have already
    used when running the configuration processor.
   </p></div><p>
   To encrypt a file using ansible-vault, execute:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/change_credentials
<code class="prompt user">ardana &gt; </code>ansible-vault encrypt <em class="replaceable ">credential change file ending in .yml or .yaml</em></pre></div><p>
   Be sure to provide the encryption key when prompted. Note that if you have
   specified the wrong ansible-vault password, the configuration-processor will
   error out with a message like the following:
  </p><div class="verbatim-wrap"><pre class="screen">################################################## Reading Persistent State ##################################################

################################################################################
# The configuration processor failed.
# PersistentStateCreds: User-supplied creds file test1.yml was not parsed properly
################################################################################</pre></div></div><div class="sect2" id="id-1.5.7.9.11"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.7.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Running the configuration processor to change passwords</span> <a title="Permalink" class="permalink" href="#id-1.5.7.9.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-change_service_passwords.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-change_service_passwords.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The directory openstack/change_credentials is not managed by git, so to rerun
   the configuration processor to generate new passwords and prepare for the
   next deployment just enter the following commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div><div id="id-1.5.7.9.11.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    The files that you placed in
    <code class="filename">~/openstack/change_credentials</code> should be removed
    once you have run the configuration processor because the old password
    values and new password values will be stored in the configuration
    processor's persistent state.
   </p></div><p>
   Note that if you see output like the following after running the
   configuration processor:
  </p><div class="verbatim-wrap"><pre class="screen">################################################################################
# The configuration processor completed with warnings.
# PersistentStateCreds: User-supplied password name 'blah' is not valid
################################################################################</pre></div><p>
   this tells you that the password name you have supplied, 'blah,' does not
   exist. A failure to correctly parse the credentials change file will result
   in the configuration processor erroring out with a message like the
   following:
  </p><div class="verbatim-wrap"><pre class="screen">################################################## Reading Persistent State ##################################################

################################################################################
# The configuration processor failed.
# PersistentStateCreds: User-supplied creds file test1.yml was not parsed properly
################################################################################</pre></div><p>
   Once you have run the configuration processor to change passwords, an
   information file
   <code class="literal">~/openstack/my_cloud/info/password_change.yml</code> similar to the
   <code class="literal">private_data_metadata_ccp.yml</code> is written to tell you which
   passwords have been changed, including metadata but not including the
   values.
  </p></div><div class="sect2" id="id-1.5.7.9.12"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.7.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Password change playbooks and tables</span> <a title="Permalink" class="permalink" href="#id-1.5.7.9.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-change_service_passwords.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-change_service_passwords.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Once you have completed the steps above to change password(s) value(s) and
   then prepare for the deployment that will actually switch over to the new
   passwords, you will need to run some high-level playbooks. The passwords
   that can be changed are grouped into six categories. The tables below list
   the password names that belong in each category. The categories are:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.7.9.12.3.1"><span class="term ">keystone</span></dt><dd><p>
      Playbook: ardana-keystone-credentials-change.yml
     </p></dd><dt id="id-1.5.7.9.12.3.2"><span class="term ">RabbitMQ</span></dt><dd><p>
      Playbook: ardana-rabbitmq-credentials-change.yml
     </p></dd><dt id="id-1.5.7.9.12.3.3"><span class="term ">MariaDB</span></dt><dd><p>
      Playbook: ardana-reconfigure.yml
     </p></dd><dt id="id-1.5.7.9.12.3.4"><span class="term ">Cluster:</span></dt><dd><p>
      Playbook: ardana-cluster-credentials-change.yml
     </p></dd><dt id="id-1.5.7.9.12.3.5"><span class="term ">monasca:</span></dt><dd><p>
      Playbook: monasca-reconfigure-credentials-change.yml
     </p></dd><dt id="id-1.5.7.9.12.3.6"><span class="term ">Other:</span></dt><dd><p>
      Playbook: ardana-other-credentials-change.yml
     </p></dd></dl></div><p>
   It is recommended that you change passwords in batches; in other words, run
   through a complete password change process for each batch of passwords,
   preferably in the above order. Once you have followed the process indicated
   above to change password(s), check the names against the tables below to see
   which password change playbook(s) you should run.
  </p><p>
   <span class="bold"><strong>Changing identity service credentials</strong></span>
  </p><p>
   The following table lists identity service credentials you can change.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col width="19em" class="c1" /></colgroup><thead><tr><th>keystone credentials </th></tr></thead><tbody><tr><td>
<div class="verbatim-wrap"><pre class="screen"><span class="bold"><strong>Password name</strong></span>
barbican_admin_password
barbican_service_password
keystone_admin_pwd
keystone_ceilometer_password
keystone_cinder_password
keystone_cinderinternal_password
keystone_demo_pwd
keystone_designate_password
keystone_glance_password
keystone_glance_swift_password
keystone_heat_password
keystone_magnum_password
keystone_monasca_agent_password
keystone_monasca_password
keystone_neutron_password
keystone_nova_password
keystone_octavia_password
keystone_swift_dispersion_password
keystone_swift_monitor_password
keystone_swift_password
nova_monasca_password</pre></div>
      </td></tr></tbody></table></div><p>
   The playbook to run to change keystone credentials is
   <code class="literal">ardana-keystone-credentials-change.yml</code>. Execute the
   following commands to make the changes:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-keystone-credentials-change.yml</pre></div><p>
   <span class="bold"><strong>Changing RabbitMQ credentials</strong></span>
  </p><p>
   The following table lists the RabbitMQ credentials you can change.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col width="19em" class="c1" /></colgroup><thead><tr><th>RabbitMQ credentials </th></tr></thead><tbody><tr><td>
<div class="verbatim-wrap"><pre class="screen"><span class="bold"><strong>Password name</strong></span>
rmq_barbican_password
rmq_ceilometer_password
rmq_cinder_password
rmq_designate_password
rmq_keystone_password
rmq_magnum_password
rmq_monasca_monitor_password
rmq_nova_password
rmq_octavia_password
rmq_service_password</pre></div>
      </td></tr></tbody></table></div><p>
   The playbook to run to change RabbitMQ credentials is
   <code class="literal">ardana-rabbitmq-credentials-change.yml</code>. Execute the
   following commands to make the changes:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-rabbitmq-credentials-change.yml</pre></div><p>
   <span class="bold"><strong>Changing MariaDB credentials</strong></span>
  </p><p>
   The following table lists the MariaDB credentials you can change.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col width="19em" class="c1" /></colgroup><thead><tr><th>MariaDB credentials </th></tr></thead><tbody><tr><td>
<div class="verbatim-wrap"><pre class="screen"><span class="bold"><strong>Password name</strong></span>
mysql_admin_password
mysql_barbican_password
mysql_clustercheck_pwd
mysql_designate_password
mysql_magnum_password
mysql_monasca_api_password
mysql_monasca_notifier_password
mysql_monasca_thresh_password
mysql_octavia_password
mysql_powerdns_password
mysql_root_pwd
mysql_sst_password
ops_mon_mdb_password
mysql_monasca_transform_password
mysql_nova_api_password
password</pre></div>
      </td></tr></tbody></table></div><p>
   The playbook to run to change MariaDB credentials is
   <code class="literal">ardana-reconfigure.yml</code>. To make the changes, execute the
   following commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml</pre></div><p>
   <span class="bold"><strong>Changing cluster credentials</strong></span>
  </p><p>
   The following table lists the cluster credentials you can change.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col width="19em" class="c1" /></colgroup><thead><tr><th>cluster credentials </th></tr></thead><tbody><tr><td>
<div class="verbatim-wrap"><pre class="screen"><span class="bold"><strong>Password name</strong></span>
haproxy_stats_password
keepalive_vrrp_password</pre></div>
      </td></tr></tbody></table></div><p>
   The playbook to run to change cluster credentials is
   <code class="literal">ardana-cluster-credentials-change.yml</code>. To make changes,
   execute the following commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-cluster-credentials-change.yml</pre></div><p>
   <span class="bold"><strong>Changing monasca credentials</strong></span>
  </p><p>
   The following table lists the monasca credentials you can change.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col width="19em" class="c1" /></colgroup><thead><tr><th>monasca credentials </th></tr></thead><tbody><tr><td>
<div class="verbatim-wrap"><pre class="screen"><span class="bold"><strong>Password name</strong></span>
cassandra_monasca_api_password
cassandra_monasca_persister_password</pre></div>
      </td></tr></tbody></table></div><p>
   The playbook to run to change monasca credentials is
   <code class="literal">monasca-reconfigure-credentials-change.yml</code>. To make the
   changes, execute the following commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-reconfigure-credentials-change.yml</pre></div><p>
   <span class="bold"><strong>Changing other credentials</strong></span>
  </p><p>
   The following table lists the other credentials you can change.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col width="19em" class="c1" /></colgroup><thead><tr><th>Other credentials </th></tr></thead><tbody><tr><td>
<div class="verbatim-wrap"><pre class="screen"><span class="bold"><strong>Password name</strong></span>
logging_beaver_password
logging_api_password
logging_monitor_password
logging_kibana_password</pre></div>
      </td></tr></tbody></table></div><p>
   The playbook to run to change these credentials is
   <code class="literal">ardana-other-credentials-change.yml</code>. To make the changes,
   execute the following commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-other-credentials-change.yml</pre></div></div><div class="sect2" id="changing-keystone-credentials-for-rgw"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.7.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Changing RADOS Gateway Credential</span> <a title="Permalink" class="permalink" href="#changing-keystone-credentials-for-rgw">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-change_service_passwords.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-change_service_passwords.xml</li><li><span class="ds-label">ID: </span>changing-keystone-credentials-for-rgw</li></ul></div></div></div></div><p>
   To change the keystone credentials of RADOS Gateway, follow the preceding
   steps documented in <a class="xref" href="#servicePasswords" title="5.7. Changing Service Passwords">Section 5.7, “Changing Service Passwords”</a> by modifying the
   <code class="literal">keystone_rgw_password</code> section in
   <code class="literal">private_data_metadata_ccp.yml</code> file in
   <a class="xref" href="#steps-to-change-password" title="5.7.4. Steps to change a password">Section 5.7.4, “Steps to change a password”</a> or
   <a class="xref" href="#specifying-password-value" title="5.7.5. Specifying password value">Section 5.7.5, “Specifying password value”</a>.
  </p></div><div class="sect2" id="id-1.5.7.9.14"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.7.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Immutable variables</span> <a title="Permalink" class="permalink" href="#id-1.5.7.9.14">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-change_service_passwords.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-change_service_passwords.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The values of certain variables are immutable, which means that once they have
   been generated by the configuration processor they cannot be changed. These
   variables are:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     barbican_master_kek_db_plugin
    </p></li><li class="listitem "><p>
     swift_hash_path_suffix
    </p></li><li class="listitem "><p>
     swift_hash_path_prefix
    </p></li><li class="listitem "><p>
     mysql_cluster_name
    </p></li><li class="listitem "><p>
     heartbeat_key
    </p></li><li class="listitem "><p>
     erlang_cookie
    </p></li></ul></div><p>
   The configuration processor will not re-generate the values of the above
   passwords, nor will it allow you to specify a value for them. In addition to
   the above variables, the following are immutable in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     All ssh keys generated by the configuration processor
    </p></li><li class="listitem "><p>
     All UUIDs generated by the configuration processor
    </p></li><li class="listitem "><p>
     metadata_proxy_shared_secret
    </p></li><li class="listitem "><p>
     horizon_secret_key
    </p></li><li class="listitem "><p>
     ceilometer_metering_secret
    </p></li></ul></div></div></div><div class="sect1" id="topic-m43-2j3-bt"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reconfiguring the Identity service</span> <a title="Permalink" class="permalink" href="#topic-m43-2j3-bt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-identity_reconfigure.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_reconfigure.xml</li><li><span class="ds-label">ID: </span>topic-m43-2j3-bt</li></ul></div></div></div></div><div class="sect2" id="id-1.5.7.10.2"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Updating the keystone Identity Service</span> <a title="Permalink" class="permalink" href="#id-1.5.7.10.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-identity_reconfigure.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_reconfigure.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   This topic explains configuration options for the Identity service.
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> lets you perform updates on the following parts of the Identity
   service configuration:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Any content in the main keystone configuration file:
     <code class="filename">/etc/keystone/keystone.conf</code>. This lets you
     manipulate keystone configuration parameters. Next, continue with
     <a class="xref" href="#idg-all-operations-configuring-identity-identity-reconfigure-xml-6" title="5.8.2. Updating the Main Identity service Configuration File">Section 5.8.2, “Updating the Main Identity service Configuration File”</a>.
    </p></li><li class="listitem "><p>
     Updating certain configuration options and enabling features, such as:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Verbosity of logs being written to keystone log files.
      </p></li><li class="listitem "><p>
       Process counts for the Apache2 WSGI module, separately for admin and
       public keystone interfaces.
      </p></li><li class="listitem "><p>
       Enabling/disabling auditing.
      </p></li><li class="listitem "><p>
       Enabling/disabling Fernet tokens.
      </p></li></ul></div><p>
     For more information, see <a class="xref" href="#enable-features" title="5.8.3. Enabling Identity Service Features">Section 5.8.3, “Enabling Identity Service Features”</a>.
    </p></li><li class="listitem "><p>
     Creating and updating domain-specific configuration files:
     <span class="bold"><strong>/etc/keystone/domains/keystone.&lt;domain_name&gt;.conf</strong></span>.
     This lets you integrate keystone with one or more external authentication
     sources, such as LDAP server. See the topic on
     <a class="xref" href="#ldap" title="5.9. Integrating LDAP with the Identity Service">Section 5.9, “Integrating LDAP with the Identity Service”</a>.
    </p></li></ul></div></div><div class="sect2" id="idg-all-operations-configuring-identity-identity-reconfigure-xml-6"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Updating the Main Identity service Configuration File</span> <a title="Permalink" class="permalink" href="#idg-all-operations-configuring-identity-identity-reconfigure-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-identity_reconfigure.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_reconfigure.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-configuring-identity-identity-reconfigure-xml-6</li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     The main keystone Identity service configuration file
     (<span class="bold"><strong>/etc/keystone/keystone.conf</strong></span>), located on
     each control plane server, is generated from the following template file
     located on a Cloud Lifecycle Manager:
     <code class="literal">~/openstack/my_cloud/config/keystone/keystone.conf.j2</code>
    </p><p>
     Modify this template file as appropriate. See
     <a class="link" href="http://docs.openstack.org/liberty/config-reference/content/keystone-configuration-file.html" target="_blank">keystone
     Liberty documentation</a> for full descriptions of all settings. This
     is a Jinja2 template, which expects certain template variables to be set.
     Do not change values inside double curly braces: <code class="literal">{{ }}</code>.
    </p><div id="id-1.5.7.10.3.2.1.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 has the following token expiration setting, which
       differs from the upstream value <code class="literal">3600</code>:
      </p><div class="verbatim-wrap"><pre class="screen">[token]
expiration = 14400</pre></div></div></li><li class="listitem "><p>
     After you modify the template, commit the change to the local git
     repository, and rerun the configuration processor / deployment area
     preparation playbooks (as suggested in <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git checkout site
<code class="prompt user">ardana &gt; </code>git add my_cloud/config/keystone/keystone.conf.j2
<code class="prompt user">ardana &gt; </code>git commit -m "Adjusting some parameters in keystone.conf"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the reconfiguration playbook in the deployment area:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</pre></div></li></ol></div></div><div class="sect2" id="enable-features"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.8.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling Identity Service Features</span> <a title="Permalink" class="permalink" href="#enable-features">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-identity_reconfigure.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_reconfigure.xml</li><li><span class="ds-label">ID: </span>enable-features</li></ul></div></div></div></div><p>
   To enable or disable keystone features, do the following:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Adjust respective parameters in
     <span class="bold"><strong>~/openstack/my_cloud/config/keystone/keystone_deploy_config.yml</strong></span>
    </p></li><li class="listitem "><p>
     Commit the change into local git repository, and rerun the configuration
     processor/deployment area preparation playbooks (as suggested in
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git checkout site
<code class="prompt user">ardana &gt; </code>git add my_cloud/config/keystone/keystone_deploy_config.yml
<code class="prompt user">ardana &gt; </code>git commit -m "Adjusting some WSGI or logging parameters for keystone"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the reconfiguration playbook in the deployment area:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</pre></div></li></ol></div></div><div class="sect2" id="fernet-tokens"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.8.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Fernet Tokens</span> <a title="Permalink" class="permalink" href="#fernet-tokens">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-identity_reconfigure.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_reconfigure.xml</li><li><span class="ds-label">ID: </span>fernet-tokens</li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 supports Fernet tokens by default. The benefit of using Fernet
   tokens is that tokens are not persisted in a database, which is helpful if
   you want to deploy the keystone Identity service as one master and multiple
   slaves; only roles, projects, and other details are replicated from master
   to slaves. The token table is not replicated.
  </p><div id="id-1.5.7.10.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    Tempest does not work with Fernet tokens in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9. If Fernet
    tokens are enabled, do not run token tests in Tempest.
   </p></div><div id="id-1.5.7.10.5.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    During reconfiguration when switching to a Fernet token provider or during
    Fernet key rotation, you may see a warning in
    <code class="literal">keystone.log</code> stating <code class="literal">[fernet_tokens]
    key_repository is world readable: /etc/keystone/fernet-keys/</code>.
    This is expected. You can safely ignore this message. For other keystone
    operations, this warning is not displayed. Directory permissions are set to
    600 (read/write by owner only), not world readable.
   </p></div><p>
   Fernet token-signing key rotation is being handled by a cron job, which is
   configured on one of the controllers. The controller with the Fernet
   token-signing key rotation cron job is also known as the Fernet Master node.
   By default, the Fernet token-signing key is rotated once every 24
   hours. The Fernet token-signing keys are distributed from the Fernet Master
   node to the rest of the controllers at each rotation. Therefore, the Fernet
    token-signing keys are consistent for all the controlers at all time.
  </p><p>
   When enabling Fernet token provider the first time, specific steps are
   needed to set up the necessary mechanisms for Fernet token-signing key
   distributions.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Set <code class="literal">keystone_configure_fernet</code> to
     <code class="literal">True</code> in
     <code class="filename">~/openstack/my_cloud/config/keystone/keystone_deploy_config.yml</code>.
    </p></li><li class="step "><p>
     Run the following commands to commit your change in Git and enable Fernet:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add my_cloud/config/keystone/keystone_deploy_config.yml
<code class="prompt user">ardana &gt; </code>git commit -m "enable Fernet token provider"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-deploy.yml</pre></div></li></ol></div></div><p>
   When the Fernet token provider is enabled, a Fernet Master alarm definition
   is also created on monasca to monitor the Fernet Master node. If the Fernet
   Master node is offline or unreachable, a <code class="literal">CRITICAL</code> alarm
   is raised for the Cloud Admin to take corrective actions. If the Fernet
   Master node is offline for a prolonged period of time, Fernet token-signing
   key rotation is not performed. This may introduce security risks to the
   cloud. The Cloud Admin must take immediate actions to resurrect the Fernet
   Master node.
  </p></div></div><div class="sect1" id="ldap"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Integrating LDAP with the Identity Service</span> <a title="Permalink" class="permalink" href="#ldap">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-identity_ldap.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_ldap.xml</li><li><span class="ds-label">ID: </span>ldap</li></ul></div></div></div></div><div class="sect2" id="id-1.5.7.11.2"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Integrating with an external LDAP server</span> <a title="Permalink" class="permalink" href="#id-1.5.7.11.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-identity_ldap.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_ldap.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The keystone identity service provides two primary functions: user
   authentication and access authorization. The user authentication function
   validates a user's identity. keystone has a very basic user management
   system that can be used to create and manage user login and password
   credentials but this system is intended only for proof of concept
   deployments due to the very limited password control functions. The internal
   identity service user management system is also commonly used to store and
   authenticate OpenStack-specific service account information.
  </p><p>
   The recommended source of authentication is external user management systems
   such as LDAP directory services. The identity service can be configured to
   connect to and use external systems as the source of user authentication.
   The identity service domain construct is used to define different
   authentication sources based on domain membership. For example, cloud
   deployment could consist of as few as two domains:
  </p><div class="itemizedlist " id="ul-msq-q3h-4v"><ul class="itemizedlist"><li class="listitem "><p>
     The default domain that is pre-configured for the service account users
     that are authenticated directly against the identity service internal user
     management system
    </p></li><li class="listitem "><p>
     A customer-defined domain that contains all user projects and membership
     definitions. This domain can then be configured to use an external LDAP
     directory such as Microsoft Active Directory as the authentication source.
    </p></li></ul></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> can support multiple domains for deployments that support multiple
   tenants. Multiple domains can be created with each domain configured to
   either the same or different external authentication sources. This
   deployment model is known as a "per-domain" model.
  </p><p>
   There are currently two ways to configure "per-domain" authentication
   sources:
  </p><div class="itemizedlist " id="ul-nsq-q3h-4v"><ul class="itemizedlist"><li class="listitem "><p>
     File store – each domain configuration is created and stored in separate
     text files. This is the older and current default method for defining
     domain configurations.
    </p></li><li class="listitem "><p>
     Database store – each domain configuration can be created using either
     the identity service manager utility (recommenced) or a
     <a class="link" href="http://developer.openstack.org/api-ref-identity-v3.html#domains-config-v3" target="_blank">Domain
     Admin API</a> (from OpenStack.org), and the results are stored in the
     identity service MariaDB database. This database store is a new method
     introduced in the OpenStack Kilo release and now available in
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
    </p></li></ul></div><p>
   Instructions for initially creating per-domain configuration files and then
   migrating to the Database store method via the identity service manager
   utility are provided as follows.
  </p></div><div class="sect2" id="filestore"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Set up domain-specific driver configuration - file store</span> <a title="Permalink" class="permalink" href="#filestore">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-identity_ldap.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_ldap.xml</li><li><span class="ds-label">ID: </span>filestore</li></ul></div></div></div></div><p>
   To update configuration to a specific LDAP domain:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Ensure that the following configuration options are in the main
     configuration file template:
     ~/openstack/my_cloud/config/keystone/keystone.conf.j2
    </p><div class="verbatim-wrap"><pre class="screen">[identity]
domain_specific_drivers_enabled = True
domain_configurations_from_database = False</pre></div></li><li class="listitem "><p>
     Create a YAML file that contains the definition of the LDAP server
     connection. The sample file below is already provided as part of the
     Cloud Lifecycle Manager in the <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>. It is available on
     the Cloud Lifecycle Manager in the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/keystone/keystone_configure_ldap_sample.yml</pre></div><p>
     Save a copy of this file with a new name, for example:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/keystone/keystone_configure_ldap_my.yml</pre></div><div id="id-1.5.7.11.3.3.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      Please refer to the LDAP section of the
      <a class="link" href="https://github.com/openstack/keystone/tree/stable/rocky/etc" target="_blank">keystone</a>
      configuration example for OpenStack for the full option list and
      description.
     </p></div><p>
     Below are samples of YAML configurations for identity service LDAP
     certificate settings, optimized for Microsoft Active Directory server.
    </p><p>
     Sample YAML configuration keystone_configure_ldap_my.yml
    </p><div class="verbatim-wrap"><pre class="screen">---
keystone_domainldap_conf:

    # CA certificates file content.
    # Certificates are stored in Base64 PEM format. This may be entire LDAP server
    # certificate (in case of self-signed certificates), certificate of authority
    # which issued LDAP server certificate, or a full certificate chain (Root CA
    # certificate, intermediate CA certificate(s), issuer certificate).
    #
    cert_settings:
      cacert: |
        -----BEGIN CERTIFICATE-----

        certificate appears here

        -----END CERTIFICATE-----

    # A domain will be created in MariaDB with this name, and associated with ldap back end.
    # Installer will also generate a config file named /etc/keystone/domains/keystone.&lt;domain_name&gt;.conf
    #
    domain_settings:
      name: ad
      description: Dedicated domain for ad users

    conf_settings:
      identity:
         driver: ldap


      # For a full list and description of ldap configuration options, please refer to
      # https://github.com/openstack/keystone/blob/master/etc/keystone.conf.sample or
      # http://docs.openstack.org/liberty/config-reference/content/keystone-configuration-file.html.
      #
      # Please note:
      #  1. LDAP configuration is read-only. Configuration which performs write operations (i.e. creates users, groups, etc)
      #     is not supported at the moment.
      #  2. LDAP is only supported for identity operations (reading users and groups from LDAP). Assignment
      #     operations with LDAP (i.e. managing roles, projects) are not supported.
      #  3. LDAP is configured as non-default domain. Configuring LDAP as a default domain is not supported.
      #
      ldap:
        url: ldap://ad.hpe.net
        suffix: DC=hpe,DC=net
        query_scope: sub
        user_tree_dn: CN=Users,DC=hpe,DC=net
        user : CN=admin,CN=Users,DC=hpe,DC=net
        password: REDACTED
        user_objectclass: user
        user_id_attribute: cn
        user_name_attribute: cn
        group_tree_dn: CN=Users,DC=hpe,DC=net
        group_objectclass: group
        group_id_attribute: cn
        group_name_attribute: cn
        use_pool: True
        user_enabled_attribute: userAccountControl
        user_enabled_mask: 2
        user_enabled_default: 512
        use_tls: True
        tls_req_cert: demand
        # if you are configuring multiple LDAP domains, and LDAP server certificates are issued
        # by different authorities, make sure that you place certs for all the LDAP backend domains in the
        # cacert parameter as seen in this sample yml file so that all the certs are combined in a single CA file
        # and every LDAP domain configuration points to the combined CA file.
        # Note:
        # 1. Please be advised that every time a new ldap domain is configured, the single CA file gets overwritten
        # and hence ensure that you place certs for all the LDAP backend domains in the cacert parameter.
        # 2. There is a known issue on one cert per CA file per domain when the system processes
        # concurrent requests to multiple LDAP domains. Using the single CA file with all certs combined
        # shall get the system working properly*.

        tls_cacertfile: /etc/keystone/ssl/certs/all_ldapdomains_ca.pem

        # The issue is in the underlying SSL library. Upstream is not investing in python-ldap package anymore.
        # It is also not python3 compliant.</pre></div><div class="verbatim-wrap"><pre class="screen">keystone_domain_MSAD_conf:

    # CA certificates file content.
    # Certificates are stored in Base64 PEM format. This may be entire LDAP server
    # certificate (in case of self-signed certificates), certificate of authority
    # which issued LDAP server certificate, or a full certificate chain (Root CA
    # certificate, intermediate CA certificate(s), issuer certificate).
    #
    cert_settings:
      cacert: |
        -----BEGIN CERTIFICATE-----

        certificate appears here

        -----END CERTIFICATE-----

    # A domain will be created in MariaDB with this name, and associated with ldap back end.
    # Installer will also generate a config file named /etc/keystone/domains/keystone.&lt;domain_name&gt;.conf
    #
        domain_settings:
          name: msad
          description: Dedicated domain for msad users

        conf_settings:
          identity:
            driver: ldap

    # For a full list and description of ldap configuration options, please refer to
    # https://github.com/openstack/keystone/blob/master/etc/keystone.conf.sample or
    # http://docs.openstack.org/liberty/config-reference/content/keystone-configuration-file.html.
    #
    # Please note:
    #  1. LDAP configuration is read-only. Configuration which performs write operations (i.e. creates users, groups, etc)
    #     is not supported at the moment.
    #  2. LDAP is only supported for identity operations (reading users and groups from LDAP). Assignment
    #     operations with LDAP (i.e. managing roles, projects) are not supported.
    #  3. LDAP is configured as non-default domain. Configuring LDAP as a default domain is not supported.
    #
    ldap:
      # If the url parameter is set to ldap then typically use_tls should be set to True. If
      # url is set to ldaps, then use_tls should be set to False
      url: ldaps://10.16.22.5
      use_tls: False
      query_scope: sub
      user_tree_dn: DC=l3,DC=local
      # this is the user and password for the account that has access to the AD server
      user: administrator@l3.local
      password: OpenStack123
      user_objectclass: user
      # For a default Active Directory schema this is where to find the user name, openldap uses a different value
      user_id_attribute: userPrincipalName
      user_name_attribute: sAMAccountName
      group_tree_dn: DC=l3,DC=local
      group_objectclass: group
      group_id_attribute: cn
      group_name_attribute: cn
      # An upstream defect requires use_pool to be set false
      use_pool: False
      user_enabled_attribute: userAccountControl
      user_enabled_mask: 2
      user_enabled_default: 512
      tls_req_cert: allow
      # Referals may contain urls that can't be resolved and will cause timeouts, ignore them
      chase_referrals: False
      # if you are configuring multiple LDAP domains, and LDAP server certificates are issued
      # by different authorities, make sure that you place certs for all the LDAP backend domains in the
      # cacert parameter as seen in this sample yml file so that all the certs are combined in a single CA file
      # and every LDAP domain configuration points to the combined CA file.
      # Note:
      # 1. Please be advised that every time a new ldap domain is configured, the single CA file gets overwritten
      # and hence ensure that you place certs for all the LDAP backend domains in the cacert parameter.
      # 2. There is a known issue on one cert per CA file per domain when the system processes
      # concurrent requests to multiple LDAP domains. Using the single CA file with all certs combined
      # shall get the system working properly.

      tls_cacertfile: /etc/keystone/ssl/certs/all_ldapdomains_ca.pem</pre></div></li><li class="listitem "><p>
     As suggested in <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>, commit the new file to the
     local git repository, and rerun the configuration processor and ready
     deployment playbooks:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git checkout site
<code class="prompt user">ardana &gt; </code>git add my_cloud/config/keystone/keystone_configure_ldap_my.yml
<code class="prompt user">ardana &gt; </code>git commit -m "Adding LDAP server integration config"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the reconfiguration playbook in a deployment area, passing the YAML
     file created in the previous step as a command-line option:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml -e@~/openstack/my_cloud/config/keystone/keystone_configure_ldap_my.yml</pre></div></li><li class="listitem "><p>
     Follow these same steps for each LDAP domain with which you are
     integrating the identity service, creating a YAML file for each and
     running the reconfigure playbook once for each additional domain.
    </p></li><li class="listitem "><p>
     Ensure that a new domain was created for LDAP (Microsoft AD in this
     example) and set environment variables for admin level access
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source keystone.osrc</pre></div><p>
     Get a list of domains
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack domain list</pre></div><p>
     As output here:
    </p><div class="verbatim-wrap"><pre class="screen">+----------------------------------+---------+---------+----------------------------------------------------------------------+
| ID                               | Name    | Enabled | Description                                                          |
+----------------------------------+---------+---------+----------------------------------------------------------------------+
| 6740dbf7465a4108a36d6476fc967dbd | heat    | True    | Owns users and projects created by heat                              |
| default                          | Default | True    | Owns users and tenants (i.e. projects) available on Identity API v2. |
| b2aac984a52e49259a2bbf74b7c4108b | ad      | True    | Dedicated domain for users managed by Microsoft AD server            |
+----------------------------------+---------+---------+----------------------------------------------------------------------+</pre></div><div id="id-1.5.7.11.3.3.6.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      LDAP domain is read-only. This means that you cannot create new user or
      group records in it.
     </p></div></li><li class="listitem "><p>
     Once the LDAP user is granted the appropriate role, you can authenticate
     within the specified domain. Set environment variables for admin-level
     access
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source keystone.osrc</pre></div><p>
     Get user record within the ad (Active Directory) domain
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack user show testuser1 --domain ad</pre></div><p>
     Note the output:
    </p><div class="verbatim-wrap"><pre class="screen">+-----------+------------------------------------------------------------------+
| Field     | Value                                                            |
+-----------+------------------------------------------------------------------+
| domain_id | 143af847018c4dc7bd35390402395886                                 |
| id        | e6d8c90abdc4510621271b73cc4dda8bc6009f263e421d8735d5f850f002f607 |
| name      | testuser1                                                        |
+-----------+------------------------------------------------------------------+</pre></div><p>
     Now, get list of LDAP groups:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack group list --domain ad</pre></div><p>
     Here you see testgroup1 and testgroup2:
    </p><div class="verbatim-wrap"><pre class="screen">+------------------------------------------------------------------+------------+
|  ID                                                              | Name       |
+------------------------------------------------------------------+------------+
|  03976b0ea6f54a8e4c0032e8f756ad581f26915c7e77500c8d4aaf0e83afcdc6| testgroup1 |
7ba52ee1c5829d9837d740c08dffa07ad118ea1db2d70e0dc7fa7853e0b79fcf   | testgroup2 |
+------------------------------------------------------------------+------------+</pre></div><p>
     Create a new role. Note that the role is not bound to the domain.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack role create testrole1</pre></div><p>
     Testrole1 has been created:
    </p><div class="verbatim-wrap"><pre class="screen">+-------+----------------------------------+
| Field | Value                            |
+-------+----------------------------------+
| id    | 02251585319d459ab847409dea527dee |
| name  | testrole1                        |
+-------+----------------------------------+</pre></div><p>
     Grant the user a role within the domain by executing the code below. Note
     that due to a current OpenStack CLI limitation, you must use the user ID
     rather than the user name when working with a non-default domain.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack role add testrole1 --user e6d8c90abdc4510621271b73cc4dda8bc6009f263e421d8735d5f850f002f607 --domain ad</pre></div><p>
     Verify that the role was successfully granted, as shown here:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack role assignment list --user e6d8c90abdc4510621271b73cc4dda8bc6009f263e421d8735d5f850f002f607 --domain ad
+----------------------------------+------------------------------------------------------------------+-------+---------+----------------------------------+
| Role                             | User                                                             | Group | Project | Domain                           |
+----------------------------------+------------------------------------------------------------------+-------+---------+----------------------------------+
| 02251585319d459ab847409dea527dee | e6d8c90abdc4510621271b73cc4dda8bc6009f263e421d8735d5f850f002f607 |       |         | 143af847018c4dc7bd35390402395886 |
+----------------------------------+------------------------------------------------------------------+-------+---------+----------------------------------+</pre></div><p>
     Authenticate (get a domain-scoped token) as a new user with a new role.
     The --os-* command-line parameters specified below override the respective
     OS_* environment variables set by the keystone.osrc script to provide
     admin access. To ensure that the command below is executed in a clean
     environment, you may want log out from the node and log in again.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack --os-identity-api-version 3 \
            --os-username testuser1 \
            --os-password testuser1_password \
            --os-auth-url http://10.0.0.6:35357/v3 \
            --os-domain-name ad \
            --os-user-domain-name ad \
            token issue</pre></div><p>
     Here is the result:
    </p><div class="verbatim-wrap"><pre class="screen">+-----------+------------------------------------------------------------------+
| Field     | Value                                                            |
+-----------+------------------------------------------------------------------+
| domain_id | 143af847018c4dc7bd35390402395886                                 |
| expires   | 2015-09-09T21:36:15.306561Z                                      |
| id        | 6f8f9f1a932a4d01b7ad9ab061eb0917                                 |
| user_id   | e6d8c90abdc4510621271b73cc4dda8bc6009f263e421d8735d5f850f002f607 |
+-----------+------------------------------------------------------------------+</pre></div></li><li class="listitem "><p>
     Users can also have a project within the domain and get a project-scoped
     token. To accomplish this, set environment variables for admin level
     access:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source keystone.osrc</pre></div><p>
     Then create a new project within the domain:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack project create testproject1 --domain ad</pre></div><p>
     The result shows that they have been created:
    </p><div class="verbatim-wrap"><pre class="screen">+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description |                                  |
| domain_id   | 143af847018c4dc7bd35390402395886 |
| enabled     | True                             |
| id          | d065394842d34abd87167ab12759f107 |
| name        | testproject1                     |
+-------------+----------------------------------+</pre></div><p>
     Grant the user a role with a project, re-using the role created in the
     previous example. Note that due to a current OpenStack CLI limitation, you
     must use user ID rather than user name when working with a non-default
     domain.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack role add testrole1 --user e6d8c90abdc4510621271b73cc4dda8bc6009f263e421d8735d5f850f002f607 --project testproject1</pre></div><p>
     Verify that the role was successfully granted by generating a list:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack role assignment list --user e6d8c90abdc4510621271b73cc4dda8bc6009f263e421d8735d5f850f002f607 --project testproject1</pre></div><p>
     The output shows the result:
    </p><div class="verbatim-wrap"><pre class="screen">+----------------------------------+------------------------------------------------------------------+-------+----------------------------------+--------+
| Role                             | User                                                             | Group | Project                          | Domain |
+----------------------------------+------------------------------------------------------------------+-------+----------------------------------+--------+
| 02251585319d459ab847409dea527dee | e6d8c90abdc4510621271b73cc4dda8bc6009f263e421d8735d5f850f002f607 |       | d065394842d34abd87167ab12759f107 |        |
+----------------------------------+------------------------------------------------------------------+-------+----------------------------------+--------+</pre></div><p>
     Authenticate (get a project-scoped token) as the new user with a new role.
     The --os-* command line parameters specified below override their
     respective OS_* environment variables set by keystone.osrc to provide
     admin access. To ensure that the command below is executed in a clean
     environment, you may want log out from the node and log in again. Note
     that both the --os-project-domain-name and --os-project-user-name
     parameters are needed to verify that both user and project are not in the
     default domain.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack --os-identity-api-version 3 \
            --os-username testuser1 \
            --os-password testuser1_password \
            --os-auth-url http://10.0.0.6:35357/v3 \
            --os-project-name testproject1 \
            --os-project-domain-name ad \
            --os-user-domain-name ad \
            token issue</pre></div><p>
     Below is the result:
    </p><div class="verbatim-wrap"><pre class="screen">+------------+------------------------------------------------------------------+
| Field      | Value                                                            |
+------------+------------------------------------------------------------------+
| expires    | 2015-09-09T21:50:49.945893Z                                      |
| id         | 328e18486f69441fb13f4842423f52d1                                 |
| project_id | d065394842d34abd87167ab12759f107                                 |
| user_id    | e6d8c90abdc4510621271b73cc4dda8bc6009f263e421d8735d5f850f002f607 |
+------------+------------------------------------------------------------------+</pre></div></li></ol></div></div><div class="sect2" id="id-1.5.7.11.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.9.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Set up or switch to domain-specific driver configuration using a database store</span> <a title="Permalink" class="permalink" href="#id-1.5.7.11.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-identity_ldap.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_ldap.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To make the switch, execute the steps below. Remember, you must have already
   set up the configuration for a file store as explained in
   <a class="xref" href="#filestore" title="5.9.2. Set up domain-specific driver configuration - file store">Section 5.9.2, “Set up domain-specific driver configuration - file store”</a>, and it must be working properly.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Ensure that the following configuration options are set in the main
     configuration file,
     ~/openstack/my_cloud/config/keystone/keystone.conf.j2:
    </p><div class="verbatim-wrap"><pre class="screen">[identity]
domain_specific_drivers_enabled = True
domain_configurations_from_database = True

[domain_config]
driver = sql</pre></div></li><li class="listitem "><p>
     Once the template is modified, commit the change to the local git
     repository, and rerun the configuration processor / deployment area
     preparation playbooks (as suggested at Using Git for Configuration
     Management):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git checkout site
<code class="prompt user">ardana &gt; </code>git add -A</pre></div><p>
     Verify that the files have been added using git status:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git status</pre></div><p>
     Then commit the changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git commit -m "Use Domain-Specific Driver Configuration - Database Store: more description here..."</pre></div><p>
     Next, run the configuration processor and ready deployment playbooks:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the reconfiguration playbook in a deployment area:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</pre></div></li><li class="listitem "><p>
     Upload the domain-specific config files to the database if they have not
     been loaded. If they have already been loaded and you want to switch back
     to database store mode, then skip this upload step and move on to step 5.
    </p><div class="orderedlist " id="ol-tff-px4-mv"><ol class="orderedlist" type="a"><li class="listitem "><p>
       Go to one of the controller nodes where keystone is deployed.
      </p></li><li class="listitem "><p>
       Verify that domain-specific driver configuration files are located under
       the directory (default /etc/keystone/domains) with the format:
       keystone.&lt;domain name&gt;.conf Use the keystone manager utility to
       load domain-specific config files to the database. There are two options
       for uploading the files:
      </p><div class="orderedlist " id="ol-uff-px4-mv"><ol class="orderedlist" type="i"><li class="listitem "><p>
         Option 1: Upload all configuration files to the SQL database:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>keystone-manage domain_config_upload --all</pre></div></li><li class="listitem "><p>
         Option 2: Upload individual domain-specific configuration files by
         specifying the domain name one by one:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>keystone-manage domain_config_upload --domain-name <em class="replaceable ">domain name</em></pre></div><p>
         Here is an example:
        </p><div class="verbatim-wrap"><pre class="screen">keystone-manage domain_config_upload --domain-name ad</pre></div><p>
         Note that the keystone manager utility does not upload the
         domain-specific driver configuration file the second time for the same
         domain. For the management of the domain-specific driver configuration
         in the database store, you may refer to
         <a class="link" href="http://developer.openstack.org/api-ref-identity-v3.html#domains-config-v3" target="_blank">OpenStack
         Identity API - Domain Configuration</a>.
        </p></li></ol></div></li></ol></div></li><li class="listitem "><p>
     Verify that the switched domain driver configuration for LDAP (Microsoft
     AD in this example) in the database store works properly. Then set the
     environment variables for admin level access:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/keystone.osrc</pre></div><p>
     Get a list of domain users:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack user list --domain ad</pre></div><p>
     Note the three users returned:
    </p><div class="verbatim-wrap"><pre class="screen">+------------------------------------------------------------------+------------+
| ID                                                               | Name       |
+------------------------------------------------------------------+------------+
| e7dbec51ecaf07906bd743debcb49157a0e8af557b860a7c1dadd454bdab03fe | testuser1  |
| 8a09630fde3180c685e0cd663427e8638151b534a8a7ccebfcf244751d6f09bd | testuser2  |
| ea463d778dadcefdcfd5b532ee122a70dce7e790786678961420ae007560f35e | testuser3  |
+------------------------------------------------------------------+------------+</pre></div><p>
     Get user records within the ad domain:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack user show testuser1 --domain ad</pre></div><p>
     Here testuser1 is returned:
    </p><div class="verbatim-wrap"><pre class="screen">+-----------+------------------------------------------------------------------+
| Field     | Value                                                            |
+-----------+------------------------------------------------------------------+
| domain_id | 143af847018c4dc7bd35390402395886                                 |
| id        | e6d8c90abdc4510621271b73cc4dda8bc6009f263e421d8735d5f850f002f607 |
| name      | testuser1                                                        |
+-----------+------------------------------------------------------------------+</pre></div><p>
     Get a list of LDAP groups:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack group list --domain ad</pre></div><p>
     Note that testgroup1 and testgroup2 are returned:
    </p><div class="verbatim-wrap"><pre class="screen">+------------------------------------------------------------------+------------+
| ID                                                               | Name       |
+------------------------------------------------------------------+------------+
| 03976b0ea6f54a8e4c0032e8f756ad581f26915c7e77500c8d4aaf0e83afcdc6 | testgroup1 |
| 7ba52ee1c5829d9837d740c08dffa07ad118ea1db2d70e0dc7fa7853e0b79fcf | testgroup2 |
+------------------------------------------------------------------+------------+</pre></div><div id="id-1.5.7.11.4.3.5.15" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      LDAP domain is read-only. This means that you cannot create new user or
      group records in it.
     </p></div></li></ol></div></div><div class="sect2" id="id-1.5.7.11.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.9.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Domain-specific driver configuration. Switching from a database to a file store</span> <a title="Permalink" class="permalink" href="#id-1.5.7.11.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-identity_ldap.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_ldap.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Following is the procedure to switch a domain-specific driver configuration
   from a database store to a file store. It is assumed that:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The domain-specific driver configuration with a database store has been
     set up and is working properly.
    </p></li><li class="listitem "><p>
     Domain-specific driver configuration files with the format:
     keystone.&lt;domain name&gt;.conf have already been located and verified
     in the specific directory (by default, /etc/keystone/domains/) on all of
     the controller nodes.
    </p></li></ul></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Ensure that the following configuration options are set in the main
     configuration file template in
     ~/openstack/my_cloud/config/keystone/keystone.conf.j2:
    </p><div class="verbatim-wrap"><pre class="screen">[identity]
 domain_specific_drivers_enabled = True
 domain_configurations_from_database = False

[domain_config]
# driver = sql</pre></div></li><li class="listitem "><p>
     Once the template is modified, commit the change to the local git
     repository, and rerun the configuration processor / deployment area
     preparation playbooks (as suggested at Using Git for Configuration
     Management):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git checkout site
<code class="prompt user">ardana &gt; </code>git add -A</pre></div><p>
     Verify that the files have been added using git status, then commit the
     changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git status
<code class="prompt user">ardana &gt; </code>git commit -m "Domain-Specific Driver Configuration - Switch From Database Store to File Store: more description here..."</pre></div><p>
     Then run the configuration processor and ready deployment playbooks:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run reconfiguration playbook in a deployment area:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</pre></div></li><li class="listitem "><p>
     Verify that the switched domain driver configuration for LDAP (Microsoft
     AD in this example) using file store works properly: Set environment
     variables for admin level access
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/keystone.osrc</pre></div><p>
     Get list of domain users:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack user list --domain ad</pre></div><p>
     Here you see the three users:
    </p><div class="verbatim-wrap"><pre class="screen">+------------------------------------------------------------------+------------+
| ID                                                               | Name       |
+------------------------------------------------------------------+------------+
| e7dbec51ecaf07906bd743debcb49157a0e8af557b860a7c1dadd454bdab03fe | testuser1  |
| 8a09630fde3180c685e0cd663427e8638151b534a8a7ccebfcf244751d6f09bd | testuser2  |
| ea463d778dadcefdcfd5b532ee122a70dce7e790786678961420ae007560f35e | testuser3  |
+------------------------------------------------------------------+------------+</pre></div><p>
     Get user records within the ad domain:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack user show testuser1 --domain ad</pre></div><p>
     Here is the result:
    </p><div class="verbatim-wrap"><pre class="screen">+-----------+------------------------------------------------------------------+
| Field     | Value                                                            |
+-----------+------------------------------------------------------------------+
| domain_id | 143af847018c4dc7bd35390402395886                                 |
| id        | e6d8c90abdc4510621271b73cc4dda8bc6009f263e421d8735d5f850f002f607 |
| name      | testuser1                                                        |
+-----------+------------------------------------------------------------------+</pre></div><p>
     Get a list of LDAP groups:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack group list --domain ad</pre></div><p>
     Here are the groups returned:
    </p><div class="verbatim-wrap"><pre class="screen">+------------------------------------------------------------------+------------+
| ID                                                               | Name       |
+------------------------------------------------------------------+------------+
| 03976b0ea6f54a8e4c0032e8f756ad581f26915c7e77500c8d4aaf0e83afcdc6 | testgroup1 |
| 7ba52ee1c5829d9837d740c08dffa07ad118ea1db2d70e0dc7fa7853e0b79fcf | testgroup2 |
+------------------------------------------------------------------+------------+</pre></div><p>
     Note: Note: LDAP domain is read-only. This means that you can not create
     new user or group record in it.
    </p></li></ol></div></div><div class="sect2" id="id-1.5.7.11.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.9.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Update LDAP CA certificates</span> <a title="Permalink" class="permalink" href="#id-1.5.7.11.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-identity_ldap.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_ldap.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   There is a chance that LDAP CA certificates may expire or for some reason
   not work anymore. Below are steps to update the LDAP CA certificates on the
   identity service side. Follow the steps below to make the updates.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Locate the file keystone_configure_ldap_certs_sample.yml
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/keystone/keystone_configure_ldap_certs_sample.yml</pre></div></li><li class="listitem "><p>
     Save a copy of this file with a new name, for example:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/keystone/keystone_configure_ldap_certs_all.yml</pre></div></li><li class="listitem "><p>
     Edit the file and specify the correct single file path name for the ldap
     certificates. This file path name has to be consistent with the one
     defined in tls_cacertfile of the domain-specific configuration. Edit the
     file and populate or update it with LDAP CA certificates for all LDAP
     domains.
    </p></li><li class="listitem "><p>
     As suggested in <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>, add the new file to the local
     git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git checkout site
<code class="prompt user">ardana &gt; </code>git add -A</pre></div><p>
     Verify that the files have been added using git status and commit the
     file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git status
<code class="prompt user">ardana &gt; </code>git commit -m "Update LDAP CA certificates: more description here..."</pre></div><p>
     Then run the configuration processor and ready deployment playbooks:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the reconfiguration playbook in the deployment area:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml -e@~/openstack/my_cloud/config/keystone/keystone_configure_ldap_certs_all.yml</pre></div></li></ol></div></div><div class="sect2" id="id-1.5.7.11.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.9.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="#id-1.5.7.11.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-identity_ldap.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_ldap.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 domain-specific configuration:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     No Global User Listing: Once domain-specific driver configuration is
     enabled, listing all users and listing all groups are not supported
     operations. Those calls require a specific domain filter and a
     domain-scoped token for the target domain.
    </p></li><li class="listitem "><p>
     You cannot have both a file store and a database store for domain-specific
     driver configuration in a single identity service instance. Once a
     database store is enabled within the identity service instance, any file
     store will be ignored, and vice versa.
    </p></li><li class="listitem "><p>
     The identity service allows a list limit configuration to globally set the
     maximum number of entities that will be returned in an identity collection
     per request but it does not support per-domain list limit setting at this
     time.
    </p></li><li class="listitem "><p>
     Each time a new domain is configured with LDAP integration the single CA
     file gets overwritten. Ensure that you place certs for all the LDAP
     back-end domains in the cacert parameter. Detailed CA file inclusion
     instructions are provided in the comments of the sample YAML configuration
     file <code class="filename">keystone_configure_ldap_my.yml</code>
     (<a class="xref" href="#filestore" title="5.9.2. Set up domain-specific driver configuration - file store">Section 5.9.2, “Set up domain-specific driver configuration - file store”</a>).
    </p></li><li class="listitem "><p>
     LDAP is only supported for identity operations (reading users and groups
     from LDAP).
    </p></li><li class="listitem "><p>
     keystone assignment operations from LDAP records such as managing or
     assigning roles and projects, are not currently supported.
    </p></li><li class="listitem "><p>
     The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 'default' domain is pre-configured to store service account
     users and is authenticated locally against the identity service. Domains
     configured for external LDAP integration are non-default domains.
    </p></li><li class="listitem "><p>
     When using the current OpenStackClient CLI you must use the user ID rather
     than the user name when working with a non-default domain.
    </p></li><li class="listitem "><p>
     Each LDAP connection with the identity service is for read-only
     operations. Configurations that require identity service write operations
     (to create users, groups, etc.) are not currently supported.
    </p></li><li class="listitem "><p>
     LDAP is only supported for identity operations (reading users and groups
     from LDAP). keystone assignment operations from LDAP records such as
     managing or assigning roles and projects, are not currently supported.
    </p></li><li class="listitem "><p>
     When using the current OpenStackClient CLI you must use the user ID rather
     than the user name when working with a non-default domain.
    </p></li></ul></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 API-based domain-specific configuration management
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     No GUI dashboard for domain-specific driver configuration management
    </p></li><li class="listitem "><p>
     API-based Domain specific config does not check for type of option.
    </p></li><li class="listitem "><p>
     API-based Domain specific config does not check for option values
     supported.
    </p></li><li class="listitem "><p>
     API-based Domain config method does not provide retrieval of default
     values of domain-specific configuration options.
    </p></li><li class="listitem "><p>
     Status: Domain-specific driver configuration database store is a non-core
     feature for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9.
    </p></li></ul></div><div id="id-1.5.7.11.7.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    When integrating with an external identity provider, cloud security is
    dependent upon the security of that identify provider. You should examine
    the security of the identity provider, and in particular the SAML 2.0 token
    generation process and decide what security properties you need to ensure
    adequate security of your cloud deployment. More information about SAML can
    be found at
    <a class="link" href="https://www.owasp.org/index.php/SAML_Security_Cheat_Sheet" target="_blank">https://www.owasp.org/index.php/SAML_Security_Cheat_Sheet</a>.
   </p></div></div></div><div class="sect1" id="k2kfed"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">keystone-to-keystone Federation</span> <a title="Permalink" class="permalink" href="#k2kfed">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-keystone_federation.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-keystone_federation.xml</li><li><span class="ds-label">ID: </span>k2kfed</li></ul></div></div></div></div><p>
  This topic explains how you can use one instance of keystone as an identity
  provider and one as a service provider.
 </p><div class="sect2" id="id-1.5.7.12.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">What Is Keystone-to-Keystone Federation?</span> <a title="Permalink" class="permalink" href="#id-1.5.7.12.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-keystone_federation.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-keystone_federation.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Identity federation lets you configure <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> using existing identity
   management systems such as an LDAP directory as the source of user access
   authentication. The keystone-to-keystone federation (K2K) function extends
   this concept for accessing resources in multiple, separate <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> clouds.
   You can configure each cloud to trust the authentication credentials of
   other clouds to provide the ability for users to authenticate with their
   home cloud and to access authorized resources in another cloud without
   having to reauthenticate with the remote cloud. This function is sometimes
   referred to as "single sign-on" or SSO.
  </p><p>
   The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud that provides the initial user authentication is called
   the identity provider (IdP). The identity provider cloud can support
   domain-based authentication against external authentication sources
   including LDAP-based directories such as Microsoft Active Directory. The
   identity provider creates the user attributes, known as assertions, which
   are used to automatically authenticate users with other <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> clouds.
  </p><p>
   An <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud that provides resources is called a service provider (SP).
   A service provider cloud accepts user authentication assertions from the
   identity provider and provides access to project resources based on the
   mapping file settings developed for each service provider cloud. The
   following are characteristics of a service provider:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Each service provider cloud has a unique set of projects, groups, and
     group role assignments that are created and managed locally.
    </p></li><li class="listitem "><p>
     The mapping file consists a set of rules that define user group
     membership.
    </p></li><li class="listitem "><p>
     The mapping file enables the ability to auto-assign incoming users to a
     specific group. Project membership and access are defined by group
     membership.
    </p></li><li class="listitem "><p>
     Project quotas are defined locally by each service provider cloud.
    </p></li></ul></div><p>
   keystone-to-keystone federation is supported and enabled in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9
   using configuration parameters in specific Ansible files. Instructions are
   provided to define and enable the required configurations.
  </p><p>
   Support for keystone-to-keystone federation happens on the API level, and
   you must implement it using your own client code by calling the supported
   APIs. Python-keystoneclient has supported APIs to access the K2K APIs.
  </p><div class="complex-example"><div class="example" id="ex-k2kclient"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 5.1: </span><span class="name">k2kclient.py </span><a title="Permalink" class="permalink" href="#ex-k2kclient">#</a></h6></div><div class="example-contents"><p>
    The following k2kclient.py file is an example, and the request diagram
    <a class="xref" href="#fig-keystone-authentication-flow" title="Keystone Authentication Flow">Figure 5.1, “Keystone Authentication Flow”</a> explains the flow of
    client requests.
   </p><div class="verbatim-wrap highlight python"><pre class="screen">import json
import os
import requests

import xml.dom.minidom

from keystoneclient.auth.identity import v3
from keystoneclient import session

class K2KClient(object):

    def __init__(self):
        # IdP auth URL
        self.auth_url = "http://192.168.245.9:35357/v3/"
        self.project_name = "admin"
        self.project_domain_name = "Default"
        self.username = "admin"
        self.password = "vvaQIZ1S"
        self.user_domain_name = "Default"
        self.session = requests.Session()
        self.verify = False
        # identity provider Id
        self.idp_id = "z420_idp"
        # service provider Id
        self.sp_id = "z620_sp"
        #self.sp_ecp_url = "https://16.103.149.44:8443/Shibboleth.sso/SAML2/ECP"
        #self.sp_auth_url = "https://16.103.149.44:8443/v3"

    def v3_authenticate(self):
        auth = v3.Password(auth_url=self.auth_url,
                           username=self.username,
                           password=self.password,
                           user_domain_name=self.user_domain_name,
                           project_name=self.project_name,
                           project_domain_name=self.project_domain_name)

        self.auth_session = session.Session(session=requests.session(),
                                       auth=auth, verify=self.verify)
        auth_ref = self.auth_session.auth.get_auth_ref(self.auth_session)
        self.token = self.auth_session.auth.get_token(self.auth_session)

    def _generate_token_json(self):
        return {
            "auth": {
                "identity": {
                    "methods": [
                        "token"
                    ],
                    "token": {
                        "id": self.token
                    }
                },
                "scope": {
                    "service_provider": {
                        "id": self.sp_id
                    }
                }
            }
        }

    def get_saml2_ecp_assertion(self):
        token = json.dumps(self._generate_token_json())
        url = self.auth_url + 'auth/OS-FEDERATION/saml2/ecp'
        r = self.session.post(url=url,
                              data=token,
                              verify=self.verify)
        if not r.ok:
            raise Exception("Something went wrong, %s" % r.__dict__)
        self.ecp_assertion = r.text

    def _get_sp_url(self):
        url = self.auth_url + 'OS-FEDERATION/service_providers/' + self.sp_id
        r = self.auth_session.get(
           url=url,
           verify=self.verify)
        if not r.ok:
            raise Exception("Something went wrong, %s" % r.__dict__)

        sp = json.loads(r.text)[u'service_provider']
        self.sp_ecp_url = sp[u'sp_url']
        self.sp_auth_url = sp[u'auth_url']

    def _handle_http_302_ecp_redirect(self, response, method, **kwargs):
        location = self.sp_auth_url + '/OS-FEDERATION/identity_providers/' + self.idp_id + '/protocols/saml2/auth'
        return self.auth_session.request(location, method, authenticated=False, **kwargs)

    def exchange_assertion(self):
        """Send assertion to a keystone SP and get token."""
        self._get_sp_url()
        print("SP ECP Url:%s" % self.sp_ecp_url)
        print("SP Auth Url:%s" % self.sp_auth_url)
        #self.sp_ecp_url = 'https://16.103.149.44:8443/Shibboleth.sso/SAML2/ECP'
        r = self.auth_session.post(
            self.sp_ecp_url,
            headers={'Content-Type': 'application/vnd.paos+xml'},
            data=self.ecp_assertion,
            authenticated=False, redirect=False)
        r = self._handle_http_302_ecp_redirect(r, 'GET',
            headers={'Content-Type': 'application/vnd.paos+xml'})
        self.fed_token_id = r.headers['X-Subject-Token']
        self.fed_token = r.text

if __name__ == "__main__":
    client = K2KClient()
    client.v3_authenticate()
    client.get_saml2_ecp_assertion()
    client.exchange_assertion()
    print('Unscoped token_id: %s' % client.fed_token_id)
    print('Unscoped token body:
%s' % client.fed_token)</pre></div></div></div></div></div><div class="sect2" id="id-1.5.7.12.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.10.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting Up a keystone Provider</span> <a title="Permalink" class="permalink" href="#id-1.5.7.12.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-keystone_federation.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-keystone_federation.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To set up keystone as a service provider, follow these steps.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Create a config file called <code class="literal">k2k.yml</code> with the following
     parameters and place it in any directory on your Cloud Lifecycle Manager, such
     as /tmp.
    </p><div class="verbatim-wrap"><pre class="screen">keystone_trusted_idp: k2k
keystone_sp_conf:
  shib_sso_idp_entity_id: &lt;protocol&gt;://&lt;idp_host&gt;:&lt;port&gt;/v3/OS-FEDERATION/saml2/idp
  shib_sso_application_entity_id: http://service_provider_uri_entityId
  target_domain:
    name: domain1
    description: my domain
  target_project:
    name: project1
    description: my project
  target_group:
    name: group1
    description: my group
  role:
    name: service
  idp_metadata_file: /tmp/idp_metadata.xml
  identity_provider:
    id: my_idp_id
    description: This is the identity service provider.
  mapping:
    id: mapping1
    <span class="bold"><strong>rules_file: /tmp/k2k_sp_mapping.json</strong></span>
  protocol:
    id: saml2
  attribute_map:
    -
      name: name1
      id: id1</pre></div><p>
     The following are descriptions of each of the attributes.
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th> Attribute</th><th>Definition</th></tr></thead><tbody><tr><td>keystone_trusted_idp</td><td>
         <p>
          A flag to indicate if this configuration is used for
          keystone-to-keystone or WebSSO. The value can be either k2k or adfs.
         </p>
        </td></tr><tr><td><span class="bold"><strong>keystone_sp_conf</strong></span>
        </td><td> </td></tr><tr><td>shib_sso_idp_entity_id</td><td>
         <p>
          The identity provider URI used as an entity Id to identity the IdP.
          You shoud use the following value:
          &lt;protocol&gt;://&lt;idp_host&gt;:&lt;port&gt;/v3/OS-FEDERATION/saml2/idp.
         </p>
        </td></tr><tr><td>shib_sso_application_entity_id</td><td>
         <p>
          The service provider URI used as an entity Id. It can be any URI here
          for keystone-to-keystone.
         </p>
        </td></tr><tr><td>target_domain</td><td>
         <p>
          A domain where the group will be created.
         </p>
        </td></tr><tr><td>name</td><td>
         <p>
          Any domain name. If it does not exist, it will be created or updated.
         </p>
        </td></tr><tr><td>description</td><td>
         <p>
          Any description.
         </p>
        </td></tr><tr><td>target_project</td><td>
         <p>
          A project scope of the group.
         </p>
        </td></tr><tr><td>name</td><td>
         <p>
          Any project name. If it does not exist, it will be created or
          updated.
         </p>
        </td></tr><tr><td>description</td><td>Any description. </td></tr><tr><td>target_group</td><td>
         <p>
          A group will be created from target_domain.
         </p>
        </td></tr><tr><td>name</td><td>
         <p>
          Any group name. If it does not exist, it will be created or updated.
         </p>
        </td></tr><tr><td>description</td><td>Any description. </td></tr><tr><td>role</td><td>
         <p>
          A role will be assigned on target_project. This role impacts the IdP
          user scoped token permission on the service provider side.
         </p>
        </td></tr><tr><td>name</td><td>Must be an existing role. </td></tr><tr><td>idp_metadata_file</td><td>
         <p>
          A reference to the IdP metadata file that validates the SAML2
          assertion.
         </p>
        </td></tr><tr><td>identity_provider</td><td>A supported IdP.</td></tr><tr><td>id</td><td>
         <p>
          Any Id. If it does not exist, it will be created or updated. This Id
          needs to be shared with the client so that the right mapping will be
          selected.
         </p>
        </td></tr><tr><td>description</td><td>Any description.</td></tr><tr><td>mapping</td><td>
         <p>
          A mapping in JSON format that maps a federated user to a
          corresponding group.
         </p>
        </td></tr><tr><td>id</td><td>
         <p>
          Any Id. If it does not exist, it will be created or updated.
         </p>
        </td></tr><tr><td>rules_file</td><td>
         <p>
          A reference to the file that has the mapping in JSON.
         </p>
        </td></tr><tr><td>protocol</td><td>
         <p>
          The supported federation protocol.
         </p>
        </td></tr><tr><td>id</td><td>
         <p>
          Security Assertion Markup Language 2.0 (SAML2) is the only supported
          protocol for K2K.
         </p>
        </td></tr><tr><td>attribute_map</td><td>
         <p>
          A shibboleth mapping that defines additional attributes to map the
          attributes from the SAML2 assertion to the K2K mapping that the
          service provider understands. K2K does not require any additional
          attribute mapping.
         </p>
        </td></tr><tr><td>name</td><td>An attribute name from the SAML2 assertion.</td></tr><tr><td>id</td><td>An Id that the preceding name will be mapped to.</td></tr></tbody></table></div></li><li class="listitem "><p>
     Create a metadata file that is referenced from <code class="literal">k2k.yml</code>,
     such as <code class="literal">/tmp/idp_metadata.xml</code>. The content of the
     metadata file comes from the identity provider and can be found in
     <code class="literal"> /etc/keystone/idp_metadata.xml</code>.
    </p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       Create a mapping file that is referenced in k2k.yml, shown previously.
       An example is <code class="literal">/tmp/k2k_sp_mapping.json</code>. You can see
       the reference in bold in the preceding k2k.yml example. The following is
       an example of the mapping file.
      </p><div class="verbatim-wrap"><pre class="screen">[
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group1",
           "domain":{
             "name": "domain1"
           }
        }
      }
    ],
    "remote":[{
      "type": "openstack_user"
    },
    {
      "type": "Shib-Identity-Provider",
      "any_one_of":[
         "https://idp_host:5000/v3/OS-FEDERATION/saml2/idp"
      ]
     }
    ]
   }
]</pre></div><p>
       You can find more information on how the K2K mapping works at
       <a class="link" href="http://docs.openstack.org" target="_blank">http://docs.openstack.org</a>.
      </p></li></ol></div></li><li class="listitem "><p>
     Go to <code class="literal">~/stack/scratch/ansible/next/ardana/ansible</code> and
     run the following playbook to enable the service provider:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml -e@/tmp/k2k.yml</pre></div></li></ol></div><p>
   <span class="bold"><strong><span class="bold"><strong>Setting Up an Identity
   Provider</strong></span></strong></span>
  </p><p>
   To set up keystone as an identity provider, follow these steps:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Create a config file <code class="literal">k2k.yml</code> with the following
     parameters and place it in any directory on your Cloud Lifecycle Manager, such
     as <code class="literal">/tmp</code>. Note that the certificate and key here are
     excerpted for space.
    </p><div class="verbatim-wrap"><pre class="screen">keystone_k2k_idp_conf:
    service_provider:
          -
            id: my_sp_id
            description: This is service provider.
            sp_url: https://sp_host:5000
            auth_url: https://sp_host:5000/v3
    signer_cert: -----BEGIN CERTIFICATE-----
MIIDmDCCAoACCQDS+ZDoUfr
    cIzANBgkqhkiG9w0BAQsFADCBjDELMAkGA1UEBhMC\ nVVMxEzARBgNVB
    AgMCkNhbGlmb3JuaWExEjAQBgNVBAcMCVN1bm55dmFsZTEMMAoG\
   
            ...
    nOpKEvhlMsl5I/tle
-----END CERTIFICATE-----
    signer_key: -----BEGIN RSA PRIVATE KEY-----
MIIEowIBAAKCAQEA1gRiHiwSO6L5PrtroHi/f17DQBOpJ1KMnS9FOHS
            
            ...</pre></div><p>
     The following are descriptions of each of the attributes under
     keystone_k2k_idp_conf
    </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.7.12.4.6.1.4.1"><span class="term ">service_provider</span></dt><dd><p>
        One or more service providers can be defined. If it does not exist, it
        will be created or updated.
       </p></dd><dt id="id-1.5.7.12.4.6.1.4.2"><span class="term ">id</span></dt><dd><p>
        Any Id. If it does not exist, it will be created or updated. This Id
        needs to be shared with the client so that it knows where the service
        provider is.
       </p></dd><dt id="id-1.5.7.12.4.6.1.4.3"><span class="term ">description</span></dt><dd><p>
        Any description.
       </p></dd><dt id="id-1.5.7.12.4.6.1.4.4"><span class="term ">sp_url</span></dt><dd><p>
        Service provider base URL.
       </p></dd><dt id="id-1.5.7.12.4.6.1.4.5"><span class="term ">auth_url</span></dt><dd><p>
        Service provider auth URL.
       </p></dd><dt id="id-1.5.7.12.4.6.1.4.6"><span class="term ">signer_cert</span></dt><dd><p>
        Content of self-signed certificate that is embedded in the metadata
        file. We recommend setting the validity for a longer period of time,
        such as 3650 days (10 years).
       </p></dd><dt id="id-1.5.7.12.4.6.1.4.7"><span class="term ">signer_key</span></dt><dd><p>
        A private key that has a key size of 2048 bits.
       </p></dd></dl></div></li><li class="listitem "><p>
     Create a private key and a self-signed certificate. The command-line tool,
     openssl, is required to generate the keys and certificates. If the system
     does not have it, you must install it.
    </p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       Create a private key of size 2048.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openssl genrsa -out myidp.key 2048</pre></div></li><li class="listitem "><p>
       Generate a certificate request named myidp.csr. When prompted, choose
       CommonName for the server's hostname.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openssl req -new -key myidp.key -out myidp.csr</pre></div></li><li class="listitem "><p>
       Generate a self-signed certificate named myidp.cer.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openssl x509 -req -days 3650 -in myidp.csr -signkey myidp.key -out myidp.cer</pre></div></li></ol></div></li><li class="listitem "><p>
     Go to <code class="literal">~/scratch/ansible/next/ardana/ansible</code> and
     run the following playbook to enable the service provider in keystone:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml -e@/tmp/k2k.yml</pre></div></li></ol></div></div><div class="sect2" id="id-1.5.7.12.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.10.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Test It Out</span> <a title="Permalink" class="permalink" href="#id-1.5.7.12.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-keystone_federation.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-keystone_federation.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   You can use the script listed earlier, <code class="literal">k2kclient.py</code>
   (<a class="xref" href="#ex-k2kclient" title="k2kclient.py">Example 5.1, “k2kclient.py”</a>), as an example for the end-to-end flows. To run
   <code class="literal">k2kclient.py</code>, follow these steps:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     A few parameters must be changed in the beginning of
     <code class="literal">k2kclient.py</code>. For example, enter your specific URL,
     project name, and user name, as follows:
    </p><div class="verbatim-wrap"><pre class="screen"># IdP auth URL
self.auth_url = "http://idp_host:5000/v3/"
self.project_name = "my_project_name"
self.project_domain_name = "my_project_domain_name"
self.username = "test"
self.password = "mypass"
self.user_domain_name = "my_domain"
# identity provider Id that is defined in the SP config
self.idp_id = "my_idp_id"
# service provider Id that is defined in the IdP config
self.sp_id = "my_sp_id"</pre></div></li><li class="listitem "><p>
     Install python-keystoneclient along with its dependencies.
    </p></li><li class="listitem "><p>
     Run the <code class="literal">k2kclient.py</code> script. An unscoped token will be
     returned from the service provider.
    </p></li></ol></div><p>
   At this point, the domain or project scope of the unscoped taken can be
   discovered by sending the following URLs:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>curl -k -X GET -H "X-Auth-Token: <em class="replaceable ">unscoped token</em>" \
 https://&lt;sp_public_endpoint&gt;:5000/v3/OS-FEDERATION/domains
<code class="prompt user">ardana &gt; </code>curl -k -X GET -H "X-Auth-Token: <em class="replaceable ">unscoped token</em>" \
 https://&lt;sp_public_endpoint:5000/v3/OS-FEDERATION/projects</pre></div></div><div class="sect2" id="id-1.5.7.12.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.10.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Inside keystone-to-keystone Federation</span> <a title="Permalink" class="permalink" href="#id-1.5.7.12.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-keystone_federation.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-keystone_federation.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   K2K federation places a lot of responsibility with the user. The complexity
   is apparent from the following diagram.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Users must first authenticate to their home or local cloud, or local
     identity provider keystone instance to obtain a scoped token.
    </p></li><li class="listitem "><p>
     Users must discover which service providers (or remote clouds) are
     available to them by querying their local cloud.
    </p></li><li class="listitem "><p>
     For a given remote cloud, users must discover which resources are
     available to them by querying the remote cloud for the projects they can
     scope to.
    </p></li><li class="listitem "><p>
     To talk to the remote cloud, users must first exchange, with the local
     cloud, their locally scoped token for a SAML2 assertion to present to the
     remote cloud.
    </p></li><li class="listitem "><p>
     Users then present the SAML2 assertion to the remote cloud. The remote
     cloud applies its mapping for the incoming SAML2 assertion to map each
     user to a local ephemeral persona (such as groups) and issues an unscoped
     token.
    </p></li><li class="listitem "><p>
     Users query the remote cloud for the list of projects they have access to.
    </p></li><li class="listitem "><p>
     Users then rescope their token to a given project.
    </p></li><li class="listitem "><p>
     Users now have access to the resources owned by the project.
    </p></li></ol></div><p>
   The following diagram illustrates the flow of authentication requests.
  </p><div class="figure" id="fig-keystone-authentication-flow"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/media-keystone-Keystone-2-Keystone-Sequence-Generic.png" target="_blank"><img src="images/media-keystone-Keystone-2-Keystone-Sequence-Generic.png" width="" alt="Keystone Authentication Flow" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 5.1: </span><span class="name">Keystone Authentication Flow </span><a title="Permalink" class="permalink" href="#fig-keystone-authentication-flow">#</a></h6></div></div></div><div class="sect2" id="id-1.5.7.12.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.10.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Additional Testing Scenarios</span> <a title="Permalink" class="permalink" href="#id-1.5.7.12.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-keystone_federation.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-keystone_federation.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following tests assume one identity provider and one service provider.
  </p><p>
   <span class="bold"><strong>Test Case 1: Any federated user in the identity
   provider maps to a single designated group in the service
   provider</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     On the identity provider side:
    </p><div class="verbatim-wrap"><pre class="screen">hostname=myidp.com
username=user1</pre></div></li><li class="listitem "><p>
     On the service provider side:
    </p><div class="verbatim-wrap"><pre class="screen">group=group1
group_domain_name=domain1
'group1' scopes to 'project1'</pre></div></li><li class="listitem "><p>
     Mapping used:
    </p><div class="verbatim-wrap"><pre class="screen">testcase1_1.json</pre></div><p>
     testcase1_1.json
    </p><div class="verbatim-wrap"><pre class="screen">[
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group1",
           "domain":{
             "name": "domain1"
           }
        }
      }
    ],
    "remote":[{
      "type": "openstack_user"
    },
    {
      "type": "Shib-Identity-Provider",
      "any_one_of":[
         "https://myidp.com:5000/v3/OS-FEDERATION/saml2/idp"
      ]
     }
    ]
   }
]</pre></div></li><li class="listitem "><p>
     Expected result: The federated user will scope to project1.
    </p></li></ol></div><p>
   <span class="bold"><strong>Test Case 2: A federated user in a specific domain in
   the identity provider maps to two different groups in the service
   provider</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     On the identity provider side:
    </p><div class="verbatim-wrap"><pre class="screen">hostname=myidp.com
username=user1
user_domain_name=Default</pre></div></li><li class="listitem "><p>
     On the service provider side:
    </p><div class="verbatim-wrap"><pre class="screen">group=group1
group_domain_name=domain1
'group1' scopes to 'project1' group=group2
group_domain_name=domain2
'group2' scopes to 'project2'</pre></div></li><li class="listitem "><p>
     Mapping used:
    </p><div class="verbatim-wrap"><pre class="screen">testcase1_2.json</pre></div><p>
     testcase1_2.json
    </p><div class="verbatim-wrap"><pre class="screen">[
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group1",
           "domain":{
             "name": "domain1"
           }
        }
      }
    ],
    "remote":[{
      "type": "openstack_user"
    },
    {
      "type": "Shib-Identity-Provider",
      "any_one_of":[
         "https://myidp.com:5000/v3/OS-FEDERATION/saml2/idp"
      ]
     }
    ]
   }
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group2",
           "domain":{
             "name": "domain2"
           }
        }
      }
    ],
    "remote":[{
      "type": "openstack_user"
    },
    {
      "type": "openstack_user_domain",
      "any_one_of": [
          "Default"
      ]
    },
    {
      "type": "Shib-Identity-Provider",
      "any_one_of":[
         "https://myidp.com:5000/v3/OS-FEDERATION/saml2/idp"
      ]
     }
    ]
   }
]</pre></div></li><li class="listitem "><p>
     Expected result: The federated user will scope to both project1 and
     project2.
    </p></li></ol></div><p>
   <span class="bold"><strong>Test Case 3: A federated user with a specific project
   in the identity provider maps to a specific group in the service
   provider</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     On the identity provider side:
    </p><div class="verbatim-wrap"><pre class="screen">hostname=myidp.com
username=user4
user_project_name=test1</pre></div></li><li class="listitem "><p>
     On the service provider side:
    </p><div class="verbatim-wrap"><pre class="screen">group=group4
group_domain_name=domain4
'group4' scopes to 'project4'</pre></div></li><li class="listitem "><p>
     Mapping used:
    </p><div class="verbatim-wrap"><pre class="screen">testcase1_3.json</pre></div><p>
     testcase1_3.json
    </p><div class="verbatim-wrap"><pre class="screen">[
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group4",
           "domain":{
             "name": "domain4"
           }
        }
      }
    ],
    "remote":[{
      "type": "openstack_user"
    },
    {
      "type": "openstack_project",
      "any_one_of": [
          "test1"
      ]
    },
    {
      "type": "Shib-Identity-Provider",
      "any_one_of":[
         "https://myidp.com:5000/v3/OS-FEDERATION/saml2/idp"
      ]
     }
    ]
   },
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group5",
           "domain":{
             "name": "domain5"
           }
        }
      }
    ],
    "remote":[{
      "type": "openstack_user"
    },
    {
      "type": "openstack_roles",
      "not_any_of": [
          "member"
      ]
    },
    {
      "type": "Shib-Identity-Provider",
      "any_one_of":[
         "https://myidp.com:5000/v3/OS-FEDERATION/saml2/idp"
      ]
     }
    ]
   }
]</pre></div></li><li class="listitem "><p>
     Expected result: The federated user will scope to project4.
    </p></li></ol></div><p>
   <span class="bold"><strong>Test Case 4: A federated user with a specific role in
   the identity provider maps to a specific group in the service
   provider</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     On the identity provider side:
    </p><div class="verbatim-wrap"><pre class="screen">hostname=myidp.com, username=user5, role_name=member</pre></div></li><li class="listitem "><p>
     On the service provider side:
    </p><div class="verbatim-wrap"><pre class="screen">group=group5, group_domain_name=domain5, 'group5' scopes to 'project5'</pre></div></li><li class="listitem "><p>
     Mapping used:
    </p><div class="verbatim-wrap"><pre class="screen">testcase1_3.json</pre></div><p>
     testcase1_3.json
    </p><div class="verbatim-wrap"><pre class="screen">[
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group4",
           "domain":{
             "name": "domain4"
           }
        }
      }
    ],
    "remote":[{
      "type": "openstack_user"
    },
    {
      "type": "openstack_project",
      "any_one_of": [
          "test1"
      ]
    },
    {
      "type": "Shib-Identity-Provider",
      "any_one_of":[
         "https://myidp.com:5000/v3/OS-FEDERATION/saml2/idp"
      ]
     }
    ]
   },
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group5",
           "domain":{
             "name": "domain5"
           }
        }
      }
    ],
    "remote":[{
      "type": "openstack_user"
    },
    {
      "type": "openstack_roles",
      "not_any_of": [
          "member"
      ]
    },
    {
      "type": "Shib-Identity-Provider",
      "any_one_of":[
         "https://myidp.com:5000/v3/OS-FEDERATION/saml2/idp"
      ]
     }
    ]
   }
]</pre></div></li><li class="listitem "><p>
     Expected result: The federated user will scope to project5.
    </p></li></ol></div><p>
   <span class="bold"><strong>Test Case 5: Retain the previous scope for a federated
   user</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     On the identity provider side:
    </p><div class="verbatim-wrap"><pre class="screen">hostname=myidp.com, username=user1, user_domain_name=Default</pre></div></li><li class="listitem "><p>
     On the service provider side:
    </p><div class="verbatim-wrap"><pre class="screen">group=group1, group_domain_name=domain1, 'group1' scopes to 'project1'</pre></div></li><li class="listitem "><p>
     Mapping used:
    </p><div class="verbatim-wrap"><pre class="screen">testcase1_1.json</pre></div><p>
     testcase1_1.json
    </p><div class="verbatim-wrap"><pre class="screen">[
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group1",
           "domain":{
             "name": "domain1"
           }
        }
      }
    ],
    "remote":[{
      "type": "openstack_user"
    },
    {
      "type": "Shib-Identity-Provider",
      "any_one_of":[
         "https://myidp.com:5000/v3/OS-FEDERATION/saml2/idp"
      ]
     }
    ]
   }
]</pre></div></li><li class="listitem "><p>
     Expected result: The federated user will scope to project1. Later, we
     would like to scope federated users who have the default domain in the
     identity provider to project2 in addition to project1.
    </p></li><li class="listitem "><p>
     On the identity provider side:
    </p><div class="verbatim-wrap"><pre class="screen">hostname=myidp.com, username=user1, user_domain_name=Default</pre></div></li><li class="listitem "><p>
     On the service provider side:
    </p><div class="verbatim-wrap"><pre class="screen">group=group1
group_domain_name=domain1
'group1' scopes to 'project1' group=group2
group_domain_name=domain2
'group2' scopes to 'project2'</pre></div></li><li class="listitem "><p>
     Mapping used:
    </p><div class="verbatim-wrap"><pre class="screen">testcase1_2.json</pre></div><p>
     testcase1_2.json
    </p><div class="verbatim-wrap"><pre class="screen">[
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group1",
           "domain":{
             "name": "domain1"
           }
        }
      }
    ],
    "remote":[{
      "type": "openstack_user"
    },
    {
      "type": "Shib-Identity-Provider",
      "any_one_of":[
         "https://myidp.com:5000/v3/OS-FEDERATION/saml2/idp"
      ]
     }
    ]
   }
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group2",
           "domain":{
             "name": "domain2"
           }
        }
      }
    ],
    "remote":[{
      "type": "openstack_user"
    },
    {
      "type": "openstack_user_domain",
      "any_one_of": [
          "Default"
      ]
    },
    {
      "type": "Shib-Identity-Provider",
      "any_one_of":[
         "https://myidp.com:5000/v3/OS-FEDERATION/saml2/idp"
      ]
     }
    ]
   }
]</pre></div></li><li class="listitem "><p>
     Expected result: The federated user will scope to project1 and project2.
    </p></li></ol></div><p>
   <span class="bold"><strong>Test Case 6: Scope a federated user to a domain
   </strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     On the identity provider side:
    </p><div class="verbatim-wrap"><pre class="screen">hostname=myidp.com, username=user1</pre></div></li><li class="listitem "><p>
     On the service provider side:
    </p><div class="verbatim-wrap"><pre class="screen">group=group1, group_domain_name=domain1, 'group1' scopes to 'project1'</pre></div></li><li class="listitem "><p>
     Mapping used:
    </p><div class="verbatim-wrap"><pre class="screen">testcase1_1.json</pre></div><p>
     testcase1_1.json
    </p><div class="verbatim-wrap"><pre class="screen">[
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group1",
           "domain":{
             "name": "domain1"
           }
        }
      }
    ],
    "remote":[{
      "type": "openstack_user"
    },
    {
      "type": "Shib-Identity-Provider",
      "any_one_of":[
         "https://myidp.com:5000/v3/OS-FEDERATION/saml2/idp"
      ]
     }
    ]
   }
]</pre></div></li><li class="listitem "><p>
     Expected result:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       The federated user will scope to project1.
      </p></li><li class="listitem "><p>
       User uses CLI/Curl to assign any existing role to group1 on domain1.
      </p></li><li class="listitem "><p>
       User uses CLI/Curl to remove project1 scope from group1.
      </p></li></ul></div></li><li class="listitem "><p>
     Final result: The federated user will scope to domain1.
    </p></li></ol></div><p>
   <span class="bold"><strong>Test Case 7: Test five remote attributes for mapping
   </strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Test all five different remote attributes, as follows, with similar test
     cases as noted previously.
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       openstack_user
      </p></li><li class="listitem "><p>
       openstack_user_domain
      </p></li><li class="listitem "><p>
       openstack_roles
      </p></li><li class="listitem "><p>
       openstack_project
      </p></li><li class="listitem "><p>
       openstack_project_domain
      </p></li></ul></div><p>
     The attribute openstack_user does not make much sense for testing because
     it is mapped only to a specific username. The preceding test cases have
     already covered the attributes openstack_user_domain, openstack_roles, and
     openstack_project.
    </p></li></ol></div><p>
    Note that similar tests have also been run for two identity providers with
    one service provider, and for one identity provider with two service
    providers.
  </p></div><div class="sect2" id="id-1.5.7.12.8"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.10.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Known Issues and Limitations</span> <a title="Permalink" class="permalink" href="#id-1.5.7.12.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-keystone_federation.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-keystone_federation.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Keep the following points in mind:
  </p><div class="itemizedlist " id="idg-all-operations-configuring-identity-keystone-federation-xml-14"><ul class="itemizedlist"><li class="listitem "><p>
     When a user is disabled in the identity provider, the issued federated
     token from the service provider still remains valid until the token is
     expired based on the keystone expiration setting.
    </p></li><li class="listitem "><p>
     An already issued federated token will retain its scope until its
     expiration. Any changes in the mapping on the service provider will not
     impact the scope of an already issued federated token. For example, if an
     already issued federated token was mapped to group1 that has scope on
     project1, and mapping is changed to group2 that has scope on project2, the
     prevously issued federated token still has scope on project1.
    </p></li><li class="listitem "><p>
     Access to service provider resources is provided only through the
     python-keystone CLI client or the keystone API. No horizon web interface
     support is currently available.
    </p></li><li class="listitem "><p>
     Domains, projects, groups, roles, and quotas are created per the service
     provider cloud. Support for federated projects, groups, roles, and quotas
     is currently not available.
    </p></li><li class="listitem "><p>
     keystone-to-keystone federation and WebSSO cannot be configured by putting
     both sets of configuration attributes in the same config file; they will
     overwrite each other. Consequently, they need to be configured
     individually.
    </p></li><li class="listitem "><p>
     Scoping the federated user to a domain is not supported by default in the
     playbook. Please follow the steps at <a class="xref" href="#scopeToDomain" title="5.10.7. Scope Federated User to Domain">Section 5.10.7, “Scope Federated User to Domain”</a>.
    </p></li></ul></div></div><div class="sect2" id="scopeToDomain"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.10.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Scope Federated User to Domain</span> <a title="Permalink" class="permalink" href="#scopeToDomain">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-keystone_federation.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-keystone_federation.xml</li><li><span class="ds-label">ID: </span>scopeToDomain</li></ul></div></div></div></div><p>
Use the following steps to scope a federated user to a domain:
</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     On the IdP side, set <code class="literal">hostname=myidp.com</code> and
     <code class="literal">username=user1</code>.
    </p></li><li class="listitem "><p>
     On the service provider side, set: <code class="literal">group=group1</code>,
     <code class="literal">group_domain_name=domain1</code>, group1 scopes to project1.
    </p></li><li class="listitem "><p>
     Mapping used: testcase1_1.json.
    </p><p>
     testcase1_1.json
    </p><div class="verbatim-wrap"><pre class="screen">[
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group1",
           "domain":{
             "name": "domain1"
           }
        }
      }
    ],
    "remote":[{
      "type": "openstack_user"
    },
    {
      "type": "Shib-Identity-Provider",
      "any_one_of":[
         "https://myidp.com:5000/v3/OS-FEDERATION/saml2/idp"
      ]
     }
    ]
   }
]</pre></div></li><li class="listitem "><p>
     Expected result: The federated user will scope to project1. Use CLI/Curl
     to assign any existing role to group1 on domain1. Use CLI/Curl to remove
     project1 scope from group1.
    </p></li><li class="listitem "><p>
     Result: The federated user will scope to domain1.
    </p></li></ol></div></div></div><div class="sect1" id="websso"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Web Single Sign-On</span> <a title="Permalink" class="permalink" href="#websso">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-websso.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-websso.xml</li><li><span class="ds-label">ID: </span>websso</li></ul></div></div></div></div><div id="id-1.5.7.13.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   The external-name in ~/openstack/my_cloud/definition/data/network_groups.yml must be set to a valid DNS-resolvable FQDN. 
   </p></div><p>
  This topic explains how to implement web single sign-on.
 </p><div class="sect2" id="id-1.5.7.13.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.11.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">What is WebSSO?</span> <a title="Permalink" class="permalink" href="#id-1.5.7.13.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-websso.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-websso.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   WebSSO, or web single sign-on, is a method for web browsers to receive
   current authentication information from an identity provider system without
   requiring a user to log in again to the application displayed by the
   browser. Users initially access the identity provider web page and supply
   their credentials. If the user successfully authenticates with the identity
   provider, the authentication credentials are then stored in the user’s web
   browser and automatically provided to all web-based applications, such as
   the horizon dashboard in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9. If users have not yet
   authenticated with an identity provider or their credentials have timed out,
   they are automatically redirected to the identity provider to renew their
   credentials.
  </p></div><div class="sect2" id="id-1.5.7.13.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.11.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="#id-1.5.7.13.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-websso.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-websso.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist " id="idg-all-operations-configuring-identity-websso-xml-6"><ul class="itemizedlist"><li class="listitem "><p>
     The WebSSO function supports only horizon web authentication. It is not
     supported for direct API or CLI access.
    </p></li><li class="listitem "><p>
     WebSSO works only with Fernet token provider. See <a class="xref" href="#fernet-tokens" title="5.8.4. Fernet Tokens">Section 5.8.4, “Fernet Tokens”</a>.
    </p></li><li class="listitem "><p>
     The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> WebSSO function was tested with Microsoft Active Directory
     Federation Services (AD FS). The instructions provided are pertinent to
     ADFS and are intended to provide a sample configuration for deploying
     WebSSO with an external identity provider. If you have a different
     identity provider such as Ping Identity or IBM Tivoli, consult with those
     vendors for specific instructions for those products.
    </p></li><li class="listitem "><p>
     The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> WebSSO function with OpenID method was tested with
     Google OAuth 2.0 APIs, which conform to the OpenID Connect specification.
     The interaction between Keystone and the external Identity Provider (IdP) is
     handled by the Apache2 <a class="link" href="https://github.com/zmartzone/mod_auth_openidc" target="_blank">auth_openidc</a> module. Please consult with the specific OpenID Connect vendor
     on whether they support <code class="literal">auth_openidc</code>.
    </p></li><li class="listitem "><p>
     Both SAML and OpenID methods are supported for WebSSO federation in
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 .
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>WebSSO has a change password option in User
     Settings, but note that this function is not accessible for users
     authenticating with external systems such as LDAP or SAML Identity
     Providers.</strong></span>
    </p></li></ul></div></div><div class="sect2" id="id-1.5.7.13.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.11.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling WebSSO</span> <a title="Permalink" class="permalink" href="#id-1.5.7.13.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-websso.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-websso.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 provides WebSSO support for the horizon web interface.
   This support requires several configuration steps including editing the
   horizon configuration file as well as ensuring that the correct keystone
   authentication configuration is enabled to receive the authentication
   assertions provided by the identity provider.
  </p><p>
   WebSSO support both SAML and OpenID methods.
   The following workflow depicts how horizon and keystone support
   WebSSO via SAML method if no current authentication assertion is available.
  </p><div class="orderedlist " id="ul-fcj-z4g-4v"><ol class="orderedlist" type="1"><li class="listitem "><p>
     horizon redirects the web browser to the keystone endpoint.
    </p></li><li class="listitem "><p>
     keystone automatically redirects the web browser to the correct identity
     provider authentication web page based on the keystone configuration file.
    </p></li><li class="listitem "><p>
     The user authenticates with the identity provider.
    </p></li><li class="listitem "><p>
     The identity provider automatically redirects the web browser back to the
     keystone endpoint.
    </p></li><li class="listitem "><p>
     keystone generates the required Javascript code to POST a token back to
     horizon.
    </p></li><li class="listitem "><p>
     keystone automatically redirects the web browser back to horizon and the
     user can then access projects and resources assigned to the user.
    </p></li></ol></div><p>
   The following diagram provides more details on the WebSSO authentication
   workflow.
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/Keystone-ADFS-WebSSO-Authentication-Sequence.png" target="_blank"><img src="images/Keystone-ADFS-WebSSO-Authentication-Sequence.png" width="" /></a></div></div><p>
   Note that the horizon dashboard service never talks directly to the keystone
   identity service until the end of the sequence, after the federated unscoped
   token negotiation has completed. The browser interacts with the horizon
   dashboard service, the keystone identity service, and ADFS on their
   respective public endpoints.
  </p><p>
   The following sequence of events is depicted in the diagram.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     The user's browser reaches the horizon dashboard service's login page. The
     user selects ADFS login from the drop-down menu.
    </p></li><li class="listitem "><p>
     The horizon dashboard service issues an HTTP Redirect (301) to redirect
     the browser to the keystone identity service's (public) SAML2 Web SSO
     endpoint (/auth/OS-FEDERATION/websso/saml2). The endpoint is protected by
     Apache mod_shib (shibboleth).
    </p></li><li class="listitem "><p>
     The browser talks to the keystone identity service. Because the user's
     browser does not have an active session with AD FS, the keystone identity
     service issues an HTTP Redirect (301) to the browser, along with the
     required SAML2 request, to the ADFS endpoint.
    </p></li><li class="listitem "><p>
     The browser talks to AD FS. ADFS returns a login form. The browser presents
     it to the user.
    </p></li><li class="listitem "><p>
     The user enters credentials (such as username and password) and submits
     the form to AD FS.
    </p></li><li class="listitem "><p>
     Upon successful validation of the user's credentials, ADFS issues an HTTP
     Redirect (301) to the browser, along with the SAML2 assertion, to the
     keystone identity service's (public) SAML2 endpoint
     (/auth/OS-FEDERATION/websso/saml2).
    </p></li><li class="listitem "><p>
     The browser talks to the keystone identity service. the keystone identity
     service validates the SAML2 assertion and issues a federated unscoped
     token. the keystone identity service returns JavaScript code to be
     executed by the browser, along with the federated unscoped token in the
     headers.
    </p></li><li class="listitem "><p>
     Upon execution of the JavaScript code, the browser is redirected to the
     horizon dashboard service with the federated unscoped token in the header.
    </p></li><li class="listitem "><p>
     The browser talks to the horizon dashboard service with the federated
     unscoped token.
    </p></li><li class="listitem "><p>
     With the unscoped token, the horizon dashboard service talks to the
     keystone identity service's (internal) endpoint to get a list of projects
     the user has access to.
    </p></li><li class="listitem "><p>
     The horizon dashboard service rescopes the token to the first project in
     the list. At this point, the user is successfully logged in.
    </p></li></ol></div><p>
   The sequence of events for WebSSO using OpenID method is similar to SAML method.
  </p></div><div class="sect2" id="id-1.5.7.13.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.11.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#id-1.5.7.13.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-websso.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-websso.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="sect3" id="id-1.5.7.13.7.2"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.11.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">WebSSO Using SAML Method</span> <a title="Permalink" class="permalink" href="#id-1.5.7.13.7.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-websso.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-websso.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="sect4" id="id-1.5.7.13.7.2.2"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.11.4.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating ADFS metadata</span> <a title="Permalink" class="permalink" href="#id-1.5.7.13.7.2.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-websso.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-websso.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   For information about creating Active Directory Federation Services
   metadata, see the section <em class="citetitle ">To create edited ADFS 2.0 metadata
   with an added scope element</em> of
   <a class="link" href="https://technet.microsoft.com/en-us/library/gg317734" target="_blank">https://technet.microsoft.com/en-us/library/gg317734</a>.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     On the ADFS computer, use a browser such as Internet Explorer to view
     <code class="literal">https://&lt;adfs_server_hostname&gt;/FederationMetadata/2007-06/FederationMetadata.xml</code>.
    </p></li><li class="listitem "><p>
     On the File menu, click Save as, and then navigate to the Windows desktop
     and save the file with the name adfs_metadata.xml. Make sure to change the
     Save as type drop-down box to All Files (*.*).
    </p></li><li class="listitem "><p>
     Use Windows Explorer to navigate to the Windows desktop, right-click
     adfs_metadata.xml, and then click Edit.
    </p></li><li class="listitem "><p>
     In Notepad, insert the following XML in the first element. Before editing,
     the EntityDescriptor appears as follows:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;EntityDescriptor ID="abc123" entityID=http://WIN-CAICP35LF2I.vlan44.domain/adfs/services/trust xmlns="urn:oasis:names:tc:SAML:2.0:metadata" &gt;</pre></div><p>
     After editing, it should look like this:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;EntityDescriptor ID="abc123" entityID="http://WIN-CAICP35LF2I.vlan44.domain/adfs/services/trust" xmlns="urn:oasis:names:tc:SAML:2.0:metadata" xmlns:shibmd="urn:mace:shibboleth:metadata:1.0"&gt;</pre></div></li><li class="listitem "><p>
     In Notepad, on the Edit menu, click Find. In Find what, type IDPSSO, and
     then click Find Next.
    </p></li><li class="listitem "><p>
     Insert the following XML in this section: Before editing, the
     IDPSSODescriptor appears as follows:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;IDPSSODescriptor protocolSupportEnumeration="urn:oasis:names:tc:SAML:2.0:protocol"&gt;&lt;KeyDescriptor use="encryption"&gt;</pre></div><p>
     After editing, it should look like this:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;IDPSSODescriptor protocolSupportEnumeration="urn:oasis:names:tc:SAML:2.0:protocol"&gt;&lt;Extensions&gt;&lt;shibmd:Scope regexp="false"&gt;vlan44.domain&lt;/shibmd:Scope&gt;&lt;/Extensions&gt;&lt;KeyDescriptor use="encryption"&gt;</pre></div></li><li class="listitem "><p>
     Delete the metadata document signature section of the file (the bold text
     shown in the following code). Because you have edited the document, the
     signature will now be invalid. Before editing the signature appears as
     follows:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;EntityDescriptor ID="abc123" entityID="http://FSWEB.contoso.com/adfs/services/trust" xmlns="urn:oasis:names:tc:SAML:2.0:metadata" xmlns:shibmd="urn:mace:shibboleth:metadata:1.0"&gt;
<span class="bold"><strong>&lt;ds:Signature xmlns:ds="http://www.w3.org/2000/09/xmldsig#"&gt;
    SIGNATURE DATA
&lt;/ds:Signature&gt;</strong></span>
&lt;RoleDescriptor xsi:type=…&gt;</pre></div><p>
     After editing it should look like this:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;EntityDescriptor ID="abc123" entityID="http://FSWEB.contoso.com/adfs/services/trust" xmlns="urn:oasis:names:tc:SAML:2.0:metadata" xmlns:shibmd="urn:mace:shibboleth:metadata:1.0"&gt;
&lt;RoleDescriptor xsi:type=…&gt;</pre></div></li><li class="listitem "><p>
     Save and close adfs_metadata.xml.
    </p></li><li class="listitem "><p>
     Copy adfs_metadata.xml to the Cloud Lifecycle Manager node and place it into
     <code class="filename">/var/lib/ardana/openstack/my_cloud/config/keystone/</code>
     directory and put it under revision control.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git checkout site
<code class="prompt user">ardana &gt; </code>git add my_cloud/config/keystone/adfs_metadata.xml
<code class="prompt user">ardana &gt; </code>git commit -m "Add ADFS metadata file for WebSSO authentication"</pre></div></li></ol></div></div><div class="sect4" id="id-1.5.7.13.7.2.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.11.4.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting Up WebSSO</span> <a title="Permalink" class="permalink" href="#id-1.5.7.13.7.2.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-websso.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-websso.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Start by creating a config file <code class="filename">adfs_config.yml</code> with the
   following parameters and place it in the
   <code class="filename">/var/lib/ardana/openstack/my_cloud/config/keystone/</code>
   directory on your Cloud Lifecycle Manager node.
  </p><div class="verbatim-wrap"><pre class="screen">keystone_trusted_idp: adfs
keystone_sp_conf:
    idp_metadata_file: /var/lib/ardana/openstack/my_cloud/config/keystone/adfs_metadata.xml
    shib_sso_application_entity_id: http://sp_uri_entityId
    shib_sso_idp_entity_id: http://default_idp_uri_entityId
    target_domain:
        name: domain1
        description: my domain
    target_project:
        name: project1
        description: my project
    target_group:
        name: group1
        description: my group
    role:
        name: service
    identity_provider:
        id: adfs_idp1
        description: This is the ADFS identity provider.
    mapping:
        id: mapping1
        rules_file: /var/lib/ardana/openstack/my_cloud/config/keystone/adfs_mapping.json
    protocol:
        id: saml2
    attribute_map:
        -
          name: http://schemas.xmlsoap.org/claims/Group
          id: ADFS_GROUP
        -
          name: urn:oid:1.3.6.1.4.1.5923.1.1.1.6
          id: ADFS_LOGIN</pre></div><p>
   A sample config file like this exists in
   roles/KEY-API/files/samples/websso/keystone_configure_adfs_sample.yml. Here
   are some detailed descriptions for each of the config options:
  </p><div class="verbatim-wrap"><pre class="screen">keystone_trusted_idp: A flag to indicate if this configuration is used for WebSSO or K2K. The value can be either 'adfs' or 'k2k'.
keystone_sp_conf:
    shib_sso_idp_entity_id: The ADFS URI used as an entity Id to identity the IdP.
    shib_sso_application_entity_id: The Service Provider URI used as a entity Id. It can be any URI here for Websso as long as it is unique to the SP.
    target_domain: A domain where the group will be created from.
        name: Any domain name. If it does not exist, it will be created or be updated.
        description: Any description.
    target_project: A project scope that the group has.
        name: Any project name. If it does not exist, it will be created or be updated.
        description: Any description.
    target_group: A group will be created from 'target_domain'.
        name: Any group name. If it does not exist, it will be created or be updated.
        description: Any description.
    role: A role will be assigned on 'target_project'. This role impacts the idp user scoped token permission at sp side.
        name: It has to be an existing role.
    idp_metadata_file: A reference to the ADFS metadata file that validates the SAML2 assertion.
    identity_provider: An ADFS IdP
        id: Any Id. If it does not exist, it will be created or be updated. This Id needs to be shared with the client so that the right mapping will be selected.
        description: Any description.
    mapping: A mapping in json format that maps a federated user to a corresponding group.
        id: Any Id. If it does not exist, it will be created or be updated.
        rules_file: A reference to the file that has the mapping in json.
    protocol: The supported federation protocol.
        id: 'saml2' is the only supported protocol for Websso.
    attribute_map: A shibboleth mapping defined additional attributes to map the attributes from the SAML2 assertion to the Websso mapping that SP understands.
        -
          name: An attribute name from the SAML2 assertion.
          id: An Id that the above name will be mapped to.</pre></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Create a mapping file, <code class="filename">adfs_mapping.json</code>, that is
     referenced from the preceding config file in
     <code class="filename">/var/lib/ardana/openstack/my_cloud/config/keystone/</code>.
    </p><div class="verbatim-wrap"><pre class="screen">     rules_file: /var/lib/ardana/openstack/my_cloud/config/keystone/adfs_mapping.json.</pre></div><p>
     The following is an example of the mapping file, existing in
     roles/KEY-API/files/samples/websso/adfs_sp_mapping.json:
    </p><div class="verbatim-wrap"><pre class="screen">[
             {
               "local": [{
                     "user": {
                         "name": "{0}"
                     }
                 }],
                 "remote": [{
                     "type": "ADFS_LOGIN"
                 }]
              },
              {
                "local": [{
                    "group": {
                        "id": "GROUP_ID"
                    }
                }],
                "remote": [{
                    "type": "ADFS_GROUP",
                "any_one_of": [
                    "Domain Users"
                    ]
                }]
              }
 ]</pre></div><p>
     You can find more details about how the WebSSO mapping works at
     <a class="link" href="http://docs.openstack.org" target="_blank">http://docs.openstack.org</a>.
     Also see <a class="xref" href="#maprules" title="5.11.4.1.3. Mapping rules">Section 5.11.4.1.3, “Mapping rules”</a> for more information.
    </p></li><li class="listitem "><p>
     Add <code class="filename">adfs_config.yml</code> and
     <code class="filename">adfs_mapping.json</code> to revision control.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git checkout site
<code class="prompt user">ardana &gt; </code>git add my_cloud/config/keystone/adfs_config.yml
<code class="prompt user">ardana &gt; </code>git add my_cloud/config/keystone/adfs_mapping.json
<code class="prompt user">ardana &gt; </code>git commit -m "Add ADFS config and mapping."</pre></div></li><li class="listitem "><p>
     Go to ~/scratch/ansible/next/ardana/ansible and run the following
     playbook to enable WebSSO in the keystone identity service:
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml -e@/var/lib/ardana/openstack/my_cloud/config/keystone/adfs_config.yml</pre></div></li><li class="listitem "><p>
     Enable WebSSO in the horizon dashboard service by setting
     horizon_websso_enabled flag to True in roles/HZN-WEB/defaults/main.yml and
     then run the horizon-reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts horizon-reconfigure.yml</pre></div></li></ol></div></div><div class="sect4" id="maprules"><div class="titlepage"><div><div><h5 class="title"><span class="number">5.11.4.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Mapping rules</span> <a title="Permalink" class="permalink" href="#maprules">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-websso.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-websso.xml</li><li><span class="ds-label">ID: </span>maprules</li></ul></div></div></div></div><p>
   One IdP-SP has only one mapping. The last mapping that the customer
   configures will be the one used and will overwrite the old mapping setting.
   Therefore, if the example mapping adfs_sp_mapping.json is used, the
   following behavior is expected because it maps the federated user only to
   the one group configured in
   <code class="literal">keystone_configure_adfs_sample.yml</code>.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Configure domain1/project1/group1, mapping1; websso login horizon, see
     project1;
    </p></li><li class="listitem "><p>
     Then reconfigure: domain1/project2/group1. mapping1, websso login horizon,
     see project1 and project2;
    </p></li><li class="listitem "><p>
     Reconfigure: domain3/project3/group3; mapping1, websso login horizon, only
     see project3; because now the IDP mapping maps the federated user to
     group3, which only has priviliges on project3.
    </p></li></ul></div><p>
   If you need a more complex mapping, you can use a custom mapping file, which
   needs to be specified in keystone_configure_adfs_sample.yml -&gt;
   rules_file.
  </p><p>
   You can use different attributes of the ADFS user in order to map to
   different or multiple groups.
  </p><p>
   An example of a more complex mapping file is
   adfs_sp_mapping_multiple_groups.json, as follows.
  </p><p>
   adfs_sp_mapping_multiple_groups.json
  </p><div class="verbatim-wrap"><pre class="screen">[
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group1",
           "domain":{
             "name": "domain1"
           }
        }
      }
    ],
    "remote":[{
      "type": "ADFS_LOGIN"
    },
    {
      "type": "ADFS_GROUP",
      "any_one_of":[
         "Domain Users"
      ]
     }
    ]
   },
  {
    "local": [
      {
        "user": {
          "name": "{0}"
        }
      },
      {
        "group": {
           "name": "group2",
           "domain":{
             "name": "domain2"
           }
        }
      }
    ],
    "remote":[{
      "type": "ADFS_LOGIN"
    },
    {
      "type": "ADFS_SCOPED_AFFILIATION",
      "any_one_of": [
          "member@contoso.com"
      ]
    },
    ]
   }
]</pre></div><p>
   The adfs_sp_mapping_multiple_groups.json must be run together with
   keystone_configure_mutiple_groups_sample.yml, which adds a new attribute for
   the shibboleth mapping. That file is as follows:
  </p><p>
   keystone_configure_mutiple_groups_sample.yml
  </p><div class="verbatim-wrap"><pre class="screen">#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---

keystone_trusted_idp: adfs
keystone_sp_conf:
    identity_provider:
        id: adfs_idp1
        description: This is the ADFS identity provider.
    idp_metadata_file: /var/lib/ardana/openstack/my_cloud/config/keystone/adfs_metadata.xml

    shib_sso_application_entity_id: http://blabla
    shib_sso_idp_entity_id: http://WIN-CAICP35LF2I.vlan44.domain/adfs/services/trust

    target_domain:
        name: domain2
        description: my domain

    target_project:
        name: project6
        description: my project

    target_group:
        name: group2
        description: my group

    role:
        name: admin

    mapping:
        id: mapping1
        rules_file: /var/lib/ardana/openstack/my_cloud/config/keystone/adfs_sp_mapping_multiple_groups.json

    protocol:
        id: saml2

    attribute_map:
        -
          name: http://schemas.xmlsoap.org/claims/Group
          id: ADFS_GROUP
        -
          name: urn:oid:1.3.6.1.4.1.5923.1.1.1.6
          id: ADFS_LOGIN
        -
          name: urn:oid:1.3.6.1.4.1.5923.1.1.1.9
          id: ADFS_SCOPED_AFFILIATION</pre></div></div></div><div class="sect3" id="id-1.5.7.13.7.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.11.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting up the ADFS server as the identity provider</span> <a title="Permalink" class="permalink" href="#id-1.5.7.13.7.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-websso.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-websso.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   For ADFS to be able to communicate with the keystone identity service, you
   need to add the keystone identity service as a trusted relying party for
   ADFS and also specify the user attributes that you want to send to the
   keystone identity service when users authenticate via WebSSO.
  </p><p>
   For more information, see the
   <a class="link" href="https://technet.microsoft.com/en-us/library/gg317734" target="_blank">Microsoft
   ADFS wiki</a>, section "Step 2: Configure ADFS 2.0 as the identity
   provider and shibboleth as the Relying Party".
  </p><p>
   Log in to the ADFS server.
  </p><p>
   <span class="bold"><strong>Add a relying party using metadata</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     From Server Manager Dashboard, click Tools on the upper right, then ADFS
     Management.
    </p></li><li class="listitem "><p>
     Right-click ADFS, and then select Add Relying Party Trust.
    </p></li><li class="listitem "><p>
     Click Start, leave the already selected option <code class="literal">Import data about
     the relying party published online or on a local network</code>.
    </p></li><li class="listitem "><p>
     In the Federation metadata address field, type
     <code class="literal">&lt;keystone_publicEndpoint&gt;/Shibboleth.sso/Metadata</code>
     (your keystone identity service Metadata endpoint), and then click Next.
     You can also import metadata from a file. Create a file with the content
     of the result of the following curl command
    </p><div class="verbatim-wrap"><pre class="screen">curl &lt;keystone_publicEndpoint&gt;/Shibboleth.sso/Metadata</pre></div><p>
     and then choose this file for importing the metadata for the relying
     party.
    </p></li><li class="listitem "><p>
     In the Specify Display Name page, choose a proper name to identify this
     trust relationship, and then click Next.
    </p></li><li class="listitem "><p>
     On the Choose Issuance Authorization Rules page, leave the default Permit
     all users to access the relying party selected, and then click Next.
    </p></li><li class="listitem "><p>
     Click Next, and then click Close.
    </p></li></ol></div><p>
   <span class="bold"><strong>Edit claim rules for relying party trust</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     The Edit Claim Rules dialog box should already be open. If not, In the
     ADFS center pane, under Relying Party Trusts, right-click your newly
     created trust, and then click Edit Claim Rules.
    </p></li><li class="listitem "><p>
     On the Issuance Transform Rules tab, click Add Rule.
    </p></li><li class="listitem "><p>
     On the Select Rule Template page, select Send LDAP Attributes as Claims,
     and then click Next.
    </p></li><li class="listitem "><p>
     On the Configure Rule page, in the Claim rule name box, type Get Data.
    </p></li><li class="listitem "><p>
     In the Attribute Store list, select Active Directory.
    </p></li><li class="listitem "><p>
     In the Mapping of LDAP attributes section, create the following mappings.
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th> LDAP Attribute</th><th>Outgoing Claim Type</th></tr></thead><tbody><tr><td>Token-Groups – Unqualified Names</td><td>Group</td></tr><tr><td>User-Principal-Name</td><td>UPN</td></tr></tbody></table></div></li><li class="listitem "><p>
     Click Finish.
    </p></li><li class="listitem "><p>
     On the Issuance Transform Rules tab, click Add Rule.
    </p></li><li class="listitem "><p>
     On the Select Rule Template page, select Send Claims Using a Custom Rule,
     and then click Next.
    </p></li><li class="listitem "><p>
     In the Configure Rule page, in the Claim rule name box, type Transform UPN
     to epPN.
    </p></li><li class="listitem "><p>
     In the Custom Rule window, type or copy and paste the following:
    </p><div class="verbatim-wrap"><pre class="screen">c:[Type == "http://schemas.xmlsoap.org/ws/2005/05/identity/claims/upn"]
=&gt; issue(Type = "urn:oid:1.3.6.1.4.1.5923.1.1.1.6", Value = c.Value, Properties["http://schemas.xmlsoap.org/ws/2005/05/identity/claimproperties/attributename"] = "urn:oasis:names:tc:SAML:2.0:attrname-format:uri");</pre></div></li><li class="listitem "><p>
     Click Finish.
    </p></li><li class="listitem "><p>
     On the Issuance Transform Rules tab, click Add Rule.
    </p></li><li class="listitem "><p>
     On the Select Rule Template page, select Send Claims Using a Custom Rule,
     and then click Next.
    </p></li><li class="listitem "><p>
     On the Configure Rule page, in the Claim rule name box, type Transform
     Group to epSA.
    </p></li><li class="listitem "><p>
     In the Custom Rule window, type or copy and paste the following:
    </p><div class="verbatim-wrap"><pre class="screen">c:[Type == "http://schemas.xmlsoap.org/claims/Group", Value == "Domain Users"]
=&gt; issue(Type = "urn:oid:1.3.6.1.4.1.5923.1.1.1.9", Value = "member@contoso.com", Properties["http://schemas.xmlsoap.org/ws/2005/05/identity/claimproperties/attributename"] = "urn:oasis:names:tc:SAML:2.0:attrname-format:uri");</pre></div></li><li class="listitem "><p>
     Click Finish, and then click OK.
    </p></li></ol></div><p>
   This list of Claim Rules is just an example and can be modified or enhanced
   based on the customer's necessities and ADFS setup specifics.
  </p><p>
   <span class="bold"><strong>Create a sample user on the ADFS server</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     From the Server Manager Dashboard, click Tools on the upper right, then
     Active Directory Users and Computer.
    </p></li><li class="listitem "><p>
     Right click User, then New, and then User.
    </p></li><li class="listitem "><p>
     Follow the on-screen instructions.
    </p></li></ol></div><p>
   You can test the horizon dashboard service "Login with ADFS" by opening a
   browser at the horizon dashboard service URL and choose
   <code class="literal">Authenticate using: ADFS Credentials</code>. You should be
   redirected to the ADFS login page and be able to log into the horizon
   dashboard service with your ADFS credentials.
  </p></div></div><div class="sect2" id="id-1.5.7.13.8"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.11.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">WebSSO Using OpenID Method</span> <a title="Permalink" class="permalink" href="#id-1.5.7.13.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-websso.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-websso.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    The interaction between Keystone and the external Identity Provider (IdP) is
    handled by the Apache2 <a class="link" href="https://github.com/zmartzone/mod_auth_openidc" target="_blank">auth_openidc</a> module.
   </p><p>
     There are two steps to enable the feature.
   </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
      Configure Keystone with the required OpenID Connect provider information.
     </p></li><li class="listitem "><p>
      Create the Identity Provider, protocol, and mapping in Keystone, using
      <span class="bold"><strong> OpenStack Command Line Tool</strong></span>.
     </p></li></ol></div><div class="sect3" id="id-1.5.7.13.8.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.11.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Keystone</span> <a title="Permalink" class="permalink" href="#id-1.5.7.13.8.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-websso.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-websso.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
      Log in to the Cloud Lifecycle Manager node and edit the <code class="filename">~/openstack/my_cloud/config/keystone/keystone_deploy_config.yml</code> file with the <code class="literal">"keystone_openid_connect_conf"</code> variable. For example:
     </p><div class="verbatim-wrap"><pre class="screen">keystone_openid_connect_conf:
    identity_provider: google
    response_type: id_token
    scope: "openid email profile"
    metadata_url: https://accounts.google.com/.well-known/openid-configuration
    client_id: [Replace with your client ID]
    client_secret: [Replace with your client secret]
    redirect_uri: https://www.myenterprise.com:5000/v3/OS-FEDERATION/identity_providers/google/protocols/openid/auth
    crypto_passphrase: ""</pre></div><p>Where:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
        <code class="literal">identity_provider</code>: name of the OpenID Connect identity provider. This must be the same as the identity provider to be created in Keystone using <span class="bold"><strong> OpenStack Command Line Tool</strong></span>. For example, if the identity provider is <code class="literal">foo</code>, we must create the identity provider with the name. For example:
       </p><div class="verbatim-wrap"><pre class="screen">openstack identity provider create foo</pre></div></li><li class="listitem "><p>
        <code class="literal">response_type</code>: corresponding to <a class="link" href="https://github.com/zmartzone/mod_auth_openidc/blob/master/auth_openidc.conf" target="_blank">auth_openidc OIDCResponseType</a>. In most cases, it should be <code class="literal">"id_token"</code>.
       </p></li><li class="listitem "><p>
        <code class="literal">scope</code>: corresponding to <a class="link" href="https://github.com/zmartzone/mod_auth_openidc/blob/master/auth_openidc.conf" target="_blank">auth_openidc OIDCScope</a>.
       </p></li><li class="listitem "><p>
        <code class="literal">metadata_url</code>: corresponding to <a class="link" href="https://github.com/zmartzone/mod_auth_openidc/blob/master/auth_openidc.conf" target="_blank">auth_openidc OIDCProviderMetadataURL</a>.
       </p></li><li class="listitem "><p>
        <code class="literal">client_id</code>: corresponding to <a class="link" href="https://github.com/zmartzone/mod_auth_openidc/blob/master/auth_openidc.conf" target="_blank">auth_openidc OIDCClientID</a>.
       </p></li><li class="listitem "><p>
        <code class="literal">client_secret</code>: corresponding to <a class="link" href="https://github.com/zmartzone/mod_auth_openidc/blob/master/auth_openidc.conf" target="_blank">auth_openidc OIDCClientSecret</a>.
       </p></li><li class="listitem "><p>
        <code class="literal">redirect_uri</code>: corresponding to <a class="link" href="https://github.com/zmartzone/mod_auth_openidc/blob/master/auth_openidc.conf" target="_blank">auth_openidc OIDCRedirectURI</a>. This must be the Keystone public endpoint for given OpenID Connect identity provider. i.e. <code class="literal">"https://keystone-public-endpoint.foo.com/v3/OS-FEDERATION/identity_providers/foo/protocols/openid/auth"</code>.
       </p><div id="id-1.5.7.13.8.5.2.1.4.7.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
         Some OpenID Connect IdPs such as Google require the hostname in the "redirect_uri" to be a public FQDN. In that case, the hostname in Keystone public endpoint must also be a public FQDN and must match the one specified in the "redirect_uri".
        </p></div></li><li class="listitem "><p>
        <code class="literal">crypto_passphrase</code>: corresponding to <a class="link" href="https://github.com/zmartzone/mod_auth_openidc/blob/master/auth_openidc.conf" target="_blank">auth_openidc OIDCCryptoPassphrase</a>. If left blank, a random cryto passphrase will be generated.
       </p></li></ul></div></li><li class="listitem "><p>
      Commit the changes to your local git repository.
     </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "add OpenID Connect configuration"</pre></div></li><li class="listitem "><p>
      Run <code class="literal">keystone-reconfigure</code> Ansible playbook.
     </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</pre></div></li></ol></div></div><div class="sect3" id="id-1.5.7.13.8.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.11.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure Horizon</span> <a title="Permalink" class="permalink" href="#id-1.5.7.13.8.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-websso.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-websso.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Complete the following steps to configure horizon to support WebSSO with OpenID method.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Edit the
     <code class="filename">~/openstack/ardana/ansible/roles/HZN-WEB/defaults/main.yml</code>
     file and set the following parameter to <code class="literal">True</code>.
    </p><div class="verbatim-wrap"><pre class="screen">horizon_websso_enabled: True</pre></div></li><li class="step "><p>
     Locate the last line in the
     <code class="filename">~/openstack/ardana/ansible/roles/HZN-WEB/defaults/main.yml</code>
     file. The default configuration for this line should look like the
     following:
    </p><div class="verbatim-wrap"><pre class="screen">horizon_websso_choices:
  - {protocol: saml2, description: "ADFS Credentials"}</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       If your cloud <span class="bold"><strong>does not</strong></span> have AD FS
       enabled, then replace the preceding
       <code class="literal">horizon_websso_choices:</code> parameter with the following.
      </p><div class="verbatim-wrap"><pre class="screen">- {protocol: openid, description: "OpenID Connect"}</pre></div><p>
       The resulting block should look like the following.
      </p><div class="verbatim-wrap"><pre class="screen">horizon_websso_choices:
    - {protocol: openid, description: "OpenID Connect"}</pre></div></li><li class="listitem "><p>
       If your cloud <span class="bold"><strong>does</strong></span> have ADFS enabled,
       then simply add the following parameter to the
       <code class="literal">horizon_websso_choices:</code> section. Do not replace the
       default parameter, add the following line to the existing block.
      </p><div class="verbatim-wrap"><pre class="screen">- {protocol: saml2, description: "ADFS Credentials"}</pre></div><p>
       If your cloud has ADFS enabled, the final block of your
       <code class="literal">~/openstack/ardana/ansible/roles/HZN-WEB/defaults/main.yml</code>
       should have the following entries.
      </p><div class="verbatim-wrap"><pre class="screen">horizon_websso_choices:
    - {protocol: openid, description: "OpenID Connect"}
    - {protocol: saml2, description: "ADFS Credentials"}</pre></div></li></ul></div></li><li class="step "><p>
     Run the following commands to add your changes to the local git
     repository, and reconfigure the horizon service, enabling the changes made
     in <span class="bold"><strong>Step 1</strong></span>:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack
git add -A
git commit -m "Configured WebSSO using OpenID Connect"
cd ~/openstack/ardana/ansible/
ansible-playbook -i hosts/localhost config-processor-run.yml
ansible-playbook -i hosts/localhost ready-deployment.yml
cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts horizon-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="id-1.5.7.13.8.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.11.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create Identity Provider, Protocol, and Mapping</span> <a title="Permalink" class="permalink" href="#id-1.5.7.13.8.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-websso.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-websso.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    To fully enable OpenID Connect, Identity Provider, Protocol, and Mapping for the given IdP must be created in Keystone. This is done by using the <span class="bold"><strong>OpenStack Command Line Tool</strong></span> using the Keystone <span class="bold"><strong>admin</strong></span> credential.
   </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
      Log in to the Cloud Lifecycle Manager node and source <code class="literal">keystone.osrc</code> file.
     </p><div class="verbatim-wrap"><pre class="screen">source ~/keystone.osrc</pre></div></li><li class="listitem "><p>
      Create the <span class="bold"><strong>Identity Provider</strong></span>. For example:
     </p><div class="verbatim-wrap"><pre class="screen">openstack identity provider create foo</pre></div><div id="id-1.5.7.13.8.7.3.2.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
       The name of the Identity Provider must be exactly the same as the "identity_provider" attribute given when configuring Keystone in the previous section.
      </p></div></li><li class="listitem "><p>
      Next, create the Mapping for the Identity Provider. Prior to creating the
      Mapping, one must fully grasp the intricacies
      of <a class="link" href="https://docs.openstack.org/keystone/latest/admin/federation/mapping_combinations.html" target="_blank">Mapping Combinations</a>
      as it may have profound security implications if done incorrectly.
      Here's an example of a mapping file.
     </p><div class="verbatim-wrap"><pre class="screen">[
    {
        "local": [
            {
                "user": {
                    "name": "{0}",
                    "email": "{1}",
                    "type": "ephemeral"
                 },
                 "group": {
                    "domain": {
                        "name": "Default"
                    },
                    "name": "openidc_demo"
                }
             }
         ],
         "remote": [
             {
                 "type": "REMOTE_USER"
             },
             {
                 "type": "HTTP_OIDC_EMAIL"
             }

        ]
    }
]</pre></div><p>
      Once the mapping file is created, now create the Mapping resource in Keystone. For example:
     </p><div class="verbatim-wrap"><pre class="screen">openstack mapping create --rule oidc_mapping.json oidc_mapping</pre></div></li><li class="listitem "><p>
      Lastly, create the Protocol for the Identity Provider and its mapping. For OpenID Connect, the protocol name must be <span class="bold"><strong>openid</strong></span>. For example:
     </p><div class="verbatim-wrap"><pre class="screen">openstack federation protocol create --identity-provider google --mapping oidc_mapping openid</pre></div></li></ol></div></div></div></div><div class="sect1" id="topic-qtp-cn3-bt"><div class="titlepage"><div><div><h2 class="title"><span class="number">5.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Identity Service Notes and Limitations</span> <a title="Permalink" class="permalink" href="#topic-qtp-cn3-bt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-identity_limitations.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_limitations.xml</li><li><span class="ds-label">ID: </span>topic-qtp-cn3-bt</li></ul></div></div></div></div><div class="sect2" id="idg-all-operations-configuring-identity-identity-limitations-xml-6"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.12.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Notes</span> <a title="Permalink" class="permalink" href="#idg-all-operations-configuring-identity-identity-limitations-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-identity_limitations.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_limitations.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-configuring-identity-identity-limitations-xml-6</li></ul></div></div></div></div><p>
   This topic describes limitations of and important notes pertaining to the
   identity service. <span class="bold"><strong>Domains</strong></span>
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Domains can be created and managed by the horizon web interface, keystone
     API and OpenStackClient CLI.
    </p></li><li class="listitem "><p>
     The configuration of external authentication systems requires the creation
     and usage of Domains.
    </p></li><li class="listitem "><p>
     All configurations are managed by creating and editing specific
     configuration files.
    </p></li><li class="listitem "><p>
     End users can authenticate to a particular project and domain via the
     horizon web interface, keystone API and OpenStackClient CLI.
    </p></li><li class="listitem "><p>
     A new horizon login page that requires a Domain entry is now installed by
     default.
    </p></li></ul></div><p>
   <span class="bold"><strong>keystone-to-keystone Federation</strong></span>
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     keystone-to-keystone (K2K) Federation provides the ability to authenticate
     once with one cloud and then use these credentials to access resources on
     other federated clouds.
    </p></li><li class="listitem "><p>
     All configurations are managed by creating and editing specific
     configuration files.
    </p></li></ul></div><p>
   <span class="bold"><strong>Multi-Factor Authentication (MFA)</strong></span>
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The keystone architecture provides support for MFA deployments.
    </p></li><li class="listitem "><p>
     MFA provides the ability to deploy non-password based authentication; for
     example: token providing hardware and text messages.
    </p></li></ul></div><p>
   <span class="bold"><strong>Hierarchical Multitenancy</strong></span>
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Provides the ability to create sub-projects within a Domain-Project
     hierarchy.
    </p></li></ul></div></div><div class="sect2" id="idg-all-operations-configuring-identity-identity-limitations-xml-7"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.12.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="#idg-all-operations-configuring-identity-identity-limitations-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-identity_limitations.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_limitations.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-configuring-identity-identity-limitations-xml-7</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Authentication with external authentication systems
   (LDAP, Active Directory (AD) or Identity Providers)</strong></span>
  </p><div class="itemizedlist " id="ul-u52-jpd-bt"><ul class="itemizedlist"><li class="listitem "><p>
     No horizon web portal support currently exists for the creation and
     management of external authentication system configurations.
    </p></li></ul></div><p>
   <span class="bold"><strong>Integration with LDAP services</strong></span>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 domain-specific configuration:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     No Global User Listing: Once domain-specific driver configuration is
     enabled, listing all users and listing all groups are not supported
     operations. Those calls require a specific domain filter and a
     domain-scoped token for the target domain.
    </p></li><li class="listitem "><p>
     You cannot have both a file store and a database store for domain-specific
     driver configuration in a single identity service instance. Once a
     database store is enabled within the identity service instance, any file
     store will be ignored, and vice versa.
    </p></li><li class="listitem "><p>
     The identity service allows a list limit configuration to globally set the
     maximum number of entities that will be returned in an identity collection
     per request but it does not support per-domain list limit setting at this
     time.
    </p></li><li class="listitem "><p>
     Each time a new domain is configured with LDAP integration the single CA
     file gets overwritten. Ensure that you place certs for all the LDAP
     back-end domains in the cacert parameter. Detailed CA file inclusion
     instructions are provided in the comments of the sample YAML configuration
     file <code class="filename">keystone_configure_ldap_my.yml</code>
     (see <a class="xref" href="#filestore" title="5.9.2. Set up domain-specific driver configuration - file store">Section 5.9.2, “Set up domain-specific driver configuration - file store”</a>).
    </p></li><li class="listitem "><p>
     LDAP is only supported for identity operations (reading users and groups
     from LDAP).
    </p></li><li class="listitem "><p>
     keystone assignment operations from LDAP records such as managing or
     assigning roles and projects, are not currently supported.
    </p></li><li class="listitem "><p>
     The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 'default' domain is pre-configured to store service account
     users and is authenticated locally against the identity service. Domains
     configured for external LDAP integration are non-default domains.
    </p></li><li class="listitem "><p>
     When using the current OpenStackClient CLI you must use the user ID rather
     than the user name when working with a non-default domain.
    </p></li><li class="listitem "><p>
     Each LDAP connection with the identity service is for read-only
     operations. Configurations that require identity service write operations
     (to create users, groups, etc.) are not currently supported.
    </p></li><li class="listitem "><p>
     LDAP is only supported for identity operations (reading users and groups
     from LDAP). keystone assignment operations from LDAP records such as
     managing or assigning roles and projects, are not currently supported.
    </p></li><li class="listitem "><p>
     When using the current OpenStackClient CLI you must use the user ID rather
     than the user name when working with a non-default domain.
    </p></li></ul></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 API-based domain-specific configuration management
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     No GUI dashboard for domain-specific driver configuration management
    </p></li><li class="listitem "><p>
     API-based Domain specific config does not check for type of option.
    </p></li><li class="listitem "><p>
     API-based Domain specific config does not check for option values
     supported.
    </p></li><li class="listitem "><p>
     API-based Domain config method does not provide retrieval of default
     values of domain-specific configuration options.
    </p></li><li class="listitem "><p>
     Status: Domain-specific driver configuration database store is a non-core
     feature for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9.
    </p></li></ul></div></div><div class="sect2" id="id-1.5.7.14.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.12.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">keystone-to-keystone federation</span> <a title="Permalink" class="permalink" href="#id-1.5.7.14.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-identity_limitations.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_limitations.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     When a user is disabled in the identity provider, the issued federated
     token from the service provider still remains valid until the token is
     expired based on the keystone expiration setting.
    </p></li><li class="listitem "><p>
     An already issued federated token will retain its scope until its
     expiration. Any changes in the mapping on the service provider will not
     impact the scope of an already issued federated token. For example, if an
     already issued federated token was mapped to group1 that has scope on
     project1, and mapping is changed to group2 that has scope on project2, the
     prevously issued federated token still has scope on project1.
    </p></li><li class="listitem "><p>
     Access to service provider resources is provided only through the
     python-keystone CLI client or the keystone API. No horizon web interface
     support is currently available.
    </p></li><li class="listitem "><p>
     Domains, projects, groups, roles, and quotas are created per the service
     provider cloud. Support for federated projects, groups, roles, and quotas
     is currently not available.
    </p></li><li class="listitem "><p>
     keystone-to-keystone federation and WebSSO cannot be configured by putting
     both sets of configuration attributes in the same config file; they will
     overwrite each other. Consequently, they need to be configured
     individually.
    </p></li><li class="listitem "><p>
     Scoping the federated user to a domain is not supported by default in the
     playbook. To enable it, see the steps in <a class="xref" href="#scopeToDomain" title="5.10.7. Scope Federated User to Domain">Section 5.10.7, “Scope Federated User to Domain”</a>.
    </p></li><li class="listitem "><p>
     No horizon web portal support currently exists for the creation and
     management of federation configurations.
    </p></li><li class="listitem "><p>
     All end user authentication is available only via the keystone API and
     OpenStackClient CLI.
    </p></li><li class="listitem "><p>
     Additional information can be found at
     <a class="link" href="http://docs.openstack.org" target="_blank">http://docs.openstack.org</a>.
    </p></li></ul></div><p>
   <span class="bold"><strong>WebSSO</strong></span>
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The WebSSO function supports only horizon web authentication. It is not
     supported for direct API or CLI access.
    </p></li><li class="listitem "><p>
     WebSSO works only with Fernet token provider. See <a class="xref" href="#fernet-tokens" title="5.8.4. Fernet Tokens">Section 5.8.4, “Fernet Tokens”</a>.
    </p></li><li class="listitem "><p>
     The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> WebSSO function with SAML method was tested with Microsoft Active Directory
     Federation Services (ADFS). The instructions provided are pertinent to
     ADFS and are intended to provide a sample configuration for deploying
     WebSSO with an external identity provider. If you have a different
     identity provider such as Ping Identity or IBM Tivoli, consult with those
     vendors for specific instructions for those products.
    </p></li><li class="listitem "><p>
     The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> WebSSO function with OpenID method was tested with
     Google OAuth 2.0 APIs, which conform to the OpenID Connect specification.
     The interaction between keystone and the external Identity Provider (IdP) is
     handled by the Apache2 <a class="link" href="https://github.com/zmartzone/mod_auth_openidc" target="_blank">auth_openidc</a> module. Please consult with the specific OpenID Connect vendor
     on whether they support <code class="literal">auth_openidc</code>
    </p></li><li class="listitem "><p>
     Both SAML and OpenID methods are supported for WebSSO federation in
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 .
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>WebSSO has a change password option in User
     Settings, but note that this function is not accessible for users
     authenticating with external systems such as LDAP or SAML Identity
     Providers.</strong></span>
    </p></li></ul></div><p>
   <span class="bold"><strong>Multi-factor authentication (MFA)</strong></span>
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> MFA support is a custom configuration requiring Sales Engineering support.
    </p></li><li class="listitem "><p>
     MFA drivers are not included with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> and need to be provided by a
     specific MFA vendor.
    </p></li><li class="listitem "><p>
     Additional information can be found at
     <a class="link" href="http://docs.openstack.org/security-guide/content/identity-authentication-methods.html#identity-authentication-methods-external-authentication-methods" target="_blank">http://docs.openstack.org/security-guide/content/identity-authentication-methods.html#identity-authentication-methods-external-authentication-methods</a>.
    </p></li></ul></div><p>
   <span class="bold"><strong>Hierarchical multitenancy</strong></span>
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     This function requires additional support from various OpenStack services
     to be functional. It is a non-core function in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> and is not ready
     for either proof of concept or production deployments.
    </p></li><li class="listitem "><p>
     Additional information can be found at
     <a class="link" href="http://specs.openstack.org/openstack/keystone-specs/specs/juno/hierarchical_multitenancy.html" target="_blank">http://specs.openstack.org/openstack/keystone-specs/specs/juno/hierarchical_multitenancy.html</a>.
    </p></li></ul></div><p>
   <span class="bold"><strong>Missing quota information for compute
   resources</strong></span>
  </p><div id="id-1.5.7.14.4.10" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    An error message that will appear in the default horizon page if you are
    running a swift-only deployment (no Compute service). In this
    configuration, you will not see any quota information for Compute
    resources and will see the following error message:
   </p></div><p>
   <span class="emphasis"><em>The Compute service is not installed or is not configured
   properly. No information is available for Compute resources.</em></span> This
   error message is expected as no Compute service is configured for this
   deployment. Please ignore the error message.
  </p><p>
   The following is the benchmark of the performance that is based on 150
   concurrent requests and run for 10 minute periods of stable load time.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="newCol3" /></colgroup><thead><tr><th>Operation </th><th>In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 (secs/request)</th><th>In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 3.0 (secs/request)</th></tr></thead><tbody><tr><td>Token Creation </td><td>0.86</td><td>0.42</td></tr><tr><td>Token Validation</td><td>0.47</td><td>0.41</td></tr></tbody></table></div><p>
   Considering that token creation operations do not happen as frequently as
   token validation operations, you are likely to experience less of a
   performance problem regardless of the extended time for token creation.
  </p></div><div class="sect2" id="sec-keystone-cron"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.12.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">System cron jobs need setup</span> <a title="Permalink" class="permalink" href="#sec-keystone-cron">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-identity-identity_limitations.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-identity-identity_limitations.xml</li><li><span class="ds-label">ID: </span>sec-keystone-cron</li></ul></div></div></div></div><p>
   keystone relies on two cron jobs to periodically clean up expired tokens and
   for token revocation. The following is how the cron jobs appear on the
   system:
  </p><div class="verbatim-wrap"><pre class="screen">1 1 * * * /opt/stack/service/keystone/venv/bin/keystone-manage token_flush
1 1,5,10,15,20 * * * /opt/stack/service/keystone/venv/bin/revocation_cleanup.sh</pre></div><p>
   By default, the two cron jobs are enabled on controller node 1 only, not on
   the other two nodes. When controller node 1 is down or has failed for any
   reason, these two cron jobs must be manually set up on one of the other two
   nodes.
  </p></div></div></div><div class="chapter " id="ops-managing-compute"><div class="titlepage"><div><div><h1 class="title"><span class="number">6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Compute</span> <a title="Permalink" class="permalink" href="#ops-managing-compute">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-managing_compute.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_compute.xml</li><li><span class="ds-label">ID: </span>ops-managing-compute</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#aggregates"><span class="number">6.1 </span><span class="name">Managing Compute Hosts using Aggregates and Scheduler Filters</span></a></span></dt><dt><span class="section"><a href="#topic-vhs-12v-vw"><span class="number">6.2 </span><span class="name">Using Flavor Metadata to Specify CPU Model</span></a></span></dt><dt><span class="section"><a href="#topic-pqr-lyx-yw"><span class="number">6.3 </span><span class="name">Forcing CPU and RAM Overcommit Settings</span></a></span></dt><dt><span class="section"><a href="#enabling-the-nova-resize"><span class="number">6.4 </span><span class="name">Enabling the Nova Resize and Migrate Features</span></a></span></dt><dt><span class="section"><a href="#resize"><span class="number">6.5 </span><span class="name">Enabling ESX Compute Instance(s) Resize Feature</span></a></span></dt><dt><span class="section"><a href="#gpu-passthrough"><span class="number">6.6 </span><span class="name">GPU passthrough</span></a></span></dt><dt><span class="section"><a href="#configure-glance"><span class="number">6.7 </span><span class="name">Configuring the Image Service</span></a></span></dt></dl></div></div><p>
  Information about managing and configuring the Compute service.
 </p><div class="sect1" id="aggregates"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Compute Hosts using Aggregates and Scheduler Filters</span> <a title="Permalink" class="permalink" href="#aggregates">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-compute-creating_aggregates.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute-creating_aggregates.xml</li><li><span class="ds-label">ID: </span>aggregates</li></ul></div></div></div></div><p>
  OpenStack nova has the concepts of availability zones and host aggregates
  that enable you to segregate your compute hosts. Availability zones are used
  to specify logical separation within your cloud based on the physical
  isolation or redundancy you have set up. Host aggregates are used to group
  compute hosts together based upon common features, such as operation system.
  For more information, read this topic.
 </p><p>
  OpenStack nova has the concepts of availability zones and host aggregates
  that enable you to segregate your Compute hosts. Availability zones are used
  to specify logical separation within your cloud based on the physical
  isolation or redundancy you have set up. Host aggregates are used to group
  compute hosts together based upon common features, such as operation system.
  For more information, see
  <a class="link" href="http://docs.openstack.org/openstack-ops/content/scaling.html" target="_blank">Scaling
  and Segregating your Cloud</a>.
 </p><p>
  The nova scheduler also has a filter scheduler, which supports both filtering
  and weighting to make decisions on where new compute instances should be
  created. For more information, see
  <a class="link" href="http://docs.openstack.org/developer/nova/filter_scheduler.html" target="_blank">Filter
  Scheduler</a> and
  <a class="link" href="http://docs.openstack.org/mitaka/config-reference/compute/scheduler.html" target="_blank">Scheduling</a>.
 </p><p>
  This document is going to show you how to set up both a nova host aggregate
  and configure the filter scheduler to further segregate your compute hosts.
 </p><div class="sect2" id="create-agg"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a nova Aggregate</span> <a title="Permalink" class="permalink" href="#create-agg">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-compute-creating_aggregates.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute-creating_aggregates.xml</li><li><span class="ds-label">ID: </span>create-agg</li></ul></div></div></div></div><p>
   These steps will show you how to create a nova aggregate and how to add a
   compute host to it. You can run these steps on any machine that contains the
   OpenStackClient that also has network access to your cloud environment. These
   requirements are met by the Cloud Lifecycle Manager.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Source the administrative creds:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc</pre></div></li><li class="listitem "><p>
     List your current nova aggregates:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack aggregate list</pre></div></li><li class="listitem "><p>
     Create a new nova aggregate with this syntax:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack aggregate create <em class="replaceable ">AGGREGATE-NAME</em></pre></div><p>
     If you wish to have the aggregate appear as an availability zone, then
     specify an availability zone with this syntax:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack aggregate create <em class="replaceable ">AGGREGATE-NAME</em> <em class="replaceable ">AVAILABILITY-ZONE-NAME</em></pre></div><p>
     So, for example, if you wish to create a new aggregate for your SUSE Linux Enterprise
     compute hosts and you wanted that to show up as the
     <code class="literal">SLE</code> availability zone, you could use this command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack aggregate create SLE SLE</pre></div><p>
     This would produce an output similar to this:
    </p><div class="verbatim-wrap"><pre class="screen">+----+------+-------------------+-------+------------------+
| Id | Name | Availability Zone | Hosts | Metadata
+----+------+-------------------+-------+--------------------------+
| 12 | SLE  | SLE               |       | 'availability_zone=SLE'
+----+------+-------------------+-------+--------------------------+</pre></div></li><li class="listitem "><p>
     Next, you need to add compute hosts to this aggregate so you can start by
     listing your current hosts. You can view the current list of hosts running 
     running the <code class="literal">compute</code> service like this:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack hypervisor list</pre></div></li><li class="listitem "><p>
     You can then add host(s) to your aggregate with this syntax:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack aggregate add host <em class="replaceable ">AGGREGATE-NAME</em> <em class="replaceable ">HOST</em></pre></div></li><li class="listitem "><p>
     Then you can confirm that this has been completed by listing the details
     of your aggregate:
    </p><div class="verbatim-wrap"><pre class="screen">openstack aggregate show <em class="replaceable ">AGGREGATE-NAME</em></pre></div><p>
     You can also list out your availability zones using this command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack availability zone list</pre></div></li></ol></div></div><div class="sect2" id="filters"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using nova Scheduler Filters</span> <a title="Permalink" class="permalink" href="#filters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-compute-creating_aggregates.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute-creating_aggregates.xml</li><li><span class="ds-label">ID: </span>filters</li></ul></div></div></div></div><p>
   The nova scheduler has two filters that can help with differentiating
   between different compute hosts that we'll describe here.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Filter</th><th>Description</th></tr></thead><tbody><tr><td>AggregateImagePropertiesIsolation</td><td>
       <p>
        Isolates compute hosts based on image properties and aggregate
        metadata. You can use commas to specify multiple values for the same
        property. The filter will then ensure at least one value matches.
       </p>
      </td></tr><tr><td>AggregateInstanceExtraSpecsFilter</td><td>
       <p>
        Checks that the aggregate metadata satisfies any extra specifications
        associated with the instance type. This uses
        <code class="literal">aggregate_instance_extra_specs</code>
       </p>
      </td></tr></tbody></table></div><div id="id-1.5.8.3.7.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    For details about other available filters, see
    <a class="link" href="http://docs.openstack.org/developer/nova/filter_scheduler.html" target="_blank">Filter
    Scheduler</a>.
   </p></div><p>
   <span class="bold"><strong>Using the AggregateImagePropertiesIsolation
   Filter</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the <code class="literal">~/openstack/my_cloud/config/nova/nova.conf.j2</code>
     file and add <code class="literal">AggregateImagePropertiesIsolation</code> to the
     scheduler_filters section. Example below, in bold:
    </p><div class="verbatim-wrap"><pre class="screen"># Scheduler
...
scheduler_available_filters = nova.scheduler.filters.all_filters
scheduler_default_filters = AvailabilityZoneFilter,RetryFilter,ComputeFilter,
 DiskFilter,RamFilter,ImagePropertiesFilter,ServerGroupAffinityFilter,
 ServerGroupAntiAffinityFilter,ComputeCapabilitiesFilter,NUMATopologyFilter,
 <span class="bold"><strong>AggregateImagePropertiesIsolation</strong></span>
...</pre></div><p>
     Optionally, you can also add these lines:
    </p><div class="verbatim-wrap"><pre class="screen">aggregate_image_properties_isolation_namespace = &lt;a prefix string&gt;</pre></div><div class="verbatim-wrap"><pre class="screen">aggregate_image_properties_isolation_separator = &lt;a separator character&gt;</pre></div><p>
     (defaults to <code class="literal">.</code>)
    </p><p>
     If these are added, the filter will only match image properties starting
     with the name space and separator - for example, setting to
     <code class="literal">my_name_space</code> and <code class="literal">:</code> would mean the
     image property <code class="literal">my_name_space:image_type=SLE</code> matches
     metadata <code class="literal">image_type=SLE</code>, but
     <code class="literal">an_other=SLE</code> would not be inspected for a match at
     all.
    </p><p>
     If these are not added all image properties will be matched against any
     similarly named aggregate metadata.
    </p></li><li class="listitem "><p>
     Add image properties to images that should be scheduled using the above
     filter
    </p></li><li class="listitem "><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "editing nova schedule filters"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Run the ready deployment playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the nova reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</pre></div></li></ol></div><p>
   <span class="bold"><strong>Using the AggregateInstanceExtraSpecsFilter
   Filter</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the <code class="literal">~/openstack/my_cloud/config/nova/nova.conf.j2</code>
     file and add <code class="literal">AggregateInstanceExtraSpecsFilter</code> to the
     scheduler_filters section. Example below, in bold:
    </p><div class="verbatim-wrap"><pre class="screen"># Scheduler
...
scheduler_available_filters = nova.scheduler.filters.all_filters
 scheduler_default_filters = AvailabilityZoneFilter,RetryFilter,ComputeFilter,
 DiskFilter,RamFilter,ImagePropertiesFilter,ServerGroupAffinityFilter,
 ServerGroupAntiAffinityFilter,ComputeCapabilitiesFilter,NUMATopologyFilter,
 <span class="bold"><strong>AggregateInstanceExtraSpecsFilter</strong></span>
...</pre></div></li><li class="listitem "><p>
     There is no additional configuration needed because the following is true:
    </p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       The filter assumes <code class="literal">:</code> is a separator
      </p></li><li class="listitem "><p>
       The filter will match all simple keys in extra_specs plus all keys with
       a separator if the prefix is
       <code class="literal">aggregate_instance_extra_specs</code> - for example,
       <code class="literal">image_type=SLE</code> and
       <code class="literal">aggregate_instance_extra_specs:image_type=SLE</code> will
       both be matched against aggregate metadata
       <code class="literal">image_type=SLE</code>
      </p></li></ol></div></li><li class="listitem "><p>
     Add <code class="literal">extra_specs</code> to flavors that should be scheduled
     according to the above.
    </p></li><li class="listitem "><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "Editing nova scheduler filters"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Run the ready deployment playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the nova reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts novan-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect1" id="topic-vhs-12v-vw"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using Flavor Metadata to Specify CPU Model</span> <a title="Permalink" class="permalink" href="#topic-vhs-12v-vw">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-compute-using_flavor_metadata.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute-using_flavor_metadata.xml</li><li><span class="ds-label">ID: </span>topic-vhs-12v-vw</li></ul></div></div></div></div><p>
  <code class="literal">Libvirt</code> is a collection of software used in <span class="productname">OpenStack</span> to
  manage virtualization. It has the ability to emulate a host CPU model in a
  guest VM. In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> nova, the ComputeCapabilitiesFilter limits this
  ability by checking the exact CPU model of the compute host against the
  requested compute instance model. It will only pick compute hosts that have
  the <code class="literal">cpu_model</code> requested by the instance model, and if the
  selected compute host does not have that <code class="literal">cpu_model</code>, the
  ComputeCapabilitiesFilter moves on to find another compute host that matches,
  if possible. Selecting an unavailable vCPU model may cause nova to fail
  with <code class="literal">no valid host found</code>.
 </p><p>
  To assist, there is a nova scheduler filter that captures
  <code class="literal">cpu_models</code> as a subset of a particular CPU family. The
  filter determines if the host CPU model is capable of emulating the guest
  CPU model by maintaining the mapping of the vCPU models and comparing it with
  the host CPU model.
 </p><p>
  There is a limitation when a particular <code class="literal">cpu_model</code> is
  specified with <code class="literal">hw:cpu_model</code> via a compute flavor: the
  <code class="literal">cpu_mode</code> will be set to <code class="literal">custom</code>. This
  mode ensures that a persistent guest virtual machine will see the same
  hardware no matter what host physical machine the guest virtual machine is
  booted on. This allows easier live migration of virtual machines. Because of
  this limitation, only some of the features of a CPU are exposed to the guest.
  Requesting particular CPU features is not supported.
 </p><div class="sect2" id="id-1.5.8.4.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Editing the flavor metadata in the horizon dashboard</span> <a title="Permalink" class="permalink" href="#id-1.5.8.4.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-compute-using_flavor_metadata.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute-using_flavor_metadata.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   These steps can be used to edit a flavor's metadata in the horizon
   dashboard to add the <code class="literal">extra_specs</code> for a
   <code class="literal">cpu_model</code>:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Access the horizon dashboard and log in with admin credentials.
    </p></li><li class="listitem "><p>
     Access the Flavors menu by (A) clicking on the menu button, (B) navigating
     to the Admin section, and then (C) clicking on Flavors:
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-operations-flavor_extraspecs_1.png" target="_blank"><img src="images/media-hos.docs-operations-flavor_extraspecs_1.png" width="" /></a></div></div></li><li class="listitem "><p>
     In the list of flavors, choose the flavor you wish to edit and click on
     the entry under the Metadata column:
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-operations-flavor_extraspecs_2.png" target="_blank"><img src="images/media-hos.docs-operations-flavor_extraspecs_2.png" width="" /></a></div></div><div id="id-1.5.8.4.5.3.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      You can also create a new flavor and then choose that one to edit.
     </p></div></li><li class="listitem "><p>
     In the Custom field, enter <code class="literal">hw:cpu_model</code> and then click
     on the <code class="literal">+</code> (plus) sign to continue:
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-operations-flavor_extraspecs_3.png" target="_blank"><img src="images/media-hos.docs-operations-flavor_extraspecs_3.png" width="" /></a></div></div></li><li class="listitem "><p>
     Then you will want to enter the CPU model into the field that you wish to
     use and then click Save:
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-operations-flavor_extraspecs_4.png" target="_blank"><img src="images/media-hos.docs-operations-flavor_extraspecs_4.png" width="" /></a></div></div></li></ol></div></div></div><div class="sect1" id="topic-pqr-lyx-yw"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Forcing CPU and RAM Overcommit Settings</span> <a title="Permalink" class="permalink" href="#topic-pqr-lyx-yw">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-compute-forcing_overcommit.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute-forcing_overcommit.xml</li><li><span class="ds-label">ID: </span>topic-pqr-lyx-yw</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> supports overcommitting of CPU and RAM resources on compute nodes.
  Overcommitting is a technique of allocating more virtualized CPUs and/or
  memory than there are physical resources.
 </p><p>
  The default settings for this are:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Setting</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td>cpu_allocation_ratio</td><td>16</td><td>
      <p>
       Virtual CPU to physical CPU allocation ratio which affects all CPU
       filters. This configuration specifies a global ratio for CoreFilter.
       For AggregateCoreFilter, it will fall back to this configuration value
       if no per-aggregate setting found.
      </p>
      <div id="id-1.5.8.5.4.1.5.1.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
        This can be set per-compute, or if set to <code class="literal">0.0</code>, the
        value set on the scheduler node(s) will be used and defaulted to
        <code class="literal">16.0</code>.
       </p></div>
     </td></tr><tr><td>ram_allocation_ratio</td><td>1.0</td><td>
      <p>
       Virtual RAM to physical RAM allocation ratio which affects all RAM
       filters. This configuration specifies a global ratio for RamFilter. For
       AggregateRamFilter, it will fall back to this configuration value if no
       per-aggregate setting found.
      </p>
      <div id="id-1.5.8.5.4.1.5.2.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
        This can be set per-compute, or if set to <code class="literal">0.0</code>, the
        value set on the scheduler node(s) will be used and defaulted to
        <code class="literal">1.5</code>.
       </p></div>
     </td></tr><tr><td>disk_allocation_ratio</td><td>1.0</td><td>
      <p>
       This is the virtual disk to physical disk allocation ratio used by the
       disk_filter.py script to determine if a host has sufficient disk space
       to fit a requested instance. A ratio greater than 1.0 will result in
       over-subscription of the available physical disk, which can be useful
       for more efficiently packing instances created with images that do not
       use the entire virtual disk,such as sparse or compressed images. It can
       be set to a value between 0.0 and 1.0 in order to preserve a percentage
       of the disk for uses other than instances.
      </p>
      <div id="id-1.5.8.5.4.1.5.3.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
        This can be set per-compute, or if set to <code class="literal">0.0</code>, the
        value set on the scheduler node(s) will be used and defaulted to
        <code class="literal">1.0</code>.
       </p></div>
     </td></tr></tbody></table></div><div class="sect2" id="change"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Changing the overcommit ratios for your entire environment</span> <a title="Permalink" class="permalink" href="#change">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-compute-forcing_overcommit.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute-forcing_overcommit.xml</li><li><span class="ds-label">ID: </span>change</li></ul></div></div></div></div><p>
   If you wish to change the CPU and/or RAM overcommit ratio settings for your
   entire environment then you can do so via your Cloud Lifecycle Manager with these
   steps.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the nova configuration settings located in this file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/nova/nova.conf.j2</pre></div></li><li class="listitem "><p>
     Add or edit the following lines to specify the ratios you wish to use:
    </p><div class="verbatim-wrap"><pre class="screen">cpu_allocation_ratio = 16
ram_allocation_ratio = 1.0</pre></div></li><li class="listitem "><p>
     Commit your configuration to the Git repository
     (<span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "setting nova overcommit settings"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the nova reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect1" id="enabling-the-nova-resize"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling the Nova Resize and Migrate Features</span> <a title="Permalink" class="permalink" href="#enabling-the-nova-resize">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-compute-enabling_resize.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute-enabling_resize.xml</li><li><span class="ds-label">ID: </span>enabling-the-nova-resize</li></ul></div></div></div></div><p>
  The nova resize and migrate features are disabled by default. If you wish
  to utilize these options, these steps will show you how to enable it in
  your cloud.
 </p><p>
  The two features below are disabled by default:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <span class="bold"><strong>Resize</strong></span> - this feature allows you to
    change the size of a Compute instance by changing its flavor. See the
    <a class="link" href="http://docs.openstack.org/user-guide/cli_change_the_size_of_your_server.html" target="_blank">OpenStack
    User Guide</a> for more details on its use.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Migrate</strong></span> - read about the differences
    between "live" migration (enabled by default) and regular migration
    (disabled by default) in <a class="xref" href="#liveInstMigration" title="15.1.3.3. Live Migration of Instances">Section 15.1.3.3, “Live Migration of Instances”</a>.
   </p></li></ul></div><p>
  These two features are disabled by default because they require passwordless
  SSH access between Compute hosts with the user having access to the file
  systems to perform the copy.
 </p><div class="sect2" id="enable"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling Nova Resize and Migrate</span> <a title="Permalink" class="permalink" href="#enable">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-compute-enabling_resize.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute-enabling_resize.xml</li><li><span class="ds-label">ID: </span>enable</li></ul></div></div></div></div><p>
   If you wish to enable these features, use these steps on your lifecycle
   manager. This will deploy a set of public and private SSH keys to the
   Compute hosts, allowing the <code class="literal">nova</code> user SSH access between
   each of your Compute hosts.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Run the nova reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml --extra-vars nova_migrate_enabled=true</pre></div></li><li class="listitem "><p>
     To ensure that the resize and migration options show up in the horizon
     dashboard, run the horizon reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts horizon-reconfigure.yml</pre></div></li></ol></div></div><div class="sect2" id="disable"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Disabling Nova Resize and Migrate</span> <a title="Permalink" class="permalink" href="#disable">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-compute-enabling_resize.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute-enabling_resize.xml</li><li><span class="ds-label">ID: </span>disable</li></ul></div></div></div></div><p>
   This feature is disabled by default. However, if you have previously enabled
   it and wish to re-disable it, you can use these steps on your lifecycle
   manager. This will remove the set of public and private SSH keys that were
   previously added to the Compute hosts, removing the <code class="literal">nova</code>
   users SSH access between each of your Compute hosts.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Run the nova reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml --extra-vars nova_migrate_enabled=false</pre></div></li><li class="listitem "><p>
     To ensure that the resize and migrate options are removed from the horizon
     dashboard, run the horizon reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts horizon-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect1" id="resize"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling ESX Compute Instance(s) Resize Feature</span> <a title="Permalink" class="permalink" href="#resize">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-compute-enabling_resize_esx_compute.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute-enabling_resize_esx_compute.xml</li><li><span class="ds-label">ID: </span>resize</li></ul></div></div></div></div><p>
  The resize of ESX compute instance is disabled by default. If you want to
  utilize this option, these steps will show you how to configure and enable
  it in your cloud.
 </p><p>
  The following feature is disabled by default:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <span class="bold"><strong>Resize</strong></span> - this feature allows you to
    change the size of a Compute instance by changing its flavor. See the
    <a class="link" href="http://docs.openstack.org/user-guide/cli_change_the_size_of_your_server.html" target="_blank">OpenStack
    User Guide</a> for more details on its use.
   </p></li></ul></div><div class="sect2" id="id-1.5.8.7.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Procedure</span> <a title="Permalink" class="permalink" href="#id-1.5.8.7.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-compute-enabling_resize_esx_compute.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute-enabling_resize_esx_compute.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If you want to configure and re-size ESX compute instance(s), perform the
   following steps:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the <code class="literal">~ /openstack/my_cloud/config/nova/nova.conf.j2</code> to
     add the following parameter under <span class="bold"><strong>Policy</strong></span>:
    </p><div class="verbatim-wrap"><pre class="screen"># Policy
allow_resize_to_same_host=True</pre></div></li><li class="listitem "><p>
     Commit your configuration:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "&lt;commit message&gt;"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div><p>
     By default the nova resize feature is disabled. To enable nova resize,
     refer to <a class="xref" href="#enabling-the-nova-resize" title="6.4. Enabling the Nova Resize and Migrate Features">Section 6.4, “Enabling the Nova Resize and Migrate Features”</a>.
    </p><p>
     By default an ESX console log is not set up. For more details about
     Hypervisor setup, refer to the <a class="link" href="https://docs.openstack.org/nova/rocky/admin/" target="_blank">OpenStack
     documentation</a>.
   </p></li></ol></div></div></div><div class="sect1" id="gpu-passthrough"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">GPU passthrough</span> <a title="Permalink" class="permalink" href="#gpu-passthrough">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-compute-gpu_passthrough.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute-gpu_passthrough.xml</li><li><span class="ds-label">ID: </span>gpu-passthrough</li></ul></div></div></div></div><p>
    GPU passthrough for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> provides the nova instance
    direct access to the GPU device for increased performance.
  </p><p>
    This section demonstrates the steps to pass
    through a Nvidia GPU card supported by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>,
  </p><div id="id-1.5.8.8.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      Resizing the VM to the same host with the same
      PCI card is not supported with PCI passthrough.
    </p></div><p>
    The following steps are necessary to leverage PCI passthrough on a SUSE <span class="productname">OpenStack</span> Cloud
    9 Compute Node: preparing the Compute Node, preparing nova
    via the input model updates and glance. Ensure you follow the
    below procedures in sequence:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     <a class="xref" href="#clm-prepare-comp-node" title="Preparing the Compute Node">Procedure 6.1, “Preparing the Compute Node”</a>
    </p></li><li class="listitem "><p>
     <a class="xref" href="#clm-prep-comp-input-model" title="6.6.1. Preparing nova via the input model updates">Section 6.6.1, “Preparing nova via the input model updates”</a>
    </p></li><li class="listitem "><p>
     <a class="xref" href="#clm-create-flavor" title="6.6.2. Create a flavor">Section 6.6.2, “Create a flavor”</a>
    </p></li></ol></div><div class="procedure " id="clm-prepare-comp-node"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 6.1: </span><span class="name">Preparing the Compute Node </span><a title="Permalink" class="permalink" href="#clm-prepare-comp-node">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
            There should be no kernel drivers or binaries with direct access to the
            PCI device. If there are kernel modules, ensure they are blacklisted.
          </p><p>
            For example, it is common to have a <code class="literal">nouveau</code> driver
            from when the node was installed. This driver is a graphics driver for
            Nvidia-based GPUs. It must be blacklisted as shown in this example:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>echo 'blacklist nouveau' &gt;&gt; /etc/modprobe.d/nouveau-default.conf</pre></div><p>
            The file location and its contents are important, however the name of the file
            is your choice. Other drivers can be blacklisted in the same manner,
            including Nvidia drivers.
          </p></li><li class="step "><p>
            On the host, <code class="literal">iommu_groups</code> is necessary and may
            already be enabled. To check if IOMMU is enabled, run the following
            commands:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code> virt-host-validate
        .....
        QEMU: Checking if IOMMU is enabled by kernel
        : WARN (IOMMU appears to be disabled in kernel. Add intel_iommu=on to kernel cmdline arguments)
        .....</pre></div><p>
            To modify the kernel command line as suggested in the warning, edit
            <code class="filename">/etc/default/grub</code> and append
            <code class="literal">intel_iommu=on</code> to the
            <code class="literal">GRUB_CMDLINE_LINUX_DEFAULT</code> variable.
            Run:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code> update-bootloader</pre></div><p>
            Reboot to enable <code class="literal">iommu_groups</code>.
          </p></li><li class="step "><p>
            After the reboot, check that IOMMU is enabled:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>virt-host-validate
        .....
        QEMU: Checking if IOMMU is enabled by kernel
        : PASS
        .....</pre></div></li><li class="step "><p>
            Confirm IOMMU groups are available by finding the group associated with
            your PCI device (for example Nvidia GPU):
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>lspci -nn | grep -i nvidia
        84:00.0 3D controller [0302]: NVIDIA Corporation GV100GL [Tesla V100 PCIe 16GB] [10de:1db4] (rev
        a1)</pre></div><p>
            In this example, <code class="literal">84:00.0</code> is the address of the PCI device. The vendorID
            is <code class="literal">10de</code>. The product ID is <code class="literal">1db4</code>.
          </p></li><li class="step "><p>
            Confirm that the devices are available for passthrough:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls -ld /sys/kernel/iommu_groups/*/devices/*84:00.?/
        drwxr-xr-x 3 root root 0 Nov 19 17:00 /sys/kernel/iommu_groups/56/devices/0000:84:00.0/</pre></div></li></ol></div></div><div class="sect2" id="clm-prep-comp-input-model"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Preparing nova via the input model updates</span> <a title="Permalink" class="permalink" href="#clm-prep-comp-input-model">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-compute-gpu_passthrough.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute-gpu_passthrough.xml</li><li><span class="ds-label">ID: </span>clm-prep-comp-input-model</li></ul></div></div></div></div><p>
        To implement the required configuration, log into the Cloud Lifecycle Manager node and update
        the Cloud Lifecycle Manager model files to enable GPU passthrough for compute nodes.
      </p><p>
        <span class="bold"><strong>Edit servers.yml</strong></span>
      </p><p>
        Add the <code class="literal">pass-through</code> section after the definition of
        servers section in the <code class="filename">servers.yml</code> file.
        The following example shows only the relevant sections:
      </p><div class="verbatim-wrap"><pre class="screen">        ---
        product:
        version: 2

        baremetal:
        netmask: 255.255.255.0
        subnet: 192.168.100.0


        servers:
        .
        .
        .
        .

          - id: compute-0001
            ip-addr: 192.168.75.5
            role: COMPUTE-ROLE
            server-group: RACK3
            nic-mapping: HP-DL360-4PORT
            ilo-ip: ****
            ilo-user: ****
            ilo-password: ****
            mac-addr: ****
          .
          .
          .

          - id: compute-0008
            ip-addr: 192.168.75.7
            role: COMPUTE-ROLE
            server-group: RACK2
            nic-mapping: HP-DL360-4PORT
            ilo-ip: ****
            ilo-user: ****
            ilo-password: ****
            mac-addr: ****

        pass-through:
          servers:
            - id: compute-0001
              data:
                gpu:
                  - vendor_id: 10de
                    product_id: 1db4
                    bus_address: 0000:84:00.0
                    pf_mode: type-PCI
                    name: a1
                  - vendor_id: 10de
                    product_id: 1db4
                    bus_address: 0000:85:00.0
                    pf_mode: type-PCI
                    name: b1
            - id: compute-0008
              data:
                gpu:
                  - vendor_id: 10de
                    product_id: 1db4
                    pf_mode: type-PCI
                    name: c1</pre></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
            Check out the site branch of the local git repository and
            change to the correct directory:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
        <code class="prompt user">ardana &gt; </code>git checkout site
        <code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition/data/</pre></div></li><li class="step "><p>
            Open the file containing the servers list, for example <code class="filename">servers.yml</code>,
            with your chosen editor. Save the changes to the file and
            commit to the local git repository:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A</pre></div><p> Confirm that the changes to the tree are relevant changes and commit:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git status
        <code class="prompt user">ardana &gt; </code>git commit -m "your commit message goes here in quotes"</pre></div></li><li class="step "><p>
            Enable your changes by running the necessary playbooks:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
        <code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
        <code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
        <code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible</pre></div><p>If you are enabling GPU passthrough for your compute nodes
            during your initial installation, run the following command:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml</pre></div><p>If you are enabling GPU passthrough for your compute nodes
            post-installation, run the following command:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</pre></div></li></ol></div></div><p>
        The above procedure updates the configuration for the nova api,
        nova compute and scheduler as defined in
        <a class="link" href="https://docs.openstack.org/nova/rocky/admin/pci-passthrough.html" target="_blank">https://docs.openstack.org/nova/rocky/admin/pci-passthrough.html</a>.
      </p><p>
        The following is the PCI configuration for the <code class="literal">compute0001</code>
        node using the above example post-playbook run:
      </p><div class="verbatim-wrap"><pre class="screen">        [pci]
        passthrough_whitelist = [{"address": "0000:84:00.0"}, {"address": "0000:85:00.0"}]
        alias = {"vendor_id": "10de", "name": "a1", "device_type": "type-PCI", "product_id": "1db4"}
        alias = {"vendor_id": "10de", "name": "b1", "device_type": "type-PCI", "product_id": "1db4"}</pre></div><p>
        The following is the PCI configuration for <code class="literal">compute0008</code>
        node using the above example post-playbook run:
      </p><div class="verbatim-wrap"><pre class="screen">        [pci]
        passthrough_whitelist = [{"vendor_id": "10de", "product_id": "1db4"}]
        alias = {"vendor_id": "10de", "name": "c1", "device_type": "type-PCI", "product_id": "1db4"}</pre></div><div id="id-1.5.8.8.8.12" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
          After running the <code class="filename">site.yml</code> playbook above,
          reboot the compute nodes that are configured with Intel PCI devices.
        </p></div></div><div class="sect2" id="clm-create-flavor"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create a flavor</span> <a title="Permalink" class="permalink" href="#clm-create-flavor">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-compute-gpu_passthrough.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute-gpu_passthrough.xml</li><li><span class="ds-label">ID: </span>clm-create-flavor</li></ul></div></div></div></div><p>
        For GPU passthrough, set the <code class="literal">pci_passthrough:alias</code>
        property. You can do so for an existing flavor or create a new flavor
        as shown in the example below:
      </p><div class="verbatim-wrap"><pre class="screen">        <code class="prompt user">ardana &gt; </code>openstack flavor create --ram 8192 --disk 100 --vcpu 8 gpuflavor
        <code class="prompt user">ardana &gt; </code>openstack flavor set gpuflavor --property "pci_passthrough:alias"="a1:1"</pre></div><p>Here the <code class="literal">a1</code> references the alias name as provided
      in the model while the <code class="literal">1</code> tells nova that a single GPU
      should be assigned.
      </p><p>
        Boot an instance using the flavor created above:
      </p><div class="verbatim-wrap"><pre class="screen">         <code class="prompt user">ardana &gt; </code>openstack server create --flavor gpuflavor --image sles12sp4 --key-name key --nic net-id=$net_id gpu-instance-1</pre></div></div></div><div class="sect1" id="configure-glance"><div class="titlepage"><div><div><h2 class="title"><span class="number">6.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Image Service</span> <a title="Permalink" class="permalink" href="#configure-glance">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-image-configure_glance.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-image-configure_glance.xml</li><li><span class="ds-label">ID: </span>configure-glance</li></ul></div></div></div></div><p>
  The Image service, based on <span class="productname">OpenStack</span> glance, works out of the box and does
  not need any special configuration. However, we show you how to enable
  glance image caching as well as how to configure your environment to allow
  the glance copy-from feature if you choose to do so. A few features
  detailed below will require some additional configuration if you choose to
  use them.
 </p><div id="image-warning" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
   glance images are assigned IDs upon creation, either automatically or
   specified by the user. The ID of an image should be unique, so if a user
   assigns an ID which already exists, a conflict (409) will occur.
  </p><p>
   This only becomes a problem if users can publicize or share images with
   others. If users can share images AND cannot publicize images then your
   system is not vulnerable. If the system has also been purged (via
   <code class="literal">glance-manage db purge</code>) then it is possible for deleted
   image IDs to be reused.
  </p><p>
   If deleted image IDs can be reused then recycling of public and shared
   images becomes a possibility. This means that a new (or modified) image can
   replace an old image, which could be malicious.
  </p><p>
   If this is a problem for you, please contact Sales Engineering.
  </p></div><div class="sect2" id="idg-all-operations-image-configure-glance-xml-6"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How to enable glance image caching</span> <a title="Permalink" class="permalink" href="#idg-all-operations-image-configure-glance-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-image-configure_glance.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-image-configure_glance.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-image-configure-glance-xml-6</li></ul></div></div></div></div><p>
   In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9, by default, the glance image caching option is not
   enabled. You have the option to have image caching enabled and these steps
   will show you how to do that.
  </p><p>
   The main benefits to using image caching is that it will allow the glance
   service to return the images faster and it will cause less load on other
   services to supply the image.
  </p><p>
   In order to use the image caching option you will need to supply a logical
   volume for the service to use for the caching.
  </p><p>
   If you wish to use the glance image caching option, you will see the
   section below in your
   <code class="literal">~/openstack/my_cloud/definition/data/disks_controller.yml</code>
   file. You will specify the mount point for the logical volume you wish to
   use for this.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Edit your
     <code class="literal">~/openstack/my_cloud/definition/data/disks_controller.yml</code>
     file and specify the volume and mount point for your <code class="literal">glance-cache</code>. Here is
     an example:
    </p><div class="verbatim-wrap"><pre class="screen"># glance cache: if a logical volume with consumer usage glance-cache
# is defined glance caching will be enabled. The logical volume can be
# part of an existing volume group or a dedicated volume group.
 - name: glance-vg
   physical-volumes:
     - /dev/sdx
   logical-volumes:
     - name: glance-cache
       size: 95%
       mount: /var/lib/glance/cache
       fstype: ext4
       mkfs-opts: -O large_file
       consumer:
         name: glance-api
         usage: glance-cache</pre></div><p>
     If you are enabling image caching during your initial installation, prior
     to running <code class="literal">site.yml</code> the first time, then continue with
     the installation steps. However, if you are making this change
     post-installation then you will need to commit your changes with the steps
     below.
    </p></li><li class="step "><p>
     Commit your configuration to the Git repository
     (<span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the glance reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml</pre></div></li></ol></div></div><p>
   An existing volume image cache is not properly deleted when cinder
   detects the source image has changed. After updating any source image,
   delete the cache volume so that the cache is refreshed.
  </p><p>
   The volume image cache must be deleted before trying to use the associated
   source image in any other volume operations. This includes creating bootable
   volumes or booting an instance with <code class="literal">create volume</code> enabled
   and the updated image as the source image.
  </p></div><div class="sect2" id="copyfrom"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Allowing the glance copy-from option in your environment</span> <a title="Permalink" class="permalink" href="#copyfrom">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-image-configure_glance.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-image-configure_glance.xml</li><li><span class="ds-label">ID: </span>copyfrom</li></ul></div></div></div></div><p>
   When creating images, one of the options you have is to copy the image from
   a remote location to your local glance store. You do this by specifying the
   <code class="literal">--copy-from</code> option when creating the image. To use this
   feature though you need to ensure the following conditions are met:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The server hosting the glance service must have network access to the
     remote location that is hosting the image.
    </p></li><li class="listitem "><p>
     There cannot be a proxy between glance and the remote location.
    </p></li><li class="listitem "><p>
     The glance v1 API must be enabled, as v2 does not currently support the
     <code class="literal">copy-from</code> function.
    </p></li><li class="listitem "><p>
     The http glance store must be enabled in the environment, following the
     steps below.
    </p></li></ul></div><p>
   <span class="bold"><strong>Enabling the HTTP glance Store</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the
     <code class="literal">~/openstack/my_cloud/config/glance/glance-api.conf.j2</code>
     file and add <code class="literal">http</code> to the list of glance stores in the
     <code class="literal">[glance_store]</code> section as seen below in bold:
    </p><div class="verbatim-wrap"><pre class="screen">[glance_store]
stores = {{ glance_stores }}<span class="bold"><strong>, http</strong></span></pre></div></li><li class="listitem "><p>
     Commit your configuration to the Git repository
     (<span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the glance reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml</pre></div></li><li class="listitem "><p>
     Run the horizon reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts horizon-reconfigure.yml</pre></div></li></ol></div></div></div></div><div class="chapter " id="ops-managing-esx"><div class="titlepage"><div><div><h1 class="title"><span class="number">7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing ESX</span> <a title="Permalink" class="permalink" href="#ops-managing-esx">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-managing_esx.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_esx.xml</li><li><span class="ds-label">ID: </span>ops-managing-esx</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#topic-odg-33x-rt"><span class="number">7.1 </span><span class="name">Networking for ESXi Hypervisor (OVSvApp)</span></a></span></dt><dt><span class="section"><a href="#verify-neutron"><span class="number">7.2 </span><span class="name">Validating the neutron Installation</span></a></span></dt><dt><span class="section"><a href="#sec-esx-remove-cluster"><span class="number">7.3 </span><span class="name">Removing a Cluster from the Compute Resource Pool</span></a></span></dt><dt><span class="section"><a href="#sec-esx-remove-esxi-host"><span class="number">7.4 </span><span class="name">Removing an ESXi Host from a Cluster</span></a></span></dt><dt><span class="section"><a href="#sec-esx-debug"><span class="number">7.5 </span><span class="name">Configuring Debug Logging</span></a></span></dt><dt><span class="section"><a href="#topic-ijt-dyh-rt"><span class="number">7.6 </span><span class="name">Making Scale Configuration Changes</span></a></span></dt><dt><span class="section"><a href="#idg-all-operations-monitoring-vcenter-clusters-xml-1"><span class="number">7.7 </span><span class="name">Monitoring vCenter Clusters</span></a></span></dt><dt><span class="section"><a href="#ovsvapp-monitoring"><span class="number">7.8 </span><span class="name">Monitoring Integration with OVSvApp Appliance</span></a></span></dt></dl></div></div><p>
  Information about managing and configuring the ESX service.
 </p><div class="sect1" id="topic-odg-33x-rt"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networking for ESXi Hypervisor (OVSvApp)</span> <a title="Permalink" class="permalink" href="#topic-odg-33x-rt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-network_esx_ovsvapp.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-network_esx_ovsvapp.xml</li><li><span class="ds-label">ID: </span>topic-odg-33x-rt</li></ul></div></div></div></div><p>
  To provide the network as a service for tenant VM's hosted on ESXi
  Hypervisor, a service VM called <code class="literal">OVSvApp VM</code> is deployed on
  each ESXi Hypervisor within a cluster managed by OpenStack nova, as shown
  in the following figure.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-esx_ovsvapp.png" target="_blank"><img src="images/media-esx-esx_ovsvapp.png" width="" /></a></div></div><p>
  The OVSvApp VM runs SLES as a guest operating system, and has Open vSwitch
  2.1.0 or above installed. It also runs an agent called <code class="literal">OVSvApp
  agent</code>, which is responsible for dynamically creating the port
  groups for the tenant VMs and manages OVS bridges, which contain the flows
  related to security groups and L2 networking.
 </p><p>
  To facilitate fault tolerance and mitigation of data path loss for tenant
  VMs, run the <span class="bold"><strong>neutron-ovsvapp-agent-monitor</strong></span>
  process as part of the <span class="bold"><strong>neutron-ovsvapp-agent
  service</strong></span>, responsible for monitoring the Open vSwitch module within
  the OVSvApp VM. It also uses a <code class="literal">nginx</code> server to provide the
  health status of the Open vSwitch module to the neutron server for mitigation
  actions. There is a mechanism to keep the
  <span class="bold"><strong>neutron-ovsvapp-agent service</strong></span> alive through
  a <code class="literal">systemd</code> script.
 </p><p>
  When a OVSvApp Service VM crashes, an agent monitoring mechanism starts a
  cluster mitigation process. You can mitigate data path traffic loss for VMs
  on the failed ESX host in that cluster by putting the failed ESX host in the
  maintenance mode. This, in turn, triggers the vCenter DRS migrates tenant VMs
  to other ESX hosts within the same cluster. This ensures data path continuity
  of tenant VMs traffic.
 </p><p>
  <span class="bold"><strong>View Cluster Mitigation</strong></span>
 </p><div id="id-1.5.9.3.8" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   Install <code class="literal">python-networking-vsphere</code> so that neutron
   ovsvapp commands will work properly.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo zypper in python-networking-vsphere</pre></div></div><p>
  An administrator can view cluster mitigation status using the following
  commands.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <code class="literal">neutron ovsvapp-mitigated-cluster-list</code>
   </p><p>
    Lists all the clusters where at least one round of host mitigation has
    happened.
   </p><p>
    Example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron ovsvapp-mitigated-cluster-list
+----------------+--------------+-----------------------+---------------------------+
| vcenter_id     | cluster_id   | being_mitigated       | threshold_reached         |
+----------------+--------------+-----------------------+---------------------------+
| vcenter1       | cluster1     | True                  | False                     |
| vcenter2       | cluster2     | False                 | True                      |
+---------------+------------+-----------------+------------------------------------+</pre></div></li><li class="listitem "><p>
    <code class="literal">neutron ovsvapp-mitigated-cluster-show --vcenter-id
    &lt;VCENTER_ID&gt; --cluster-id &lt;CLUSTER_ID&gt;</code>
   </p><p>
    Shows the status of a particular cluster.
   </p><p>
    Example :
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron ovsvapp-mitigated-cluster-show --vcenter-id vcenter1 --cluster-id cluster1
+---------------------------+-------------+
| Field                     | Value       |
+---------------------------+-------------+
| being_mitigated           | True        |
| cluster_id                | cluster1    |
| threshold_reached         | False       |
| vcenter_id                | vcenter1    |
+---------------------------+-------------+</pre></div><p>
    There can be instances where a triggered mitigation may not succeed and the
    neutron server is not informed of such failure (for example, if the
    selected agent which had to mitigate the host, goes down before finishing
    the task). In this case, the cluster will be locked. To unlock the cluster
    for further mitigations, use the update command.
   </p></li><li class="listitem "><p>
    <code class="literal">neutron ovsvapp-mitigated-cluster-update --vcenter-id
    &lt;VCENTER_ID&gt; --cluster-id &lt;CLUSTER_ID&gt;</code>
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Update the status of a mitigated cluster:
     </p><p>
      Modify the values of <span class="bold"><strong>being-mitigated</strong></span>
      from <span class="bold"><strong>True</strong></span> to
      <span class="bold"><strong>False</strong></span> to unlock the cluster.
     </p><p>
      Example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron ovsvapp-mitigated-cluster-update --vcenter-id vcenter1 --cluster-id cluster1 --being-mitigated False</pre></div></li><li class="listitem "><p>
      Update the threshold value:
     </p><p>
      Update the threshold-reached value to
      <span class="bold"><strong>True</strong></span>, if no further migration is
      required in the selected cluster.
     </p><p>
      Example :
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>neutron ovsvapp-mitigated-cluster-update --vcenter-id vcenter1 --cluster-id cluster1 --being-mitigated False --threshold-reached True</pre></div></li></ul></div><p>
    <span class="bold"><strong>Rest API</strong></span>
   </p><div class="itemizedlist " id="ul-rkp-4kx-rt"><ul class="itemizedlist"><li class="listitem "><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>curl -i -X GET http://&lt;ip&gt;:9696/v2.0/ovsvapp_mitigated_clusters \
  -H "User-Agent: python-neutronclient" -H "Accept: application/json" -H \
  "X-Auth-Token: &lt;token_id&gt;"</pre></div></li></ul></div></li></ul></div><div class="sect2" id="id-1.5.9.3.11"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">More Information</span> <a title="Permalink" class="permalink" href="#id-1.5.9.3.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-network_esx_ovsvapp.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-network_esx_ovsvapp.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   For more information on the Networking for ESXi Hypervisor (OVSvApp), see
   the following references:
  </p><div class="itemizedlist " id="ul-b2c-n2c-vt"><ul class="itemizedlist"><li class="listitem "><p>
     VBrownBag session in Vancouver OpenStack Liberty Summit:
    </p><p>
     <a class="link" href="https://www.youtube.com/watch?v=icYA_ixhwsM&amp;feature=youtu.be" target="_blank">https://www.youtube.com/watch?v=icYA_ixhwsM&amp;feature=youtu.be</a>
    </p></li><li class="listitem "><p>
     Wiki Link:
    </p><p>
     <a class="link" href="https://wiki.openstack.org/wiki/Neutron/Networking-vSphere" target="_blank">https://wiki.openstack.org/wiki/Neutron/Networking-vSphere</a>
    </p></li><li class="listitem "><p>
     Codebase:
    </p><p>
     <a class="link" href="https://github.com/openstack/networking-vsphere/" target="_blank">https://github.com/openstack/networking-vsphere/</a>
    </p></li><li class="listitem "><p>
     Whitepaper:
    </p><p>
     <a class="link" href="https://github.com/hp-networking/ovsvapp/blob/master/OVSvApp_Solution.pdf" target="_blank">https://github.com/hp-networking/ovsvapp/blob/master/OVSvApp_Solution.pdf</a>
    </p></li></ul></div></div></div><div class="sect1" id="verify-neutron"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Validating the neutron Installation</span> <a title="Permalink" class="permalink" href="#verify-neutron">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-enable_new_cluster_compute_resource.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-enable_new_cluster_compute_resource.xml</li><li><span class="ds-label">ID: </span>verify-neutron</li></ul></div></div></div></div><p>
   You can validate that the ESX compute cluster is added to the cloud
   successfully using the following command:
  </p><div class="verbatim-wrap"><pre class="screen"># openstack network agent list

+------------------+----------------------+-----------------------+-------------------+-------+----------------+---------------------------+
| id               | agent_type           | host                  | availability_zone | alive | admin_state_up | binary                    |
+------------------+----------------------+-----------------------+-------------------+-------+----------------+---------------------------+
| 05ca6ef...999c09 | L3 agent             | doc-cp1-comp0001-mgmt | nova              | :-)   | True           | neutron-l3-agent          |
| 3b9179a...28e2ef | Metadata agent       | doc-cp1-comp0001-mgmt |                   | :-)   | True           | neutron-metadata-agent    |
| 4e8f84f...c9c58f | Metadata agent       | doc-cp1-comp0002-mgmt |                   | :-)   | True           | neutron-metadata-agent    |
| 55a5791...c17451 | L3 agent             | doc-cp1-c1-m1-mgmt    | nova              | :-)   | True           | neutron-vpn-agent         |
| 5e3db8f...87f9be | Open vSwitch agent   | doc-cp1-c1-m1-mgmt    |                   | :-)   | True           | neutron-openvswitch-agent |
| 6968d9a...b7b4e9 | L3 agent             | doc-cp1-c1-m2-mgmt    | nova              | :-)   | True           | neutron-vpn-agent         |
| 7b02b20...53a187 | Metadata agent       | doc-cp1-c1-m2-mgmt    |                   | :-)   | True           | neutron-metadata-agent    |
| 8ece188...5c3703 | Open vSwitch agent   | doc-cp1-comp0002-mgmt |                   | :-)   | True           | neutron-openvswitch-agent |
| 8fcb3c7...65119a | Metadata agent       | doc-cp1-c1-m1-mgmt    |                   | :-)   | True           | neutron-metadata-agent    |
| 9f48967...36effe | OVSvApp agent        | doc-cp1-comp0002-mgmt |                   | :-)   | True           | ovsvapp-agent             |
| a2a0b78...026da9 | Open vSwitch agent   | doc-cp1-comp0001-mgmt |                   | :-)   | True           | neutron-openvswitch-agent |
| a2fbd4a...28a1ac | DHCP agent           | doc-cp1-c1-m2-mgmt    | nova              | :-)   | True           | neutron-dhcp-agent        |
| b2428d5...ee60b2 | DHCP agent           | doc-cp1-c1-m1-mgmt    | nova              | :-)   | True           | neutron-dhcp-agent        |
| c0983a6...411524 | Open vSwitch agent   | doc-cp1-c1-m2-mgmt    |                   | :-)   | True           | neutron-openvswitch-agent |
| c32778b...a0fc75 | L3 agent             | doc-cp1-comp0002-mgmt | nova              | :-)   | True           | neutron-l3-agent          |
+------------------+----------------------+-----------------------+-------------------+-------+----------------+---------------------------+</pre></div></div><div class="sect1" id="sec-esx-remove-cluster"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Removing a Cluster from the Compute Resource Pool</span> <a title="Permalink" class="permalink" href="#sec-esx-remove-cluster">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-remove_existing_cluster_compute_resource_pool.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-remove_existing_cluster_compute_resource_pool.xml</li><li><span class="ds-label">ID: </span>sec-esx-remove-cluster</li></ul></div></div></div></div><div class="sect2" id="idg-all-esx-remove-existing-cluster-compute-resource-pool-xml-6"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#idg-all-esx-remove-existing-cluster-compute-resource-pool-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-remove_existing_cluster_compute_resource_pool.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-remove_existing_cluster_compute_resource_pool.xml</li><li><span class="ds-label">ID: </span>idg-all-esx-remove-existing-cluster-compute-resource-pool-xml-6</li></ul></div></div></div></div><p>
   Write down the Hostname and ESXi configuration IP addresses of OVSvAPP VMs
   of that ESX cluster before deleting the VMs. These IP address and Hostname
   will be used to cleanup monasca alarm definitions.
  </p><p>
   Perform the following steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Login to vSphere client.
    </p></li><li class="step "><p>
     Select the ovsvapp node running on each ESXi host and click
     <span class="bold"><strong>Summary</strong></span> tab as shown in the following
     example.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-esx_hostname.png" target="_blank"><img src="images/media-esx-esx_hostname.png" width="" /></a></div></div><p>
     Similarly you can retrieve the compute-proxy node information.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-esx_cluster2.png" target="_blank"><img src="images/media-esx-esx_cluster2.png" width="" /></a></div></div></li></ol></div></div></div><div class="sect2" id="id-1.5.9.5.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Removing an existing cluster from the compute resource pool</span> <a title="Permalink" class="permalink" href="#id-1.5.9.5.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-remove_existing_cluster_compute_resource_pool.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-remove_existing_cluster_compute_resource_pool.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Perform the following steps to remove an existing cluster from the compute
   resource pool.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Run the following command to check for the instances launched in that
     cluster:
    </p><div class="verbatim-wrap"><pre class="screen"># openstack server list --host &lt;hostname&gt;
+--------------------------------------+------+--------+------------+-------------+------------------+
| ID                                   | Name | Status | Task State | Power State | Networks         |
+--------------------------------------+------+--------+------------+-------------+------------------+
| 80e54965-758b-425e-901b-9ea756576331 | VM1  | ACTIVE | -          | Running     | private=10.0.0.2 |
+--------------------------------------+------+--------+------------+-------------+------------------+</pre></div><p>
     where:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>hostname</strong></span>: Specifies hostname of the
       compute proxy present in that cluster.
      </p></li></ul></div></li><li class="step "><p>
     Delete all instances spawned in that cluster:
    </p><div class="verbatim-wrap"><pre class="screen"># openstack server delete &lt;server&gt; [&lt;server ...&gt;]</pre></div><p>
     where:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>server</strong></span>: Specifies the name or ID of
       server (s)
      </p></li></ul></div><p>
     OR
    </p><p>
     Migrate all instances spawned in that cluster.
    </p><div class="verbatim-wrap"><pre class="screen"># openstack server migrate &lt;server&gt;</pre></div></li><li class="step "><p>
     Run the following playbooks for stop the Compute (nova) and Networking
     (neutron) services:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-stop --limit &lt;hostname&gt;;
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-stop --limit &lt;hostname&gt;;</pre></div><p>
     where:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="bold"><strong>hostname</strong></span>: Specifies hostname of the
       compute proxy present in that cluster.
      </p></li></ul></div></li></ol></div></div></div><div class="sect2" id="id-1.5.9.5.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cleanup monasca-agent for OVSvAPP Service</span> <a title="Permalink" class="permalink" href="#id-1.5.9.5.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-remove_existing_cluster_compute_resource_pool.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-remove_existing_cluster_compute_resource_pool.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Perform the following procedure to cleanup monasca agents for ovsvapp-agent
   service.
  </p><div class="procedure " id="idg-all-esx-remove-existing-cluster-compute-resource-pool-xml-14"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     If monasca-API is installed on different node, copy the
     <code class="literal">service.orsc</code> from Cloud Lifecycle Manager to monasca API server.
    </p><div class="verbatim-wrap"><pre class="screen">scp service.orsc $USER@ardana-cp1-mtrmon-m1-mgmt:</pre></div></li><li class="step "><p>
     SSH to monasca API server. You must SSH to each monasca API server for
     cleanup.
    </p><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">ssh ardana-cp1-mtrmon-m1-mgmt</pre></div></li><li class="step "><p>
     Edit <code class="literal">/etc/monasca/agent/conf.d/host_alive.yaml</code> file to
     remove the reference to the OVSvAPP you removed. This requires
     <code class="command">sudo</code> access.
    </p><div class="verbatim-wrap"><pre class="screen">sudo vi /etc/monasca/agent/conf.d/host_alive.yaml</pre></div><p>
     A sample of <code class="literal">host_alive.yaml</code>:
    </p><div class="verbatim-wrap"><pre class="screen">- alive_test: ping
  built_by: HostAlive
  host_name: esx-cp1-esx-ovsvapp0001-mgmt
  name: esx-cp1-esx-ovsvapp0001-mgmt ping
  target_hostname: esx-cp1-esx-ovsvapp0001-mgmt</pre></div><p>
     where <em class="replaceable ">HOST_NAME</em> and
     <em class="replaceable ">TARGET_HOSTNAME</em> is mentioned at the DNS name
     field at the vSphere client. (Refer to
     <a class="xref" href="#idg-all-esx-remove-existing-cluster-compute-resource-pool-xml-6" title="7.3.1. Prerequisites">Section 7.3.1, “Prerequisites”</a>).
    </p></li><li class="step "><p>
     After removing the reference on each of the monasca API servers, restart
     the monasca-agent on each of those servers by executing the following
     command.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo service openstack-monasca-agent restart</pre></div></li><li class="step "><p>
     With the OVSvAPP references removed and the monasca-agent restarted, you
     can delete the corresponding alarm to complete the cleanup process. We
     recommend using the monasca CLI which is installed on each of your monasca
     API servers by default. Execute the following command from the monasca API
     server (for example: <code class="literal">ardana-cp1-mtrmon-mX-mgmt</code>).
    </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-list --metric-name host_alive_status --metric-dimensions hostname=&lt;ovsvapp deleted&gt;</pre></div><p>
     For example: You can execute the following command to get the alarm ID, if
     the OVSvAPP appears as a preceding example.
    </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-list --metric-name host_alive_status --metric-dimensions hostname=MCP-VCP-cpesx-esx-ovsvapp0001-mgmt
+--------------------------------------+--------------------------------------+-----------------------+-------------------+-------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+
| id                                   | alarm_definition_id                  | alarm_definition_name | metric_name       | metric_dimensions                         | severity | state | lifecycle_state | link | state_updated_timestamp  | updated_timestamp        | created_timestamp        |
+--------------------------------------+--------------------------------------+-----------------------+-------------------+-------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+
| cfc6bfa4-2485-4319-b1e5-0107886f4270 | cca96c53-a927-4b0a-9bf3-cb21d28216f3 | Host Status           | host_alive_status | service: system                           | HIGH     | OK    | None            | None | 2016-10-27T06:33:04.256Z | 2016-10-27T06:33:04.256Z | 2016-10-23T13:41:57.258Z |
|                                      |                                      |                       |                   | cloud_name: entry-scale-kvm-esx-mml       |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | test_type: ping                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | hostname: ardana-cp1-esx-ovsvapp0001-mgmt |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | control_plane: control-plane-1            |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cluster: mtrmon                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | observer_host: ardana-cp1-mtrmon-m1-mgmt  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       | host_alive_status | service: system                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cloud_name: entry-scale-kvm-esx-mml       |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | test_type: ping                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | hostname: ardana-cp1-esx-ovsvapp0001-mgmt |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | control_plane: control-plane-1            |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cluster: mtrmon                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | observer_host: ardana-cp1-mtrmon-m3-mgmt  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       | host_alive_status | service: system                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cloud_name: entry-scale-kvm-esx-mml       |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | test_type: ping                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | hostname: ardana-cp1-esx-ovsvapp0001-mgmt |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | control_plane: control-plane-1            |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cluster: mtrmon                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | observer_host: ardana-cp1-mtrmon-m2-mgmt  |          |       |                 |      |                          |                          |                          |
+--------------------------------------+--------------------------------------+-----------------------+-------------------+-------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+</pre></div></li><li class="step "><p>
     Delete the monasca alarm.
    </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-delete &lt;alarm ID&gt;</pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-delete cfc6bfa4-2485-4319-b1e5-0107886f4270Successfully deleted alarm</pre></div><p>
     After deleting the alarms and updating the monasca-agent configuration,
     those alarms will be removed from the Operations Console UI. You can login to
     Operations Console and view the status.
    </p></li></ol></div></div></div><div class="sect2" id="id-1.5.9.5.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Removing the Compute Proxy from Monitoring</span> <a title="Permalink" class="permalink" href="#id-1.5.9.5.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-remove_existing_cluster_compute_resource_pool.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-remove_existing_cluster_compute_resource_pool.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Once you have removed the Compute proxy, the alarms against them will still
   trigger. Therefore to resolve this, you must perform the following steps.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     SSH to monasca API server. You must SSH to each monasca API server for
     cleanup.
    </p><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">ssh ardana-cp1-mtrmon-m1-mgmt</pre></div></li><li class="step "><p>
     Edit <code class="literal">/etc/monasca/agent/conf.d/host_alive.yaml</code> file to
     remove the reference to the Compute proxy you removed. This requires
     <code class="command">sudo</code> access.
    </p><div class="verbatim-wrap"><pre class="screen">sudo vi /etc/monasca/agent/conf.d/host_alive.yaml</pre></div><p>
     A sample of <code class="literal">host_alive.yaml</code> file.
    </p><div class="verbatim-wrap"><pre class="screen">- alive_test: ping
  built_by: HostAlive
  host_name: MCP-VCP-cpesx-esx-comp0001-mgmt
  name: MCP-VCP-cpesx-esx-comp0001-mgmt ping</pre></div></li><li class="step "><p>
     Once you have removed the references on each of your monasca API servers,
     execute the following command to restart the monasca-agent on each of
     those servers.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo service openstack-monasca-agent restart</pre></div></li><li class="step "><p>
     With the Compute proxy references removed and the monasca-agent restarted,
     delete the corresponding alarm to complete this process. complete the
     cleanup process. We recommend using the monasca CLI which is installed on
     each of your monasca API servers by default.
    </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-list --metric-dimensions hostname= &lt;compute node deleted&gt;</pre></div><p>
     For example: You can execute the following command to get the alarm ID, if
     the Compute proxy appears as a preceding example.
    </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-list --metric-dimensions hostname=ardana-cp1-comp0001-mgmt</pre></div></li><li class="step "><p>
     Delete the monasca alarm
    </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-delete &lt;alarm ID&gt;</pre></div></li></ol></div></div></div><div class="sect2" id="sec-esx-clean-monasca"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cleaning the monasca Alarms Related to ESX Proxy and vCenter Cluster</span> <a title="Permalink" class="permalink" href="#sec-esx-clean-monasca">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-remove_existing_cluster_compute_resource_pool.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-remove_existing_cluster_compute_resource_pool.xml</li><li><span class="ds-label">ID: </span>sec-esx-clean-monasca</li></ul></div></div></div></div><p>
   Perform the following procedure:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step " id="st-esx-clean-monasca-alarm"><p>
     Using the ESX proxy hostname, execute the following command to list all
     alarms.
    </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-list --metric-dimensions hostname=<em class="replaceable ">COMPUTE_NODE_DELETED</em></pre></div><p>
     where <em class="replaceable ">COMPUTE_NODE_DELETED</em> - hostname is taken
     from the vSphere client (refer to
     <a class="xref" href="#idg-all-esx-remove-existing-cluster-compute-resource-pool-xml-6" title="7.3.1. Prerequisites">Section 7.3.1, “Prerequisites”</a>).
    </p><div id="id-1.5.9.5.6.3.1.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      Make a note of all the alarm IDs that are displayed after executing the
      preceding command.
     </p></div><p>
     For example, the compute proxy hostname is
     <code class="literal">MCP-VCP-cpesx-esx-comp0001-mgmt</code>.
    </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-list --metric-dimensions hostname=MCP-VCP-cpesx-esx-comp0001-mgmt
ardana@R28N6340-701-cp1-c1-m1-mgmt:~$ monasca alarm-list --metric-dimensions hostname=R28N6340-701-cp1-esx-comp0001-mgmt
+--------------------------------------+--------------------------------------+------------------------+------------------------+--------------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+
| id                                   | alarm_definition_id                  | alarm_definition_name  | metric_name            | metric_dimensions                                | severity | state | lifecycle_state | link | state_updated_timestamp  | updated_timestamp        | created_timestamp        |
+--------------------------------------+--------------------------------------+------------------------+------------------------+--------------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+
| 02342bcb-da81-40db-a262-09539523c482 | 3e302297-0a36-4f0e-a1bd-03402b937a4e | HTTP Status            | http_status            | service: compute                                 | HIGH     | OK    | None            | None | 2016-11-11T06:58:11.717Z | 2016-11-11T06:58:11.717Z | 2016-11-10T08:55:45.136Z |
|                                      |                                      |                        |                        | cloud_name: entry-scale-esx-kvm                  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | url: https://10.244.209.9:8774                   |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | hostname: R28N6340-701-cp1-esx-comp0001-mgmt     |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | component: nova-api                              |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | control_plane: control-plane-1                   |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | cluster: esx-compute                             |          |       |                 |      |                          |                          |                          |
| 04cb36ce-0c7c-4b4c-9ebc-c4011e2f6c0a | 15c593de-fa54-4803-bd71-afab95b980a4 | Disk Usage             | disk.space_used_perc   | mount_point: /proc/sys/fs/binfmt_misc            | HIGH     | OK    | None            | None | 2016-11-10T08:52:52.886Z | 2016-11-10T08:52:52.886Z | 2016-11-10T08:51:29.197Z |
|                                      |                                      |                        |                        | service: system                                  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | cloud_name: entry-scale-esx-kvm                  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | hostname: R28N6340-701-cp1-esx-comp0001-mgmt     |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | control_plane: control-plane-1                   |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | cluster: esx-compute                             |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                        |                        | device: systemd-1                                |          |       |                 |      |                          |                          |                          |
+--------------------------------------+--------------------------------------+------------------------+------------------------+--------------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+</pre></div></li><li class="step "><p>
     Delete the alarm using the alarm IDs.
    </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-delete &lt;alarm ID&gt;</pre></div><p>
     Perform this step for all alarm IDs listed from the preceding step
     (<a class="xref" href="#st-esx-clean-monasca-alarm" title="Step 1">Step 1</a>).
    </p><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-delete 1cc219b1-ce4d-476b-80c2-0cafa53e1a12</pre></div></li></ol></div></div></div></div><div class="sect1" id="sec-esx-remove-esxi-host"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Removing an ESXi Host from a Cluster</span> <a title="Permalink" class="permalink" href="#sec-esx-remove-esxi-host">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-removing_esx_host_from_cluster.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-removing_esx_host_from_cluster.xml</li><li><span class="ds-label">ID: </span>sec-esx-remove-esxi-host</li></ul></div></div></div></div><p>
  This topic describes how to remove an existing ESXi host from a cluster and
  clean up of services for OVSvAPP VM.
 </p><div id="id-1.5.9.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   Before performing this procedure, wait until VCenter migrates all the tenant
   VMs to other active hosts in that same cluster.
  </p></div><div class="sect2" id="sec-esxi-host-pre"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisite</span> <a title="Permalink" class="permalink" href="#sec-esxi-host-pre">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-removing_esx_host_from_cluster.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-removing_esx_host_from_cluster.xml</li><li><span class="ds-label">ID: </span>sec-esxi-host-pre</li></ul></div></div></div></div><p>
   Write down the Hostname and ESXi configuration IP addresses of OVSvAPP VMs
   of that ESX cluster before deleting the VMs. These IP address and Hostname
   will be used to clean up monasca alarm definitions.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Login to vSphere client.
    </p></li><li class="listitem "><p>
     Select the ovsvapp node running on the ESXi host and click
     <span class="bold"><strong>Summary</strong></span> tab.
    </p></li></ol></div></div><div class="sect2" id="id-1.5.9.6.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Procedure</span> <a title="Permalink" class="permalink" href="#id-1.5.9.6.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-removing_esx_host_from_cluster.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-removing_esx_host_from_cluster.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Right-click and put the host in the maintenance mode. This will
     automatically migrate all the tenant VMs except OVSvApp.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-eon_maintenance.png" target="_blank"><img src="images/media-esx-eon_maintenance.png" width="" /></a></div></div></li><li class="listitem "><p>
     Cancel the maintenance mode task.
    </p></li><li class="listitem "><p>
     Right-click the <span class="bold"><strong>ovsvapp VM (IP Address)</strong></span>
     node, select <span class="bold"><strong>Power</strong></span>, and then click
     <span class="bold"><strong>Power Off</strong></span>.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-eon_poweroff_ovsvapp.png" target="_blank"><img src="images/media-esx-eon_poweroff_ovsvapp.png" width="" /></a></div></div></li><li class="listitem "><p>
     Right-click the node and then click <span class="bold"><strong>Delete from
     Disk</strong></span>.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-eon_delete_ovsvapp.png" target="_blank"><img src="images/media-esx-eon_delete_ovsvapp.png" width="" /></a></div></div></li><li class="listitem "><p>
     Right-click the <span class="bold"><strong>Host</strong></span>, and then click
     <span class="bold"><strong>Enter Maintenance Mode</strong></span>.
    </p></li><li class="listitem "><p>
     Disconnect the VM. Right-click the VM, and then click
     <span class="bold"><strong>Disconnect</strong></span>.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-eon_disconnect_maintenance.png" target="_blank"><img src="images/media-esx-eon_disconnect_maintenance.png" width="" /></a></div></div></li></ol></div><p>
   The ESXi node is removed from the vCenter.
  </p></div><div class="sect2" id="id-1.5.9.6.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Clean up neutron-agent for OVSvAPP Service</span> <a title="Permalink" class="permalink" href="#id-1.5.9.6.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-removing_esx_host_from_cluster.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-removing_esx_host_from_cluster.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   After removing ESXi node from a vCenter, perform the following procedure to
   clean up neutron agents for ovsvapp-agent service.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Login to Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Source the credentials.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source service.osrc</pre></div></li><li class="listitem "><p>
     Execute the following command.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network agent list | grep &lt;OVSvapp hostname&gt;</pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network agent list | grep MCP-VCP-cpesx-esx-ovsvapp0001-mgmt
| 92ca8ada-d89b-43f9-b941-3e0cd2b51e49 | OVSvApp Agent      | MCP-VCP-cpesx-esx-ovsvapp0001-mgmt |                   | :-)   | True           | ovsvapp-agent             |</pre></div></li><li class="listitem "><p>
     Delete the OVSvAPP agent.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network agent delete &lt;Agent -ID&gt;</pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network agent delete 92ca8ada-d89b-43f9-b941-3e0cd2b51e49</pre></div></li></ol></div><p>
   If you have more than one host, perform the preceding procedure for all the
   hosts.
  </p></div><div class="sect2" id="id-1.5.9.6.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Clean up monasca-agent for OVSvAPP Service</span> <a title="Permalink" class="permalink" href="#id-1.5.9.6.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-removing_esx_host_from_cluster.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-removing_esx_host_from_cluster.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Perform the following procedure to clean up monasca agents for ovsvapp-agent
   service.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     If monasca-API is installed on different node, copy the
     <code class="literal">service.orsc</code> from Cloud Lifecycle Manager to monasca API server.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>scp service.orsc $USER@ardana-cp1-mtrmon-m1-mgmt:</pre></div></li><li class="listitem "><p>
     SSH to monasca API server. You must SSH to each monasca API server for
     cleanup.
    </p><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ssh ardana-cp1-mtrmon-m1-mgmt</pre></div></li><li class="listitem "><p>
     Edit <code class="literal">/etc/monasca/agent/conf.d/host_alive.yaml</code> file to
     remove the reference to the OVSvAPP you removed. This requires
     <code class="command">sudo</code> access.
    </p><div class="verbatim-wrap"><pre class="screen">sudo vi /etc/monasca/agent/conf.d/host_alive.yaml</pre></div><p>
     A sample of <code class="literal">host_alive.yaml</code>:
    </p><div class="verbatim-wrap"><pre class="screen">- alive_test: ping
  built_by: HostAlive
  host_name: MCP-VCP-cpesx-esx-ovsvapp0001-mgmt
  name: MCP-VCP-cpesx-esx-ovsvapp0001-mgmt ping
  target_hostname: MCP-VCP-cpesx-esx-ovsvapp0001-mgmt</pre></div><p>
     where <code class="literal">host_name</code> and <code class="literal">target_hostname</code>
     are mentioned at the DNS name field at the vSphere client. (Refer to
     <a class="xref" href="#sec-esxi-host-pre" title="7.4.1. Prerequisite">Section 7.4.1, “Prerequisite”</a>).
    </p></li><li class="listitem "><p>
     After removing the reference on each of the monasca API servers, restart
     the monasca-agent on each of those servers by executing the following
     command.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo service openstack-monasca-agent restart</pre></div></li><li class="listitem "><p>
     With the OVSvAPP references removed and the monasca-agent restarted, you
     can delete the corresponding alarm to complete the cleanup process. We
     recommend using the monasca CLI which is installed on each of your monasca
     API servers by default. Execute the following command from the monasca API
     server (for example: <code class="literal">ardana-cp1-mtrmon-mX-mgmt</code>).
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>monasca alarm-list --metric-name host_alive_status --metric-dimensions hostname=&lt;ovsvapp deleted&gt;</pre></div><p>
     For example: You can execute the following command to get the alarm ID, if
     the OVSvAPP appears as a preceding example.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>monasca alarm-list --metric-name host_alive_status --metric-dimensions hostname=MCP-VCP-cpesx-esx-ovsvapp0001-mgmt
+--------------------------------------+--------------------------------------+-----------------------+-------------------+-------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+
| id                                   | alarm_definition_id                  | alarm_definition_name | metric_name       | metric_dimensions                         | severity | state | lifecycle_state | link | state_updated_timestamp  | updated_timestamp        | created_timestamp        |
+--------------------------------------+--------------------------------------+-----------------------+-------------------+-------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+
| cfc6bfa4-2485-4319-b1e5-0107886f4270 | cca96c53-a927-4b0a-9bf3-cb21d28216f3 | Host Status           | host_alive_status | service: system                           | HIGH     | OK    | None            | None | 2016-10-27T06:33:04.256Z | 2016-10-27T06:33:04.256Z | 2016-10-23T13:41:57.258Z |
|                                      |                                      |                       |                   | cloud_name: entry-scale-kvm-esx-mml       |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | test_type: ping                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | hostname: ardana-cp1-esx-ovsvapp0001-mgmt |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | control_plane: control-plane-1            |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cluster: mtrmon                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | observer_host: ardana-cp1-mtrmon-m1-mgmt  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       | host_alive_status | service: system                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cloud_name: entry-scale-kvm-esx-mml       |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | test_type: ping                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | hostname: ardana-cp1-esx-ovsvapp0001-mgmt |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | control_plane: control-plane-1            |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cluster: mtrmon                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | observer_host: ardana-cp1-mtrmon-m3-mgmt  |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       | host_alive_status | service: system                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cloud_name: entry-scale-kvm-esx-mml       |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | test_type: ping                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | hostname: ardana-cp1-esx-ovsvapp0001-mgmt |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | control_plane: control-plane-1            |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | cluster: mtrmon                           |          |       |                 |      |                          |                          |                          |
|                                      |                                      |                       |                   | observer_host: ardana-cp1-mtrmon-m2-mgmt  |          |       |                 |      |                          |                          |                          |
+--------------------------------------+--------------------------------------+-----------------------+-------------------+-------------------------------------------+----------+-------+-----------------+------+--------------------------+--------------------------+--------------------------+</pre></div></li><li class="listitem "><p>
     Delete the monasca alarm.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>monasca alarm-delete &lt;alarm ID&gt;</pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>monasca alarm-delete cfc6bfa4-2485-4319-b1e5-0107886f4270Successfully deleted alarm</pre></div><p>
     After deleting the alarms and updating the monasca-agent configuration,
     those alarms will be removed from the Operations Console UI. You can login to
     Operations Console and view the status.
    </p></li></ol></div></div><div class="sect2" id="id-1.5.9.6.8"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Clean up the entries of OVSvAPP VM from /etc/host</span> <a title="Permalink" class="permalink" href="#id-1.5.9.6.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-removing_esx_host_from_cluster.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-removing_esx_host_from_cluster.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Perform the following procedure to clean up the entries of OVSvAPP VM from
   <code class="literal">/etc/hosts</code>.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Login to Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit <code class="literal">/etc/host</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>vi /etc/host</pre></div><p>
     For example: <code class="literal">MCP-VCP-cpesx-esx-ovsvapp0001-mgmt</code> VM is
     present in the <code class="literal">/etc/host</code>.
    </p><div class="verbatim-wrap"><pre class="screen">192.168.86.17    MCP-VCP-cpesx-esx-ovsvapp0001-mgmt</pre></div></li><li class="listitem "><p>
     Delete the OVSvAPP entries from <code class="literal">/etc/hosts</code>.
    </p></li></ol></div></div><div class="sect2" id="id-1.5.9.6.9"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Remove the OVSVAPP VM from the servers.yml and pass_through.yml files and run the Configuration Processor</span> <a title="Permalink" class="permalink" href="#id-1.5.9.6.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-removing_esx_host_from_cluster.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-removing_esx_host_from_cluster.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Complete these steps from the Cloud Lifecycle Manager to remove the OVSvAPP VM:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager
    </p></li><li class="listitem "><p>
     Edit <code class="literal">servers.yml</code> file to remove references to the
     OVSvAPP VM(s) you want to remove:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/servers.yml</pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">- ip-addr:192.168.86.17
  server-group: AZ1    role:
  OVSVAPP-ROLE    id:
  6afaa903398c8fc6425e4d066edf4da1a0f04388</pre></div></li><li class="listitem "><p>
     Edit
     <code class="literal">~/openstack/my_cloud/definition/data/pass_through.yml</code>
     file to remove the OVSvAPP VM references using the server-id above section
     to find the references.
    </p><div class="verbatim-wrap"><pre class="screen">- data:
  vmware:
  vcenter_cluster: Clust1
  cluster_dvs_mapping: 'DC1/host/Clust1:TRUNK-DVS-Clust1'
  esx_hostname: MCP-VCP-cpesx-esx-ovsvapp0001-mgmt
  vcenter_id: 0997E2ED9-5E4F-49EA-97E6-E2706345BAB2
id: 6afaa903398c8fc6425e4d066edf4da1a0f04388</pre></div></li><li class="listitem "><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git commit -a -m "Remove ESXi host &lt;name&gt;"</pre></div></li><li class="listitem "><p>
     Run the configuration processor. You may want to use the
     <code class="literal">remove_deleted_servers</code> and
     <code class="literal">free_unused_addresses</code> switches to free up the resources
     when running the configuration processor. See
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 7 “Other Topics”, Section 7.3 “Persisted Data”</span> for more details.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml -e remove_deleted_servers="y" -e free_unused_addresses="y"</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li></ol></div></div><div class="sect2" id="id-1.5.9.6.10"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.4.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Clean Up nova Agent for ESX Proxy</span> <a title="Permalink" class="permalink" href="#id-1.5.9.6.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-removing_esx_host_from_cluster.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-removing_esx_host_from_cluster.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager
    </p></li><li class="step "><p>
     Source the credentials.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source service.osrc</pre></div></li><li class="step "><p>
     Find the nova ID for ESX Proxy with <code class="command">openstack compute service
     list</code>.
    </p></li><li class="step "><p>
     Delete the ESX Proxy service.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service delete
    <em class="replaceable ">ESX_PROXY_ID</em></pre></div></li></ol></div></div><p>
   If you have more than one host, perform the preceding procedure for all the
   hosts.
  </p></div><div class="sect2" id="id-1.5.9.6.11"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.4.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Clean Up monasca Agent for ESX Proxy</span> <a title="Permalink" class="permalink" href="#id-1.5.9.6.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-removing_esx_host_from_cluster.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-removing_esx_host_from_cluster.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Using the ESX proxy hostname, execute the following command to list all
     alarms.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>monasca alarm-list --metric-dimensions hostname=<em class="replaceable ">COMPUTE_NODE_DELETED</em></pre></div><p>
     where <em class="replaceable ">COMPUTE_NODE_DELETED</em> - hostname is taken
     from the vSphere client (refer to
     <a class="xref" href="#idg-all-esx-remove-existing-cluster-compute-resource-pool-xml-6" title="7.3.1. Prerequisites">Section 7.3.1, “Prerequisites”</a>).
    </p><div id="id-1.5.9.6.11.2.1.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      Make a note of all the alarm IDs that are displayed after executing the
      preceding command.
     </p></div></li><li class="step "><p>
     Delete the ESX Proxy alarm using the alarm IDs.
    </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-delete &lt;alarm ID&gt;</pre></div><p>
     This step has to be performed for all alarm IDs listed with the
     <code class="command">monasca alarm-list</code> command.
    </p></li></ol></div></div></div><div class="sect2" id="id-1.5.9.6.12"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.4.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Clean Up ESX Proxy Entries in <code class="filename">/etc/host</code></span> <a title="Permalink" class="permalink" href="#id-1.5.9.6.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-removing_esx_host_from_cluster.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-removing_esx_host_from_cluster.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager
    </p></li><li class="step "><p>
     Edit the <code class="filename">/etc/hosts</code> file, removing ESX Proxy entries.
    </p></li></ol></div></div></div><div class="sect2" id="id-1.5.9.6.13"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.4.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Remove ESX Proxy from <code class="filename">servers.yml</code> and <code class="filename">pass_through.yml</code> files; run the Configuration Processor</span> <a title="Permalink" class="permalink" href="#id-1.5.9.6.13">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-removing_esx_host_from_cluster.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-removing_esx_host_from_cluster.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager
    </p></li><li class="step "><p>
     Edit <code class="filename">servers.yml</code> file to remove references to ESX
     Proxy:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/servers.yml</pre></div></li><li class="step "><p>
     Edit
     <code class="literal">~/openstack/my_cloud/definition/data/pass_through.yml</code>
     file to remove the ESX Proxy references using the
     <code class="literal">server-id</code> fromm the <code class="filename">servers.yml</code>
     file.
    </p></li><li class="step "><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen">git commit -a -m "Remove ESX Proxy references"</pre></div></li><li class="step "><p>
     Run the configuration processor. You may want to use the
     <code class="literal">remove_deleted_servers</code> and
     <code class="literal">free_unused_addresses</code> switches to free up the resources
     when running the configuration processor. See
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 7 “Other Topics”, Section 7.3 “Persisted Data”</span> for more details.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml \
-e remove_deleted_servers="y" -e free_unused_addresses="y"</pre></div></li><li class="step "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li></ol></div></div></div><div class="sect2" id="id-1.5.9.6.14"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.4.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Remove Distributed Resource Scheduler (DRS) Rules</span> <a title="Permalink" class="permalink" href="#id-1.5.9.6.14">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-removing_esx_host_from_cluster.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-removing_esx_host_from_cluster.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Perform the following procedure to remove DRS rules, which is added by
   OVSvAPP installer to ensure that OVSvAPP does not get migrated to other
   hosts.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Login to vCenter.
    </p></li><li class="listitem "><p>
     Right click on cluster and select <span class="bold"><strong>Edit
     settings</strong></span>.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-drs-rule1.png" target="_blank"><img src="images/media-esx-drs-rule1.png" width="" /></a></div></div><p>
     A cluster settings page appears.
    </p></li><li class="listitem "><p>
     Click <span class="bold"><strong>DRS Groups Manager</strong></span> on the left hand
     side of the pop-up box. Select the group which is created for deleted
     OVSvAPP and click Remove.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-drs-group2.png" target="_blank"><img src="images/media-esx-drs-group2.png" width="" /></a></div></div></li><li class="listitem "><p>
     Click <span class="bold"><strong>Rules</strong></span> on the left hand side of the
     pop-up box and select the checkbox for deleted OVSvAPP and click
     <span class="bold"><strong>Remove</strong></span>.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-rules3.png" target="_blank"><img src="images/media-esx-rules3.png" width="" /></a></div></div></li><li class="listitem "><p>
     Click <span class="bold"><strong>OK</strong></span>.
    </p></li></ol></div></div></div><div class="sect1" id="sec-esx-debug"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Debug Logging</span> <a title="Permalink" class="permalink" href="#sec-esx-debug">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-eon_logging.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-eon_logging.xml</li><li><span class="ds-label">ID: </span>sec-esx-debug</li></ul></div></div></div></div><div class="sect2" id="id-1.5.9.7.2"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To Modify the OVSVAPP VM Log Level</span> <a title="Permalink" class="permalink" href="#id-1.5.9.7.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-eon_logging.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-eon_logging.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To change the OVSVAPP log level to DEBUG, do the following:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the file below:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/neutron-common/templates/ovsvapp-agent-logging.conf.j2</pre></div></li><li class="listitem "><p>
     Set the logging level value of the <code class="literal">logger_root</code> section
     to <code class="literal">DEBUG</code>, like this:
    </p><div class="verbatim-wrap"><pre class="screen">[logger_root]
qualname: root
handlers: watchedfile, logstash
level: DEBUG</pre></div></li><li class="listitem "><p>
     Commit your configuration to the Git repository
     (<span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Deploy your changes:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></li></ol></div></div><div class="sect2" id="id-1.5.9.7.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To Enable OVSVAPP Service for Centralized Logging</span> <a title="Permalink" class="permalink" href="#id-1.5.9.7.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-eon_logging.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-eon_logging.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To enable OVSVAPP Service for centralized logging:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the file below:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/logging/vars/neutron-ovsvapp-clr.yml</pre></div></li><li class="listitem "><p>
     Set the value of <code class="literal">centralized_logging</code> to
     <span class="bold"><strong>true</strong></span> as shown in the following sample:
    </p><div class="verbatim-wrap"><pre class="screen">logr_services:
  neutron-ovsvapp:
    logging_options:
    - centralized_logging:
        enabled: <span class="bold"><strong>true</strong></span>
        format: json
        ...</pre></div></li><li class="listitem "><p>
     Commit your configuration to the Git repository
     (<span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Deploy your changes, specifying the hostname for your OVSAPP host:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml --limit &lt;hostname&gt;</pre></div><p>
     The hostname of the node can be found in the list generated from the
     output of the following command:
    </p><div class="verbatim-wrap"><pre class="screen">grep hostname ~/openstack/my_cloud/info/server_info.yml</pre></div></li></ol></div></div></div><div class="sect1" id="topic-ijt-dyh-rt"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Making Scale Configuration Changes</span> <a title="Permalink" class="permalink" href="#topic-ijt-dyh-rt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-scale_config_changes.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-scale_config_changes.xml</li><li><span class="ds-label">ID: </span>topic-ijt-dyh-rt</li></ul></div></div></div></div><p>
  This procedure describes how to make the recommended configuration changes to
  achieve 8,000 virtual machine instances.
 </p><div id="id-1.5.9.8.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   In a scale environment for ESX computes, the configuration of vCenter Proxy
   VM has to be increased to 8 vCPUs and 16 GB RAM. By default it is 4 vCPUs
   and 4 GB RAM.
  </p></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Change the directory. The <code class="literal">nova.conf.j2</code> file is present
    in following directories:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible/roles/nova-common/templates</pre></div></li><li class="listitem "><p>
    Edit the DEFAULT section in the <code class="literal">nova.conf.j2</code> file as
    below:
   </p><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
rpc_responce_timeout = 180
server_down_time = 300
report_interval = 30</pre></div></li><li class="listitem "><p>
    Commit your configuration:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "&lt;commit message&gt;"</pre></div></li><li class="listitem "><p>
    Prepare your environment for deployment:
   </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost ready-deployment.yml;
cd ~/scratch/ansible/next/ardana/ansible;</pre></div></li><li class="listitem "><p>
    Execute the <code class="literal">nova-reconfigure</code> playbook:
   </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</pre></div></li></ol></div></div><div class="sect1" id="idg-all-operations-monitoring-vcenter-clusters-xml-1"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring vCenter Clusters</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-vcenter-clusters-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring_vcenter_clusters.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring_vcenter_clusters.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-vcenter-clusters-xml-1</li></ul></div></div></div></div><p>
  Remote monitoring of activated ESX cluster is enabled through vCenter Plugin
  of monasca. The monasca-agent running in each ESX Compute proxy node is
  configured with the vcenter plugin, to monitor the cluster.
 </p><p>
  Alarm definitions are created with the default threshold values and whenever
  the threshold limit breaches respective alarms (OK/ALARM/UNDETERMINED) are
  generated.
 </p><p>
  The configuration file details is given below:
 </p><div class="verbatim-wrap"><pre class="screen">init_config: {}
instances:
  - vcenter_ip: &lt;vcenter-ip&gt;
      username: &lt;vcenter-username&gt;
      password: &lt;center-password&gt;
      clusters: &lt;[cluster list]&gt;</pre></div><p>
  Metrics List of metrics posted to monasca by vCenter Plugin are listed below:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    vcenter.cpu.total_mhz
   </p></li><li class="listitem "><p>
    vcenter.cpu.used_mhz
   </p></li><li class="listitem "><p>
    vcenter.cpu.used_perc
   </p></li><li class="listitem "><p>
    vcenter.cpu.total_logical_cores
   </p></li><li class="listitem "><p>
    vcenter.mem.total_mb
   </p></li><li class="listitem "><p>
    vcenter.mem.used_mb
   </p></li><li class="listitem "><p>
    vcenter.mem.used_perc
   </p></li><li class="listitem "><p>
    vcenter.disk.total_space_mb
   </p></li><li class="listitem "><p>
    vcenter.disk.total_used_space_mb
   </p></li><li class="listitem "><p>
    vcenter.disk.total_used_space_perc
   </p></li></ul></div><p>
  monasca measurement-list --dimensions
  esx_cluster_id=domain-c7.D99502A9-63A8-41A2-B3C3-D8E31B591224
  vcenter.disk.total_used_space_mb 2016-08-30T11:20:08
 </p><div class="verbatim-wrap"><pre class="screen">+----------------------------------------------+----------------------------------------------------------------------------------------------+-----------------------------------+------------------+-----------------+
| name                                         | dimensions                                                                                   | timestamp                         | value            | value_meta      |
+----------------------------------------------+----------------------------------------------------------------------------------------------+-----------------------------------+------------------+-----------------+
| vcenter.disk.total_used_space_mb             | vcenter_ip: 10.1.200.91                                                                      | 2016-08-30T11:20:20.703Z          | 100371.000       |                 |
|                                              | esx_cluster_id: domain-c7.D99502A9-63A8-41A2-B3C3-D8E31B591224                               | 2016-08-30T11:20:50.727Z          | 100371.000       |                 |
|                                              | hostname: MCP-VCP-cpesx-esx-comp0001-mgmt                                                    | 2016-08-30T11:21:20.707Z          | 100371.000       |                 |
|                                              |                                                                                              | 2016-08-30T11:21:50.700Z          | 100371.000       |                 |
|                                              |                                                                                              | 2016-08-30T11:22:20.700Z          | 100371.000       |                 |
|                                              |                                                                                              | 2016-08-30T11:22:50.700Z          | 100371.000       |                 |
|                                              |                                                                                              | 2016-08-30T11:23:20.620Z          | 100371.000       |                 |
+----------------------------------------------+-----------------------------------------------------------------------------------------------+-----------------------------------+------------------+-----------------+</pre></div><p>
  <span class="bold"><strong>Dimensions</strong></span>
 </p><p>
  Each metric will have the dimension as below
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.9.9.12.1"><span class="term ">vcenter_ip</span></dt><dd><p>
     FQDN/IP Address of the registered vCenter
    </p></dd><dt id="id-1.5.9.9.12.2"><span class="term ">server esx_cluster_id</span></dt><dd><p>
     clusterName.vCenter-id, as seen in the openstack hypervisor list
    </p></dd><dt id="id-1.5.9.9.12.3"><span class="term ">hostname</span></dt><dd><p>
     ESX compute proxy name
    </p></dd></dl></div><p>
  <span class="bold"><strong>Alarms</strong></span>
 </p><p>
  Alarms are created for monitoring cpu, memory and disk usages for each
  activated clusters. The alarm definitions details are
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /></colgroup><thead><tr><th>Name</th><th>Expression</th><th>Severity</th><th>Match_by</th></tr></thead><tbody><tr><td>ESX cluster CPU Usage</td><td>avg(vcenter.cpu.used_perc) &gt; 90 times 3</td><td>High</td><td>esx_cluster_id</td></tr><tr><td>ESX cluster Memory Usage</td><td>avg(vcenter.mem.used_perc) &gt; 90 times 3</td><td>High</td><td>esx_cluster_id</td></tr><tr><td>ESX cluster Disk Usage</td><td>vcenter.disk.total_used_space_perc &gt; 90</td><td>High</td><td>esx_cluster_id</td></tr></tbody></table></div></div><div class="sect1" id="ovsvapp-monitoring"><div class="titlepage"><div><div><h2 class="title"><span class="number">7.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring Integration with OVSvApp Appliance</span> <a title="Permalink" class="permalink" href="#ovsvapp-monitoring">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-esx_monitoring.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-esx_monitoring.xml</li><li><span class="ds-label">ID: </span>ovsvapp-monitoring</li></ul></div></div></div></div><div class="sect2" id="processes"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Processes Monitored with monasca-agent</span> <a title="Permalink" class="permalink" href="#processes">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-esx_monitoring.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-esx_monitoring.xml</li><li><span class="ds-label">ID: </span>processes</li></ul></div></div></div></div><p>
   Using the monasca agent, the following services are monitored on the OVSvApp
   appliance:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>neutron_ovsvapp_agent service</strong></span> - This is
     the neutron agent which runs in the appliance which will help enable
     networking for the tenant virtual machines.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Openvswitch</strong></span> - This service is used by the
     neutron_ovsvapp_agent service for enabling the datapath and security for
     the tenant virtual machines.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Ovsdb-server</strong></span> - This service is used by
     the neutron_ovsvapp_agent service.
    </p></li></ul></div><p>
   If any of the above three processes fail to run on the OVSvApp appliance it
   will lead to network disruption for the tenant virtual machines. This is why
   they are monitored.
  </p><p>
   The monasca-agent periodically reports the status of these processes and
   metrics data ('load' - cpu.load_avg_1min, 'process' - process.pid_count,
   'memory' - mem.usable_perc, 'disk' - disk.space_used_perc, 'cpu' -
   cpu.idle_perc for examples) to the monasca server.
  </p></div><div class="sect2" id="how"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How It Works</span> <a title="Permalink" class="permalink" href="#how">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/esx-esx_monitoring.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>esx-esx_monitoring.xml</li><li><span class="ds-label">ID: </span>how</li></ul></div></div></div></div><p>
   Once the vApp is configured and up, the monasca-agent will attempt to
   register with the monasca server. After successful registration, the
   monitoring begins on the processes listed above and you will be able to see
   status updates on the server side.
  </p><p>
   The monasca-agent monitors the processes at the system level so, in the case
   of failures of any of the configured processes, updates should be seen
   immediately from monasca.
  </p><p>
   To check the events from the server side, log into the Operations Console.
  </p></div></div></div><div class="chapter " id="ops-managing-blockstorage"><div class="titlepage"><div><div><h1 class="title"><span class="number">8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Block Storage</span> <a title="Permalink" class="permalink" href="#ops-managing-blockstorage">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-managing_blockstorage.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_blockstorage.xml</li><li><span class="ds-label">ID: </span>ops-managing-blockstorage</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#topic-e5g-z3h-gt"><span class="number">8.1 </span><span class="name">Managing Block Storage using Cinder</span></a></span></dt></dl></div></div><p>
  Information about managing and configuring the Block Storage service.
 </p><div class="sect1" id="topic-e5g-z3h-gt"><div class="titlepage"><div><div><h2 class="title"><span class="number">8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Block Storage using Cinder</span> <a title="Permalink" class="permalink" href="#topic-e5g-z3h-gt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-blockstorage_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-blockstorage_overview.xml</li><li><span class="ds-label">ID: </span>topic-e5g-z3h-gt</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Block Storage volume operations use the OpenStack cinder service to
  manage storage volumes, which includes creating volumes, attaching/detaching
  volumes to nova instances, creating volume snapshots, and configuring
  volumes.
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> supports the following storage back ends for block storage
  volumes and backup datastore configuration:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Volumes
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      SUSE Enterprise Storage; for more information, see
      <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 35 “Integrations”, Section 35.3 “SUSE Enterprise Storage Integration”</span>.
     </p></li><li class="listitem "><p>
      3PAR FC or iSCSI; for more information, see
      <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 35 “Integrations”, Section 35.1 “Configuring for 3PAR Block Storage Backend”</span>.
     </p></li></ul></div></li><li class="listitem "><p>
    Backup
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      swift
     </p></li></ul></div></li></ul></div><div class="sect2" id="multiple"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting Up Multiple Block Storage Back-ends</span> <a title="Permalink" class="permalink" href="#multiple">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-blockstorage_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-blockstorage_overview.xml</li><li><span class="ds-label">ID: </span>multiple</li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> supports setting up multiple block storage backends and multiple
   volume types.
  </p><p>
   Whether you have a single or multiple block storage back-ends defined in
   your <code class="filename">cinder.conf.j2</code> file, you can create one or more
   volume types using the specific attributes associated with the back-end. You
   can find details on how to do that for each of the supported back-end types
   here:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 35 “Integrations”, Section 35.3 “SUSE Enterprise Storage Integration”</span>
    </p></li><li class="listitem "><p>
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 35 “Integrations”, Section 35.1 “Configuring for 3PAR Block Storage Backend”</span>
    </p></li></ul></div></div><div class="sect2" id="creating-voltype"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a Volume Type for your Volumes</span> <a title="Permalink" class="permalink" href="#creating-voltype">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-blockstorage-creating_voltype.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-blockstorage-creating_voltype.xml</li><li><span class="ds-label">ID: </span>creating-voltype</li></ul></div></div></div></div><p>
  Creating volume types allows you to create standard specifications for your
  volumes.
 </p><p>
  Volume types are used to specify a standard Block Storage back-end and
  collection of extra specifications for your volumes. This allows an
  administrator to give its users a variety of options while simplifying the
  process of creating volumes.
 </p><p>
  The tasks involved in this process are:
 </p><div class="sect3" id="create-volumetype"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create a Volume Type for your Volumes</span> <a title="Permalink" class="permalink" href="#create-volumetype">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-blockstorage-creating_voltype.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-blockstorage-creating_voltype.xml</li><li><span class="ds-label">ID: </span>create-volumetype</li></ul></div></div></div></div><p>
   The default volume type will be thin provisioned and will have no fault
   tolerance (RAID 0). You should configure cinder to fully provision volumes,
   and you may want to configure fault tolerance. Follow the instructions below
   to create a new volume type that is fully provisioned and fault tolerant:
  </p><p>
   Perform the following steps to create a volume type using the horizon GUI:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the horizon dashboard.
    </p></li><li class="step "><p>
     Ensure that you are scoped to your <code class="literal">admin</code> Project. Then
     under the <span class="guimenu ">Admin</span> menu in the navigation pane, click on
     <span class="guimenu ">Volumes</span> under the <span class="guimenu ">System</span> subheading.
    </p></li><li class="step "><p>
     Select the <span class="guimenu ">Volume Types</span> tab and then click the
     <span class="guimenu ">Create Volume Type</span> button to display a dialog box.
    </p></li><li class="step "><p>
     Enter a unique name for the volume type and then click the
     <span class="guimenu ">Create Volume Type</span> button to complete the
     action.
    </p></li></ol></div></div><p>
   The newly created volume type will be displayed in the <code class="literal">Volume
   Types</code> list confirming its creation.
  </p><div id="id-1.5.10.3.6.5.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    You must set a <code class="literal">default_volume_type</code> in
    <code class="filename">cinder.conf.j2</code>, whether it is
    <code class="literal">default_type</code> or one you have created. For more
    information, see <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 35 “Integrations”, Section 35.1 “Configuring for 3PAR Block Storage Backend”, Section 35.1.4 “Configure 3PAR FC as a Cinder Backend”</span>.
   </p></div></div><div class="sect3" id="associate-volumetype"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.1.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Associate the Volume Type to the Back-end</span> <a title="Permalink" class="permalink" href="#associate-volumetype">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-blockstorage-creating_voltype.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-blockstorage-creating_voltype.xml</li><li><span class="ds-label">ID: </span>associate-volumetype</li></ul></div></div></div></div><p>
   After the volume type(s) have been created, you can assign extra
   specification attributes to the volume types. Each Block Storage back-end
   option has unique attributes that can be used.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <a class="xref" href="#extraspecs-3par" title="8.1.2.3. Extra Specification Options for 3PAR">Section 8.1.2.3, “Extra Specification Options for 3PAR”</a>
    </p></li></ul></div><p>
   To map a volume type to a back-end, do the following:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log into the horizon dashboard.
    </p></li><li class="step "><p>
     Ensure that you are scoped to your <span class="guimenu ">admin</span> Project (for
     more information, see <a class="xref" href="#scopeToDomain" title="5.10.7. Scope Federated User to Domain">Section 5.10.7, “Scope Federated User to Domain”</a>. Then under the
     <span class="guimenu ">Admin</span> menu in the navigation pane, click on
     <span class="guimenu ">Volumes</span> under the <span class="guimenu ">System</span> subheading.
    </p></li><li class="step "><p>
     Click the <span class="guimenu ">Volume Type</span> tab to list the volume types.
    </p></li><li class="step "><p>
     In the <span class="guimenu ">Actions</span> column of the Volume Type you created
     earlier, click the drop-down option and select <span class="guimenu ">View Extra
     Specs</span> which will bring up the <span class="guimenu ">Volume Type Extra
     Specs</span> options.
    </p></li><li class="step "><p>
     Click the <span class="guimenu ">Create</span> button on the <code class="literal">Volume Type
     Extra Specs</code> screen.
    </p></li><li class="step "><p>
     In the <code class="literal">Key</code> field, enter one of the key values in the
     table in the next section. In the <code class="literal">Value</code> box, enter its
     corresponding value. Once you have completed that, click the
     <span class="guimenu ">Create</span> button to create the extra volume type specs.
    </p></li></ol></div></div><p>
   Once the volume type is mapped to a back-end, you can create volumes with
   this volume type.
  </p></div><div class="sect3" id="extraspecs-3par"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.1.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Extra Specification Options for 3PAR</span> <a title="Permalink" class="permalink" href="#extraspecs-3par">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-blockstorage-creating_voltype.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-blockstorage-creating_voltype.xml</li><li><span class="ds-label">ID: </span>extraspecs-3par</li></ul></div></div></div></div><p>
   3PAR supports volumes creation with additional attributes. These attributes
   can be specified using the extra specs options for your volume type. The
   administrator is expected to define appropriate extra spec for 3PAR volume
   type as per the guidelines provided at
   <a class="link" href="http://docs.openstack.org/liberty/config-reference/content/hp-3par-supported-ops.html" target="_blank">http://docs.openstack.org/liberty/config-reference/content/hp-3par-supported-ops.html</a>.
  </p><p>
   The following cinder Volume Type extra-specs options enable control over the
   3PAR storage provisioning type:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Key</th><th>Value</th><th>Description</th></tr></thead><tbody><tr><td>volume_backend_name</td><td><span class="emphasis"><em>volume backend name</em></span>
      </td><td>
       <p>
        The name of the back-end to which you want to associate the volume type,
        which you also specified earlier in the
        <code class="literal">cinder.conf.j2</code> file.
       </p>
      </td></tr><tr><td>hp3par:provisioning (optional)</td><td>thin, full, or dedup</td><td> </td></tr></tbody></table></div><p>
   For more information, see <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 35 “Integrations”, Section 35.1 “Configuring for 3PAR Block Storage Backend”</span>.
  </p></div></div><div class="sect2" id="sec-operation-manage-block-storage"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing cinder Volume and Backup Services</span> <a title="Permalink" class="permalink" href="#sec-operation-manage-block-storage">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-blockstorage-managing_cinder_volumebackup_services.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-blockstorage-managing_cinder_volumebackup_services.xml</li><li><span class="ds-label">ID: </span>sec-operation-manage-block-storage</li></ul></div></div></div></div><div id="id-1.5.10.3.7.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Use Only When Needed</h6><p>
   If the host running the <code class="literal">cinder-volume</code> service fails for
   any reason, it should be restarted as quickly as possible. Often, the host
   running cinder services also runs high availability (HA) services
   such as MariaDB and RabbitMQ. These HA services are at risk while one of the
   nodes in the cluster is down. If it will take a significant amount of time
   to recover the failed node, then you may migrate the
   <code class="literal">cinder-volume</code> service and its backup service to one of
   the other controller nodes. When the node has been recovered, you should
   migrate the <code class="literal">cinder-volume</code> service and its backup service
   to the original (default) node.
  </p><p>
   The <code class="literal">cinder-volume</code> service and its backup service migrate
   as a pair. If you migrate the <code class="literal">cinder-volume</code> service, its
   backup service will also be migrated.
  </p></div><div class="sect3" id="idg-all-operations-blockstorage-managing-cinder-volumebackup-services-xml-5"><div class="titlepage"><div><div><h4 class="title"><span class="number">8.1.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Migrating the cinder-volume service</span> <a title="Permalink" class="permalink" href="#idg-all-operations-blockstorage-managing-cinder-volumebackup-services-xml-5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-blockstorage-managing_cinder_volumebackup_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-blockstorage-managing_cinder_volumebackup_services.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-blockstorage-managing-cinder-volumebackup-services-xml-5</li></ul></div></div></div></div><p>
   The following steps will migrate the cinder-volume service and its backup service.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager node.
    </p></li><li class="listitem "><p>
     Determine the host index numbers for each of your control plane nodes.
     This host index number will be used in a later step. They can be obtained
     by running this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts cinder-show-volume-hosts.yml</pre></div><p>
     Here is an example snippet showing the output of a single three node
     control plane, with the host index numbers in bold:
    </p><div class="verbatim-wrap"><pre class="screen">TASK: [_CND-CMN | show_volume_hosts | Show cinder Volume hosts index and hostname] ***
ok: [ardana-cp1-c1-m1] =&gt; (item=(0, 'ardana-cp1-c1-m1')) =&gt; {
    "item": [
        <span class="bold"><strong>0</strong></span>,
        "ardana-cp1-c1-m1"
    ],
    "msg": "Index 0 Hostname ardana-cp1-c1-m1"
}
ok: [ardana-cp1-c1-m1] =&gt; (item=(1, 'ardana-cp1-c1-m2')) =&gt; {
    "item": [
        <span class="bold"><strong>1</strong></span>,
        "ardana-cp1-c1-m2"
    ],
    "msg": "Index 1 Hostname ardana-cp1-c1-m2"
}
ok: [ardana-cp1-c1-m1] =&gt; (item=(2, 'ardana-cp1-c1-m3')) =&gt; {
    "item": [
        <span class="bold"><strong>2</strong></span>,
        "ardana-cp1-c1-m3"
    ],
    "msg": "Index 2 Hostname ardana-cp1-c1-m3"
}</pre></div></li><li class="listitem "><p>
     Locate the control plane fact file for the control plane you need to
     migrate the service from. It will be located in the following directory:
    </p><div class="verbatim-wrap"><pre class="screen">/etc/ansible/facts.d/</pre></div><p>
     These fact files use the following naming convention:
    </p><div class="verbatim-wrap"><pre class="screen">cinder_volume_run_location_<span class="emphasis"><em>&lt;control_plane_name&gt;</em></span>.fact</pre></div></li><li class="listitem "><p>
     Edit the fact file to include the host index number of the control plane
     node you wish to migrate the <code class="literal">cinder-volume</code> services to.
     For example, if they currently reside on your first controller node, host
     index 0, and you wish to migrate them to your second controller, you would
     change the value in the fact file to <code class="literal">1</code>.
    </p></li><li class="listitem "><p>
     If you are using data encryption on your Cloud Lifecycle Manager, ensure you
     have included the encryption key in your environment variables. For more
     information see <span class="intraxref">Book “Security Guide”, Chapter 10 “Encryption of Passwords and Sensitive Data”</span>.
    </p><div class="verbatim-wrap"><pre class="screen">export HOS_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</pre></div></li><li class="listitem "><p>
     After you have edited the control plane fact file, run the cinder volume
     migration playbook for the control plane nodes involved in the migration.
     At minimum this includes the one to start cinder-volume manager on and the
     one on which to stop it:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts cinder-migrate-volume.yml --limit=&lt;limit_pattern1,limit_pattern2&gt;</pre></div><div id="id-1.5.10.3.7.3.3.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      <span class="emphasis"><em>&lt;limit_pattern&gt;</em></span> is the pattern used to limit
      the hosts that are selected to those within a specific control plane. For
      example, with the nodes in the snippet shown above,
      <code class="literal">--limit=&gt;ardana-cp1-c1-m1,ardana-cp1-c1-m2&lt;</code>
     </p></div></li><li class="listitem "><p>
     Even though the playbook summary reports no errors, you may disregard
     informational messages such as:
    </p><div class="verbatim-wrap"><pre class="screen">msg: Marking ardana_notify_cinder_restart_required to be cleared from the fact cache</pre></div></li><li class="listitem "><p>
     Ensure that once your maintenance or other tasks are completed that you
     migrate the <code class="literal">cinder-volume</code> services back to their
     original node using these same steps.
    </p></li></ol></div></div></div></div></div><div class="chapter " id="ops-managing-objectstorage"><div class="titlepage"><div><div><h1 class="title"><span class="number">9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Object Storage</span> <a title="Permalink" class="permalink" href="#ops-managing-objectstorage">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-managing_objectstorage.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_objectstorage.xml</li><li><span class="ds-label">ID: </span>ops-managing-objectstorage</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#swift-healthcheck"><span class="number">9.1 </span><span class="name">Running the swift Dispersion Report</span></a></span></dt><dt><span class="section"><a href="#swift-recon"><span class="number">9.2 </span><span class="name">Gathering Swift Data</span></a></span></dt><dt><span class="section"><a href="#topic-pcv-fy4-nt"><span class="number">9.3 </span><span class="name">Gathering Swift Monitoring Metrics</span></a></span></dt><dt><span class="section"><a href="#topic-m13-dgp-nt"><span class="number">9.4 </span><span class="name">Using the swift Command-line Client (CLI)</span></a></span></dt><dt><span class="section"><a href="#swift-ring-management"><span class="number">9.5 </span><span class="name">Managing swift Rings</span></a></span></dt><dt><span class="section"><a href="#topic-el2-cqv-mv"><span class="number">9.6 </span><span class="name">Configuring your swift System to Allow Container Sync</span></a></span></dt></dl></div></div><p>
  Information about managing and configuring the Object Storage service.
 </p><p>
  The Object Storage service may be deployed in a full-fledged manner, with
  proxy nodes engaging rings for managing the accounts, containers,
  and objects being stored. Or, it may simply be deployed as a front-end to
  SUSE Enterprise Storage, offering Object Storage APIs with an external back-end.
 </p><p>
  In the former case, managing your Object Storage environment includes tasks
  related to ensuring your swift rings stay balanced, and that and other
  topics are discussed in more detail in this section. swift includes many
  commands and utilities for these purposes.
 </p><p>
  When used as a front-end to SUSE Enterprise Storage, many swift constructs such as rings
  and ring balancing, replica dispersion, etc. do not apply, as swift itself
  is not responsible for the mechanics of object storage.
 </p><div class="sect1" id="swift-healthcheck"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Running the swift Dispersion Report</span> <a title="Permalink" class="permalink" href="#swift-healthcheck">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-swift_dispersion_report.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_dispersion_report.xml</li><li><span class="ds-label">ID: </span>swift-healthcheck</li></ul></div></div></div></div><p>
  swift contains a tool called <code class="literal">swift-dispersion-report</code> that
  can be used to determine whether your containers and objects have three
  replicas like they are supposed to. This tool works by populating a
  percentage of partitions in the system with containers and objects (using
  <code class="literal">swift-dispersion-populate</code>) and then running the report to
  see if all the replicas of these containers and objects are in the correct
  place. For a more detailed explanation of this tool in Openstack swift,
  please see
  <a class="link" href="http://docs.openstack.org/developer/swift/admin_guide.html#cluster-health" target="_blank">OpenStack
  swift - Administrator's Guide</a>.
 </p><div class="sect2" id="id-1.5.11.6.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the swift dispersion populate</span> <a title="Permalink" class="permalink" href="#id-1.5.11.6.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-swift_dispersion_report.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_dispersion_report.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Once a swift system has been fully deployed in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9, you can
   setup the swift-dispersion-report using the default parameters found in
   <code class="filename">~/openstack/ardana/ansible/roles/swift-dispersion/templates/dispersion.conf.j2</code>.
   This populates 1% of the partitions on the system and if you are happy with
   this figure, please proceed to step 2 below. Otherwise, follow step 1 to
   edit the configuration file.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     If you wish to change the dispersion coverage percentage, then connect
     to the Cloud Lifecycle Manager server and change the value of
     <code class="literal">dispersion_coverage</code> in the
     <code class="filename">~/openstack/ardana/ansible/roles/swift-dispersion/templates/dispersion.conf.j2</code>
     file to the value you wish to use. In the example below we have altered
     the file to create 5% dispersion:
    </p><div class="verbatim-wrap"><pre class="screen">...
[dispersion]
auth_url = {{ keystone_identity_uri }}/v3
auth_user = {{ swift_dispersion_tenant }}:{{ swift_dispersion_user }}
auth_key = {{ swift_dispersion_password  }}
endpoint_type = {{ endpoint_type }}
auth_version = {{ disp_auth_version }}
# Set this to the percentage coverage. We recommend a value
# of 1%. You can increase this to get more coverage. However, if you
# decrease the value, the dispersion containers and objects are
# not deleted.
<span class="bold"><strong>dispersion_coverage = 5.0</strong></span></pre></div></li><li class="listitem "><p>
     Commit your configuration to the Git repository
     (<span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Reconfigure the swift servers:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li><li class="listitem "><p>
     Run this playbook to populate your swift system for the health check:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-dispersion-populate.yml</pre></div></li></ol></div></div><div class="sect2" id="id-1.5.11.6.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Running the swift dispersion report</span> <a title="Permalink" class="permalink" href="#id-1.5.11.6.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-swift_dispersion_report.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_dispersion_report.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Check the status of the swift system by running the swift dispersion report
   with this playbook:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-dispersion-report.yml</pre></div><p>
   The output of the report will look similar to this:
  </p><div class="verbatim-wrap"><pre class="screen">TASK: [swift-dispersion | report | Display dispersion report results] *********
ok: [padawan-ccp-c1-m1-mgmt] =&gt; {
    "var": {
        "dispersion_report_result.stdout_lines": [
            "Using storage policy: General ",
            "",
            "[KQueried 40 containers for dispersion reporting, 0s, 0 retries",
            "100.00% of container copies found (120 of 120)",
            "Sample represents 0.98% of the container partition space",
            "",
            "[KQueried 40 objects for dispersion reporting, 0s, 0 retries",
            "There were 40 partitions missing 0 copies.",
            "100.00% of object copies found (120 of 120)",
            "Sample represents 0.98% of the object partition space"
        ]
    }
}
...</pre></div><p>
   In addition to being able to run the report above, there will be a cron-job
   scheduled to run every 2 hours located on the primary proxy node of your
   cloud environment. It will run <code class="literal">dispersion-report</code> and save
   the results to the following location on its local filesystem:
  </p><div class="verbatim-wrap"><pre class="screen">/var/cache/swift/dispersion-report</pre></div><p>
   When interpreting the results you get from this report, we recommend using
   <a class="link" href="http://docs.openstack.org/developer/swift/admin_guide.html#cluster-health" target="_blank">swift
   Administrator's Guide - Cluster Health</a>
  </p></div></div><div class="sect1" id="swift-recon"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Gathering Swift Data</span> <a title="Permalink" class="permalink" href="#swift-recon">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-validating_swift_recon.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-validating_swift_recon.xml</li><li><span class="ds-label">ID: </span>swift-recon</li></ul></div></div></div></div><p>
  The <code class="literal">swift-recon</code> command retrieves data from swift servers
  and displays the results. To use this command, log on as a root user to any
  node which is running the swift-proxy service.
 </p><div class="sect2" id="idg-all-operations-objectstorage-validating-swift-recon-xml-7"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Notes</span> <a title="Permalink" class="permalink" href="#idg-all-operations-objectstorage-validating-swift-recon-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-validating_swift_recon.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-validating_swift_recon.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-objectstorage-validating-swift-recon-xml-7</li></ul></div></div></div></div><p>
   For help with the <code class="literal">swift-recon</code> command you can use this:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo swift-recon --help</pre></div><div id="id-1.5.11.7.3.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
    The <code class="literal">--driveaudit</code> option is not supported.
   </p></div><div id="id-1.5.11.7.3.5" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> does not support ec_type <code class="literal">isa_l_rs_vand</code> and
    <code class="literal">ec_num_parity_fragments</code> greater than or equal to
    <span class="bold"><strong>5</strong></span> in the storage-policy configuration.
    This particular policy is known to harm data durability.
   </p></div></div><div class="sect2" id="idg-all-operations-objectstorage-validating-swift-recon-xml-8"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the swift-recon Command</span> <a title="Permalink" class="permalink" href="#idg-all-operations-objectstorage-validating-swift-recon-xml-8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-validating_swift_recon.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-validating_swift_recon.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-objectstorage-validating-swift-recon-xml-8</li></ul></div></div></div></div><p>
   The following command retrieves and displays disk usage information:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo swift-recon --diskusage</pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo swift-recon --diskusage
===============================================================================
--&gt; Starting reconnaissance on 3 hosts
===============================================================================
[2015-09-14 16:01:40] Checking disk usage now
Distribution Graph:
 10%    3 *********************************************************************
 11%    1 ***********************
 12%    2 **********************************************
Disk usage: space used: 13745373184 of 119927734272
Disk usage: space free: 106182361088 of 119927734272
Disk usage: lowest: 10.39%, highest: 12.96%, avg: 11.4613798613%
===============================================================================</pre></div><p>
   In the above example, the results for several nodes are combined together.
   You can also view the results from individual nodes by adding the
   <span class="bold"><strong>-v</strong></span> option as shown in the following
   example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo swift-recon --diskusage -v
===============================================================================
--&gt; Starting reconnaissance on 3 hosts
===============================================================================
[2015-09-14 16:12:30] Checking disk usage now
-&gt; http://192.168.245.3:6000/recon/diskusage: [{'device': 'disk1', 'avail': 17398411264, 'mounted': True, 'used': 2589544448, 'size': 19987955712}, {'device': 'disk0', 'avail': 17904222208, 'mounted': True, 'used': 2083733504, 'size': 19987955712}]
-&gt; http://192.168.245.2:6000/recon/diskusage: [{'device': 'disk1', 'avail': 17769721856, 'mounted': True, 'used': 2218233856, 'size': 19987955712}, {'device': 'disk0', 'avail': 17793581056, 'mounted': True, 'used': 2194374656, 'size': 19987955712}]
-&gt; http://192.168.245.4:6000/recon/diskusage: [{'device': 'disk1', 'avail': 17912147968, 'mounted': True, 'used': 2075807744, 'size': 19987955712}, {'device': 'disk0', 'avail': 17404235776, 'mounted': True, 'used': 2583719936, 'size': 19987955712}]
Distribution Graph:
 10%    3 *********************************************************************
 11%    1 ***********************
 12%    2 **********************************************
Disk usage: space used: 13745414144 of 119927734272
Disk usage: space free: 106182320128 of 119927734272
Disk usage: lowest: 10.39%, highest: 12.96%, avg: 11.4614140152%
===============================================================================</pre></div><p>
   By default, <code class="literal">swift-recon</code> uses the object-0 ring for
   information about nodes and drives. For some commands, it is appropriate to
   specify <span class="bold"><strong>account</strong></span>,
   <span class="bold"><strong>container</strong></span>, or
   <span class="bold"><strong>object</strong></span> to indicate the type of ring. For
   example, to check the checksum of the account ring, use the following:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo swift-recon --md5 account
===============================================================================
--&gt; Starting reconnaissance on 3 hosts
===============================================================================
[2015-09-14 16:17:28] Checking ring md5sums
3/3 hosts matched, 0 error[s] while checking hosts.
===============================================================================
[2015-09-14 16:17:28] Checking swift.conf md5sum
3/3 hosts matched, 0 error[s] while checking hosts.
===============================================================================</pre></div></div></div><div class="sect1" id="topic-pcv-fy4-nt"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Gathering Swift Monitoring Metrics</span> <a title="Permalink" class="permalink" href="#topic-pcv-fy4-nt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-lm_scan_metering.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-lm_scan_metering.xml</li><li><span class="ds-label">ID: </span>topic-pcv-fy4-nt</li></ul></div></div></div></div><p>
  The <code class="literal">swiftlm-scan</code> command is the mechanism used to gather
  metrics for the monasca system. These metrics are used to derive alarms. For
  a list of alarms that can be generated from this data, see
  <a class="xref" href="#alarmdefinitions" title="18.1.1. Alarm Resolution Procedures">Section 18.1.1, “Alarm Resolution Procedures”</a>.
 </p><p>
  To view the metrics, use the <code class="literal">swiftlm-scan</code> command
  directly. Log on to the swift node as the root user. The following example
  shows the command and a snippet of the output:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo swiftlm-scan --pretty
. . .
  {
    "dimensions": {
      "device": "sdc",
      "hostname": "padawan-ccp-c1-m2-mgmt",
      "service": "object-storage"
    },
    "metric": "swiftlm.swift.drive_audit",
    "timestamp": 1442248083,
    "value": 0,
    "value_meta": {
      "msg": "No errors found on device: sdc"
    }
  },
. . .</pre></div><div id="id-1.5.11.8.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   To make the JSON file easier to read, use the <code class="literal">--pretty</code>
   option.
  </p></div><p>
  The fields are as follows:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /></colgroup><tbody><tr><td><code class="literal">metric</code>
     </td><td>
      <p>
       Specifies the name of the metric.
      </p>
     </td></tr><tr><td><code class="literal">dimensions</code>
     </td><td>
      <p>
       Provides information about the source or location of the metric. The
       dimensions differ depending on the metric in question. The following
       dimensions are used by <code class="literal">swiftlm-scan</code>:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         <span class="bold"><strong>service</strong></span>: This is always
         <span class="bold"><strong>object-storage</strong></span>.
        </p></li><li class="listitem "><p>
         <span class="bold"><strong>component</strong></span>: This identifies the
         component. For example,
         <span class="bold"><strong>swift-object-server</strong></span> indicates that
         the metric is about the swift-object-server process.
        </p></li><li class="listitem "><p>
         <span class="bold"><strong>hostname</strong></span>: This is the name of the
         node the metric relates to. This is not necessarily the name of the
         current node.
        </p></li><li class="listitem "><p>
         <span class="bold"><strong>url</strong></span>: If the metric is associated with
         a URL, this is the URL.
        </p></li><li class="listitem "><p>
         <span class="bold"><strong>port</strong></span>: If the metric relates to
         connectivity to a node, this is the port used.
        </p></li><li class="listitem "><p>
         <span class="bold"><strong>device</strong></span>: This is the block device a
         metric relates to.
        </p></li></ul></div>
     </td></tr><tr><td><code class="literal">value</code></td><td>
      <p>
       The value of the metric. For many metrics, this is simply the value of
       the metric. However, if the value indicates a status. If
       <code class="literal">value_meta</code> contains a
       <span class="bold"><strong>msg</strong></span> field, the value is a status. The
       following status values are used:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         0 - no error
        </p></li><li class="listitem "><p>
         1 - warning
        </p></li><li class="listitem "><p>
         2 - failure
        </p></li></ul></div>
     </td></tr><tr><td><code class="literal">value_meta</code>
     </td><td>
      <p>
       Additional information. The <span class="bold"><strong>msg</strong></span> field
       is the most useful of this information.
      </p>
     </td></tr></tbody></table></div><div class="sect2" id="id-1.5.11.8.8"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Optional Parameters</span> <a title="Permalink" class="permalink" href="#id-1.5.11.8.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-lm_scan_metering.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-lm_scan_metering.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   You can focus on specific sets of metrics by using one of the following
   optional parameters:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /></colgroup><tbody><tr><td><code class="literal">--replication</code></td><td>
       <p>
        Checks replication and health status.
       </p>
      </td></tr><tr><td><code class="literal">--file-ownership</code></td><td>
       <p>
        Checks that swift owns its relevant files and directories.
       </p>
      </td></tr><tr><td><code class="literal">--drive-audit</code></td><td>
       <p>
        Checks for logged events about corrupted sectors (unrecoverable read
        errors) on drives.
       </p>
      </td></tr><tr><td><code class="literal">--connectivity</code></td><td>
       <p>
        Checks connectivity to various servers used by the swift system,
        including:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          Checks this node can connect to all memcachd servers
         </p></li><li class="listitem "><p>
          Checks that this node can connect to the keystone service (only
          applicable if this is a proxy server node)
         </p></li></ul></div>
      </td></tr><tr><td><code class="literal">--swift-services</code></td><td>
       <p>
        Check that the relevant swift processes are running.
       </p>
      </td></tr><tr><td><code class="literal">--network-interface</code></td><td>
       <p>
        Checks NIC speed and reports statistics for each interface.
       </p>
      </td></tr><tr><td><code class="literal">--check-mounts</code></td><td>
       <p>
        Checks that the node has correctly mounted drives used by swift.
       </p>
      </td></tr><tr><td><code class="literal">--hpssacli</code></td><td>
       <p>
        If this server uses a Smart Array Controller, this checks the operation
        of the controller and disk drives.
       </p>
      </td></tr></tbody></table></div></div></div><div class="sect1" id="topic-m13-dgp-nt"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the swift Command-line Client (CLI)</span> <a title="Permalink" class="permalink" href="#topic-m13-dgp-nt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-swift_cli.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_cli.xml</li><li><span class="ds-label">ID: </span>topic-m13-dgp-nt</li></ul></div></div></div></div><p>
  OpenStackClient (OSC) is a command-line client for OpenStack with a uniform
  command structure for OpenStack services. Some swift commands do not
  have OSC equivalents. The <code class="literal">swift</code> utility (or swift
  CLI) is installed on the Cloud Lifecycle Manager node and also on all other nodes running the
  swift proxy service. To use this utility on the Cloud Lifecycle Manager, you can use the
  <code class="literal">~/service.osrc</code> file as a basis and then edit it with the
  credentials of another user if you need to.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cp ~/service.osrc ~/swiftuser.osrc</pre></div><p>
  Then you can use your preferred editor to edit swiftuser.osrc so you can
  authenticate using the <code class="literal">OS_USERNAME</code>,
  <code class="literal">OS_PASSWORD</code>, and <code class="literal">OS_PROJECT_NAME</code> you
  wish to use. For example, if you want use the <code class="literal">demo</code> user
  that is created automatically for you, it would look like this:
 </p><div class="verbatim-wrap"><pre class="screen">unset OS_DOMAIN_NAME
export OS_IDENTITY_API_VERSION=3
export OS_AUTH_VERSION=3
export OS_PROJECT_NAME=demo
export OS_PROJECT_DOMAIN_NAME=Default
export OS_USERNAME=demo
export OS_USER_DOMAIN_NAME=Default
export OS_PASSWORD=&lt;password&gt;
export OS_AUTH_URL=&lt;auth_URL&gt;
export OS_ENDPOINT_TYPE=internalURL
# OpenstackClient uses OS_INTERFACE instead of OS_ENDPOINT
export OS_INTERFACE=internal
export OS_CACERT=/etc/ssl/certs/ca-certificates.crt
export OS_COMPUTE_API_VERSION=2</pre></div><p>
  You must use the appropriate password for the demo user and select the
  correct endpoint for the <span class="bold"><strong>OS_AUTH_URL</strong></span> value,
  which should be in the <code class="literal">~/service.osrc</code> file you copied.
 </p><p>
  You can then examine the following account data using this command:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack object store account show</pre></div><p>
  Example showing an environment with no containers or objects:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack object store account show
        Account: AUTH_205804d000a242d385b8124188284998
     Containers: 0
        Objects: 0
          Bytes: 0
X-Put-Timestamp: 1442249536.31989
     Connection: keep-alive
    X-Timestamp: 1442249536.31989
     X-Trans-Id: tx5493faa15be44efeac2e6-0055f6fb3f
   Content-Type: text/plain; charset=utf-8</pre></div><p>
  Use the following command to create a container:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack container create <em class="replaceable ">CONTAINER_NAME</em></pre></div><p>
  Example, creating a container named <code class="literal">documents</code>:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack container create documents</pre></div><p>
  The newly created container appears. But there are no objects:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack container show documents
         Account: AUTH_205804d000a242d385b8124188284998
       Container: documents
         Objects: 0
           Bytes: 0
        Read ACL:
       Write ACL:
         Sync To:
        Sync Key:
   Accept-Ranges: bytes
X-Storage-Policy: General
      Connection: keep-alive
     X-Timestamp: 1442249637.69486
      X-Trans-Id: tx1f59d5f7750f4ae8a3929-0055f6fbcc
    Content-Type: text/plain; charset=utf-8</pre></div><p>
  Upload a document:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack object create <em class="replaceable ">CONTAINER_NAME</em> <em class="replaceable ">FILENAME</em></pre></div><p>
  Example:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack object create documents mydocument
mydocument</pre></div><p>
  List objects in the container:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack object list <em class="replaceable ">CONTAINER_NAME</em></pre></div><p>
  Example using a container called <code class="literal">documents</code>:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack object list documents
mydocument</pre></div><div id="id-1.5.11.9.25" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   This is a brief introduction to the <code class="command">swift</code> CLI. Use the
   <code class="command">swift --help</code> command for more information. You can also
   use the OpenStack CLI, see <code class="command">openstack -h</code> for more
   information.
  </p></div></div><div class="sect1" id="swift-ring-management"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing swift Rings</span> <a title="Permalink" class="permalink" href="#swift-ring-management">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-ring_management.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-ring_management.xml</li><li><span class="ds-label">ID: </span>swift-ring-management</li></ul></div></div></div></div><p>
  swift rings are a machine-readable description of which disk drives are used
  by the Object Storage service (for example, a drive is used to store account
  or object data). Rings also specify the policy for data storage (for example,
  defining the number of replicas). The rings are automatically built during
  the initial deployment of your cloud, with the configuration provided during
  setup of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Input Model. For more information, see
  <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 5 “Input Model”</span>.
 </p><p>
  After successful deployment of your cloud, you may want to change or modify
  the configuration for swift. For example, you may want to add or remove swift
  nodes, add additional storage policies, or upgrade the size of the disk
  drives. For instructions, see <a class="xref" href="#change-swift-rings" title="9.5.5. Applying Input Model Changes to Existing Rings">Section 9.5.5, “Applying Input Model Changes to Existing Rings”</a> and
  <a class="xref" href="#add-storage-policy" title="9.5.6. Adding a New Swift Storage Policy">Section 9.5.6, “Adding a New Swift Storage Policy”</a>.
 </p><div id="id-1.5.11.10.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   The process of modifying or adding a configuration is similar to other
   configuration or topology changes in the cloud. Generally, you make the
   changes to the input model files at
   <code class="literal">~/openstack/my_cloud/definition/</code> on the Cloud Lifecycle Manager and then
   run Ansible playbooks to reconfigure the system.
  </p></div><p>
  Changes to the rings require several phases to complete, therefore, you may
  need to run the playbooks several times over several days.
 </p><p>
  The following topics cover ring management.
 </p><div class="sect2" id="swift-ring-rebalance"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Rebalancing Swift Rings</span> <a title="Permalink" class="permalink" href="#swift-ring-rebalance">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-rebalanced_explained.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-rebalanced_explained.xml</li><li><span class="ds-label">ID: </span>swift-ring-rebalance</li></ul></div></div></div></div><p>
  The swift ring building process tries to distribute data evenly among the
  available disk drives. The data is stored in partitions. (For more
  information, see <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 11 “Modifying Example Configurations for Object Storage using Swift”, Section 11.10 “Understanding Swift Ring Specifications”</span>.) If you, for example,
  double the number of disk drives in a ring, you need to move 50% of the
  partitions to the new drives so that all drives contain the same number of
  partitions (and hence same amount of data). However, it is not possible to
  move the partitions in a single step. It can take minutes to hours to move
  partitions from the original drives to their new drives (this process is
  called the replication process).
 </p><p>
  If you move all partitions at once, there would be a period where swift would
  expect to find partitions on the new drives, but the data has not yet
  replicated there so that swift could not return the data to the user.
  Therefore, swift will not be able to find all of the data in the middle of
  replication because some data has finished replication while other bits of
  data are still in the old locations and have not yet been moved. So it is
  considered best practice to move only one replica at a time. If the replica
  count is 3, you could first move 16.6% of the partitions and then wait until
  all data has replicated. Then move another 16.6% of partitions. Wait again
  and then finally move the remaining 16.6% of partitions. For any given
  object, only one of the replicas is moved at a time.
 </p><div class="sect3" id="id-1.5.11.10.7.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.5.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reasons to Move Partitions Gradually</span> <a title="Permalink" class="permalink" href="#id-1.5.11.10.7.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-rebalanced_explained.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-rebalanced_explained.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Due to the following factors, you must move the partitions gradually:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Not all devices are of the same size. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 automatically
     assigns different weights to drives so that smaller drives store fewer
     partitions than larger drives.
    </p></li><li class="listitem "><p>
     The process attempts to keep replicas of the same partition in different
     servers.
    </p></li><li class="listitem "><p>
     Making a large change in one step (for example, doubling the number of
     drives in the ring), would result in a lot of network traffic due to the
     replication process and the system performance suffers. There are two ways
     to mitigate this:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Add servers in smaller groups
      </p></li><li class="listitem "><p>
       Set the weight-step attribute in the ring specification. For more
       information, see <a class="xref" href="#swift-weight-att" title="9.5.2. Using the Weight-Step Attributes to Prepare for Ring Changes">Section 9.5.2, “Using the Weight-Step Attributes to Prepare for Ring Changes”</a>.
      </p></li></ul></div></li></ul></div></div></div><div class="sect2" id="swift-weight-att"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Weight-Step Attributes to Prepare for Ring Changes</span> <a title="Permalink" class="permalink" href="#swift-weight-att">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-swift_weight_attribute.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_weight_attribute.xml</li><li><span class="ds-label">ID: </span>swift-weight-att</li></ul></div></div></div></div><p>
  swift rings are built during a deployment and this process sets the weights
  of disk drives such that smaller disk drives have a smaller weight than
  larger disk drives. When making changes in the ring, you should limit the
  amount of change that occurs. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 does this by limiting the
  weights of the new drives to a smaller value and then building new rings.
  Once the replication process has finished, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 will increase the
  weight and rebuild rings to trigger another round of replication. (For more
  information, see <a class="xref" href="#swift-ring-rebalance" title="9.5.1. Rebalancing Swift Rings">Section 9.5.1, “Rebalancing Swift Rings”</a>.)
 </p><p>
  In addition, you should become familiar with how the replication process
  behaves on your system during normal operation. Before making ring changes,
  use the <code class="literal">swift-recon</code> command to determine the typical
  oldest replication times for your system. For instructions, see
  <a class="xref" href="#topic-ohx-j1t-4t" title="9.5.4. Determining When to Rebalance and Deploy a New Ring">Section 9.5.4, “Determining When to Rebalance and Deploy a New Ring”</a>.
 </p><p>
  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the weight-step attribute is set in the ring specification of
  the input model. The weight-step value specifies a maximum value for the
  change of the weight of a drive in any single rebalance. For example, if you
  add a drive of 4TB, you would normally assign a weight of 4096. However, if
  the weight-step attribute is set to 1024 instead then when you add that drive
  the weight is initially set to 1024. The next time you rebalance the ring,
  the weight is set to 2048. The subsequent rebalance would then set the weight
  to the final value of 4096.
 </p><p>
  The value of the weight-step attribute is dependent on the size of the
  drives, number of the servers being added, and how experienced you are with
  the replication process. A common starting value is to use 20% of the size of
  an individual drive. For example, when adding X number of 4TB drives a value
  of 820 would be appropriate. As you gain more experience with your system,
  you may increase or reduce this value.
 </p><div class="sect3" id="setting-weight-step-attribute"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.5.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting the weight-step attribute</span> <a title="Permalink" class="permalink" href="#setting-weight-step-attribute">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-swift_weight_attribute.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_weight_attribute.xml</li><li><span class="ds-label">ID: </span>setting-weight-step-attribute</li></ul></div></div></div></div><p>
   Perform the following steps to set the weight-step attribute:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the
     <code class="filename">~/openstack/my_cloud/definition/data/swift/swift_config.yml</code> file
     containing the ring-specifications for the account, container, and object
     rings.
    </p><p>
     Add the weight-step attribute to the ring in this format:
    </p><div class="verbatim-wrap"><pre class="screen">- name: account
  weight-step: <em class="replaceable ">WEIGHT_STEP_VALUE</em>
  display-name: Account Ring
  min-part-hours: 16
  ...</pre></div><p>
     For example, to set weight-step to 820, add the attribute like this:
    </p><div class="verbatim-wrap"><pre class="screen">- name: account
  weight-step: <span class="bold"><strong>820</strong></span>
  display-name: Account Ring
  min-part-hours: 16
  ...</pre></div></li><li class="listitem "><p>
     Repeat step 2 for the other rings, if necessary (container, object-0,
     etc).
    </p></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Use the playbook to create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     To complete the configuration, use the ansible playbooks documented in
     <a class="xref" href="#swift-ansible-playbooks" title="9.5.3. Managing Rings Using swift Playbooks">Section 9.5.3, “Managing Rings Using swift Playbooks”</a>.
    </p></li></ol></div></div></div><div class="sect2" id="swift-ansible-playbooks"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Rings Using swift Playbooks</span> <a title="Permalink" class="permalink" href="#swift-ansible-playbooks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-swift_ring_mgmt.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_ring_mgmt.xml</li><li><span class="ds-label">ID: </span>swift-ansible-playbooks</li></ul></div></div></div></div><p>
  The following table describes how playbooks relate to ring management.
 </p><p>
  All of these playbooks will be run from the Cloud Lifecycle Manager from the
  <code class="literal">~/scratch/ansible/next/ardana/ansible</code> directory.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Playbook</th><th>Description</th><th>Notes</th></tr></thead><tbody><tr><td><code class="literal">swift-update-from-model-rebalance-rings.yml</code>
     </td><td>
      <p>
       There are two steps in this playbook:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Make delta
        </p><p>
         It processes the input model and compares it against the existing
         rings. After comparison, it produces a list of differences between
         the input model and the existing rings. This is called the ring
         delta. The ring delta covers drives being added, drives being
         removed, weight changes, and replica count changes.
        </p></li></ul></div>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Rebalance
        </p><p>
         The ring delta is then converted into a series of commands (such as
         <code class="literal">add</code>) to the swift-ring-builder program. Finally,
         the <code class="literal">rebalance</code> command is issued to the
         swift-ring-builder program.
        </p></li></ul></div>
     </td><td>
      <p>
       This playbook performs its actions on the first node running the
       swift-proxy service. (For more information, see
       <a class="xref" href="#topic-rtc-s3t-mt" title="18.6.2.4. Identifying the Swift Ring Building Server">Section 18.6.2.4, “Identifying the Swift Ring Building Server”</a>.) However, it also scans all swift
       nodes to find the size of disk drives.
      </p>
      <p>
       If there are no changes in the ring delta, the
       <code class="literal">rebalance</code> command is still executed to rebalance the
       rings. If <code class="literal">min-part-hours</code> has not yet elapsed or if
       no partitions need to be moved, new rings are not written.
      </p>
     </td></tr><tr><td><code class="literal">swift-compare-model-rings.yml</code>
     </td><td>
      <p>
       There are two steps in this playbook:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Make delta
        </p><p>
         This is the same as described for
         <code class="literal">swift-update-from-model-rebalance-rings.yml</code>.
        </p></li></ul></div>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Report
        </p><p>
         This prints a summary of the proposed changes that will be made to
         the rings (that is, what would happen if you rebalanced).
        </p></li></ul></div>
      <p>
       The playbook reports any issues or problems it finds with the input
       model.
      </p>
      <p>
       This playbook can be useful to confirm that there are no errors in the
       input model. It also allows you to check that when you change the input
       model, that the proposed ring changes are as expected. For example, if
       you have added a server to the input model, but this playbook reports
       that no drives are being added, you should determine the cause.
      </p>
     </td><td>
      <p>
       There is troubleshooting information related to the information that
       you receive in this report that you can view on this page:
       <a class="xref" href="#sec-input-swift-error" title="18.6.2.3. Interpreting Swift Input Model Validation Errors">Section 18.6.2.3, “Interpreting Swift Input Model Validation Errors”</a>.
      </p>
     </td></tr><tr><td><code class="literal">swift-deploy.yml</code>
     </td><td>
      <p>
       <code class="literal">swift-deploy.yml</code> is responsible for installing
       software and configuring swift on nodes. As part of installing and
       configuring, it runs the
       <code class="literal">swift-update-from-model-rebalance-rings.yml</code> and
       <code class="literal">swift-reconfigure.yml</code> playbooks.
      </p>
     </td><td>
      <p>
       This playbook is included in the <code class="literal">ardana-deploy.yml</code> and
       <code class="literal">site.yml</code> playbooks, so if you run either of those
       playbooks, the <code class="literal">swift-deploy.yml</code> playbook is also
       run.
      </p>
     </td></tr><tr><td><code class="literal">swift-reconfigure.yml</code>
     </td><td>
      <p>
       <code class="literal">swift-reconfigure.yml</code> takes rings that the
       <code class="literal">swift-update-from-model-rebalance-rings.yml</code>
       playbook has changed and copies those rings to all swift nodes.
      </p>
     </td><td>
      <p>
       Every time that you directly use the
       <code class="literal">swift-update-from-model-rebalance-rings.yml</code>
       playbook, you must copy these rings to the system using the
       <code class="literal">swift-reconfigure.yml</code> playbook. If you forget and
       run <code class="literal">swift-update-from-model-rebalance-rings.yml</code>
       twice, the process may move two replicates of some partitions at the
       same time.
      </p>
     </td></tr></tbody></table></div><div class="sect3" id="variables"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.5.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Optional Ansible variables related to ring management</span> <a title="Permalink" class="permalink" href="#variables">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-swift_ring_mgmt.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_ring_mgmt.xml</li><li><span class="ds-label">ID: </span>variables</li></ul></div></div></div></div><p>
   The following optional variables may be specified when running the playbooks
   outlined above. They are specified using the <code class="literal">--extra-vars</code>
   option.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Variable</th><th>Description and Use</th></tr></thead><tbody><tr><td><code class="literal">limit_ring</code>
      </td><td>
       <p>
        Limit changes to the named ring. Other rings will not be examined or
        updated. This option may be used with any of the swift playbooks. For
        example, to only update the <code class="literal">object-1</code> ring, use the
        following command:
       </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-update-from-model-rebalance-rings.yml --extra-vars "limit-ring=object-1"</pre></div>
      </td></tr><tr><td>drive_detail</td><td>
       <p>
        Used only with the swift-compare-model-rings.yml playbook. The playbook
        will include details of changes to every drive where the model and
        existing rings differ. If you omit the drive_detail variable, only
        summary information is provided. The following shows how to use the
        drive_detail variable:
       </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml --extra-vars "drive_detail=yes"</pre></div>
      </td></tr></tbody></table></div></div><div class="sect3" id="id-1.5.11.10.9.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.5.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Interpreting the report from the swift-compare-model-rings.yml playbook</span> <a title="Permalink" class="permalink" href="#id-1.5.11.10.9.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-swift_ring_mgmt.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_ring_mgmt.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The <code class="literal">swift-compare-model-rings.yml</code> playbook compares the
   existing swift rings with the input model and prints a report telling you
   how the rings and the model differ. Specifically, it will tell you what
   actions will take place when you next run the
   <code class="literal">swift-update-from-model-rebalance-rings.yml</code> playbook (or
   a playbook such as <code class="literal">ardana-deploy.yml</code> that runs
   <code class="literal">swift-update-from-model-rebalance-rings.yml</code>).
  </p><p>
   The <code class="literal">swift-compare-model-rings.yml</code> playbook will make no
   changes, but is just an advisory report.
  </p><p>
   Here is an example output from the playbook. The report is between
   "report.stdout_lines" and "PLAY RECAP":
  </p><div class="verbatim-wrap"><pre class="screen">TASK: [swiftlm-ring-supervisor | validate-input-model | Print report] *********
ok: [ardana-cp1-c1-m1-mgmt] =&gt; {
    "var": {
        "report.stdout_lines": [
            "Rings:",
            "  ACCOUNT:",
            "    ring exists (minimum time to next rebalance: 8:07:33)",
            "    will remove 1 devices (18.00GB)",
            "    ring will be rebalanced",
            "  CONTAINER:",
            "    ring exists (minimum time to next rebalance: 8:07:35)",
            "    no device changes",
            "    ring will be rebalanced",
            "  OBJECT-0:",
            "    ring exists (minimum time to next rebalance: 8:07:34)",
            "    no device changes",
            "    ring will be rebalanced"
        ]
    }
}</pre></div><p>
   The following describes the report in more detail:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Message</th><th>Description</th></tr></thead><tbody><tr><td>
       <p>
        ring exists
       </p>
      </td><td>
       <p>
        The ring already exists on the system.
       </p>
      </td></tr><tr><td>
       <p>
        ring will be created
       </p>
      </td><td>
       <p>
        The ring does not yet exist on the system.
       </p>
      </td></tr><tr><td>
       <p>
        no device changes
       </p>
      </td><td>
       <p>
        The devices in the ring exactly match the input model. There are no
        servers being added or removed and the weights are appropriate for the
        size of the drives.
       </p>
      </td></tr><tr><td>
       <p>
        minimum time to next rebalance
       </p>
      </td><td>
       <p>
        If this time is <code class="literal">0:00:00</code>, if you run one of the swift
        playbooks that update rings, the ring will be rebalanced.
       </p>
       <p>
        If the time is non-zero, it means that not enough time has elapsed
        since the ring was last rebalanced. Even if you run a swift playbook
        that attempts to change the ring, the ring will not actually
        rebalance. This time is determined by the
        <code class="literal">min-part-hours</code> attribute.
       </p>
      </td></tr><tr><td>
       <p>
        set-weight ardana-ccp-c1-m1-mgmt:disk0:/dev/sdc 8.00 &gt; 12.00 &gt; 18.63
       </p>
      </td><td>
       <p>
        The weight of disk0 (mounted on /dev/sdc) on server
        <code class="literal">ardana-ccp-c1-m1-mgmt</code> is currently set to 8.0 but
        should be 18.83 given the size of the drive. However, in this example,
        we cannot go directly from 8.0 to 18.63 because of the weight-step
        attribute. Hence, the proposed weight change is from 8.0 to 12.0.
       </p>
       <p>
        This information is only shown when you the
        <code class="literal">drive_detail=yes</code> argument when running the playbook.
       </p>
      </td></tr><tr><td>
       <p>
        will change weight on 12 devices (6.00TB)
       </p>
      </td><td>
       <p>
        The weight of 12 devices will be increased. This might happen for
        example, if a server had been added in a prior ring update. However,
        with use of the <code class="literal">weight-step</code> attribute, the system
        gradually increases the weight of these new devices. In this example,
        the change in weight represents 6TB of total available storage. For
        example, if your system currently has 100TB of available storage, when
        the weight of these devices is changed, there will be 106TB of
        available storage. If your system is 50% utilized, this means that when
        the ring is rebalanced, up to 3TB of data may be moved by the
        replication process. This is an estimate - in practice, because only
        one copy of a given replica is moved in any given rebalance, it may not
        be possible to move this amount of data in a single ring rebalance.
       </p>
      </td></tr><tr><td>
       <p>
        add: ardana-ccp-c1-m1-mgmt:disk0:/dev/sdc
       </p>
      </td><td>
       <p>
        The disk0 device will be added to the ardana-ccp-c1-m1-mgmt server. This
        happens when a server is added to the input model or if a disk model is
        changed to add additional devices.
       </p>
       <p>
        This information is only shown when you the
        <code class="literal">drive_detail=yes</code> argument when running the
        playbook.
       </p>
      </td></tr><tr><td>
       <p>
        remove: ardana-ccp-c1-m1-mgmt:disk0:/dev/sdc
       </p>
      </td><td>
       <p>
        The device is no longer in the input model and will be removed from the
        ring. This happens if a server is removed from the model, a disk drive
        is removed from a disk model or the server is marked for removal using
        the pass-through feature.
       </p>
       <p>
        This information is only shown when you the
        <code class="literal">drive_detail=yes</code> argument when running the
        playbook.
       </p>
      </td></tr><tr><td>
       <p>
        will add 12 devices (6TB)
       </p>
      </td><td>
       <p>
        There are 12 devices in the input model that have not yet been added to
        the ring. Usually this is because one or more servers have been added.
        In this example, this could be one server with 12 drives or two
        servers, each with 6 drives. The size in the report is the change in
        total available capacity. When the weight-step attribute is used, this
        may be a fraction of the total size of the disk drives. In this
        example, 6TB of capacity is being added. For example, if your system
        currently has 100TB of available storage, when these devices are added,
        there will be 106TB of available storage. If your system is 50%
        utilized, this means that when the ring is rebalanced, up to 3TB of
        data may be moved by the replication process. This is an estimate - in
        practice, because only one copy of a given replica is moved in any
        given rebalance, it may not be possible to move this amount of data in
        a single ring rebalance.
       </p>
      </td></tr><tr><td>
       <p>
        will remove 12 devices (6TB)
       </p>
      </td><td>
       <p>
        There are 12 devices in rings that no longer appear in the input model.
        Usually this is because one or more servers have been removed. In this
        example, this could be one server with 12 drives or two servers, each
        with 6 drives. The size in the report is the change in total removed
        capacity. In this example, 6TB of capacity is being removed. For
        example, if your system currently has 100TB of available storage, when
        these devices are removed, there will be 94TB of available storage. If
        your system is 50% utilized, this means that when the ring is
        rebalanced, approximately 3TB of data must be moved by the replication
        process.
       </p>
      </td></tr><tr><td>
       <p>
        min-part-hours will be changed
       </p>
      </td><td>
       <p>
        The <code class="literal">min-part-hours</code> attribute has been changed in the
        ring specification in the input model.
       </p>
      </td></tr><tr><td>
       <p>
        replica-count will be changed
       </p>
      </td><td>
       <p>
        The <code class="literal">replica-count</code> attribute has been changed in the
        ring specification in the input model.
       </p>
      </td></tr><tr><td>
       <p>
        ring will be rebalanced
       </p>
      </td><td>
       <p>
        This is always reported. Every time the
        <code class="literal">swift-update-from-model-rebalance-rings.yml</code> playbook
        is run, it will execute the swift-ring-builder rebalance command. This
        happens even if there were no input model changes. If the ring is
        already well balanced, the swift-ring-builder will not rewrite the
        ring.
       </p>
      </td></tr></tbody></table></div></div></div><div class="sect2" id="topic-ohx-j1t-4t"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Determining When to Rebalance and Deploy a New Ring</span> <a title="Permalink" class="permalink" href="#topic-ohx-j1t-4t">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-safe_rebalance_deploy_ring.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-safe_rebalance_deploy_ring.xml</li><li><span class="ds-label">ID: </span>topic-ohx-j1t-4t</li></ul></div></div></div></div><p>
  Before deploying a new ring, you must be sure the change that has been
  applied to the last ring is complete (that is, all the partitions are in
  their correct location). There are three aspects to this:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Is the replication system busy?
   </p><p>
    You might want to postpone a ring change until after replication has
    finished. If the replication system is busy repairing a failed drive, a
    ring change will place additional load on the system. To check that
    replication has finished, use the <code class="literal">swift-recon</code> command
    with the <span class="bold"><strong>--replication</strong></span> argument. (For more
    information, see <a class="xref" href="#swift-recon" title="9.2. Gathering Swift Data">Section 9.2, “Gathering Swift Data”</a>.) The
    oldest completion time can indicate that the replication process is very
    busy. If it is more than 15 or 20 minutes then the object replication
    process are probably still very busy. The following example indicates
    that the oldest completion is 120 seconds, so that the replication
    process is probably not busy:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>swift-recon --replication
===============================================================================
--&gt; Starting reconnaissance on 3 hosts
===============================================================================
[2015-10-02 15:31:45] Checking on replication
[replication_time] low: 0, high: 0, avg: 0.0, total: 0, Failed: 0.0%, no_result: 0, reported: 3
Oldest completion was 2015-10-02 15:31:32 (120 seconds ago) by 192.168.245.4:6000.
Most recent completion was 2015-10-02 15:31:43 (10 seconds ago) by 192.168.245.3:6000.
===============================================================================</pre></div></li></ul></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Are there drive or server failures?
   </p><p>
    A drive failure does not preclude deploying a new ring. In principle, there
    should be two copies elsewhere. However, another drive failure in the
    middle of replication might make data temporary unavailable. If possible,
    postpone ring changes until all servers and drives are operating normally.
   </p></li><li class="listitem "><p>
    Has <code class="literal">min-part-hours</code> elapsed?
   </p><p>
    The <code class="literal">swift-ring-builder</code> will refuse to build a new ring
    until the <code class="literal">min-part-hours</code> has elapsed since the last time
    it built rings. You must postpone changes until this time has elapsed.
   </p><p>
    You can determine how long you must wait by running the
    <code class="literal">swift-compare-model-rings.yml</code> playbook, which will tell
    you how long you until the <code class="literal">min-part-hours</code> has elapsed.
    For more details, see <a class="xref" href="#swift-ansible-playbooks" title="9.5.3. Managing Rings Using swift Playbooks">Section 9.5.3, “Managing Rings Using swift Playbooks”</a>.
   </p><p>
    You can change the value of <code class="literal">min-part-hours</code>. (For
    instructions, see <a class="xref" href="#min-part-hours" title="9.5.7. Changing min-part-hours in Swift">Section 9.5.7, “Changing min-part-hours in Swift”</a>).
   </p></li><li class="listitem "><p>
    Is the swift dispersion report clean?
   </p><p>
    Run the <code class="literal">swift-dispersion-report.yml</code> playbook (as
    described in <a class="xref" href="#swift-healthcheck" title="9.1. Running the swift Dispersion Report">Section 9.1, “Running the swift Dispersion Report”</a>) and
    examine the results. If the replication process has not yet replicated
    partitions that were moved to new drives in the last ring rebalance, the
    dispersion report will indicate that some containers or objects are
    missing a copy.
   </p><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen">There were 462 partitions missing one copy.</pre></div><p>
    Assuming all servers and disk drives are operational, the reason for the
    missing partitions is that the replication process has not yet managed to
    copy a replica into the partitions.
   </p><p>
    You should wait an hour and rerun the dispersion report process and examine
    the report. The number of partitions missing one copy should have reduced.
    Continue to wait until this reaches zero before making any further ring
    rebalances.
   </p><div id="id-1.5.11.10.10.4.3.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     It is normal to see partitions missing one copy if disk drives or
     servers are down. If all servers and disk drives are mounted, and you
     did not recently perform a ring rebalance, you should investigate
     whether there are problems with the replication process. You can use the
     Operations Console to investigate replication issues.
    </p></div><div id="id-1.5.11.10.10.4.3.8" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
     If there are any partitions missing two copies, you must reboot or repair
     any failed servers and disk drives as soon as possible. Do not shutdown
     any swift nodes in this situation. Assuming a replica count of 3, if you
     are missing two copies you are in danger of losing the only remaining
     copy.
    </p></div></li></ul></div></div><div class="sect2" id="change-swift-rings"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.5.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Applying Input Model Changes to Existing Rings</span> <a title="Permalink" class="permalink" href="#change-swift-rings">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-input_model_change_existing_rings.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-input_model_change_existing_rings.xml</li><li><span class="ds-label">ID: </span>change-swift-rings</li></ul></div></div></div></div><p>
  This page describes a general approach for making changes to your existing
  swift rings. This approach applies to actions such as adding and removing a
  server and replacing and upgrading disk drives, and must be performed as a
  series of phases, as shown below:
 </p><div class="sect3" id="change-inputmodel"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.5.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Changing the Input Model Configuration Files</span> <a title="Permalink" class="permalink" href="#change-inputmodel">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-input_model_change_existing_rings.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-input_model_change_existing_rings.xml</li><li><span class="ds-label">ID: </span>change-inputmodel</li></ul></div></div></div></div><p>
   The first step to apply new changes to the swift environment is to update
   the configuration files. Follow these steps:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Set the weight-step attribute, as needed, for the nodes you are altering.
     (For instructions, see <a class="xref" href="#swift-weight-att" title="9.5.2. Using the Weight-Step Attributes to Prepare for Ring Changes">Section 9.5.2, “Using the Weight-Step Attributes to Prepare for Ring Changes”</a>).
    </p></li><li class="listitem "><p>
     Edit the configuration files as part of the Input Model as appropriate.
     (For general information about the Input Model, see
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.14 “Networks”</span>. For more specific information about
     the swift parts of the configuration files, see
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 11 “Modifying Example Configurations for Object Storage using Swift”</span>)
    </p></li><li class="listitem "><p>
     Once you have completed all of the changes, commit your configuration to
     the local git repository. (For more information,
     see<span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>.) :
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">root # </code>git commit -m "commit message"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the swift playbook that will validate your configuration files and
     give you a report as an output:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">root # </code>ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml</pre></div></li><li class="listitem "><p>
     Use the report to validate that the number of drives proposed to be added
     or deleted, or the weight change, is correct. Fix any errors in your input
     model. At this stage, no changes have been made to rings.
    </p></li></ol></div></div><div class="sect3" id="first-rebalance"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.5.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">First phase of Ring Rebalance</span> <a title="Permalink" class="permalink" href="#first-rebalance">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-input_model_change_existing_rings.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-input_model_change_existing_rings.xml</li><li><span class="ds-label">ID: </span>first-rebalance</li></ul></div></div></div></div><p>
   To begin the rebalancing of the swift rings, follow these steps:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     After going through the steps in the section above, deploy your changes to
     all of the swift nodes in your environment by running this playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-deploy.yml</pre></div></li><li class="listitem "><p>
     Wait until replication has finished or <code class="literal">min-part-hours</code>
     has elapsed (whichever is longer). For more information, see
     <a class="xref" href="#topic-ohx-j1t-4t" title="9.5.4. Determining When to Rebalance and Deploy a New Ring">Section 9.5.4, “Determining When to Rebalance and Deploy a New Ring”</a>
    </p></li></ol></div></div><div class="sect3" id="weight-change"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.5.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Weight Change Phase of Ring Rebalance</span> <a title="Permalink" class="permalink" href="#weight-change">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-input_model_change_existing_rings.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-input_model_change_existing_rings.xml</li><li><span class="ds-label">ID: </span>weight-change</li></ul></div></div></div></div><p>
   At this stage, no changes have been made to the input model. However, when
   you set the <code class="literal">weight-step</code> attribute, the rings that were
   rebuilt in the previous rebalance phase have weights that are different than
   their target/final value. You gradually move to the target/final weight by
   rebalancing a number of times as described on this page. For more
   information about the weight-step attribute, see
   <a class="xref" href="#swift-weight-att" title="9.5.2. Using the Weight-Step Attributes to Prepare for Ring Changes">Section 9.5.2, “Using the Weight-Step Attributes to Prepare for Ring Changes”</a>.
  </p><p>
   To begin the re-balancing of the rings, follow these steps:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Rebalance the rings by running the playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-update-from-model-rebalance-rings.yml</pre></div></li><li class="listitem "><p>
     Run the reconfiguration:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li><li class="listitem "><p>
     Wait until replication has finished or <code class="literal">min-part-hours</code>
     has elapsed (whichever is longer). For more information, see
     <a class="xref" href="#topic-ohx-j1t-4t" title="9.5.4. Determining When to Rebalance and Deploy a New Ring">Section 9.5.4, “Determining When to Rebalance and Deploy a New Ring”</a>
    </p></li><li class="listitem "><p>
     Run the following command and review the report:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml --limit SWF*</pre></div><p>
     The following is an example of the output after executing the above
     command. In the example <span class="bold"><strong>no</strong></span> weight changes
     are proposed:
    </p><div class="verbatim-wrap"><pre class="screen">TASK: [swiftlm-ring-supervisor | validate-input-model | Print report] *********
ok: [padawan-ccp-c1-m1-mgmt] =&gt; {
    "var": {
        "report.stdout_lines": [
            "Need to add 0 devices",
            "Need to remove 0 devices",
            "<span class="bold"><strong>Need to set weight on 0 devices</strong></span>"
        ]
    }
}</pre></div></li><li class="listitem "><p>
     When there are no proposed weight changes, you proceed to the final phase.
    </p></li><li class="listitem "><p>
     If there are proposed weight changes repeat this phase again.
    </p></li></ol></div></div><div class="sect3" id="final-rebalance"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.5.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Final Rebalance Phase</span> <a title="Permalink" class="permalink" href="#final-rebalance">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-input_model_change_existing_rings.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-input_model_change_existing_rings.xml</li><li><span class="ds-label">ID: </span>final-rebalance</li></ul></div></div></div></div><p>
   The final rebalance phase moves all replicas to their final destination.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Rebalance the rings by running the playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-update-from-model-rebalance-rings.yml | tee /tmp/rebalance.log</pre></div><div id="id-1.5.11.10.11.6.3.1.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The output is saved for later reference.
     </p></div></li><li class="listitem "><p>
     Review the output from the previous step. If the output for all rings is
     similar to the following, the rebalance had no effect. That is, the rings
     are balanced and no further changes are needed. In addition, the ring
     files were not changed so you do not need to deploy them to the swift
     nodes:
    </p><div class="verbatim-wrap"><pre class="screen">"Running: swift-ring-builder /etc/swiftlm/cloud1/cp1/builder_dir/account.builder rebalance 999",
      "NOTE: No partitions could be reassigned.",
      "Either none need to be or none can be due to min_part_hours [16]."</pre></div><p>
     The text <span class="bold"><strong>No partitions could be
     reassigned</strong></span> indicates that no further rebalances are necessary.
     If this is true for all the rings, you have completed the final phase.
    </p><div id="id-1.5.11.10.11.6.3.2.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      You must have allowed enough time to elapse since the last rebalance.
      As mentioned in the above example, <code class="literal">min_part_hours [16]</code>
      means that you must wait at least 16 hours since the last rebalance. If
      not, you should wait until enough time has elapsed and repeat this
      phase.
     </p></div></li><li class="listitem "><p>
     Run the <code class="literal">swift-reconfigure.yml</code> playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li><li class="listitem "><p>
     Wait until replication has finished or <code class="literal">min-part-hours</code>
     has elapsed (whichever is longer). For more information see
     <a class="xref" href="#topic-ohx-j1t-4t" title="9.5.4. Determining When to Rebalance and Deploy a New Ring">Section 9.5.4, “Determining When to Rebalance and Deploy a New Ring”</a>
    </p></li><li class="listitem "><p>
     Repeat the above steps until the ring is rebalanced.
    </p></li></ol></div></div><div class="sect3" id="system-changes"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.5.5.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">System Changes that Change Existing Rings</span> <a title="Permalink" class="permalink" href="#system-changes">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-input_model_change_existing_rings.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-input_model_change_existing_rings.xml</li><li><span class="ds-label">ID: </span>system-changes</li></ul></div></div></div></div><p>
   There are many system changes ranging from adding servers to replacing
   drives, which might require you to rebuild and rebalance your rings.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th><span class="bold"><strong>Actions</strong></span>
      </th><th><span class="bold"><strong>Process</strong></span>
      </th></tr></thead><tbody><tr><td>Adding Servers(s)</td><td>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          If not already done, set the weight change attribute. For
          instructions, see <a class="xref" href="#swift-weight-att" title="9.5.2. Using the Weight-Step Attributes to Prepare for Ring Changes">Section 9.5.2, “Using the Weight-Step Attributes to Prepare for Ring Changes”</a>
         </p></li><li class="listitem "><p>
          Add servers in phases:
         </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
            This reduces the impact of the changes on your system.
           </p></li><li class="listitem "><p>
            If your rings use swift zones, ensure that you add the same
            number of servers to each zone at each phase.
           </p></li></ul></div></li></ul></div>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          To add a server, see <a class="xref" href="#sec-swift-add-object-node" title="15.1.5.1.1. Adding a Swift Object Node">Section 15.1.5.1.1, “Adding a Swift Object Node”</a>.
         </p></li><li class="listitem "><p>
          Incrementally change the weights and perform the final rebalance. For
          instructions, see <a class="xref" href="#final-rebalance" title="9.5.5.4. Final Rebalance Phase">Section 9.5.5.4, “Final Rebalance Phase”</a>.
         </p></li></ul></div>
      </td></tr><tr><td>Removing Server(s)</td><td>
       <p>
        In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, when you remove servers from the input model, the
        disk drives are removed from the ring - the weight is not gradually
        reduced using the <code class="literal">weight-step</code> attribute.
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          Remove servers in phases:
         </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
            This reduces the impact of the changes on your system.
           </p></li><li class="listitem "><p>
            If your rings use swift zones, ensure you remove the same number of
            servers for each zone at each phase.
           </p></li></ul></div></li></ul></div>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          To remove a server, see <a class="xref" href="#remove-swift-node" title="15.1.5.1.4. Removing a Swift Node">Section 15.1.5.1.4, “Removing a Swift Node”</a>.
         </p></li><li class="listitem "><p>
          To perform the final rebalance, see <a class="xref" href="#final-rebalance" title="9.5.5.4. Final Rebalance Phase">Section 9.5.5.4, “Final Rebalance Phase”</a>.
         </p></li></ul></div>
      </td></tr><tr><td>
       Adding Disk Drive(s)
      </td><td>
	<div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
	   If not already done, set the weight attribute. For instructions, see <a class="xref" href="#swift-weight-att" title="9.5.2. Using the Weight-Step Attributes to Prepare for Ring Changes">Section 9.5.2, “Using the Weight-Step Attributes to Prepare for Ring Changes”</a>.
	  </p></li><li class="listitem "><p>
	   Add the drives to your input model.
	  </p></li></ul></div>
      </td></tr><tr><td>Replacing Disk Drive(s)</td><td>
       <p>
        When a drive fails, replace it as soon as possible. Do not attempt to
        remove it from the ring - this creates operator overhead. swift will
        continue to store the correct number of replicas by handing off objects
        to other drives instead of the failed drive.
       </p>
       <p>
        If the disk drives are of the same size as the original when the
        drive is replaced, no ring changes are required. You can confirm this
        by running the
        <code class="literal">swift-update-from-model-rebalance-rings.yml</code>
        playbook. It should report that no weight changes are needed.
       </p>
       <p>
        For a single drive replacement, even if the drive is significantly
        larger than the original drives, you do not need to rebalance the ring
        (however, the extra space on the drive will not be used).
       </p>
      </td></tr><tr><td>Upgrading Disk Drives</td><td>
       <p>
        If the drives are different size (for example, you are upgrading your
        system), you can proceed as follows:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          If not already done, set the weight-step attribute
         </p></li><li class="listitem "><p>
          Replace drives in phases:
         </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
            Avoid replacing too many drives at once.
           </p></li><li class="listitem "><p>
            If your rings use swift zones, upgrade a number of drives in the
            same zone at the same time - not drives in several zones.
           </p></li><li class="listitem "><p>
            It is also safer to upgrade one server instead of drives in several
            servers at the same time.
           </p></li><li class="listitem "><p>
            Remember that the final size of all swift zones must be the same,
            so you may need to replace a small number of drives in one zone,
            then a small number in second zone, then return to the first zone
            and replace more drives, etc.
           </p></li></ul></div></li></ul></div>
      </td></tr><tr><td>
       Removing Disk Drive(s)
      </td><td>
       <p>
	When removing a disk drive from the input model, keep in mind that this
	drops the disk out of the ring without allowing Swift to move the data
	off it first. While it should be fine in a properly replicated healthy
	cluster, we do not recommend this approach. A better solution is to
	step down <code class="literal">weight_step</code> to 0 to allow Swift to move
	data.
       </p>
      </td></tr></tbody></table></div></div></div><div class="sect2" id="add-storage-policy"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.5.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a New Swift Storage Policy</span> <a title="Permalink" class="permalink" href="#add-storage-policy">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-add_new_storage_policy.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-add_new_storage_policy.xml</li><li><span class="ds-label">ID: </span>add-storage-policy</li></ul></div></div></div></div><p>
  This page describes how to add an additional storage policy to an existing
  system. For an overview of storage policies, see
  <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 11 “Modifying Example Configurations for Object Storage using Swift”, Section 11.11 “Designing Storage Policies”</span>.
 </p><p>
  <span class="bold"><strong>To Add a Storage Policy</strong></span>
 </p><p>
  Perform the following steps to add the storage policy to an existing system.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="listitem "><p>
    Select a storage policy index and ring name.
   </p><p>
    For example, if you already have object-0 and object-1 rings in your
    ring-specifications (usually in the
    <code class="filename">~/openstack/my_cloud/definition/data/swift/swift_config.yml</code> file),
    the next index is 2 and the ring name is object-2.
   </p></li><li class="listitem "><p>
    Select a user-visible name so that you can see when you examine container
    metadata or when you want to specify the storage policy used when you
    create a container. The name should be a single word (hyphen and dashes are
    allowed).
   </p></li><li class="listitem "><p>
    Decide if this new policy will be the default for all new containers.
   </p></li><li class="listitem "><p>
    Decide on other attributes such as <code class="literal">partition-power</code> and
    <code class="literal">replica-count</code> if you are using a standard replication
    ring. However, if you are using an erasure coded ring, you also need to
    decide on other attributes: <code class="literal">ec-type</code>,
    <code class="literal">ec-num-data-fragments</code>,
    <code class="literal">ec-num-parity-fragments</code>, and
    <code class="literal">ec-object-segment-size</code>. For more details on the required
    attributes, see <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 11 “Modifying Example Configurations for Object Storage using Swift”, Section 11.10 “Understanding Swift Ring Specifications”</span>.
   </p></li><li class="listitem "><p>
    Edit the <code class="literal">ring-specifications</code> attribute (usually in the
    <code class="filename">~/openstack/my_cloud/definition/data/swift/swift_config.yml</code> file)
    and add the new ring specification. If this policy is to be the default
    storage policy for new containers, set the <code class="literal">default</code>
    attribute to <span class="bold"><strong>yes</strong></span>.
   </p><div id="id-1.5.11.10.12.5.6.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       Ensure that only one object ring has the
       <code class="literal">default</code> attribute set to <code class="literal">yes</code>. If
       you set two rings as default, swift processes will not start.
      </p></li><li class="listitem "><p>
       Do not specify the <code class="literal">weight-step</code> attribute for the new
       object ring. Since this is a new ring there is no need to gradually
       increase device weights.
      </p></li></ol></div></div></li><li class="listitem "><p>
    Update the appropriate disk model to use the new storage policy (for
    example, the <code class="literal">data/disks_swobj.yml</code> file). The following
    sample shows that the <span class="bold"><strong>object-2</strong></span> has been
    added to the list of existing rings that use the drives:
   </p><div class="verbatim-wrap"><pre class="screen">disk-models:
- name: SWOBJ-DISKS
  ...
  device-groups:
  - name: swobj
    devices:
       ...
    consumer:
        name: swift
        attrs:
            rings:
            - object-0
            - object-1
            - <span class="bold"><strong>object-2</strong></span>
  ...</pre></div><div id="id-1.5.11.10.12.5.7.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     You must use the new object ring on at least one node that runs the
     <code class="literal">swift-object</code> service. If you skip this step and
     continue to run the <code class="literal">swift-compare-model-rings.yml</code> or
     <code class="literal">swift-deploy.yml</code> playbooks, they will fail with an
     error <span class="emphasis"><em>There are no devices in this ring, or all devices have
     been deleted</em></span>, as shown below:
    </p><div class="verbatim-wrap"><pre class="screen">TASK: [swiftlm-ring-supervisor | build-rings | Build ring (make-delta, rebalance)] ***
failed: [padawan-ccp-c1-m1-mgmt] =&gt; {"changed": true, "cmd": ["swiftlm-ring-supervisor", "--make-delta", "--rebalance"], "delta": "0:00:03.511929", "end": "2015-10-07 14:02:03.610226", "rc": 2, "start": "2015-10-07 14:02:00.098297", "warnings": []}
...
Running: swift-ring-builder /etc/swiftlm/cloud1/cp1/builder_dir/object-2.builder rebalance 999
ERROR: -------------------------------------------------------------------------------
An error has occurred during ring validation. Common
causes of failure are rings that are empty or do not
have enough devices to accommodate the replica count.
Original exception message:
There are no devices in this ring, or all devices have been deleted
-------------------------------------------------------------------------------</pre></div></div></li><li class="listitem "><p>
    Commit your configuration:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "commit message"</pre></div></li><li class="listitem "><p>
    Run the configuration processor:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
    Create a deployment directory:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
    Validate the changes by running the
    <code class="literal">swift-compare-model-rings.yml</code> playbook:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml</pre></div><p>
    If any errors occur, correct them. For instructions, see
    <a class="xref" href="#sec-input-swift-error" title="18.6.2.3. Interpreting Swift Input Model Validation Errors">Section 18.6.2.3, “Interpreting Swift Input Model Validation Errors”</a>. Then, re-run steps
    <span class="bold"><strong>5 - 10</strong></span>.
    
   </p></li><li class="listitem "><p>
    Create the new ring (for example, object-2). Then verify the swift service
    status and reconfigure the swift node to use a new storage policy, by
    running these playbooks:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-status.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-deploy.yml</pre></div></li></ol></div><p>
  After adding a storage policy, there is no need to rebalance the ring.
 </p></div><div class="sect2" id="min-part-hours"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.5.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Changing min-part-hours in Swift</span> <a title="Permalink" class="permalink" href="#min-part-hours">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-swift_min_part_hours.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_min_part_hours.xml</li><li><span class="ds-label">ID: </span>min-part-hours</li></ul></div></div></div></div><p>
  The <code class="literal">min-part-hours</code> parameter specifies the number of
  hours you must wait before swift will allow a given partition to be moved.
  In other words, it constrains how often you perform ring rebalance
  operations. Before changing this value, you should get some experience with
  how long it takes your system to perform replication after you make ring
  changes (for example, when you add servers).
 </p><p>
  See <a class="xref" href="#topic-ohx-j1t-4t" title="9.5.4. Determining When to Rebalance and Deploy a New Ring">Section 9.5.4, “Determining When to Rebalance and Deploy a New Ring”</a> for more information about
  determining when replication has completed.
 </p><div class="sect3" id="idg-all-operations-objectstorage-swift-min-part-hours-xml-7"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.5.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Changing the min-part-hours Value</span> <a title="Permalink" class="permalink" href="#idg-all-operations-objectstorage-swift-min-part-hours-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-swift_min_part_hours.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_min_part_hours.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-objectstorage-swift-min-part-hours-xml-7</li></ul></div></div></div></div><p>
   To change the <code class="literal">min-part-hours</code> value, following these
   steps:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit your
     <code class="filename">~/openstack/my_cloud/definition/data/swift/swift_config.yml</code> file
     and change the value(s) of <code class="literal">min-part-hours</code> for the rings
     you desire. The value is expressed in hours and a value of zero is not
     allowed.
    </p></li><li class="listitem "><p>
     Commit your configuration to the local Git repository
     (<span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Apply the changes by running this playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-deploy.yml</pre></div></li></ol></div></div></div><div class="sect2" id="changing-swift-zone"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.5.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Changing Swift Zone Layout</span> <a title="Permalink" class="permalink" href="#changing-swift-zone">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-changing_swift_zone.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-changing_swift_zone.xml</li><li><span class="ds-label">ID: </span>changing-swift-zone</li></ul></div></div></div></div><p>
  Before changing the number of swift zones or the assignment of servers to
  specific zones, you must ensure that your system has sufficient storage
  available to perform the operation. Specifically, if you are adding a new
  zone, you may need additional storage. There are two reasons for this:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    You cannot simply change the swift zone number of disk drives in the ring.
    Instead, you need to remove the server(s) from the ring and then re-add
    the server(s) with a new swift zone number to the ring. At the point where
    the servers are removed from the ring, there must be sufficient spare
    capacity on the remaining servers to hold the data that was originally
    hosted on the removed servers.
   </p></li><li class="listitem "><p>
    The total amount of storage in each swift zone must be the same. This is
    because new data is added to each zone at the same rate. If one zone has a
    lower capacity than the other zones, once that zone becomes full, you
    cannot add more data to the system – even if there is unused space in
    the other zones.
   </p></li></ul></div><p>
  As mentioned above, you cannot simply change the swift zone number of disk
  drives in an existing ring. Instead, you must remove and then re-add
  servers. This is a summary of the process:
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Identify appropriate server groups that correspond to the desired swift
    zone layout.
   </p></li><li class="listitem "><p>
    Remove the servers in a server group from the rings. This process may be
    protracted, either by removing servers in small batches or by using the
    weight-step attribute so that you limit the amount of replication traffic
    that happens at once.
   </p></li><li class="listitem "><p>
    Once all the targeted servers are removed, edit the
    <code class="literal">swift-zones</code> attribute in the ring specifications to add
    or remove a swift zone.
   </p></li><li class="listitem "><p>
    Re-add the servers you had temporarily removed to the rings. Again you may
    need to do this in batches or rely on the weight-step attribute.
   </p></li><li class="listitem "><p>
    Continue removing and re-adding servers until you reach your final
    configuration.
   </p></li></ol></div><div class="sect3" id="idg-all-operations-objectstorage-changing-swift-zone-xml-5"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.5.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Process for Changing Swift Zones</span> <a title="Permalink" class="permalink" href="#idg-all-operations-objectstorage-changing-swift-zone-xml-5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-changing_swift_zone.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-changing_swift_zone.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-objectstorage-changing-swift-zone-xml-5</li></ul></div></div></div></div><p>
   This section describes the detailed process or reorganizing swift zones. As
   a concrete example, we assume we start with a single swift zone and the
   target is three swift zones. The same general process would apply if you
   were reducing the number of zones as well.
  </p><p>
   The process is as follows:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Identify the appropriate server groups that represent the desired final
     state. In this example, we are going to change the swift zone layout as
     follows:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Original Layout</th><th>Target Layout</th></tr></thead><tbody><tr><td>
<div class="verbatim-wrap"><pre class="screen">swift-zones:
  - 1d: 1
    server-groups:
       - AZ1
       - AZ2
       - AZ3</pre></div>
        </td><td>
<div class="verbatim-wrap"><pre class="screen">swift-zones:
   - 1d: 1
     server-groups:
        - AZ1
   - id: 2
        - AZ2
   - id: 3
        - AZ3</pre></div>
        </td></tr></tbody></table></div><p>
     The plan is to move servers from server groups <code class="literal">AZ2</code> and
     <code class="literal">AZ3</code> to a new swift zone number. The servers in
     <code class="literal">AZ1</code> will remain in swift zone 1.
    </p></li><li class="listitem "><p>
     If you have not already done so, consider setting the weight-step
     attribute as described in <a class="xref" href="#swift-weight-att" title="9.5.2. Using the Weight-Step Attributes to Prepare for Ring Changes">Section 9.5.2, “Using the Weight-Step Attributes to Prepare for Ring Changes”</a>.
    </p></li><li class="listitem "><p>
     Identify the servers in the <code class="literal">AZ2</code> server group. You may
     remove all servers at once or remove them in batches. If this is the first
     time you have performed a major ring change, we suggest you remove one or
     two servers only in the first batch. When you see how long this takes and
     the impact replication has on your system you can then use that experience
     to decide whether you can remove a larger batch of servers, or increase or
     decrease the weight-step attribute for the next server-removal cycle. To
     remove a server, use steps 2-9 as described in
     <a class="xref" href="#remove-swift-node" title="15.1.5.1.4. Removing a Swift Node">Section 15.1.5.1.4, “Removing a Swift Node”</a> ensuring that you
     do not remove the servers from the input model.
     
    </p></li><li class="listitem "><p>
     This process may take a number of ring rebalance cycles until the disk
     drives are removed from the ring files. Once this happens, you can edit
     the ring specifications and add swift zone 2 as shown in this example:
    </p><div class="verbatim-wrap"><pre class="screen">swift-zones:
  - id: 1
    server-groups:
      - AZ1
      - AZ3
  - id: 2
       - AZ2</pre></div></li><li class="listitem "><p>
     The server removal process in step #3
     
     set the "remove" attribute in the
     <code class="literal">pass-through</code> attribute of the servers in server group
     <code class="literal">AZ2</code>. Edit the input model files and remove this
     <code class="literal">pass-through</code> attribute. This signals to the system that
     the servers should be used the next time we rebalance the rings (that is,
     the server should be added to the rings).
    </p></li><li class="listitem "><p>
     Commit your configuration to the local Git repository
     (<span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Use the playbook to create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Rebuild and deploy the swift rings containing the re-added servers by
     running this playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-deploy.yml</pre></div></li><li class="listitem "><p>
     Wait until replication has finished. For more details, see
     <a class="xref" href="#topic-ohx-j1t-4t" title="9.5.4. Determining When to Rebalance and Deploy a New Ring">Section 9.5.4, “Determining When to Rebalance and Deploy a New Ring”</a>.
    </p></li><li class="listitem "><p>
     You may need to continue to rebalance the rings. For instructions, see the
     "Final Rebalance Stage" steps at <a class="xref" href="#change-swift-rings" title="9.5.5. Applying Input Model Changes to Existing Rings">Section 9.5.5, “Applying Input Model Changes to Existing Rings”</a>.
    </p></li><li class="listitem "><p>
     At this stage, the servers in server group <code class="literal">AZ2</code> are
     responsible for swift zone 2. Repeat the process in steps #3-9 to remove
     the servers in server group <code class="literal">AZ3</code> from the rings and then
     re-add them to swift zone 3. The ring specifications for zones (step 4)
     should be as follows:
    </p><div class="verbatim-wrap"><pre class="screen">swift-zones:
  - 1d: 1
    server-groups:
      - AZ1
  - id: 2
      - AZ2
  - id: 3
      - AZ3</pre></div></li><li class="listitem "><p>
     Once complete, all data should be dispersed (that is, each replica is
     located) in the swift zones as specified in the input model.
    </p></li></ol></div></div></div></div><div class="sect1" id="topic-el2-cqv-mv"><div class="titlepage"><div><div><h2 class="title"><span class="number">9.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring your swift System to Allow Container Sync</span> <a title="Permalink" class="permalink" href="#topic-el2-cqv-mv">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-swift_container_sync.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_container_sync.xml</li><li><span class="ds-label">ID: </span>topic-el2-cqv-mv</li></ul></div></div></div></div><p>
  swift has a feature where all the contents of a container can be mirrored to
  another container through background synchronization. swift operators
  configure their system to allow/accept sync requests to/from other systems,
  and the user specifies where to sync their container to along with a secret
  synchronization key. For an overview of this feature, refer to
  <a class="link" href="http://docs.openstack.org/developer/swift/overview_container_sync.html" target="_blank">OpenStack
  swift - Container to Container Synchronization</a>.
 </p><div class="sect2" id="idg-all-operations-objectstorage-swift-container-sync-xml-5"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Notes and limitations</span> <a title="Permalink" class="permalink" href="#idg-all-operations-objectstorage-swift-container-sync-xml-5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-swift_container_sync.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_container_sync.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-objectstorage-swift-container-sync-xml-5</li></ul></div></div></div></div><p>
   The container synchronization is done as a background action. When you put
   an object into the source container, it will take some time before it
   becomes visible in the destination container. Storage services will not
   necessarily copy objects in any particular order, meaning they may be
   transferred in a different order to which they were created.
  </p><p>
   Container sync may not be able to keep up with a moderate upload rate to a
   container. For example, if the average object upload rate to a container is
   greater than one object per second, then container sync may not be able to
   keep the objects synced.
  </p><p>
   If container sync is enabled on a container that already has a large number
   of objects then container sync may take a long time to sync the data. For
   example, a container with one million 1KB objects could take more than 11
   days to complete a sync.
  </p><p>
   You may operate on the destination container just like any other container
   -- adding or deleting objects -- including the objects that are in the
   destination container because they were copied from the source container. To
   decide how to handle object creation, replacement or deletion, the system
   uses timestamps to determine what to do. In general, the latest timestamp
   "wins". That is, if you create an object, replace it, delete it and the
   re-create it, the destination container will eventually contain the most
   recently created object. However, if you also create and delete objects in
   the destination container, you get some subtle behaviours as follows:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     If an object is copied to the destination container and then deleted, it
     remains deleted in the destination even though there is still a copy in
     the source container. If you modify the object (replace or change its
     metadata) in the source container, it will reappear in the destination
     again.
    </p></li><li class="listitem "><p>
     The same applies to a replacement or metadata modification of an object in
     the destination container -- the object will remain as-is unless there is
     a replacement or modification in the source container.
    </p></li><li class="listitem "><p>
     If you replace or modify metadata of an object in the destination
     container and then delete it in the source container, it is
     <span class="bold"><strong>not</strong></span> deleted from the destination. This is
     because your modified object has a later timestamp than the object you
     deleted in the source.
    </p></li><li class="listitem "><p>
     If you create an object in the source container and before the system has
     a chance to copy it to the destination, you also create an object of the
     same name in the destination, then the object in the destination is
     <span class="bold"><strong>not</strong></span> overwritten by the source container's
     object.
    </p></li></ul></div><p>
   <span class="bold"><strong>Segmented objects</strong></span>
  </p><p>
   Segmented objects (objects larger than 5GB) will not work seamlessly with
   container synchronization. If the manifest object is copied to the
   destination container before the object segments, when you perform a GET
   operation on the manifest object, the system may fail to find some or all of
   the object segments. If your manifest and object segments are in different
   containers, do not forget that both containers must be synchonized and that
   the container name of the object segments must be the same on both source
   and destination.
  </p></div><div class="sect2" id="idg-all-operations-objectstorage-swift-container-sync-xml-6"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#idg-all-operations-objectstorage-swift-container-sync-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-swift_container_sync.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_container_sync.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-objectstorage-swift-container-sync-xml-6</li></ul></div></div></div></div><p>
   Container to container synchronization requires that SSL certificates are
   configured on both the source and destination systems. For more information
   on how to implement SSL, see <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 41 “Configuring Transport Layer Security (TLS)”</span>.
  </p></div><div class="sect2" id="configure"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring container sync</span> <a title="Permalink" class="permalink" href="#configure">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-swift_container_sync.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_container_sync.xml</li><li><span class="ds-label">ID: </span>configure</li></ul></div></div></div></div><p>
   Container to container synchronization requires that both the source and
   destination swift systems involved be configured to allow/accept this.  In
   the context of container to container synchronization, swift uses the term
   <span class="emphasis"><em>cluster</em></span> to denote a swift system. swift
   <span class="emphasis"><em>clusters</em></span> correspond to <span class="emphasis"><em>Control
   Planes</em></span> in <span class="productname">OpenStack</span> terminology.
   </p><p>
   <span class="bold"><strong>Gather the public API endpoints for both swift
   systems</strong></span>
  </p><p>
   Gather information about the external/public URL used by each system, as
   follows:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     On the Cloud Lifecycle Manager of one system, get the public API endpoint of the
     system by running the following commands:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>openstack endpoint list | grep swift</pre></div><p>
     The output of the command will look similar to this:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack endpoint list | grep swift
| 063a84b205c44887bc606c3ba84fa608 | region0 | swift           | object-store    | True    | admin     | https://10.13.111.176:8080/v1/AUTH_%(tenant_id)s |
| 3c46a9b2a5f94163bb5703a1a0d4d37b | region0 | swift           | object-store    | True    | public    | <span class="bold"><strong>https://10.13.120.105:8080/v1</strong></span>/AUTH_%(tenant_id)s |
| a7b2f4ab5ad14330a7748c950962b188 | region0 | swift           | object-store    | True    | internal  | https://10.13.111.176:8080/v1/AUTH_%(tenant_id)s |</pre></div><p>
     The portion that you want is the endpoint up to, but not including, the
     <code class="literal">AUTH</code> part. It is bolded in the above example,
     <code class="literal">https://10.13.120.105:8080/v1</code>.
    </p></li><li class="listitem "><p>
     Repeat these steps on the other swift system so you have both of the
     public API endpoints for them.
    </p></li></ol></div><p>
   <span class="bold"><strong>Validate connectivity between both systems</strong></span>
  </p><p>
   The swift nodes running the <code class="literal">swift-container</code> service must
   be able to connect to the public API endpoints of each other for the
   container sync to work. You can validate connectivity on each system using
   these steps.
  </p><p>
   For the sake of the examples, we will use the terms
   <span class="emphasis"><em>source</em></span> and <span class="emphasis"><em>destination</em></span> to notate
   the nodes doing the synchronization.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to a swift node running the <code class="literal">swift-container</code>
     service on the source system. You can determine this by looking at the
     service list in your
     <code class="literal">~/openstack/my_cloud/info/service_info.yml</code> file for a list
     of the servers containing this service.
    </p></li><li class="listitem "><p>
     Verify the SSL certificates by running this command against the
     destination swift server:
    </p><div class="verbatim-wrap"><pre class="screen">echo | openssl s_client -connect <em class="replaceable ">PUBLIC_API_ENDPOINT</em>:8080 -CAfile /etc/ssl/certs/ca-certificates.crt</pre></div><p>
     If the connection was successful you should see a return code of
     <code class="literal">0 (ok)</code> similar to this:
    </p><div class="verbatim-wrap"><pre class="screen">...
Timeout   : 300 (sec)
Verify return code: 0 (ok)</pre></div></li><li class="listitem "><p>
     Also verify that the source node can connect to the destination swift
     system using this command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>curl -k <em class="replaceable ">DESTINATION_IP OR HOSTNAME</em>:8080/healthcheck</pre></div><p>
     If the connection was successful, you should see a response of
     <code class="literal">OK</code>.
    </p></li><li class="listitem "><p>
     Repeat these verification steps on any system involved in your container
     synchronization setup.
    </p></li></ol></div><p>
   <span class="bold"><strong>Configure container to container
   synchronization</strong></span>
  </p><p>
   Both the source and destination swift systems must be configured the same
   way, using sync realms. For more details on how sync realms work, see
   <a class="link" href="http://docs.openstack.org/developer/swift/overview_container_sync.html#configuring-container-sync" target="_blank">OpenStack
   swift - Configuring Container Sync</a>.
  </p><p>
   To configure one of the systems, follow these steps:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the
     <code class="literal">~/openstack/my_cloud/config/swift/container-sync-realms.conf.j2</code>
     file and uncomment the sync realm section.
    </p><p>
     Here is a sample showing this section in the file:
    </p><div class="verbatim-wrap"><pre class="screen">#Add sync realms here, for example:
# [realm1]
# key = realm1key
# key2 = realm1key2
# cluster_name1 = https://host1/v1/
# cluster_name2 = https://host2/v1/</pre></div></li><li class="listitem "><p>
     Add in the details for your source and destination systems. Each realm you
     define is a set of clusters that have agreed to allow container syncing
     between them. These values are case sensitive.
    </p><p>
     Only one <code class="literal">key</code> is required. The second key is optional
     and can be provided to allow an operator to rotate keys if desired. The
     values for the clusters must contain the prefix
     <code class="literal">cluster_</code> and will be populated with the public API
     endpoints for the systems.
    </p></li><li class="listitem "><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "Add node &lt;name&gt;"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update the deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the swift reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li><li class="listitem "><p>
     Run this command to validate that your container synchronization is
     configured:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>swift capabilities</pre></div><p>
     Here is a snippet of the output showing the container sync information.
     This should be populated with your cluster names:
    </p><div class="verbatim-wrap"><pre class="screen">...
Additional middleware: container_sync
 Options:
  realms: {u'INTRACLUSTER': {u'clusters': {u'THISCLUSTER': {}}}}</pre></div></li><li class="listitem "><p>
     Repeat these steps on any other swift systems that will be involved in
     your sync realms.
    </p></li></ol></div></div><div class="sect2" id="intra-cluster-sync"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Intra Cluster Container Sync</span> <a title="Permalink" class="permalink" href="#intra-cluster-sync">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-objectstorage-swift_container_sync.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-objectstorage-swift_container_sync.xml</li><li><span class="ds-label">ID: </span>intra-cluster-sync</li></ul></div></div></div></div><p>
   It is possible to use the swift container sync functionality to sync objects
   between containers within the same swift system. swift is automatically
   configured to allow intra cluster container sync. Each swift PAC server will
   have an intracluster container sync realm defined in
   <code class="literal">/etc/swift/container-sync-realms.conf</code>.
  </p><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"># The intracluster realm facilitates syncing containers on this system
[intracluster]
key = lQ8JjuZfO
# key2 =
cluster_thiscluster = http://<em class="replaceable ">SWIFT-PROXY-VIP</em>:8080/v1/</pre></div><p>
   The keys defined in <code class="literal">/etc/swift/container-sync-realms.conf</code>
   are used by the container-sync daemon to determine trust. On top of this
   the containers that will be in sync will need a seperate shared key they
   both define in container metadata to establish their trust between each other.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Create two containers, for example container-src and container-dst. In
     this example we will sync one way from container-src to container-dst.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack container create container-src
<code class="prompt user">ardana &gt; </code>openstack container create container-dst</pre></div></li><li class="listitem "><p>
     Determine your swift account. In the following example it is AUTH_1234
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack container show
                                 Account: AUTH_1234
                              Containers: 3
                                 Objects: 42
                                   Bytes: 21692421
Containers in policy "erasure-code-ring": 3
   Objects in policy "erasure-code-ring": 42
     Bytes in policy "erasure-code-ring": 21692421
                            Content-Type: text/plain; charset=utf-8
             X-Account-Project-Domain-Id: default
                             X-Timestamp: 1472651418.17025
                              X-Trans-Id: tx81122c56032548aeae8cd-0057cee40c
                           Accept-Ranges: bytes</pre></div></li><li class="listitem "><p>
     Configure container-src to sync to container-dst using a key specified
     by both containers. Replace <em class="replaceable ">KEY</em> with your key.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack container set -t '//intracluster/thiscluster/AUTH_1234/container-dst' -k '<em class="replaceable ">KEY</em>' container-src</pre></div></li><li class="listitem "><p>
     Configure container-dst to accept synced objects with this key
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack container set -k '<em class="replaceable ">KEY</em>' container-dst</pre></div></li><li class="listitem "><p>
     Upload objects to container-src. Within a number of minutes the objects
     should be automatically synced to container-dst.
    </p></li></ol></div><p>
   <span class="bold"><strong>Changing the intracluster realm key</strong></span>
  </p><p>
   The intracluster realm key used by container sync to sync objects between
   containers in the same swift system is automatically generated. The process
   for changing passwords is described in
   <a class="xref" href="#servicePasswords" title="5.7. Changing Service Passwords">Section 5.7, “Changing Service Passwords”</a>.
  </p><p>
   The steps to change the intracluster realm key are as follows.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     On the Cloud Lifecycle Manager create a file called
     <code class="literal">~/openstack/change_credentials/swift_data_metadata.yml</code>
     with the contents included below. The <code class="literal">consuming-cp</code> and
     <code class="literal">cp</code> are the control plane name specified in
     <code class="literal">~/openstack/my_cloud/definition/data/control_plane.yml</code>
     where the swift-container service is running.
    </p><div class="verbatim-wrap"><pre class="screen">swift_intracluster_sync_key:
 metadata:
 - clusters:
   - swpac
   component: swift-container
   consuming-cp: control-plane-1
   cp: control-plane-1
 version: '2.0'</pre></div></li><li class="listitem "><p>
     Run the following commands
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Reconfigure the swift credentials
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure-credentials-change.yml</pre></div></li><li class="listitem "><p>
     Delete
     <code class="literal">~/openstack/change_credentials/swift_data_metadata.yml</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>rm ~/openstack/change_credentials/swift_data_metadata.yml</pre></div></li><li class="listitem "><p>
     On a swift PAC server check that the intracluster realm key has been
     updated in <code class="literal">/etc/swift/container-sync-realms.conf</code>
    </p><div class="verbatim-wrap"><pre class="screen"># The intracluster realm facilitates syncing containers on this system
[intracluster]
key = aNlDn3kWK</pre></div></li><li class="listitem "><p>
     Update any containers using the intracluster container sync to use the new
     intracluster realm key
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack container set -k 'aNlDn3kWK' container-src
<code class="prompt user">ardana &gt; </code>openstack container set -k 'aNlDn3kWK' container-dst</pre></div></li></ol></div></div></div></div><div class="chapter " id="ops-managing-networking"><div class="titlepage"><div><div><h1 class="title"><span class="number">10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Networking</span> <a title="Permalink" class="permalink" href="#ops-managing-networking">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-managing_networking.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_networking.xml</li><li><span class="ds-label">ID: </span>ops-managing-networking</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#topic-gll-nsn-15"><span class="number">10.1 </span><span class="name">SUSE <span class="productname">OpenStack</span> Cloud Firewall</span></a></span></dt><dt><span class="section"><a href="#UsingVPNaaS"><span class="number">10.2 </span><span class="name">Using VPN as a Service (VPNaaS)</span></a></span></dt><dt><span class="section"><a href="#designateOverview"><span class="number">10.3 </span><span class="name">DNS Service Overview</span></a></span></dt><dt><span class="section"><a href="#neutron-overview"><span class="number">10.4 </span><span class="name">Networking Service Overview</span></a></span></dt><dt><span class="section"><a href="#CreateHARouter"><span class="number">10.5 </span><span class="name">Creating a Highly Available Router</span></a></span></dt></dl></div></div><p>
  Information about managing and configuring the Networking service.
 </p><div class="sect1" id="topic-gll-nsn-15"><div class="titlepage"><div><div><h2 class="title"><span class="number">10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE <span class="productname">OpenStack</span> Cloud Firewall</span> <a title="Permalink" class="permalink" href="#topic-gll-nsn-15">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configure_firewall.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configure_firewall.xml</li><li><span class="ds-label">ID: </span>topic-gll-nsn-15</li></ul></div></div></div></div><p>
  Firewall as a Service (FWaaS) provides the ability to assign network-level,
  port security for all traffic entering an existing tenant network. More
  information on this service can be found in the public OpenStack
  documentation located at <a class="link" href="http://specs.openstack.org/openstack/neutron-specs/specs/api/firewall_as_a_service__fwaas_.html" target="_blank">http://specs.openstack.org/openstack/neutron-specs/specs/api/firewall_as_a_service__fwaas_.html</a>.
  The following documentation provides command-line interface example
  instructions for configuring and testing a SUSE <span class="productname">OpenStack</span> Cloud firewall. FWaaS can also
  be configured and managed by the horizon web interface.
 </p><p>
  With SUSE <span class="productname">OpenStack</span> Cloud, FWaaS is implemented directly in the L3 agent
  (<span class="emphasis"><em>neutron-l3-agent</em></span>). However if VPNaaS is enabled, FWaaS
  is implemented in the VPNaaS agent (<span class="emphasis"><em>neutron-vpn-agent</em></span>).
  Because FWaaS does not use a separate agent process or start a specific
  service, there currently are no monasca alarms for it.
 </p><p>
  If DVR is enabled, the firewall service currently does not filter traffic
  between OpenStack private networks, also known as <span class="emphasis"><em>east-west
  traffic</em></span> and will only filter traffic from external networks, also
  known as <span class="emphasis"><em>north-south traffic</em></span>.
 </p><div id="id-1.5.12.3.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   The L3 agent must be restarted on each compute node hosting a DVR router
   when removing the FWaaS or adding a new FWaaS. This condition only applies
   when updating existing instances connected to DVR routers. For more
   information, see the <a class="link" href="https://bugs.launchpad.net/neutron/+bug/1845557" target="_blank">upstream
   bug</a>.
  </p></div><div class="sect2" id="id-1.5.12.3.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Overview of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Firewall configuration</span> <a title="Permalink" class="permalink" href="#id-1.5.12.3.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configure_firewall.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configure_firewall.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
  The following instructions provide information about how to identify and
  modify the overall <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> firewall that is configured in front of the
  control services. This firewall is administered only by a cloud admin and is
  not available for tenant use for private network firewall services.
 </p><p>
  During the installation process, the configuration processor will
  automatically generate "allow" firewall rules for each server based on the
  services deployed and block all other ports. These are populated in
  <code class="literal">~/openstack/my_cloud/info/firewall_info.yml</code>, which includes
  a list of all the ports by network, including the addresses on which the
  ports will be opened. This is described in more detail in
  <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 5 “Input Model”, Section 5.2 “Concepts”, Section 5.2.10 “Networking”, Section 5.2.10.5 “Firewall Configuration”</span>.
 </p><p>
  The <code class="literal">firewall_rules.yml</code> file in the input model allows you
  to define additional rules for each network group. You can read more about
  this in <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.15 “Firewall Rules”</span>.
 </p><p>
  The purpose of this document is to show you how to make post-installation
  changes to the firewall rules if the need arises.
 </p><div id="id-1.5.12.3.6.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   This process is not to be confused with Firewall-as-a-Service,
   which is a separate service that enables the ability for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> tenants
   to create north-south, network-level firewalls to provide stateful
   protection to all instances in a private, tenant network. This service is
   optional and is tenant-configured.
  </p></div></div><div class="sect2" id="idg-all-networking-fwaas-xml-9"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 FWaaS Configuration</span> <a title="Permalink" class="permalink" href="#idg-all-networking-fwaas-xml-9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configure_firewall.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configure_firewall.xml</li><li><span class="ds-label">ID: </span>idg-all-networking-fwaas-xml-9</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Check for an enabled firewall.</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     You should check to determine if the firewall is enabled. The output of
     the <span class="emphasis"><em>openstack extension list</em></span> should contain a
     firewall entry.
    </p><div class="verbatim-wrap"><pre class="screen">openstack extension list</pre></div></li><li class="listitem "><p>
     Assuming the external network is already created by the admin, this
     command will show the external network.
    </p><div class="verbatim-wrap"><pre class="screen">openstack network list</pre></div></li></ol></div><p>
   <span class="bold"><strong>Create required assets.</strong></span>
  </p><p>
   Before creating firewalls, you will need to create a network, subnet,
   router, security group rules, start an instance and assign it a floating IP
   address.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Create the network, subnet and router.
    </p><div class="verbatim-wrap"><pre class="screen">openstack network create private
openstack subnet create --name sub private 10.0.0.0/24 --gateway 10.0.0.1
openstack router create router
openstack router add subnet router sub
openstack router set router ext-net</pre></div></li><li class="listitem "><p>
     Create security group rules. Security group rules filter traffic at VM
     level.
    </p><div class="verbatim-wrap"><pre class="screen">openstack security group rule create default --protocol icmp
openstack security group rule create default --protocol tcp --port-range-min 22 --port-range-max 22
openstack security group rule create default --protocol tcp --port-range-min 80 --port-range-max 80</pre></div></li><li class="listitem "><p>
     Boot a VM.
    </p><div class="verbatim-wrap"><pre class="screen">NET=$(openstack network list | awk '/private/ {print $2}')
openstack server create --flavor 1 --image &lt;image&gt; --nic net-id=$NET vm1 --poll</pre></div></li><li class="listitem "><p>
     Verify if the instance is ACTIVE and is assigned an IP address.
    </p><div class="verbatim-wrap"><pre class="screen">openstack server list</pre></div></li><li class="listitem "><p>
     Get the port id of the vm1 instance.
    </p><div class="verbatim-wrap"><pre class="screen">fixedip=$(openstack server list | awk '/vm1/ {print $12}' | awk -F '=' '{print $2}' | awk -F ',' '{print $1}')
vmportuuid=$(openstack port list | grep $fixedip | awk '{print $2}')</pre></div></li><li class="listitem "><p>
     Create and associate a floating IP address to the vm1 instance.
    </p><div class="verbatim-wrap"><pre class="screen">openstack floating ip create ext-net --port-id $vmportuuid</pre></div></li><li class="listitem "><p>
     Verify if the floating IP is assigned to the instance. The following
     command should show an assigned floating IP address from the external
     network range.
    </p><div class="verbatim-wrap"><pre class="screen">openstack server show vm1</pre></div></li><li class="listitem "><p>
     Verify if the instance is reachable from the external network. SSH into
     the instance from a node in (or has route to) the external network.
    </p><div class="verbatim-wrap"><pre class="screen">ssh cirros@FIP-VM1
password: &lt;password&gt;</pre></div></li></ol></div><p>
   <span class="bold"><strong>Create and attach the firewall.</strong></span>
  </p><div id="id-1.5.12.3.7.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    By default, an internal "drop all" rule is enabled in IP tables if none of
    the defined rules match the real-time data packets.
   </p></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Create new firewall rules using <code class="literal">firewall-rule-create</code>
     command and providing the protocol, action (allow, deny, reject) and name
     for the new rule.
    </p><p>
     Firewall actions provide rules in which data traffic can be handled. An
     <span class="bold"><strong>allow</strong></span> rule will allow traffic to pass
     through the firewall,
     <span class="bold"><strong>deny</strong></span>
     will stop and prevent data traffic from passing through the firewall and
     <span class="bold"><strong>reject</strong></span>
     will reject the data traffic and return a
     <span class="emphasis"><em>destination-unreachable</em></span> response. Using
     <span class="bold"><strong>reject</strong></span> will speed up failure detection
     time dramatically for legitimate users, since they will not be required to
     wait for retransmission timeouts or submit retries. Some customers should
     stick with <span class="bold"><strong>deny</strong></span> where prevention of port
     scanners and similar methods may be attempted by hostile attackers. Using
     <span class="bold"><strong>deny</strong></span>
     will drop all of the packets, making it more difficult for malicious
     intent. The firewall action, <span class="bold"><strong>deny</strong></span> is the
     default behavior.
    </p><p>
     The example below demonstrates how to allow icmp and ssh while denying
     access to http. See the <code class="literal">OpenStackClient</code> command-line reference
     at <a class="link" href="https://docs.openstack.org/python-openstackclient/rocky/" target="_blank">https://docs.openstack.org/python-openstackclient/rocky/</a>
     on additional options such as source IP, destination IP, source port and
     destination port.
    </p><div id="id-1.5.12.3.7.9.1.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      You can create a firewall rule with an identical name and each instance
      will have a unique id associated with the created rule, however for
      clarity purposes this is not recommended.
     </p></div><div class="verbatim-wrap"><pre class="screen">neutron firewall-rule-create --protocol icmp --action allow --name allow-icmp
neutron firewall-rule-create --protocol tcp --destination-port 80 --action deny --name deny-http
neutron firewall-rule-create --protocol tcp --destination-port 22 --action allow --name allow-ssh</pre></div></li><li class="listitem "><p>
     Once the rules are created, create the firewall policy by using the
     <code class="literal">firewall-policy-create</code> command with the
     <code class="literal">--firewall-rules</code> option and rules to include in quotes,
     followed by the name of the new policy. The order of the rules is
     important.
    </p><div class="verbatim-wrap"><pre class="screen">neutron firewall-policy-create --firewall-rules "allow-icmp deny-http allow-ssh" policy-fw</pre></div></li><li class="listitem "><p>
     Finish the firewall creation by using the
     <code class="literal">firewall-create</code> command, the policy name and the new
     name you want to give to your new firewall.
    </p><div class="verbatim-wrap"><pre class="screen">neutron firewall-create policy-fw --name user-fw</pre></div></li><li class="listitem "><p>
     You can view the details of your new firewall by using the
     <code class="literal">firewall-show</code> command and the name of your firewall.
     This will verify that the status of the firewall is ACTIVE.
    </p><div class="verbatim-wrap"><pre class="screen">neutron firewall-show user-fw</pre></div></li></ol></div><p>
   <span class="bold"><strong>Verify the FWaaS is functional.</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Since allow-icmp firewall rule is set you can ping the floating IP address
     of the instance from the external network.
    </p><div class="verbatim-wrap"><pre class="screen">ping &lt;FIP-VM1&gt;</pre></div></li><li class="listitem "><p>
     Similarly, you can connect via ssh to the instance due to the allow-ssh
     firewall rule.
    </p><div class="verbatim-wrap"><pre class="screen">ssh cirros@&lt;FIP-VM1&gt;
password: &lt;password&gt;</pre></div></li><li class="listitem "><p>
     Run a web server on vm1 instance that listens over port 80, accepts
     requests and sends a WELCOME response.
    </p><div class="verbatim-wrap"><pre class="screen">$ vi webserv.sh

#!/bin/bash

MYIP=$(/sbin/ifconfig eth0|grep 'inet addr'|awk -F: '{print $2}'| awk '{print $1}');
while true; do
  echo -e "HTTP/1.0 200 OK

Welcome to $MYIP" | sudo nc -l -p 80
done

# Give it Exec rights
$ chmod 755 webserv.sh

# Execute the script
$ ./webserv.sh</pre></div></li><li class="listitem "><p>
     You should expect to see curl fail over port 80 because of the deny-http
     firewall rule. If curl succeeds, the firewall is not blocking incoming
     http requests.
    </p><div class="verbatim-wrap"><pre class="screen">curl -vvv &lt;FIP-VM1&gt;</pre></div></li></ol></div><div id="id-1.5.12.3.7.12" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
    When using reference implementation, new networks, FIPs and routers created
    after the Firewall creation will not be automatically updated with firewall
    rules. Thus, execute the firewall-update command by passing the current and
    new router Ids such that the rules are reconfigured across all the routers
    (both current and new).
   </p><p>
    For example if router-1 is created before and router-2 is created after the
    firewall creation
   </p><div class="verbatim-wrap"><pre class="screen">$ neutron firewall-update —router &lt;router-1-id&gt; —router &lt;router-2-id&gt; &lt;firewall-name&gt;</pre></div></div></div><div class="sect2" id="idg-all-operations-configure-firewall-xml-7"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Making Changes to the Firewall Rules</span> <a title="Permalink" class="permalink" href="#idg-all-operations-configure-firewall-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configure_firewall.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configure_firewall.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-configure-firewall-xml-7</li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit your
     <code class="literal">~/openstack/my_cloud/definition/data/firewall_rules.yml</code>
     file and add the lines necessary to allow the port(s) needed through the
     firewall.
    </p><p>
     In this example we are going to open up port range 5900-5905 to allow VNC
     traffic through the firewall:
    </p><div class="verbatim-wrap"><pre class="screen">  - name: VNC
    network-groups:
  - MANAGEMENT
    rules:
     - type: allow
       remote-ip-prefix:  0.0.0.0/0
       port-range-min: 5900
       port-range-max: 5905
       protocol: tcp</pre></div><div id="id-1.5.12.3.8.2.2.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The example above shows a <code class="literal">remote-ip-prefix</code> of
      <code class="literal">0.0.0.0/0</code> which opens the ports up to all IP ranges.
      To be more secure you can specify your local IP address CIDR you will
      be running the VNC connect from.
     </p></div></li><li class="listitem "><p>
     Commit those changes to your local git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "firewall rule update"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Create the deployment directory structure:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Change to the deployment directory and run the
     <code class="literal">osconfig-iptables-deploy.yml</code> playbook to update your
     iptable rules to allow VNC:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-iptables-deploy.yml</pre></div></li></ol></div><p>
   You can repeat these steps as needed to add, remove, or edit any of these
   firewall rules.
  </p></div><div class="sect2" id="sec-hp20fwaas-more"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">More Information</span> <a title="Permalink" class="permalink" href="#sec-hp20fwaas-more">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configure_firewall.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configure_firewall.xml</li><li><span class="ds-label">ID: </span>sec-hp20fwaas-more</li></ul></div></div></div></div><p>
   Firewalls are based in IPtable settings.
  </p><p>
   Each firewall that is created is known as an instance.
  </p><p>
   A firewall instance can be deployed on selected project routers. If no
   specific project router is selected, a firewall instance is automatically
   applied to all project routers.
  </p><p>
   Only 1 firewall instance can be applied to a project router.
  </p><p>
   Only 1 firewall policy can be applied to a firewall instance.
  </p><p>
   Multiple firewall rules can be added and applied to a firewall policy.
  </p><p>
   Firewall rules can be shared across different projects via the Share API
   flag.
  </p><p>
   Firewall rules supersede the Security Group rules that are applied at the
   Instance level for all traffic entering or leaving a private, project
   network.
  </p><p>
   For more information on the command-line interface (CLI) and
   firewalls, see the OpenStack networking command-line client reference:
   <a class="link" href="https://docs.openstack.org/python-openstackclient/rocky/" target="_blank">https://docs.openstack.org/python-openstackclient/rocky/</a>
  </p></div></div><div class="sect1" id="UsingVPNaaS"><div class="titlepage"><div><div><h2 class="title"><span class="number">10.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using VPN as a Service (VPNaaS)</span> <a title="Permalink" class="permalink" href="#UsingVPNaaS">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-networking-vpnaas.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-networking-vpnaas.xml</li><li><span class="ds-label">ID: </span>UsingVPNaaS</li></ul></div></div></div></div><p>
  <span class="bold"><strong><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 VPNaaS Configuration</strong></span>
 </p><p>
  This document describes the configuration process and requirements for the
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 Virtual Private Network (VPN) as a Service
  (VPNaaS) module.
 </p><div class="sect2" id="idg-all-networking-vpnaas-xml-7"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#idg-all-networking-vpnaas-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-networking-vpnaas.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-networking-vpnaas.xml</li><li><span class="ds-label">ID: </span>idg-all-networking-vpnaas-xml-7</li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> must be installed.
    </p></li><li class="listitem "><p>
     Before setting up VPNaaS, you will need to have created an external
     network and a subnet with access to the internet. Information on how to
     create the external network and subnet can be found in
     <a class="xref" href="#sec-vpnaas-more" title="10.2.4. More Information">Section 10.2.4, “More Information”</a>.
    </p></li><li class="listitem "><p>
     You should assume 172.16.0.0/16 as the ext-net CIDR in this document.
    </p></li></ol></div></div><div class="sect2" id="Considerations"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Considerations</span> <a title="Permalink" class="permalink" href="#Considerations">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-networking-vpnaas.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-networking-vpnaas.xml</li><li><span class="ds-label">ID: </span>Considerations</li></ul></div></div></div></div><p>
   Using the neutron plugin-based VPNaaS causes additional processes to be run
   on the Network Service Nodes. One of these processes, the ipsec charon
   process from StrongSwan, runs as root and listens on an external network. A
   vulnerability in that process can lead to remote root compromise of the
   Network Service Nodes. If this is a concern customers should consider using
   a VPN solution other than the neutron plugin-based VPNaaS and/or deploying
   additional protection mechanisms.
  </p></div><div class="sect2" id="idg-all-networking-vpnaas-xml-9"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuration</span> <a title="Permalink" class="permalink" href="#idg-all-networking-vpnaas-xml-9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-networking-vpnaas.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-networking-vpnaas.xml</li><li><span class="ds-label">ID: </span>idg-all-networking-vpnaas-xml-9</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Setup Networks</strong></span> You can setup VPN as a
   Service (VPNaaS) by first creating networks, subnets and routers using the
   <code class="literal">neutron</code> command line. The VPNaaS module enables the
   ability to extend access between private networks across two different
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> clouds or between a <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud and a non-cloud network. VPNaaS
   is based on the open source software application called StrongSwan.
   StrongSwan (more information available
   at <a class="link" href="http://www.strongswan.org/" target="_blank">http://www.strongswan.org/</a>)
   is an IPsec implementation and provides basic VPN gateway functionality.
  </p><div id="id-1.5.12.4.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    You can execute the included commands from any shell with access to the
    service APIs. In the included examples, the commands are executed from the
    lifecycle manager, however you could execute the commands from the
    controller node or any other shell with aforementioned service API access.
   </p></div><div id="id-1.5.12.4.6.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    The use of floating IP's is not possible with the current version of
    VPNaaS when DVR is enabled. Ensure that no floating IP is associated to
    instances that will be using VPNaaS when using a DVR router. Floating IP
    associated to instances are ok when using CVR router.
   </p></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     From the Cloud Lifecycle Manager, create first private network, subnet and
     router assuming that <span class="emphasis"><em>ext-net</em></span> is created by admin.
    </p><div class="verbatim-wrap"><pre class="screen">openstack network create privateA
openstack subnet create --name subA privateA 10.1.0.0/24 --gateway 10.1.0.1
openstack router create router1
openstack router add subnet router1 subA
openstack router set router1 ext-net</pre></div></li><li class="step "><p>
     Create second private network, subnet and router.
    </p><div class="verbatim-wrap"><pre class="screen">openstack network create privateB
openstack subnet create --name subB privateB 10.2.0.0/24 --gateway 10.2.0.1
openstack router create router2
openstack router add subnet router2 subB
openstack router set router2 ext-net</pre></div></li></ol></div></div><div class="procedure " id="pro-vpnaas-start"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 10.1: </span><span class="name">Starting Virtual Machines </span><a title="Permalink" class="permalink" href="#pro-vpnaas-start">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     From the Cloud Lifecycle Manager run the following to start the virtual
     machines. Begin with adding secgroup rules for SSH and ICMP.
    </p><div class="verbatim-wrap"><pre class="screen">openstack security group rule create default --protocol icmp
openstack security group rule create default --protocol tcp --port-range-min 22 --port-range-max 22</pre></div></li><li class="step "><p>
     Start the virtual machine in the privateA subnet. Using <span class="emphasis"><em>nova
     images-list</em></span>, use the image id to boot image instead of the
     image name. After executing this step, it is recommended that you wait
     approximately 10 seconds to allow the virtual machine to become active.
    </p><div class="verbatim-wrap"><pre class="screen">NETA=$(openstack network list | awk '/privateA/ {print $2}')
openstack server create --flavor 1 --image &lt;id&gt; --nic net-id=$NETA vm1</pre></div></li><li class="step "><p>
     Start the virtual machine in the privateB subnet.
    </p><div class="verbatim-wrap"><pre class="screen">NETB=$(openstack network list | awk '/privateB/ {print $2}')
openstack server create --flavor 1 --image &lt;id&gt; --nic net-id=$NETB vm2</pre></div></li><li class="step " id="st-vpnaas-obtain-ip"><p>
     Verify private IP's are allocated to the respective vms. Take note of IP's
     for later use.
    </p><div class="verbatim-wrap"><pre class="screen">openstack server show vm1
openstack server show vm2</pre></div></li></ol></div></div><div class="procedure " id="id-1.5.12.4.6.7"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 10.2: </span><span class="name">Create VPN </span><a title="Permalink" class="permalink" href="#id-1.5.12.4.6.7">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     You can set up the VPN by executing the below commands from the lifecycle
     manager or any shell with access to the service APIs. Begin with creating
     the policies with <code class="literal">vpn-ikepolicy-create</code> and
     <code class="literal">vpn-ipsecpolicy-create </code>.
    </p><div class="verbatim-wrap"><pre class="screen">neutron vpn-ikepolicy-create ikepolicy
neutron vpn-ipsecpolicy-create ipsecpolicy</pre></div></li><li class="step "><p>
     Create the VPN service at router1.
    </p><div class="verbatim-wrap"><pre class="screen">neutron vpn-service-create --name myvpnA --description "My vpn service" router1 subA</pre></div></li><li class="step "><p>
     Wait at least 5 seconds and then run
     <code class="literal">ipsec-site-connection-create</code> to create a ipsec-site
     connection. Note that <code class="literal">--peer-address</code> is the assign
     ext-net IP from router2 and <code class="literal">--peer-cidr</code> is subB cidr.
    </p><div class="verbatim-wrap"><pre class="screen">neutron ipsec-site-connection-create --name vpnconnection1 --vpnservice-id myvpnA \
--ikepolicy-id ikepolicy --ipsecpolicy-id ipsecpolicy --peer-address 172.16.0.3 \
--peer-id 172.16.0.3 --peer-cidr 10.2.0.0/24 --psk secret</pre></div></li><li class="step "><p>
     Create the VPN service at router2.
    </p><div class="verbatim-wrap"><pre class="screen">neutron vpn-service-create --name myvpnB --description "My vpn serviceB" router2 subB</pre></div></li><li class="step "><p>
     Wait at least 5 seconds and then run
     <code class="literal">ipsec-site-connection-create</code> to create a ipsec-site
     connection. Note that <code class="literal">--peer-address</code> is the assigned
     ext-net IP from router1 and <code class="literal">--peer-cidr</code> is subA cidr.
    </p><div class="verbatim-wrap"><pre class="screen">neutron ipsec-site-connection-create --name vpnconnection2 --vpnservice-id myvpnB \
--ikepolicy-id ikepolicy --ipsecpolicy-id ipsecpolicy --peer-address 172.16.0.2 \
--peer-id 172.16.0.2 --peer-cidr 10.1.0.0/24 --psk secret</pre></div></li><li class="step "><p>
     On the Cloud Lifecycle Manager, run the
     <code class="literal">ipsec-site-connection-list</code> command to see the active
     connections. Be sure to check that the vpn_services are ACTIVE. You can
     check this by running <code class="literal">vpn-service-list</code> and then
     checking ipsec-site-connections status. You should expect that the time
     for both vpn-services and ipsec-site-connections to become ACTIVE could
     take as long as 1 to 3 minutes.
    </p><div class="verbatim-wrap"><pre class="screen">neutron ipsec-site-connection-list
+--------------------------------------+----------------+--------------+---------------+------------+-----------+--------+
| id                                   | name           | peer_address | peer_cidrs    | route_mode | auth_mode | status |
+--------------------------------------+----------------+--------------+---------------+------------+-----------+--------+
| 1e8763e3-fc6a-444c-a00e-426a4e5b737c | vpnconnection2 | 172.16.0.2   | "10.1.0.0/24" | static     | psk       | ACTIVE |
| 4a97118e-6d1d-4d8c-b449-b63b41e1eb23 | vpnconnection1 | 172.16.0.3   | "10.2.0.0/24" | static     | psk       | ACTIVE |
+--------------------------------------+----------------+--------------+---------------+------------+-----------+--------+</pre></div></li></ol></div></div><p>
   <span class="bold"><strong>Verify VPN</strong></span> In the case of non-admin users,
   you can verify the VPN connection by pinging the virtual machines.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Check the VPN connections.
    </p><div id="id-1.5.12.4.6.9.1.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      vm1-ip and vm2-ip denotes private IP's for vm1 and vm2 respectively.
      The private IPs are obtained, as described in of
      <a class="xref" href="#st-vpnaas-obtain-ip" title="Step 4">Step 4</a>. If you are unable to SSH to the
      private network due to a lack
      of direct access, the VM console can be accessed through horizon.
     </p></div><div class="verbatim-wrap"><pre class="screen">ssh cirros@vm1-ip
password: &lt;password&gt;

# ping the private IP address of vm2
ping ###.###.###.###</pre></div></li><li class="step "><p>
     In another terminal.
    </p><div class="verbatim-wrap"><pre class="screen">ssh cirros@vm2-ip
password: &lt;password&gt;

# ping the private IP address of vm1
ping ###.###.###.###</pre></div></li><li class="step "><p>
     You should see ping responses from both virtual machines.
    </p></li></ol></div></div><p>
   As the admin user, you should check to make sure that a route exists between
   the router gateways. Once the gateways have been checked, packet encryption
   can be verified by using traffic analyzer (tcpdump) by tapping on the
   respective namespace (qrouter-* in case of non-DVR and snat-* in case of
   DVR) and tapping the right interface (qg-***).
  </p><div id="id-1.5.12.4.6.11" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    When using DVR namespaces, all the occurrences of qrouter-xxxxxx in the
    following commands should be replaced with respective snat-xxxxxx.
   </p></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Check the if the route exists between two router gateways. You can get the
     right qrouter namespace id by executing <span class="emphasis"><em>sudo ip
     netns</em></span>. Once you have the qrouter namespace id, you can get the
     interface by executing <span class="emphasis"><em>sudo ip netns qrouter-xxxxxxxx ip
     addr</em></span> and from the result the interface can be found.
    </p><div class="verbatim-wrap"><pre class="screen">sudo ip netns
sudo ip netns exec qrouter-&lt;router1 UUID&gt; ping &lt;router2 gateway&gt;
sudo ip netns exec qrouter-&lt;router2 UUID&gt; ping &lt;router1 gateway&gt;</pre></div></li><li class="step "><p>
     Initiate a tcpdump on the interface.
    </p><div class="verbatim-wrap"><pre class="screen">sudo ip netns exec qrouter-xxxxxxxx tcpdump -i qg-xxxxxx</pre></div></li><li class="step "><p>
     Check the VPN connection.
    </p><div class="verbatim-wrap"><pre class="screen">ssh cirros@vm1-ip
password: &lt;password&gt;

# ping the private IP address of vm2
ping ###.###.###.###</pre></div></li><li class="step "><p>
     Repeat for other namespace and right tap interface.
    </p><div class="verbatim-wrap"><pre class="screen">sudo ip netns exec qrouter-xxxxxxxx tcpdump -i qg-xxxxxx</pre></div></li><li class="step "><p>
     In another terminal.
    </p><div class="verbatim-wrap"><pre class="screen">ssh cirros@vm2-ip
password: &lt;password&gt;

# ping the private IP address of vm1
ping ###.###.###.###</pre></div></li><li class="step "><p>
     You will find encrypted packets containing ‘ESP’ in the tcpdump trace.
    </p></li></ol></div></div></div><div class="sect2" id="sec-vpnaas-more"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">More Information</span> <a title="Permalink" class="permalink" href="#sec-vpnaas-more">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-networking-vpnaas.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-networking-vpnaas.xml</li><li><span class="ds-label">ID: </span>sec-vpnaas-more</li></ul></div></div></div></div><p>
   VPNaaS currently only supports Pre-shared Keys (PSK) security between VPN
   gateways. A different VPN gateway solution should be considered if stronger,
   certificate-based security is required.
  </p><p>
   For more information on the neutron command-line interface (CLI) and VPN as
   a Service (VPNaaS), see the OpenStack networking command-line client
   reference:
   <a class="link" href="https://docs.openstack.org/python-openstackclient/rocky/" target="_blank">https://docs.openstack.org/python-openstackclient/rocky/</a>
  </p><p>
   For information on how to create an external network and subnet, see the
   OpenStack manual:
   <a class="link" href="http://docs.openstack.org/user-guide/dashboard_create_networks.html" target="_blank">http://docs.openstack.org/user-guide/dashboard_create_networks.html</a>
  </p></div></div><div class="sect1" id="designateOverview"><div class="titlepage"><div><div><h2 class="title"><span class="number">10.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DNS Service Overview</span> <a title="Permalink" class="permalink" href="#designateOverview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-dns-designate_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_overview.xml</li><li><span class="ds-label">ID: </span>designateOverview</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> DNS service provides multi-tenant Domain Name Service with REST
  API management for domain and records.
 </p><div id="id-1.5.12.5.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
   The DNS Service is not intended to be used as an
   <span class="emphasis"><em>internal</em></span> or
   <span class="emphasis"><em>private</em></span> DNS service. The name records in
   DNSaaS should be treated as public information that anyone could query.
   There are controls to prevent tenants from creating records for domains they
   do not own. TSIG provides a <span class="bold"><strong>T</strong></span>ransaction
   <span class="bold"><strong>SIG</strong></span> nature to ensure integrity during zone
   transfer to other DNS servers.
  </p></div><div class="sect2" id="id-1.5.12.5.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="#id-1.5.12.5.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-dns-designate_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_overview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     For more information about designate REST APIs, see the <span class="productname">OpenStack</span> REST
     API Documentation at
     <a class="link" href="http://docs.openstack.org/developer/designate/rest.html" target="_blank">http://docs.openstack.org/developer/designate/rest.html</a>.
    </p></li><li class="listitem "><p>
     For a glossary of terms for designate, see the <span class="productname">OpenStack</span> glossary at
     <a class="link" href="http://docs.openstack.org/developer/designate/glossary.html" target="_blank">http://docs.openstack.org/developer/designate/glossary.html</a>.
    </p></li></ul></div></div><div class="sect2" id="designateInitialConfig"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">designate Initial Configuration</span> <a title="Permalink" class="permalink" href="#designateInitialConfig">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-dns-designate_initialconfig.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_initialconfig.xml</li><li><span class="ds-label">ID: </span>designateInitialConfig</li></ul></div></div></div></div><p>
  After the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installation
  has been completed, designate requires initial configuration to operate.
 </p><div class="sect3" id="sec-designate-identify"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.3.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Identifying Name Server Public IPs</span> <a title="Permalink" class="permalink" href="#sec-designate-identify">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-dns-designate_initialconfig.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_initialconfig.xml</li><li><span class="ds-label">ID: </span>sec-designate-identify</li></ul></div></div></div></div><p>
   Depending on the back-end, the method used to identify the name servers'
   public IPs will differ.
  </p><div class="sect4" id="sec-designate-infoblox"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.3.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">InfoBlox</span> <a title="Permalink" class="permalink" href="#sec-designate-infoblox">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-dns-designate_initialconfig.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_initialconfig.xml</li><li><span class="ds-label">ID: </span>sec-designate-infoblox</li></ul></div></div></div></div><p>
    InfoBlox will act as your public name servers, consult the InfoBlox
    management UI to identify the IPs.
   </p></div><div class="sect4" id="sec-designate-bind"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.3.2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">BIND Back-end</span> <a title="Permalink" class="permalink" href="#sec-designate-bind">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-dns-designate_initialconfig.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_initialconfig.xml</li><li><span class="ds-label">ID: </span>sec-designate-bind</li></ul></div></div></div></div><p>
    You can find the name server IPs in <code class="filename">/etc/hosts</code> by
    looking for the <code class="literal">ext-api</code> addresses, which are the
    addresses of the controllers. For example:
   </p><div class="verbatim-wrap"><pre class="screen">192.168.10.1 example-cp1-c1-m1-extapi
192.168.10.2 example-cp1-c1-m2-extapi
192.168.10.3 example-cp1-c1-m3-extapi</pre></div></div><div class="sect4" id="sec-designate-a-record"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.3.2.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating Name Server A Records</span> <a title="Permalink" class="permalink" href="#sec-designate-a-record">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-dns-designate_initialconfig.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_initialconfig.xml</li><li><span class="ds-label">ID: </span>sec-designate-a-record</li></ul></div></div></div></div><p>
    Each name server requires a public name, for example
    <code class="literal">ns1.example.com.</code>, to which designate-managed domains will
    be delegated. There are two common locations where these may be registered,
    either within a zone hosted on designate itself, or within a zone hosted on a
    external DNS service.
   </p><p>
    <span class="bold"><strong>If you are using an externally managed zone for these
    names:</strong></span>
   </p><div class="procedure "><div class="procedure-contents"><ul class="procedure"><li class="step "><p>
      For each name server public IP, create the necessary A records in the
      external system.
     </p></li></ul></div></div><p>
    <span class="bold"><strong>If you are using a designate-managed zone for these
    names:</strong></span>
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Create the zone in designate which will contain the records:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack zone create --email hostmaster@example.com example.com.
+----------------+--------------------------------------+
| Field          | Value                                |
+----------------+--------------------------------------+
| action         | CREATE                               |
| created_at     | 2016-03-09T13:16:41.000000           |
| description    | None                                 |
| email          | hostmaster@example.com               |
| id             | 23501581-7e34-4b88-94f4-ad8cec1f4387 |
| masters        |                                      |
| name           | example.com.                         |
| pool_id        | 794ccc2c-d751-44fe-b57f-8894c9f5c842 |
| project_id     | a194d740818942a8bea6f3674e0a3d71     |
| serial         | 1457529400                           |
| status         | PENDING                              |
| transferred_at | None                                 |
| ttl            | 3600                                 |
| type           | PRIMARY                              |
| updated_at     | None                                 |
| version        | 1                                    |
+----------------+--------------------------------------+</pre></div></li><li class="step "><p>
      For each name server public IP, create an A record. For example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack recordset create --records 192.168.10.1 --type A example.com. ns1.example.com.
+-------------+--------------------------------------+
| Field       | Value                                |
+-------------+--------------------------------------+
| action      | CREATE                               |
| created_at  | 2016-03-09T13:18:36.000000           |
| description | None                                 |
| id          | 09e962ed-6915-441a-a5a1-e8d93c3239b6 |
| name        | ns1.example.com.                     |
| records     | 192.168.10.1                         |
| status      | PENDING                              |
| ttl         | None                                 |
| type        | A                                    |
| updated_at  | None                                 |
| version     | 1                                    |
| zone_id     | 23501581-7e34-4b88-94f4-ad8cec1f4387 |
+-------------+--------------------------------------+</pre></div></li><li class="step "><p>
      When records have been added, list the record sets in the zone to
      validate:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack recordset list example.com.
+--------------+------------------+------+---------------------------------------------------+
| id           | name             | type | records                                           |
+--------------+------------------+------+---------------------------------------------------+
| 2d6cf...655b | example.com.     | SOA  | ns1.example.com. hostmaster.example.com 145...600 |
| 33466...bd9c | example.com.     | NS   | ns1.example.com.                                  |
| da98c...bc2f | example.com.     | NS   | ns2.example.com.                                  |
| 672ee...74dd | example.com.     | NS   | ns3.example.com.                                  |
| 09e96...39b6 | ns1.example.com. | A    | 192.168.10.1                                      |
| bca4f...a752 | ns2.example.com. | A    | 192.168.10.2                                      |
| 0f123...2117 | ns3.example.com. | A    | 192.168.10.3                                      |
+--------------+------------------+------+---------------------------------------------------+</pre></div></li><li class="step "><p>
      Contact your domain registrar requesting <span class="emphasis"><em>Glue
      Records</em></span> to be registered in the
      <code class="literal">com.</code> zone for the nameserver and public
      IP address pairs above. If you are using a sub-zone of an existing
      company zone (for example, <code class="literal">ns1.cloud.mycompany.com.</code>),
      the Glue must be placed in the <code class="literal">mycompany.com.</code> zone.
     </p></li></ol></div></div></div><div class="sect4" id="sec-designate-more"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.3.2.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="#sec-designate-more">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-dns-designate_initialconfig.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_initialconfig.xml</li><li><span class="ds-label">ID: </span>sec-designate-more</li></ul></div></div></div></div><p>
    For additional DNS integration and configuration information, see the
    <span class="productname">OpenStack</span> designate documentation at
    <a class="link" href="https://docs.openstack.org/designate/rocky/" target="_blank">https://docs.openstack.org/designate/rocky/</a>.
   </p><p>
    For more information on creating servers, domains and examples, see the
    <span class="productname">OpenStack</span> REST API documentation at
    <a class="link" href="https://developer.openstack.org/api-ref/dns/" target="_blank">https://developer.openstack.org/api-ref/dns/</a>.
   </p></div></div></div><div class="sect2" id="designateMonitoringSupport"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DNS Service Monitoring Support</span> <a title="Permalink" class="permalink" href="#designateMonitoringSupport">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-dns-designate_monitor_support.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_monitor_support.xml</li><li><span class="ds-label">ID: </span>designateMonitoringSupport</li></ul></div></div></div></div><div class="sect3" id="MonitoringSupport"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.3.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DNS Service Monitoring Support</span> <a title="Permalink" class="permalink" href="#MonitoringSupport">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-dns-designate_monitor_support.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-dns-designate_monitor_support.xml</li><li><span class="ds-label">ID: </span>MonitoringSupport</li></ul></div></div></div></div><p>
   Additional monitoring support for the DNS Service (designate) has been added
   to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  </p><p>
   In the Networking section of the Operations Console, you can see alarms for all of
   the DNS Services (designate), such as designate-zone-manager, designate-api,
   designate-pool-manager, designate-mdns, and designate-central after running
   <code class="literal">designate-stop.yml</code>.
  </p><p>
   You can run <code class="literal">designate-start.yml</code> to start the DNS Services
   back up and the alarms will change from a red status to green and be removed
   from the <span class="bold"><strong>New Alarms</strong></span> panel of the
   Operations Console.
  </p><p>
   An example of the generated alarms from the Operations Console is provided below
   after running <code class="literal">designate-stop.yml</code>:
  </p><div class="verbatim-wrap"><pre class="screen">ALARM:  STATE:  ALARM ID:  LAST CHECK:  DIMENSION:
Process Check
0f221056-1b0e-4507-9a28-2e42561fac3e 2016-10-03T10:06:32.106Z hostname=ardana-cp1-c1-m1-mgmt,
service=dns,
cluster=cluster1,
process_name=designate-zone-manager,
component=designate-zone-manager,
control_plane=control-plane-1,
cloud_name=entry-scale-kvm

Process Check
50dc4c7b-6fae-416c-9388-6194d2cfc837 2016-10-03T10:04:32.086Z hostname=ardana-cp1-c1-m1-mgmt,
service=dns,
cluster=cluster1,
process_name=designate-api,
component=designate-api,
control_plane=control-plane-1,
cloud_name=entry-scale-kvm

Process Check
55cf49cd-1189-4d07-aaf4-09ed08463044 2016-10-03T10:05:32.109Z hostname=ardana-cp1-c1-m1-mgmt,
service=dns,
cluster=cluster1,
process_name=designate-pool-manager,
component=designate-pool-manager,
control_plane=control-plane-1,
cloud_name=entry-scale-kvm

Process Check
c4ab7a2e-19d7-4eb2-a9e9-26d3b14465ea 2016-10-03T10:06:32.105Z hostname=ardana-cp1-c1-m1-mgmt,
service=dns,
cluster=cluster1,
process_name=designate-mdns,
component=designate-mdns,
control_plane=control-plane-1,
cloud_name=entry-scale-kvm
HTTP Status
c6349bbf-4fd1-461a-9932-434169b86ce5 2016-10-03T10:05:01.731Z service=dns,
cluster=cluster1,
url=http://100.60.90.3:9001/,
hostname=ardana-cp1-c1-m3-mgmt,
component=designate-api,
control_plane=control-plane-1,
api_endpoint=internal,
cloud_name=entry-scale-kvm,
monitored_host_type=instance

Process Check
ec2c32c8-3b91-4656-be70-27ff0c271c89 2016-10-03T10:04:32.082Z hostname=ardana-cp1-c1-m1-mgmt,
service=dns,
cluster=cluster1,
process_name=designate-central,
component=designate-central,
control_plane=control-plane-1,
cloud_name=entry-scale-kvm</pre></div></div></div></div><div class="sect1" id="neutron-overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">10.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networking Service Overview</span> <a title="Permalink" class="permalink" href="#neutron-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-networking_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-networking_overview.xml</li><li><span class="ds-label">ID: </span>neutron-overview</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Networking is a virtual Networking service that leverages the
  OpenStack neutron service to provide network connectivity and addressing to
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Compute service devices.
 </p><p>
  The Networking service also provides an API to configure and manage a variety
  of network services.
 </p><p>
  You can use the Networking service to connect guest servers or you can define
  and configure your own virtual network topology.
 </p><div class="sect2" id="installing-the-networking-service"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing the Networking Service</span> <a title="Permalink" class="permalink" href="#installing-the-networking-service">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-networking_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-networking_overview.xml</li><li><span class="ds-label">ID: </span>installing-the-networking-service</li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Network Administrators are responsible for planning for the neutron
   Networking service, and once installed, to configure the service to meet the
   needs of their cloud network users.
  </p></div><div class="sect2" id="working-with-the-networking-service"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Working with the Networking service</span> <a title="Permalink" class="permalink" href="#working-with-the-networking-service">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-networking_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-networking_overview.xml</li><li><span class="ds-label">ID: </span>working-with-the-networking-service</li></ul></div></div></div></div><p>
   To perform tasks using the Networking service, you can use the dashboard,
   API or CLI.
  </p></div><div class="sect2" id="restarting"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reconfiguring the Networking service</span> <a title="Permalink" class="permalink" href="#restarting">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-networking_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-networking_overview.xml</li><li><span class="ds-label">ID: </span>restarting</li></ul></div></div></div></div><p>
   If you change any of the network configuration after installation, it is
   recommended that you reconfigure the Networking service by running the
   neutron-reconfigure playbook.
  </p><p>
   On the Cloud Lifecycle Manager:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></div><div class="sect2" id="for-more-information"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For more information</span> <a title="Permalink" class="permalink" href="#for-more-information">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-networking_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-networking_overview.xml</li><li><span class="ds-label">ID: </span>for-more-information</li></ul></div></div></div></div><p>
   For information on how to operate your cloud we suggest you read the
   <a class="link" href="http://docs.openstack.org/ops/" target="_blank">OpenStack Operations
   Guide</a>. The <span class="emphasis"><em>Architecture</em></span> section contains useful
   information about how an OpenStack Cloud is put together. However, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   takes care of these details for you. The <span class="emphasis"><em>Operations</em></span>
   section contains information on how to manage the system.
  </p></div><div class="sect2" id="neutron-external-networks"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Neutron External Networks</span> <a title="Permalink" class="permalink" href="#neutron-external-networks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_external_networks.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_external_networks.xml</li><li><span class="ds-label">ID: </span>neutron-external-networks</li></ul></div></div></div></div><div class="sect3" id="id-1.5.12.6.9.2"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">External networks overview</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.9.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_external_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_external_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   This topic explains how to create a neutron external network.
  </p><p>
   External networks provide access to the internet.
  </p><p>
   The typical use is to provide an IP address that can be used to reach a VM
   from an external network which can be a public network like the internet or
   a network that is private to an organization.
  </p></div><div class="sect3" id="idg-all-networking-neutron-external-networks-xml-4"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Ansible Playbook</span> <a title="Permalink" class="permalink" href="#idg-all-networking-neutron-external-networks-xml-4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_external_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_external_networks.xml</li><li><span class="ds-label">ID: </span>idg-all-networking-neutron-external-networks-xml-4</li></ul></div></div></div></div><p>
   This playbook will query the Networking service for an existing external
   network, and then create a new one if you do not already have one. The
   resulting external network will have the name <code class="literal">ext-net</code>
   with a subnet matching the CIDR you specify in the command below.
  </p><p>
   If you need to specify more granularity, for example specifying an
   allocation pool for the subnet, use the
   <a class="xref" href="#idg-all-networking-neutron-external-networks-xml-6" title="10.4.5.3. Using the python-neutronclient CLI">Section 10.4.5.3, “Using the python-neutronclient CLI”</a>.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-cloud-configure.yml -e EXT_NET_CIDR=&lt;CIDR&gt;</pre></div><p>
   The table below shows the optional switch that you can use as part of this
   playbook to specify environment-specific information:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Switch</th><th>Description</th></tr></thead><tbody><tr><td>
       <p>
        <code class="literal">-e EXT_NET_CIDR=&lt;CIDR&gt;</code>
       </p>
      </td><td>
       <p>
        Optional. You can use this switch to specify the external network CIDR.
        If you choose not to use this switch, or use a wrong value, the VMs
        will not be accessible over the network.
       </p>
       <p>
        This CIDR will be from the <code class="literal">EXTERNAL VM</code> network.
       </p>
      </td></tr></tbody></table></div></div><div class="sect3" id="idg-all-networking-neutron-external-networks-xml-6"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the python-neutronclient CLI</span> <a title="Permalink" class="permalink" href="#idg-all-networking-neutron-external-networks-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_external_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_external_networks.xml</li><li><span class="ds-label">ID: </span>idg-all-networking-neutron-external-networks-xml-6</li></ul></div></div></div></div><p>
   For more granularity you can utilize the OpenStackClient tool to create your
   external network.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Source the Admin creds:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc</pre></div></li><li class="listitem "><p>
     Create the external network and then the subnet using these commands
     below.
    </p><p>
     Creating the network:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network create --router:external &lt;external-network-name&gt;</pre></div><p>
     Creating the subnet:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack subnet create <em class="replaceable ">EXTERNAL-NETWORK-NAME</em> <em class="replaceable ">CIDR</em> --gateway <em class="replaceable ">GATEWAY</em> --allocation-pool start=<em class="replaceable ">IP_START</em>,end=<em class="replaceable ">IP_END</em> [--disable-dhcp]</pre></div><p>
     Where:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Value</th><th>Description</th></tr></thead><tbody><tr><td>external-network-name</td><td>
         <p>
          This is the name given to your external network. This is a unique
          value that you will choose. The value <code class="literal">ext-net</code> is
          usually used.
         </p>
        </td></tr><tr><td>CIDR</td><td>
         <p>
          Use this switch to specify the external network CIDR. If you do not
          use this switch or use a wrong value, the VMs will not be accessible
          over the network.
         </p>
         <p>
          This CIDR will be from the EXTERNAL VM network.
         </p>
        </td></tr><tr><td>--gateway</td><td>
         <p>
          Optional switch to specify the gateway IP for your subnet. If this is
          not included, it will choose the first available IP.
         </p>
        </td></tr><tr><td>
         <p>
          --allocation-pool start end
         </p>
        </td><td>
         <p>
          Optional switch to specify start and end IP addresses to use as the
          allocation pool for this subnet.
         </p>
        </td></tr><tr><td>--disable-dhcp</td><td>
         <p>
          Optional switch if you want to disable DHCP on this subnet. If this
          is not specified, DHCP will be enabled.
         </p>
        </td></tr></tbody></table></div></li></ol></div></div><div class="sect3" id="MultipleExternalNetworks"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Multiple External Networks</span> <a title="Permalink" class="permalink" href="#MultipleExternalNetworks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_external_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_external_networks.xml</li><li><span class="ds-label">ID: </span>MultipleExternalNetworks</li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> provides the ability to have multiple external networks, by using
   the Network Service (neutron) provider networks for external networks. You
   can configure <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to allow the use of provider VLANs as external
   networks by following these steps.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Do NOT include the
     <code class="literal">neutron.l3_agent.external_network_bridge</code> tag in the
     network_groups definition for your cloud. This results in the
     <code class="literal">l3_agent.ini external_network_bridge</code> being set to an
     empty value (rather than the traditional br-ex).
    </p></li><li class="listitem "><p>
     Configure your cloud to use provider VLANs, by specifying the
     <code class="literal">provider_physical_network</code> tag on one of the
     network_groups defined for your cloud.
    </p><p>
     For example, to run provider VLANS over the EXAMPLE network group: (some
     attributes omitted for brevity)
    </p><div class="verbatim-wrap"><pre class="screen">network-groups:

  - name: EXAMPLE
    tags:
      - neutron.networks.vlan:
          provider-physical-network: physnet1</pre></div></li><li class="listitem "><p>
     After the cloud has been deployed, you can create external networks using
     provider VLANs.
    </p><p>
     For example, using the OpenStackClient:
    </p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       Create external network 1 on vlan101
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network create --provider-network-type vlan
--provider-physical-network physnet1 --provider-segment 101 --external ext-net1</pre></div></li><li class="listitem "><p>
       Create external network 2 on vlan102
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network create --provider-network-type vlan
--provider-physical-network physnet1 --provider-segment 102 --external ext-net2</pre></div></li></ol></div></li></ol></div></div></div><div class="sect2" id="neutron-provider-networks"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Neutron Provider Networks</span> <a title="Permalink" class="permalink" href="#neutron-provider-networks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span>neutron-provider-networks</li></ul></div></div></div></div><p>
  This topic explains how to create a neutron provider network.
 </p><p>
  A provider network is a virtual network created in the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud that
  is consumed by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services. The distinctive element of a provider
  network is that it does not create a virtual router; rather, it depends on
  L3 routing that is provided by the infrastructure.
 </p><p>
  A provider network is created by adding the specification to the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  input model. It consists of at least one network and one or more subnets.
 </p><div class="sect3" id="id-1.5.12.6.10.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> input model</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.10.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The input model is the primary mechanism a cloud admin uses in defining a
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installation. It exists as a directory with a data subdirectory that
   contains YAML files. By convention, any service that creates a neutron
   provider network will create a subdirectory under the data directory and the
   name of the subdirectory shall be the project name. For example, the Octavia
   project will use neutron provider networks so it will have a subdirectory
   named 'octavia' and the config file that specifies the neutron network will
   exist in that subdirectory.
  </p><div class="verbatim-wrap"><pre class="screen">├── cloudConfig.yml
    ├── data
    │   ├── control_plane.yml
    │   ├── disks_compute.yml
    │   ├── disks_controller_1TB.yml
    │   ├── disks_controller.yml
    │   ├── firewall_rules.yml
    │   ├── net_interfaces.yml
    │   ├── network_groups.yml
    │   ├── networks.yml
    │   ├── neutron
    │   │   └── neutron_config.yml
    │   ├── nic_mappings.yml
    │   ├── server_groups.yml
    │   ├── server_roles.yml
    │   ├── servers.yml
    │   ├── swift
    │   │   └── swift_config.yml
    │   └── octavia
    │       └── octavia_config.yml
    ├── README.html
    └── README.md</pre></div></div><div class="sect3" id="id-1.5.12.6.10.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network/Subnet specification</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.10.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The elements required in the input model for you to define a network are:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     name
    </p></li><li class="listitem "><p>
     network_type
    </p></li><li class="listitem "><p>
     physical_network
    </p></li></ul></div><p>
   Elements that are optional when defining a network are:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     segmentation_id
    </p></li><li class="listitem "><p>
     shared
    </p></li></ul></div><p>
   Required elements for the subnet definition are:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     cidr
    </p></li></ul></div><p>
   Optional elements for the subnet definition are:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     allocation_pools which will require start and end addresses
    </p></li><li class="listitem "><p>
     host_routes which will require a destination and nexthop
    </p></li><li class="listitem "><p>
     gateway_ip
    </p></li><li class="listitem "><p>
     no_gateway
    </p></li><li class="listitem "><p>
     enable-dhcp
    </p></li></ul></div><p>
   NOTE: Only IPv4 is supported at the present time.
  </p></div><div class="sect3" id="id-1.5.12.6.10.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network details</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.10.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following table outlines the network values to be set, and what they
   represent.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /></colgroup><thead><tr><th>Attribute</th><th>Required/optional</th><th>Allowed Values</th><th>Usage</th></tr></thead><tbody><tr><td>name</td><td>Required</td><td> </td><td> </td></tr><tr><td>network_type</td><td>Required</td><td>flat, vlan, vxlan</td><td>The type of desired network</td></tr><tr><td>physical_network</td><td>Required</td><td>Valid</td><td>Name of physical network that is overlayed with the virtual network</td></tr><tr><td>segmentation_id</td><td>Optional</td><td>vlan or vxlan ranges</td><td>VLAN id for vlan or tunnel id for vxlan</td></tr><tr><td>shared</td><td>Optional</td><td>True</td><td>Shared by all projects or private to a single project</td></tr></tbody></table></div></div><div class="sect3" id="id-1.5.12.6.10.8"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Subnet details</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.10.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following table outlines the subnet values to be set, and what they
   represent.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /></colgroup><thead><tr><th>Attribute</th><th>Req/Opt</th><th>Allowed Values</th><th>Usage</th></tr></thead><tbody><tr><td>cidr</td><td>Required</td><td>Valid CIDR range</td><td>for example, 172.30.0.0/24</td></tr><tr><td>allocation_pools</td><td>Optional</td><td>See allocation_pools table below</td><td> </td></tr><tr><td>host_routes</td><td>Optional</td><td>See host_routes table below</td><td> </td></tr><tr><td>gateway_ip</td><td>Optional</td><td>Valid IP addr</td><td>Subnet gateway to other nets</td></tr><tr><td>no_gateway</td><td>Optional</td><td>True</td><td>No distribution of gateway</td></tr><tr><td>enable-dhcp</td><td>Optional</td><td>True</td><td>Enable dhcp for this subnet</td></tr></tbody></table></div></div><div class="sect3" id="id-1.5.12.6.10.9"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.6.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">ALLOCATION_POOLS details</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.10.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following table explains allocation pool settings.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /></colgroup><thead><tr><th>Attribute</th><th>Req/Opt</th><th>Allowed Values</th><th>Usage</th></tr></thead><tbody><tr><td>start</td><td>Required</td><td>Valid IP addr</td><td>First ip address in pool</td></tr><tr><td>end</td><td>Required</td><td>Valid IP addr</td><td>Last ip address in pool</td></tr></tbody></table></div></div><div class="sect3" id="id-1.5.12.6.10.10"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.6.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HOST_ROUTES details</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.10.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following table explains host route settings.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /></colgroup><thead><tr><th>Attribute</th><th>Req/Opt</th><th>Allowed Values</th><th>Usage</th></tr></thead><tbody><tr><td>destination</td><td>Required</td><td>Valid CIDR</td><td>Destination subnet</td></tr><tr><td>nexthop</td><td>Required</td><td>Valid IP addr</td><td>Hop to take to destination subnet</td></tr></tbody></table></div><div id="id-1.5.12.6.10.10.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    Multiple destination/nexthop values can be used.
   </p></div></div><div class="sect3" id="id-1.5.12.6.10.11"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.6.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Examples</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.10.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following examples show the configuration file settings for neutron and
   Octavia.
  </p><p>
   <span class="bold"><strong>Octavia configuration</strong></span>
  </p><p>
   This file defines the mapping. It does not need to be edited unless you want
   to change the name of your VLAN.
  </p><p>
   Path:
   <code class="literal">~/openstack/my_cloud/definition/data/octavia/octavia_config.yml</code>
  </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  configuration-data:
    - name: OCTAVIA-CONFIG-CP1
      services:
        - octavia
      data:
        amp_network_name: OCTAVIA-MGMT-NET</pre></div><p>
   <span class="bold"><strong>neutron configuration</strong></span>
  </p><p>
   Input your network configuration information for your provider VLANs in
   <code class="literal">neutron_config.yml</code> found here:
  </p><p>
   <code class="literal">~/openstack/my_cloud/definition/data/neutron/</code>.
  </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  configuration-data:
    - name:  NEUTRON-CONFIG-CP1
      services:
        - neutron
      data:
        neutron_provider_networks:
        - name: OCTAVIA-MGMT-NET
          provider:
            - network_type: vlan
              physical_network: physnet1
              segmentation_id: 2754
          cidr: 10.13.189.0/24
          no_gateway:  True
          enable_dhcp: True
          allocation_pools:
            - start: 10.13.189.4
              end: 10.13.189.252
          host_routes:
            # route to MANAGEMENT-NET
            - destination: 10.13.111.128/26
              nexthop:  10.13.189.5</pre></div></div><div class="sect3" id="id-1.5.12.6.10.12"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.6.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Implementing your changes</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.10.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "configuring provider network"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Then continue with your clean cloud installation.
    </p></li><li class="listitem "><p>
     If you are only adding a neutron Provider network to an existing model,
     then run the neutron-deploy.yml playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-deploy.yml</pre></div></li></ol></div></div><div class="sect3" id="MultipleProviderNetworks"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.6.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Multiple Provider Networks</span> <a title="Permalink" class="permalink" href="#MultipleProviderNetworks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span>MultipleProviderNetworks</li></ul></div></div></div></div><p>
   The physical network infrastructure must be configured to convey the
   provider VLAN traffic as tagged VLANs to the cloud compute nodes and network
   service network nodes. Configuration of the physical network infrastructure
   is outside the scope of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 software.
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 automates the server networking configuration and the
   Network Service configuration based on information in the cloud definition.
   To configure the system for provider VLANs, specify the<span class="emphasis"><em>
   neutron.networks.vlan</em></span> tag with a
   <span class="emphasis"><em>provider-physical-network</em></span> attribute on one or more
   network groups. For example (some attributes omitted for brevity):
  </p><div class="verbatim-wrap"><pre class="screen">network-groups:

        - name: NET_GROUP_A
        tags:
        - neutron.networks.vlan:
        provider-physical-network: physnet1

        - name: NET_GROUP_B
        tags:
        - neutron.networks.vlan:
        provider-physical-network: physnet2</pre></div><p>
   A network group is associated with a server network interface via an
   interface model. For example (some attributes omitted for brevity):
  </p><div class="verbatim-wrap"><pre class="screen">interface-models:
        - name: INTERFACE_SET_X
        network-interfaces:
        - device:
        name: bond0
        network-groups:
        - NET_GROUP_A
        - device:
        name: eth3
        network-groups:
        - NET_GROUP_B</pre></div><p>
   A network group used for provider VLANs may contain only a single <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   network, because that VLAN must span all compute nodes and any Network
   Service network nodes/controllers (that is, it is a single L2 segment). The
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> network must be defined with tagged-vlan false, otherwise a Linux
   VLAN network interface will be created. For example:
  </p><div class="verbatim-wrap"><pre class="screen">networks:

        - name: NET_A
        tagged-vlan: false
        network-group: NET_GROUP_A

        - name: NET_B
        tagged-vlan: false
        network-group: NET_GROUP_B</pre></div><p>
   When the cloud is deployed, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 will create the appropriate
   bridges on the servers, and set the appropriate attributes in the neutron
   configuration files (for example, bridge_mappings).
  </p><p>
   After the cloud has been deployed, create Network Service network objects
   for each provider VLAN. For example, using the Network Service CLI:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network create --provider:network_type vlan --provider:physical_network physnet1 --provider-segment 101 mynet101
<code class="prompt user">ardana &gt; </code>openstack network create --provider:network_type vlan --provider:physical_network physnet2 --provider-segment 234 mynet234</pre></div></div><div class="sect3" id="idg-all-networking-neutron-provider-networks-xml-10"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.6.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">More Information</span> <a title="Permalink" class="permalink" href="#idg-all-networking-neutron-provider-networks-xml-10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_provider_networks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_provider_networks.xml</li><li><span class="ds-label">ID: </span>idg-all-networking-neutron-provider-networks-xml-10</li></ul></div></div></div></div><p>
   For more information on the Network Service command-line interface (CLI),
   see the OpenStack networking command-line client reference:
   <a class="link" href="http://docs.openstack.org/cli-reference/content/neutronclient_commands.html" target="_blank">http://docs.openstack.org/cli-reference/content/neutronclient_commands.html</a>
  </p></div></div><div class="sect2" id="ipam"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using IPAM Drivers in the Networking Service</span> <a title="Permalink" class="permalink" href="#ipam">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-using_ipam.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-using_ipam.xml</li><li><span class="ds-label">ID: </span>ipam</li></ul></div></div></div></div><p>
  This topic describes how to choose and implement an IPAM driver.
 </p><div class="sect3" id="id-1.5.12.6.11.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Selecting and implementing an IPAM driver</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.11.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-using_ipam.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-using_ipam.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Beginning with the Liberty release, OpenStack networking includes a
   pluggable interface for the IP Address Management (IPAM) function. This
   interface creates a driver framework for the allocation and de-allocation of
   subnets and IP addresses, enabling the integration of alternate IPAM
   implementations or third-party IP Address Management systems.
  </p><p>
   There are three possible IPAM driver options:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Non-pluggable driver. This option is the default when the ipam_driver
     parameter is not specified in neutron.conf.
    </p></li><li class="listitem "><p>
     Pluggable reference IPAM driver. The pluggable IPAM driver interface was
     introduced in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 (OpenStack Liberty). It is a refactoring of
     the Kilo non-pluggable driver to use the new pluggable interface. The
     setting in neutron.conf to specify this driver is <code class="literal">ipam_driver =
     internal</code>.
    </p></li><li class="listitem "><p>
     Pluggable Infoblox IPAM driver. The pluggable Infoblox IPAM driver is a
     third-party implementation of the pluggable IPAM interface. the
     corresponding setting in neutron.conf to specify this driver is
     <code class="literal">ipam_driver =
     networking_infoblox.ipam.driver.InfobloxPool</code>.
    </p><div id="id-1.5.12.6.11.3.4.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      You can use either the non-pluggable IPAM driver or a pluggable one.
      However, you cannot use both.
     </p></div></li></ul></div></div><div class="sect3" id="id-1.5.12.6.11.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Pluggable reference IPAM driver</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.11.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-using_ipam.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-using_ipam.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To indicate that you want to use the Pluggable reference IPAM driver, the
   only parameter needed is "ipam_driver." You can set it by looking for the
   following commented line in the
   <code class="literal">neutron.conf.j2</code> template (ipam_driver = internal)
   uncommenting it, and committing the file. After following the standard
   steps to deploy neutron, neutron will be configured to run using the
   Pluggable reference IPAM driver.
  </p><p>
   As stated, the file you must edit is <code class="literal">neutron.conf.j2</code> on
   the Cloud Lifecycle Manager in the directory
   <code class="literal">~/openstack/my_cloud/config/neutron</code>. Here is the relevant
   section where you can see the <code class="literal">ipam_driver</code> parameter
   commented out:
  </p><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
  ...
  l3_ha_net_cidr = 169.254.192.0/18

  # Uncomment the line below if the Reference Pluggable IPAM driver is to be used
  # ipam_driver = internal
  ...</pre></div><p>
   After uncommenting the line <code class="literal">ipam_driver = internal</code>,
   commit the file using git commit from the <code class="literal">openstack/my_cloud</code>
   directory:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git commit -a -m 'My config for enabling the internal IPAM Driver'</pre></div><p>
   Then follow the steps to deploy <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> in the
   <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 13 “Overview”</span> appropriate to your cloud configuration.
  </p><div id="id-1.5.12.6.11.4.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    Currently there is no migration path from the non-pluggable driver to a
    pluggable IPAM driver because changes are needed to database tables and
    neutron currently cannot make those changes.
   </p></div></div><div class="sect3" id="id-1.5.12.6.11.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.7.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Infoblox IPAM driver</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.11.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-using_ipam.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-using_ipam.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   As suggested above, using the Infoblox IPAM driver requires changes to
   existing parameters in <code class="literal">nova.conf</code> and
   <code class="literal">neutron.conf</code>. If you want to use the infoblox appliance,
   you will need to add the "infoblox service-component" to the service-role
   containing the neutron API server. To use the infoblox appliance for IPAM,
   both the agent <span class="emphasis"><em>and</em></span> the Infoblox IPAM driver are
   required. The <code class="literal">infoblox-ipam-agent</code> should be deployed on
   the same node where the neutron-server component is running. Usually this is
   a Controller node.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Have the Infoblox appliance running on the management network (the
     Infoblox appliance admin or the datacenter administrator should know how
     to perform this step).
    </p></li><li class="listitem "><p>
     Change the control plane definition to add
     i<code class="literal">nfoblox-ipam-agent</code> as a service in the controller node
     cluster (see change in bold). Make the changes in
     <code class="literal">control_plane.yml</code> found here:
     <code class="literal">~/openstack/my_cloud/definition/data/control_plane.yml</code>
    </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  control-planes:
    - name: ccp
      control-plane-prefix: ccp
 ...
      clusters:
        - name: cluster0
          cluster-prefix: c0
          server-role: ARDANA-ROLE
          member-count: 1
          allocation-policy: strict
          service-components:
            - lifecycle-manager
        - name: cluster1
          cluster-prefix: c1
          server-role: CONTROLLER-ROLE
          member-count: 3
          allocation-policy: strict
          service-components:
            - ntp-server
...
            - neutron-server
            <span class="bold"><strong>- infoblox-ipam-agent</strong></span>
...
            - designate-client
            - bind
      resources:
        - name: compute
          resource-prefix: comp
          server-role: COMPUTE-ROLE
          allocation-policy: any</pre></div></li><li class="listitem "><p>
     Modify the
     <code class="literal">~/openstack/my_cloud/config/neutron/neutron.conf.j2</code> file
     on the controller node to comment and uncomment the lines noted below to
     enable use with the Infoblox appliance:
    </p><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
            ...
            l3_ha_net_cidr = 169.254.192.0/18


            # Uncomment the line below if the Reference Pluggable IPAM driver is to be used
            # ipam_driver = internal


            # Comment out the line below if the Infoblox IPAM Driver is to be used
            # notification_driver = messaging

            # Uncomment the lines below if the Infoblox IPAM driver is to be used
            ipam_driver = networking_infoblox.ipam.driver.InfobloxPool
            notification_driver = messagingv2


            # Modify the infoblox sections below to suit your cloud environment

            [infoblox]
            cloud_data_center_id = 1
            # This name of this section is formed by "infoblox-dc:&lt;infoblox.cloud_data_center_id&gt;"
            # If cloud_data_center_id is 1, then the section name is "infoblox-dc:1"

            [infoblox-dc:0]
            http_request_timeout = 120
            http_pool_maxsize = 100
            http_pool_connections = 100
            ssl_verify = False
            wapi_version = 2.2
            admin_user_name = admin
            admin_password = infoblox
            grid_master_name = infoblox.localdomain
            grid_master_host = 1.2.3.4


            [QUOTAS]
            ...</pre></div></li><li class="listitem "><p>
     Change <code class="literal">nova.conf.j2</code> to replace the notification driver
     "messaging" to "messagingv2"
    </p><div class="verbatim-wrap"><pre class="screen"> ...

 # Oslo messaging
 notification_driver = log

 #  Note:
 #  If the infoblox-ipam-agent is to be deployed in the cloud, change the
 #  notification_driver setting from "messaging" to "messagingv2".
 notification_driver = messagingv2
 notification_topics = notifications

 # Policy
 ...</pre></div></li><li class="listitem "><p>
     Commit the changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud
<code class="prompt user">ardana &gt; </code>git commit –a –m 'My config for enabling the Infoblox IPAM driver'</pre></div></li><li class="listitem "><p>
     Deploy the cloud with the changes. Due to changes to the
     control_plane.yml, you will need to rerun the config-processor-run.yml
     playbook if you have run it already during the install process.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml</pre></div></li></ol></div></div><div class="sect3" id="id-1.5.12.6.11.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.7.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuration parameters for using the Infoblox IPAM driver</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.11.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-using_ipam.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-using_ipam.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Changes required in the notification parameters in nova.conf:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /></colgroup><thead><tr><th>Parameter Name</th><th>Section in nova.conf</th><th>Default Value</th><th>Current Value </th><th>Description</th></tr></thead><tbody><tr><td>notify_on_state_change</td><td>DEFAULT</td><td>None</td><td>vm_and_task_state</td><td>
       <p>
        Send compute.instance.update notifications on instance state changes.
       </p>
       <p>
        Vm_and_task_state means notify on vm and task state changes.
       </p>
       <p>
        Infoblox requires the value to be vm_state (notify on vm state change).
       </p>
       <p>
        <span class="bold"><strong> Thus NO CHANGE is needed for infoblox</strong></span>
       </p>
      </td></tr><tr><td>notification_topics</td><td>DEFAULT</td><td>empty list</td><td>notifications</td><td>
       <p>
        <span class="bold"><strong>NO CHANGE is needed for infoblox.</strong></span>
       </p>
       <p>
        The infoblox installation guide requires the notifications to be
        "notifications"
       </p>
      </td></tr><tr><td>notification_driver</td><td>DEFAULT</td><td>None</td><td>messaging</td><td>
       <p>
        <span class="bold"><strong>Change needed.</strong></span>
       </p>
       <p>
        The infoblox installation guide requires the notification driver to be
        "messagingv2".
       </p>
      </td></tr></tbody></table></div><p>
   Changes to existing parameters in neutron.conf
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /></colgroup><thead><tr><th>Parameter Name</th><th>Section in neutron.conf</th><th>Default Value</th><th>Current Value </th><th>Description</th></tr></thead><tbody><tr><td>ipam_driver</td><td>DEFAULT</td><td>None</td><td>
       <p>
        None
       </p>
       <p>
        (param is undeclared in neutron.conf)
       </p>
      </td><td>
       <p>
        Pluggable IPAM driver to be used by neutron API server.
       </p>
       <p>
        For infoblox, the value is
        "networking_infoblox.ipam.driver.InfobloxPool"
       </p>
      </td></tr><tr><td>notification_driver</td><td>DEFAULT</td><td>empty list</td><td>messaging</td><td>
       <p>
        The driver used to send notifications from the neutron API server to
        the neutron agents.
       </p>
       <p>
        The installation guide for networking-infoblox calls for the
        notification_driver to be "messagingv2"
       </p>
      </td></tr><tr><td>notification_topics</td><td>DEFAULT</td><td>None</td><td>notifications</td><td>
       <p>
        <span class="bold"><strong>No change needed</strong></span>.
       </p>
       <p>
        The row is here show the changes in the neutron parameters described in
        the installation guide for networking-infoblox
       </p>
      </td></tr></tbody></table></div><p>
   Parameters specific to the Networking Infoblox Driver. All the parameters
   for the Infoblox IPAM driver must be defined in neutron.conf.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /></colgroup><thead><tr><th>Parameter Name</th><th>Section in neutron.conf</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td>cloud_data_center_id</td><td>infoblox</td><td>0</td><td>ID for selecting a particular grid from one or more grids to serve networks in
                the Infoblox back end</td></tr><tr><td>ipam_agent_workers</td><td>infoblox</td><td>1</td><td>Number of Infoblox IPAM agent works to run</td></tr><tr><td>grid_master_host</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>empty string</td><td>IP address of the grid master. WAPI requests are sent to the
                grid_master_host</td></tr><tr><td>ssl_verify</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>False</td><td>Ensure whether WAPI requests sent over HTTPS require SSL verification</td></tr><tr><td>WAPI Version</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>1.4</td><td>The WAPI version. Value should be 2.2.</td></tr><tr><td>admin_user_name</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>empty string</td><td>Admin user name to access the grid master or cloud platform appliance</td></tr><tr><td>admin_password</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>empty string</td><td>Admin user password</td></tr><tr><td>http_pool_connections</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>100</td><td> </td></tr><tr><td>http_pool_maxsize</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>100</td><td> </td></tr><tr><td>http_request_timeout</td><td>infoblox-dc.&lt;cloud_data_center_id&gt;</td><td>120</td><td> </td></tr></tbody></table></div><p>
  The diagram below shows nova compute sending notification to the
  infoblox-ipam-agent
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-networking-ipam.png" target="_blank"><img src="images/media-networking-ipam.png" width="" /></a></div></div></div><div class="sect3" id="id-1.5.12.6.11.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.7.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.11.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-using_ipam.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-using_ipam.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     There is no IPAM migration path from non-pluggable to pluggable IPAM
     driver
     (<a class="link" href="https://bugs.launchpad.net/neutron/+bug/1516156" target="_blank">https://bugs.launchpad.net/neutron/+bug/1516156</a>).
     This means there is no way to reconfigure the neutron database if you
     wanted to change neutron to use a pluggable IPAM driver. Unless you change
     the default of non-pluggable IPAM configuration to a pluggable driver at
     install time, you will have no other opportunity to make that change
     because reconfiguration of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9from using the default
     non-pluggable IPAM configuration to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 using a pluggable IPAM
     driver is not supported.
    </p></li><li class="listitem "><p>
     Upgrade from previous versions of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 to use a
     pluggable IPAM driver is not supported.
    </p></li><li class="listitem "><p>
     The Infoblox appliance does not allow for overlapping IPs. For example,
     only one tenant can have a CIDR of 10.0.0.0/24.
    </p></li><li class="listitem "><p>
     The infoblox IPAM driver fails the creation of a subnet when a there is no
     gateway-ip supplied. For example, the command <code class="command">openstack subnet create ...
     --no-gateway ...</code> will fail.
    </p></li></ul></div></div></div><div class="sect2" id="HP2-0LBaaSAdmin"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Load Balancing as a Service (LBaaS)</span> <a title="Permalink" class="permalink" href="#HP2-0LBaaSAdmin">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-lbaas_admin.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-lbaas_admin.xml</li><li><span class="ds-label">ID: </span>HP2-0LBaaSAdmin</li></ul></div></div></div></div><p>
  <span class="bold"><strong><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 LBaaS Configuration</strong></span>
 </p><p>
  Load Balancing as a Service (LBaaS) is an advanced networking service that
  allows load balancing of multi-node environments. It provides the ability to
  spread requests across multiple servers thereby reducing the load on any
  single server. This document describes the installation steps and the
  configuration for LBaaS v2.
 </p><div id="id-1.5.12.6.12.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
   The LBaaS architecture is based on a driver model to support different load
   balancers. LBaaS-compatible drivers are provided by load balancer vendors
   including F5 and Citrix. A new software load balancer driver was introduced
   in the OpenStack Liberty release called "<span class="emphasis"><em>Octavia</em></span>". The
   Octavia driver deploys a software load balancer called HAProxy. Octavia is
   the default load balancing provider in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 for LBaaS V2.
   Until Octavia is configured the creation of load balancers will fail with an
   error. Refer to <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 43 “Configuring Load Balancer as a Service”</span> document for
   information on installing Octavia.
  </p></div><div id="id-1.5.12.6.12.5" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
   Before upgrading to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9, contact F5 and
   SUSE to determine which F5 drivers have been certified for use with
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. Loading drivers not certified by SUSE may result in
   failure of your cloud deployment.
  </p></div><p>
  LBaaS V2 offers with <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 43 “Configuring Load Balancer as a Service”</span> a software load
  balancing solution that supports both a highly available control plane and
  data plane. However, should an external hardware load balancer be selected
  the cloud operation can achieve additional performance and availability.
 </p><p>
  <span class="bold"><strong>LBaaS v2</strong></span>
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Your vendor already has a driver that supports LBaaS v2. Many hardware load
    balancer vendors already support LBaaS v2 and this list is growing all the
    time.
   </p></li><li class="listitem "><p>
    You intend to script your load balancer creation and management so a UI is
    not important right now (horizon support will be added in a future
    release).
   </p></li><li class="listitem "><p>
    You intend to support TLS termination at the load balancer.
   </p></li><li class="listitem "><p>
    You intend to use the Octavia software load balancer (adding HA and
    scalability).
   </p></li><li class="listitem "><p>
    You do not want to take your load balancers offline to perform
    subsequent LBaaS upgrades.
   </p></li><li class="listitem "><p>
    You intend in future releases to need L7 load balancing.
   </p></li></ol></div><p>
  Reasons not to select this version.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Your LBaaS vendor does not have a v2 driver.
   </p></li><li class="listitem "><p>
    You must be able to manage your load balancers from horizon.
   </p></li><li class="listitem "><p>
    You have legacy software which utilizes the LBaaS v1 API.
   </p></li></ol></div><p>
  LBaaS v2 is installed by default with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> and requires
  minimal configuration to start the service.
 </p><div id="id-1.5.12.6.12.12" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   LBaaS V2 API currently supports load balancer failover with Octavia.
   LBaaS v2 API includes automatic failover of a deployed load balancer with
   Octavia. More information about this driver can be found in
   <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 43 “Configuring Load Balancer as a Service”</span>.
  </p></div><div class="sect3" id="idg-all-networking-lbaas-admin-xml-7"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#idg-all-networking-lbaas-admin-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-lbaas_admin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-lbaas_admin.xml</li><li><span class="ds-label">ID: </span>idg-all-networking-lbaas-admin-xml-7</li></ul></div></div></div></div><p>
   <span class="bold"><strong><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> LBaaS v2</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> must be installed for LBaaS v2.
    </p></li><li class="listitem "><p>
     Follow the instructions to install <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 43 “Configuring Load Balancer as a Service”</span>
    </p></li></ol></div></div></div><div class="sect2" id="OctaviaAdmin"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Load Balancer: Octavia Driver Administration</span> <a title="Permalink" class="permalink" href="#OctaviaAdmin">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-octavia_admin.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-octavia_admin.xml</li><li><span class="ds-label">ID: </span>OctaviaAdmin</li></ul></div></div></div></div><p>
  This document provides the instructions on how to enable and manage various
  components of the Load Balancer Octavia driver if that driver is enabled.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="#Alerts" title="10.4.9.1. Monasca Alerts">Section 10.4.9.1, “Monasca Alerts”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#Tuning" title="10.4.9.2. Tuning Octavia Installation">Section 10.4.9.2, “Tuning Octavia Installation”</a>
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Homogeneous Compute Configuration
     </p></li><li class="listitem "><p>
      Octavia and Floating IP's
     </p></li><li class="listitem "><p>
      Configuration Files
     </p></li><li class="listitem "><p>
      Spare Pools
     </p></li></ul></div></li><li class="listitem "><p>
    <a class="xref" href="#Amphora" title="10.4.9.3. Managing Amphora">Section 10.4.9.3, “Managing Amphora”</a>
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Updating the Cryptographic Certificates
     </p></li><li class="listitem "><p>
      Accessing VM information in nova
     </p></li><li class="listitem "><p>
      Initiating Failover of an Amphora VM
     </p></li></ul></div></li></ul></div><div class="sect3" id="Alerts"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monasca Alerts</span> <a title="Permalink" class="permalink" href="#Alerts">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-octavia_admin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-octavia_admin.xml</li><li><span class="ds-label">ID: </span>Alerts</li></ul></div></div></div></div><p>
   The monasca-agent has the following Octavia-related plugins:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Process checks – checks if octavia processes are running. When it
     starts, it detects which processes are running and then monitors them.
    </p></li><li class="listitem "><p>
     http_connect check – checks if it can connect to octavia api servers.
    </p></li></ul></div><p>
   Alerts are displayed in the Operations Console.
  </p></div><div class="sect3" id="Tuning"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Tuning Octavia Installation</span> <a title="Permalink" class="permalink" href="#Tuning">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-octavia_admin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-octavia_admin.xml</li><li><span class="ds-label">ID: </span>Tuning</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Homogeneous Compute Configuration</strong></span>
  </p><p>
   Octavia works only with homogeneous compute node configurations. Currently,
   Octavia does not support multiple nova flavors. If Octavia needs to be
   supported on multiple compute nodes, then all the compute nodes should carry
   same set of physnets (which will be used for Octavia).
  </p><p>
   <span class="bold"><strong>Octavia and Floating IPs</strong></span>
  </p><p>
   Due to a neutron limitation Octavia will only work with CVR routers. Another
   option is to use VLAN provider networks which do not require a router.
  </p><p>
   You cannot currently assign a floating IP address as the VIP (user facing)
   address for a load balancer created by the Octavia driver if the underlying
   neutron network is configured to support Distributed Virtual Router (DVR).
   The Octavia driver uses a neutron function known as
   <span class="emphasis"><em>allowed address pairs</em></span>
   to support load balancer fail over.
  </p><p>
   There is currently a neutron bug that does not support this function in a
   DVR configuration
  </p><p>
   <span class="bold"><strong>Octavia Configuration Files</strong></span>
  </p><p>
   The system comes pre-tuned and should not need any adjustments for most
   customers. If in rare instances manual tuning is needed, follow these steps:
  </p><div id="id-1.5.12.6.13.5.10" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
    Changes might be lost during <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> upgrades.
   </p></div><p>
   Edit the Octavia configuration files in
   <code class="literal">my_cloud/config/octavia</code>. It is recommended that any
   changes be made in all of the Octavia configuration files.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     octavia-api.conf.j2
    </p></li><li class="listitem "><p>
     octavia-health-manager.conf.j2
    </p></li><li class="listitem "><p>
     octavia-housekeeping.conf.j2
    </p></li><li class="listitem "><p>
     octavia-worker.conf.j2
    </p></li></ul></div><p>
   After the changes are made to the configuration files, redeploy the service.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Commit changes to git.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My Octavia Config"</pre></div></li><li class="listitem "><p>
     Run the configuration processor and ready deployment.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the Octavia reconfigure.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts octavia-reconfigure.yml</pre></div></li></ol></div><p>
   <span class="bold"><strong>Spare Pools</strong></span>
  </p><p>
    The Octavia driver provides support for creating spare pools of
    the HAProxy software installed in VMs. This means instead of
    creating a new load balancer when loads increase, create new load
    balancer calls will pull a load balancer from the spare pool. The
    spare pools feature consumes resources, therefore the load
    balancers in the spares pool has been set to 0, which is the
    default and also disables the feature.
  </p><p>
    Reasons to enable a load balancing spare pool in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
        You expect a large number of load balancers to be provisioned all at once
        (puppet scripts, or ansible scripts) and you want them to come up quickly.
      </p></li><li class="listitem "><p>
        You want to reduce the wait time a customer has while requesting a new
        load balancer.
      </p></li></ol></div><p>
    To increase the number of load balancers in your spares pool, edit
    the Octavia configuration files by uncommenting the
    <code class="literal">spare_amphora_pool_size</code> and adding the number of load
    balancers you would like to include in your spares pool.
  </p><div class="verbatim-wrap"><pre class="screen"># Pool size for the spare pool
# spare_amphora_pool_size = 0</pre></div></div><div class="sect3" id="Amphora"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.9.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Amphora</span> <a title="Permalink" class="permalink" href="#Amphora">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-octavia_admin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-octavia_admin.xml</li><li><span class="ds-label">ID: </span>Amphora</li></ul></div></div></div></div><p>
   Octavia starts a separate VM for each load balancing function. These VMs are
   called amphora.
  </p><p>
   <span class="bold"><strong>Updating the Cryptographic Certificates</strong></span>
  </p><p>
   Octavia uses two-way SSL encryption for communication between amphora and
   the control plane. Octavia keeps track of the certificates on the amphora
   and will automatically recycle them. The certificates on the control plane
   are valid for one year after installation of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  </p><p>
   You can check on the status of the certificate by logging into the
   controller node as root and running:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd /opt/stack/service/octavia-<em class="replaceable ">SOME UUID</em>/etc/certs/
openssl x509 -in client.pem  -text –noout</pre></div><p>
   This prints the certificate out where you can check on the expiration dates.
  </p><p>
   To renew the certificates, reconfigure Octavia. Reconfiguring causes Octavia
   to automatically generate new certificates and deploy them to the controller
   hosts.
  </p><p>
   On the Cloud Lifecycle Manager execute octavia-reconfigure:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts octavia-reconfigure.yml</pre></div><p>
   <span class="bold"><strong>Accessing VM information in nova</strong></span>
  </p><p>
   You can use <code class="literal">openstack project list</code> as an administrative
   user to obtain information about the tenant or project-id of the Octavia
   project. In the example below, the Octavia project has a project-id of
   <code class="literal">37fd6e4feac14741b6e75aba14aea833</code>.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack project list
+----------------------------------+------------------+
| ID                               | Name             |
+----------------------------------+------------------+
| 055071d8f25d450ea0b981ca67f7ccee | glance-swift     |
| 37fd6e4feac14741b6e75aba14aea833 | octavia          |
| 4b431ae087ef4bd285bc887da6405b12 | swift-monitor    |
| 8ecf2bb5754646ae97989ba6cba08607 | swift-dispersion |
| b6bd581f8d9a48e18c86008301d40b26 | services         |
| bfcada17189e4bc7b22a9072d663b52d | cinderinternal   |
| c410223059354dd19964063ef7d63eca | monitor          |
| d43bc229f513494189422d88709b7b73 | admin            |
| d5a80541ba324c54aeae58ac3de95f77 | demo             |
| ea6e039d973e4a58bbe42ee08eaf6a7a | backup           |
+----------------------------------+------------------+</pre></div><p>
   You can then use <code class="literal">openstack server list --tenant &lt;project-id&gt;</code> to
   list the VMs for the Octavia tenant. Take particular note of the IP address
   on the OCTAVIA-MGMT-NET; in the example below it is
   <code class="literal">172.30.1.11</code>. For additional nova command-line options see
   <a class="xref" href="#idg-all-networking-octavia-admin-xml-10" title="10.4.9.5. For More Information">Section 10.4.9.5, “For More Information”</a>.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list --tenant 37fd6e4feac14741b6e75aba14aea833
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+
| ID                                   | Name                                         | Tenant ID                        | Status | Task State | Power State | Networks                                       |
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+
| 1ed8f651-de31-4208-81c5-817363818596 | amphora-1c3a4598-5489-48ea-8b9c-60c821269e4c | 37fd6e4feac14741b6e75aba14aea833 | ACTIVE | -          | Running     | private=10.0.0.4; OCTAVIA-MGMT-NET=172.30.1.11 |
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+</pre></div><div id="id-1.5.12.6.13.6.16" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    The Amphora VMs do not have SSH or any other access. In the rare case that
    there is a problem with the underlying load balancer the whole amphora will
    need to be replaced.
   </p></div><p>
   <span class="bold"><strong>Initiating Failover of an Amphora VM</strong></span>
  </p><p>
   Under normal operations Octavia will monitor the health of the amphora
   constantly and automatically fail them over if there are any issues. This
   helps to minimize any potential downtime for load balancer users. There are,
   however, a few cases a failover needs to be initiated manually:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     The Loadbalancer has become unresponsive and Octavia has not detected an
     error.
    </p></li><li class="listitem "><p>
     A new image has become available and existing load balancers need to start
     using the new image.
    </p></li><li class="listitem "><p>
     The cryptographic certificates to control and/or the HMAC password to
     verify Health information of the amphora have been compromised.
    </p></li></ol></div><p>
   To minimize the impact for end users we will keep the existing load balancer
   working until shortly before the new one has been provisioned. There will be
   a short interruption for the load balancing service so keep that in mind
   when scheduling the failovers. To achieve that follow these steps (assuming
   the management ip from the previous step):
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Assign the IP to a SHELL variable for better readability.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>export MGM_IP=172.30.1.11</pre></div></li><li class="listitem "><p>
     Identify the port of the vm on the management network.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port list | grep $MGM_IP
| 0b0301b9-4ee8-4fb6-a47c-2690594173f4 |                                                   | fa:16:3e:d7:50:92 |
{"subnet_id": "3e0de487-e255-4fc3-84b8-60e08564c5b7", "ip_address": "172.30.1.11"} |</pre></div></li><li class="listitem "><p>
     Disable the port to initiate a failover. Note the load balancer will still
     function but cannot be controlled any longer by Octavia.
    </p><div id="id-1.5.12.6.13.6.21.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      Changes after disabling the port will result in errors.
     </p></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port set --admin-state-up False 0b0301b9-4ee8-4fb6-a47c-2690594173f4
Updated port: 0b0301b9-4ee8-4fb6-a47c-2690594173f4</pre></div></li><li class="listitem "><p>
     You can check to see if the amphora failed over with <code class="literal">openstack
     server list --tenant &lt;project-id&gt;</code>. This may take some time
     and in some cases may need to be repeated several times. You can tell that
     the failover has been successful by the changed IP on the management
     network.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list --tenant 37fd6e4feac14741b6e75aba14aea833
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+
| ID                                   | Name                                         | Tenant ID                        | Status | Task State | Power State | Networks                                       |
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+
| 1ed8f651-de31-4208-81c5-817363818596 | amphora-1c3a4598-5489-48ea-8b9c-60c821269e4c | 37fd6e4feac14741b6e75aba14aea833 | ACTIVE | -          | Running     | private=10.0.0.4; OCTAVIA-MGMT-NET=172.30.1.12 |
+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+------------------------------------------------+</pre></div></li></ol></div><div id="id-1.5.12.6.13.6.22" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
    Do not issue too many failovers at once. In a big installation you might be
    tempted to initiate several failovers in parallel for instance to speed up
    an update of amphora images. This will put a strain on the nova service and
    depending on the size of your installation you might need to throttle the
    failover rate.
   </p></div></div><div class="sect3" id="OctaviaMaintenance"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.9.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Load Balancer: Octavia Administration</span> <a title="Permalink" class="permalink" href="#OctaviaMaintenance">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-octavia-maintenance.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-octavia-maintenance.xml</li><li><span class="ds-label">ID: </span>OctaviaMaintenance</li></ul></div></div></div></div><div class="sect4" id="octavia-admin-delete"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.9.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Removing load balancers</span> <a title="Permalink" class="permalink" href="#octavia-admin-delete">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-octavia-maintenance.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-octavia-maintenance.xml</li><li><span class="ds-label">ID: </span>octavia-admin-delete</li></ul></div></div></div></div><p>
      The following procedures demonstrate how to delete a load
      balancer that is in the <code class="literal">ERROR</code>,
      <code class="literal">PENDING_CREATE</code>, or
      <code class="literal">PENDING_DELETE</code> state.
    </p><div class="procedure " id="id-1.5.12.6.13.7.2.3"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 10.3: </span><span class="name">
        Manually deleting load balancers created with neutron lbaasv2
        (in an upgrade/migration scenario)
       </span><a title="Permalink" class="permalink" href="#id-1.5.12.6.13.7.2.3">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Query the Neutron service for the loadbalancer ID:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron lbaas-loadbalancer-list
neutron CLI is deprecated and will be removed in the future. Use openstack CLI instead.
+--------------------------------------+---------+----------------------------------+--------------+---------------------+----------+
| id                                   | name    | tenant_id                        | vip_address  | provisioning_status | provider |
+--------------------------------------+---------+----------------------------------+--------------+---------------------+----------+
| 7be4e4ab-e9c6-4a57-b767-da9af5ba7405 | test-lb | d62a1510b0f54b5693566fb8afeb5e33 | 192.168.1.10 | ERROR               | haproxy  |
+--------------------------------------+---------+----------------------------------+--------------+---------------------+----------+</pre></div></li><li class="step "><p>
          Connect to the neutron database:
        </p><div id="id-1.5.12.6.13.7.2.3.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
            The default database name depends on the life cycle
            manager. Ardana uses <code class="literal">ovs_neutron</code> while
            Crowbar uses <code class="literal">neutron</code>.
          </p></div><p>Ardana:</p><div class="verbatim-wrap"><pre class="screen">mysql&gt; use ovs_neutron</pre></div><p>Crowbar:</p><div class="verbatim-wrap"><pre class="screen">mysql&gt; use neutron</pre></div></li><li class="step "><p>
          Get the pools and healthmonitors associated with the loadbalancer:
        </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; select id, healthmonitor_id, loadbalancer_id from lbaas_pools where loadbalancer_id = '7be4e4ab-e9c6-4a57-b767-da9af5ba7405';
+--------------------------------------+--------------------------------------+--------------------------------------+
| id                                   | healthmonitor_id                     | loadbalancer_id                      |
+--------------------------------------+--------------------------------------+--------------------------------------+
| 26c0384b-fc76-4943-83e5-9de40dd1c78c | 323a3c4b-8083-41e1-b1d9-04e1fef1a331 | 7be4e4ab-e9c6-4a57-b767-da9af5ba7405 |
+--------------------------------------+--------------------------------------+--------------------------------------+</pre></div></li><li class="step "><p>
          Get the members associated with the pool:
        </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; select id, pool_id from lbaas_members where pool_id = '26c0384b-fc76-4943-83e5-9de40dd1c78c';
+--------------------------------------+--------------------------------------+
| id                                   | pool_id                              |
+--------------------------------------+--------------------------------------+
| 6730f6c1-634c-4371-9df5-1a880662acc9 | 26c0384b-fc76-4943-83e5-9de40dd1c78c |
| 06f0cfc9-379a-4e3d-ab31-cdba1580afc2 | 26c0384b-fc76-4943-83e5-9de40dd1c78c |
+--------------------------------------+--------------------------------------+</pre></div></li><li class="step "><p>
          Delete the pool members:
        </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; delete from lbaas_members where id = '6730f6c1-634c-4371-9df5-1a880662acc9';
mysql&gt; delete from lbaas_members where id = '06f0cfc9-379a-4e3d-ab31-cdba1580afc2';</pre></div></li><li class="step "><p>
          Find and delete the listener associated with the loadbalancer:
        </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; select id, loadbalancer_id, default_pool_id from lbaas_listeners where loadbalancer_id = '7be4e4ab-e9c6-4a57-b767-da9af5ba7405';
+--------------------------------------+--------------------------------------+--------------------------------------+
| id                                   | loadbalancer_id                      | default_pool_id                      |
+--------------------------------------+--------------------------------------+--------------------------------------+
| 3283f589-8464-43b3-96e0-399377642e0a | 7be4e4ab-e9c6-4a57-b767-da9af5ba7405 | 26c0384b-fc76-4943-83e5-9de40dd1c78c |
+--------------------------------------+--------------------------------------+--------------------------------------+
mysql&gt; delete from lbaas_listeners where id = '3283f589-8464-43b3-96e0-399377642e0a';</pre></div></li><li class="step "><p>
          Delete the pool associated with the loadbalancer:
        </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; delete from lbaas_pools where id = '26c0384b-fc76-4943-83e5-9de40dd1c78c';</pre></div></li><li class="step "><p>
          Delete the healthmonitor associated with the pool:
        </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; delete from lbaas_healthmonitors where id = '323a3c4b-8083-41e1-b1d9-04e1fef1a331';</pre></div></li><li class="step "><p>
          Delete the loadbalancer:
        </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; delete from lbaas_loadbalancer_statistics where loadbalancer_id = '7be4e4ab-e9c6-4a57-b767-da9af5ba7405';
mysql&gt; delete from lbaas_loadbalancers where id = '7be4e4ab-e9c6-4a57-b767-da9af5ba7405';</pre></div></li></ol></div></div><div class="procedure " id="id-1.5.12.6.13.7.2.4"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 10.4: </span><span class="name">Manually Deleting Load Balancers Created With Octavia </span><a title="Permalink" class="permalink" href="#id-1.5.12.6.13.7.2.4">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Query the Octavia service for the loadbalancer ID:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack loadbalancer list --column id --column name --column provisioning_status
+--------------------------------------+---------+---------------------+
| id                                   | name    | provisioning_status |
+--------------------------------------+---------+---------------------+
| d8ac085d-e077-4af2-b47a-bdec0c162928 | test-lb | ERROR               |
+--------------------------------------+---------+---------------------+</pre></div></li><li class="step "><p>
          Query the Octavia service for the amphora IDs (in this
          example we use <code class="literal">ACTIVE/STANDBY</code> topology with 1 spare Amphora):
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack loadbalancer amphora list
+--------------------------------------+--------------------------------------+-----------+--------+---------------+-------------+
| id                                   | loadbalancer_id                      | status    | role   | lb_network_ip | ha_ip       |
+--------------------------------------+--------------------------------------+-----------+--------+---------------+-------------+
| 6dc66d41-e4b6-4c33-945d-563f8b26e675 | d8ac085d-e077-4af2-b47a-bdec0c162928 | ALLOCATED | BACKUP | 172.30.1.7    | 192.168.1.8 |
| 1b195602-3b14-4352-b355-5c4a70e200cf | d8ac085d-e077-4af2-b47a-bdec0c162928 | ALLOCATED | MASTER | 172.30.1.6    | 192.168.1.8 |
| b2ee14df-8ac6-4bb0-a8d3-3f378dbc2509 | None                                 | READY     | None   | 172.30.1.20   | None        |
+--------------------------------------+--------------------------------------+-----------+--------+---------------+-------------+</pre></div></li><li class="step "><p>
          Query the Octavia service for the loadbalancer pools:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack loadbalancer pool list
+--------------------------------------+-----------+----------------------------------+---------------------+----------+--------------+----------------+
| id                                   | name      | project_id                       | provisioning_status | protocol | lb_algorithm | admin_state_up |
+--------------------------------------+-----------+----------------------------------+---------------------+----------+--------------+----------------+
| 39c4c791-6e66-4dd5-9b80-14ea11152bb5 | test-pool | 86fba765e67f430b83437f2f25225b65 | ACTIVE              | TCP      | ROUND_ROBIN  | True           |
+--------------------------------------+-----------+----------------------------------+---------------------+----------+--------------+----------------+</pre></div></li><li class="step "><p>
          Connect to the octavia database:
        </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; use octavia</pre></div></li><li class="step "><p>
          Delete any listeners, pools, health monitors, and members
          from the load balancer:
        </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; delete from listener where load_balancer_id = 'd8ac085d-e077-4af2-b47a-bdec0c162928';
mysql&gt; delete from health_monitor where pool_id = '39c4c791-6e66-4dd5-9b80-14ea11152bb5';
mysql&gt; delete from member where pool_id = '39c4c791-6e66-4dd5-9b80-14ea11152bb5';
mysql&gt; delete from pool where load_balancer_id = 'd8ac085d-e077-4af2-b47a-bdec0c162928';</pre></div></li><li class="step "><p>
          Delete the amphora entries in the database:
        </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; delete from amphora_health where amphora_id = '6dc66d41-e4b6-4c33-945d-563f8b26e675';
mysql&gt; update amphora set status = 'DELETED' where id = '6dc66d41-e4b6-4c33-945d-563f8b26e675';
mysql&gt; delete from amphora_health where amphora_id = '1b195602-3b14-4352-b355-5c4a70e200cf';
mysql&gt; update amphora set status = 'DELETED' where id = '1b195602-3b14-4352-b355-5c4a70e200cf';</pre></div></li><li class="step "><p>
          Delete the load balancer instance:
        </p><div class="verbatim-wrap"><pre class="screen">mysql&gt; update load_balancer set provisioning_status = 'DELETED' where id = 'd8ac085d-e077-4af2-b47a-bdec0c162928';</pre></div></li><li class="step "><p>
          The following script automates the above steps:
        </p><div class="verbatim-wrap"><pre class="screen">#!/bin/bash

if (( $# != 1 )); then
echo "Please specify a loadbalancer ID"
exit 1
fi

LB_ID=$1

set -u -e -x

readarray -t AMPHORAE &lt; &lt;(openstack loadbalancer amphora list \
--format value \
--column id \
--column loadbalancer_id \
| grep ${LB_ID} \
| cut -d ' ' -f 1)

readarray -t POOLS &lt; &lt;(openstack loadbalancer show ${LB_ID} \
--format value \
--column pools)

mysql octavia --execute "delete from listener where load_balancer_id = '${LB_ID}';"
for p in "${POOLS[@]}"; do
mysql octavia --execute "delete from health_monitor where pool_id = '${p}';"
mysql octavia --execute "delete from member where pool_id = '${p}';"
done
mysql octavia --execute "delete from pool where load_balancer_id = '${LB_ID}';"
for a in "${AMPHORAE[@]}"; do
mysql octavia --execute "delete from amphora_health where amphora_id = '${a}';"
mysql octavia --execute "update amphora set status = 'DELETED' where id = '${a}';"
done
mysql octavia --execute "update load_balancer set provisioning_status = 'DELETED' where id = '${LB_ID}';"</pre></div></li></ol></div></div></div></div><div class="sect3" id="idg-all-networking-octavia-admin-xml-10"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.9.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="#idg-all-networking-octavia-admin-xml-10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-octavia_admin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-octavia_admin.xml</li><li><span class="ds-label">ID: </span>idg-all-networking-octavia-admin-xml-10</li></ul></div></div></div></div><p>
   For more information on the OpenStackClient and Octavia terminology, see the
   <a class="link" href="https://docs.openstack.org/python-openstackclient/latest/" target="_blank">OpenStackClient</a>
   guide.
  </p></div></div><div class="sect2" id="cha-network-rbac"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Role-based Access Control in neutron</span> <a title="Permalink" class="permalink" href="#cha-network-rbac">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span>cha-network-rbac</li></ul></div></div></div></div><p>
   This topic explains how to achieve more granular access control for your
   neutron networks.
  </p><p>
   Previously in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, a network object was either private to a project or
   could be used by all projects. If the network's shared attribute was True,
   then the network could be used by every project in the cloud. If false, only
   the members of the owning project could use it. There was no way for the
   network to be shared by only a subset of the projects.
  </p><p>
  <span class="phrase">neutron Role Based Access Control (RBAC) solves this problem for
   networks. Now the network owner can create RBAC policies that give network
   access to target projects. Members of a targeted project can use the
   network named in the RBAC policy the same way as if the network was owned
   by the project. Constraints are described in the
   section</span>
  <a class="xref" href="#sec-network-rbac-limitation" title="10.4.10.10. Limitations">Section 10.4.10.10, “Limitations”</a>.
 </p><p>
   With RBAC you are able to let another tenant use a network that you created,
   but as the owner of the network, you need to create the subnet and the
   router for the network.
  </p><div class="sect3" id="id-1.5.12.6.14.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a Network</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.14.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network create demo-net
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | UP                                   |
| availability_zone_hints   |                                      |
| availability_zones        |                                      |
| created_at                | 2018-07-25T17:43:59Z                 |
| description               |                                      |
| dns_domain                |                                      |
| id                        | 9c801954-ec7f-4a65-82f8-e313120aabc4 |
| ipv4_address_scope        | None                                 |
| ipv6_address_scope        | None                                 |
| is_default                | False                                |
| is_vlan_transparent       | None                                 |
| mtu                       | 1450                                 |
| name                      | demo-net                             |
| port_security_enabled     | False                                |
| project_id                | cb67c79e25a84e328326d186bf703e1b     |
| provider:network_type     | vxlan                                |
| provider:physical_network | None                                 |
| provider:segmentation_id  | 1009                                 |
| qos_policy_id             | None                                 |
| revision_number           | 2                                    |
| router:external           | Internal                             |
| segments                  | None                                 |
| shared                    | False                                |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tags                      |                                      |
| updated_at                | 2018-07-25T17:43:59Z                 |
+---------------------------+--------------------------------------+</pre></div></div><div class="sect3" id="id-1.5.12.6.14.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.10.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating an RBAC Policy</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.14.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Here we will create an RBAC policy where a member of the project called
   'demo' will share the network with members of project 'demo2'
  </p><p>
   To create the RBAC policy, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac create  --target-project <em class="replaceable ">DEMO2-PROJECT-ID</em> --type network --action access_as_shared demo-net</pre></div><p>
   Here is an example where the <em class="replaceable ">DEMO2-PROJECT-ID</em> is
   5a582af8b44b422fafcd4545bd2b7eb5
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac create --target-tenant 5a582af8b44b422fafcd4545bd2b7eb5 \
  --type network --action access_as_shared demo-net</pre></div></div><div class="sect3" id="id-1.5.12.6.14.8"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.10.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Listing RBACs</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.14.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To list all the RBAC rules/policies, execute:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac list
+--------------------------------------+-------------+--------------------------------------+
| ID                                   | Object Type | Object ID                            |
+--------------------------------------+-------------+--------------------------------------+
| 0fdec7f0-9b94-42b4-a4cd-b291d04282c1 | network     | 7cd94877-4276-488d-b682-7328fc85d721 |
+--------------------------------------+-------------+--------------------------------------+</pre></div></div><div class="sect3" id="id-1.5.12.6.14.9"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.10.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Listing the Attributes of an RBAC</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.14.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To see the attributes of a specific RBAC policy, run
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac show <em class="replaceable ">POLICY-ID</em></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac show 0fd89dcb-9809-4a5e-adc1-39dd676cb386</pre></div><p>
   Here is the output:
  </p><div class="verbatim-wrap"><pre class="screen">+---------------+--------------------------------------+
| Field         | Value                                |
+---------------+--------------------------------------+
| action        | access_as_shared                     |
| id            | 0fd89dcb-9809-4a5e-adc1-39dd676cb386 |
| object_id     | c3d55c21-d8c9-4ee5-944b-560b7e0ea33b |
| object_type   | network                              |
| target_tenant | 5a582af8b44b422fafcd4545bd2b7eb5     |
| tenant_id     | 75eb5efae5764682bca2fede6f4d8c6f     |
+---------------+--------------------------------------+</pre></div></div><div class="sect3" id="id-1.5.12.6.14.10"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.10.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deleting an RBAC Policy</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.14.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To delete an RBAC policy, run <code class="literal">openstack network rbac delete</code> passing the policy id:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac delete <em class="replaceable ">POLICY-ID</em></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac delete 0fd89dcb-9809-4a5e-adc1-39dd676cb386</pre></div><p>
   Here is the output:
  </p><div class="verbatim-wrap"><pre class="screen">Deleted rbac_policy: 0fd89dcb-9809-4a5e-adc1-39dd676cb386</pre></div></div><div class="sect3" id="id-1.5.12.6.14.11"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.10.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Sharing a Network with All Tenants</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.14.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Either the administrator or the network owner can make a network shareable
   by all tenants.
  </p><p>
   The administrator can make a tenant's network shareable by all tenants.
   To make the network <code class="literal">demo-shareall-net</code> accessible by all
   tenants in the cloud:
  </p><p>
   To share a network with all tenants:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Get a list of all projects
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>~/service.osrc
<code class="prompt user">ardana &gt; </code>openstack project list</pre></div><p>
      which produces the list:
    </p><div class="verbatim-wrap"><pre class="screen">+----------------------------------+------------------+
| ID                               | Name             |
+----------------------------------+------------------+
| 1be57778b61645a7a1c07ca0ac488f9e | demo             |
| 5346676226274cd2b3e3862c2d5ceadd | admin            |
| 749a557b2b9c482ca047e8f4abf348cd | swift-monitor    |
| 8284a83df4df429fb04996c59f9a314b | swift-dispersion |
| c7a74026ed8d4345a48a3860048dcb39 | demo-sharee      |
| e771266d937440828372090c4f99a995 | glance-swift     |
| f43fb69f107b4b109d22431766b85f20 | services         |
+----------------------------------+------------------+</pre></div></li><li class="step "><p>
     Get a list of networks:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network list</pre></div><p>
     This produces the following list:
    </p><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+-------------------+----------------------------------------------------+
| id                                   | name              | subnets                                            |
+--------------------------------------+-------------------+----------------------------------------------------+
| f50f9a63-c048-444d-939d-370cb0af1387 | ext-net           | ef3873db-fc7a-4085-8454-5566fb5578ea 172.31.0.0/16 |
| 9fb676f5-137e-4646-ac6e-db675a885fd3 | demo-net          | 18fb0b77-fc8b-4f8d-9172-ee47869f92cc 10.0.1.0/24   |
| 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e | demo-shareall-net | 2bbc85a9-3ffe-464c-944b-2476c7804877 10.0.250.0/24 |
| 73f946ee-bd2b-42e9-87e4-87f19edd0682 | demo-share-subset | c088b0ef-f541-42a7-b4b9-6ef3c9921e44 10.0.2.0/24   |
+--------------------------------------+-------------------+----------------------------------------------------+</pre></div></li><li class="step "><p>
     Set the network you want to share to a shared value of True:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network set --share 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e</pre></div><p>
     You should see the following output:
    </p><div class="verbatim-wrap"><pre class="screen">Updated network: 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e</pre></div></li><li class="step "><p>
     Check the attributes of that network by running the following command
     using the ID of the network in question:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network show 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e</pre></div><p>
     The output will look like this:
    </p><div class="verbatim-wrap"><pre class="screen">+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | UP                                   |
| availability_zone_hints   |                                      |
| availability_zones        |                                      |
| created_at                | 2018-07-25T17:43:59Z                 |
| description               |                                      |
| dns_domain                |                                      |
| id                        | 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e |
| ipv4_address_scope        | None                                 |
| ipv6_address_scope        | None                                 |
| is_default                | None                                 |
| is_vlan_transparent       | None                                 |
| mtu                       | 1450                                 |
| name                      | demo-net                             |
| port_security_enabled     | False                                |
| project_id                | cb67c79e25a84e328326d186bf703e1b     |
| provider:network_type     | vxlan                                |
| provider:physical_network | None                                 |
| provider:segmentation_id  | 1009                                 |
| qos_policy_id             | None                                 |
| revision_number           | 2                                    |
| router:external           | Internal                             |
| segments                  | None                                 |
| shared                    | False                                |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tags                      |                                      |
| updated_at                | 2018-07-25T17:43:59Z                 |
+---------------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     As the owner of the <code class="literal">demo-shareall-net</code> network, view
     the RBAC attributes for
     <code class="literal">demo-shareall-net</code>
     (<code class="literal">id=8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e</code>) by first
     getting an RBAC list:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>echo $OS_USERNAME ; echo $OS_PROJECT_NAME
demo
demo
<code class="prompt user">ardana &gt; </code>openstack network rbac list</pre></div><p>
     This produces the list:
    </p><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+--------------------------------------+
| id                                   | object_id                            |
+--------------------------------------+--------------------------------------+
| ...                                                                         |
| 3e078293-f55d-461c-9a0b-67b5dae321e8 | 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e |
+--------------------------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     View the RBAC information:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac show 3e078293-f55d-461c-9a0b-67b5dae321e8

+---------------+--------------------------------------+
| Field         | Value                                |
+---------------+--------------------------------------+
| action        | access_as_shared                     |
| id            | 3e078293-f55d-461c-9a0b-67b5dae321e8 |
| object_id     | 8eada4f7-83cf-40ba-aa8c-5bf7d87cca8e |
| object_type   | network                              |
| target_tenant | *                                    |
| tenant_id     | 1be57778b61645a7a1c07ca0ac488f9e     |
+---------------+--------------------------------------+</pre></div></li><li class="step "><p>
     With network RBAC, the owner of the network can also make the network
     shareable by all tenants. First create the network:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>echo $OS_PROJECT_NAME ; echo $OS_USERNAME
demo
demo
<code class="prompt user">ardana &gt; </code>openstack network create test-net</pre></div><p>
     The network is created:
    </p><div class="verbatim-wrap"><pre class="screen">+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | UP                                   |
| availability_zone_hints   |                                      |
| availability_zones        |                                      |
| created_at                | 2018-07-25T18:04:25Z                 |
| description               |                                      |
| dns_domain                |                                      |
| id                        | a4bd7c3a-818f-4431-8cdb-fedf7ff40f73 |
| ipv4_address_scope        | None                                 |
| ipv6_address_scope        | None                                 |
| is_default                | False                                |
| is_vlan_transparent       | None                                 |
| mtu                       | 1450                                 |
| name                      | test-net                             |
| port_security_enabled     | False                                |
| project_id                | cb67c79e25a84e328326d186bf703e1b     |
| provider:network_type     | vxlan                                |
| provider:physical_network | None                                 |
| provider:segmentation_id  | 1073                                 |
| qos_policy_id             | None                                 |
| revision_number           | 2                                    |
| router:external           | Internal                             |
| segments                  | None                                 |
| shared                    | False                                |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tags                      |                                      |
| updated_at                | 2018-07-25T18:04:25Z                 |
+---------------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     Create the RBAC. It is important that the asterisk is surrounded by
     single-quotes to prevent the shell from expanding it to all files in the
     current directory.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network rbac create --type network \
  --action access_as_shared --target-project '*' test-net</pre></div><p>
     Here are the resulting RBAC attributes:
    </p><div class="verbatim-wrap"><pre class="screen">+---------------+--------------------------------------+
| Field         | Value                                |
+---------------+--------------------------------------+
| action        | access_as_shared                     |
| id            | 0b797cc6-debc-48a1-bf9d-d294b077d0d9 |
| object_id     | a4bd7c3a-818f-4431-8cdb-fedf7ff40f73 |
| object_type   | network                              |
| target_tenant | *                                    |
| tenant_id     | 1be57778b61645a7a1c07ca0ac488f9e     |
+---------------+--------------------------------------+</pre></div></li></ol></div></div></div><div class="sect3" id="id-1.5.12.6.14.12"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.10.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Target Project (<code class="literal">demo2</code>) View of Networks and Subnets</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.14.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Note that the owner of the network and subnet is not the tenant named
   <code class="literal">demo2</code>. Both the network and subnet are owned by tenant <code class="literal">demo</code>.
   <code class="literal">Demo2</code>members cannot create subnets of the network. They also cannot
   modify or delete subnets owned by <code class="literal">demo</code>.
  </p><p>
   As the tenant <code class="literal">demo2</code>, you can get a list of neutron networks:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network list</pre></div><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+-----------+--------------------------------------------------+
| id                                   | name      | subnets                                          |
+--------------------------------------+-----------+--------------------------------------------------+
| f60f3896-2854-4f20-b03f-584a0dcce7a6 | ext-net   | 50e39973-b2e3-466b-81c9-31f4d83d990b             |
| c3d55c21-d8c9-4ee5-944b-560b7e0ea33b | demo-net  | d9b765da-45eb-4543-be96-1b69a00a2556 10.0.1.0/24 |
   ...
+--------------------------------------+-----------+--------------------------------------------------+</pre></div><p>
   And get a list of subnets:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack subnet list --network c3d55c21-d8c9-4ee5-944b-560b7e0ea33b</pre></div><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+---------+--------------------------------------+---------------+
| ID                                   | Name    | Network                              | Subnet        |
+--------------------------------------+---------+--------------------------------------+---------------+
| a806f28b-ad66-47f1-b280-a1caa9beb832 | ext-net | c3d55c21-d8c9-4ee5-944b-560b7e0ea33b | 10.0.1.0/24   |
+--------------------------------------+---------+--------------------------------------+---------------+</pre></div><p>
To show details of the subnet:
</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack subnet show d9b765da-45eb-4543-be96-1b69a00a2556</pre></div><div class="verbatim-wrap"><pre class="screen">+-------------------+--------------------------------------------+
| Field             | Value                                      |
+-------------------+--------------------------------------------+
| allocation_pools  | {"start": "10.0.1.2", "end": "10.0.1.254"} |
| cidr              | 10.0.1.0/24                                |
| dns_nameservers   |                                            |
| enable_dhcp       | True                                       |
| gateway_ip        | 10.0.1.1                                   |
| host_routes       |                                            |
| id                | d9b765da-45eb-4543-be96-1b69a00a2556       |
| ip_version        | 4                                          |
| ipv6_address_mode |                                            |
| ipv6_ra_mode      |                                            |
| name              | sb-demo-net                                |
| network_id        | c3d55c21-d8c9-4ee5-944b-560b7e0ea33b       |
| subnetpool_id     |                                            |
| tenant_id         | 75eb5efae5764682bca2fede6f4d8c6f           |
+-------------------+--------------------------------------------+</pre></div></div><div class="sect3" id="id-1.5.12.6.14.13"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.10.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Target Project: Creating a Port Using demo-net</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.14.13">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The owner of the port is <code class="literal">demo2</code>. Members of the network owner project
   (<code class="literal">demo</code>) will not see this port.
  </p><p>
   Running the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port create c3d55c21-d8c9-4ee5-944b-560b7e0ea33b</pre></div><p>
   Creates a new port:
  </p><div class="verbatim-wrap"><pre class="screen">+-----------------------+-----------------------------------------------------------------------------------------------------+
| Field                 | Value                                                                                               |
+-----------------------+-----------------------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                                |
| allowed_address_pairs |                                                                                                     |
| binding:vnic_type     | normal                                                                                              |
| device_id             |                                                                                                     |
| device_owner          |                                                                                                     |
| dns_assignment        | {"hostname": "host-10-0-1-10", "ip_address": "10.0.1.10", "fqdn": "host-10-0-1-10.openstacklocal."} |
| dns_name              |                                                                                                     |
| fixed_ips             | {"subnet_id": "d9b765da-45eb-4543-be96-1b69a00a2556", "ip_address": "10.0.1.10"}                    |
| id                    | 03ef2dce-20dc-47e5-9160-942320b4e503                                                                |
| mac_address           | fa:16:3e:27:8d:ca                                                                                   |
| name                  |                                                                                                     |
| network_id            | c3d55c21-d8c9-4ee5-944b-560b7e0ea33b                                                                |
| security_groups       | 275802d0-33cb-4796-9e57-03d8ddd29b94                                                                |
| status                | DOWN                                                                                                |
| tenant_id             | 5a582af8b44b422fafcd4545bd2b7eb5                                                                    |
+-----------------------+-----------------------------------------------------------------------------------------------------+</pre></div></div><div class="sect3" id="id-1.5.12.6.14.14"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.10.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Target Project Booting a VM Using Demo-Net</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.14.14">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Here the tenant <code class="literal">demo2</code> boots a VM that uses the <code class="literal">demo-net</code> shared network:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server create --flavor 1 --image $OS_IMAGE --nic net-id=c3d55c21-d8c9-4ee5-944b-560b7e0ea33b demo2-vm-using-demo-net-nic</pre></div><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+------------------------------------------------+
| Property                             | Value                                          |
+--------------------------------------+------------------------------------------------+
| OS-EXT-AZ:availability_zone          |                                                |
| OS-EXT-STS:power_state               | 0                                              |
| OS-EXT-STS:task_state                | scheduling                                     |
| OS-EXT-STS:vm_state                  | building                                       |
| OS-SRV-USG:launched_at               | -                                              |
| OS-SRV-USG:terminated_at             | -                                              |
| accessIPv4                           |                                                |
| accessIPv6                           |                                                |
| adminPass                            | sS9uSv9PT79F                                   |
| config_drive                         |                                                |
| created                              | 2016-01-04T19:23:24Z                           |
| flavor                               | m1.tiny (1)                                    |
| hostId                               |                                                |
| id                                   | 3a4dc44a-027b-45e9-acf8-054a7c2dca2a           |
| image                                | cirros-0.3.3-x86_64 (6ae23432-8636-4e...1efc5) |
| key_name                             | -                                              |
| metadata                             | {}                                             |
| name                                 | demo2-vm-using-demo-net-nic                    |
| os-extended-volumes:volumes_attached | []                                             |
| progress                             | 0                                              |
| security_groups                      | default                                        |
| status                               | BUILD                                          |
| tenant_id                            | 5a582af8b44b422fafcd4545bd2b7eb5               |
| updated                              | 2016-01-04T19:23:24Z                           |
| user_id                              | a0e6427b036344fdb47162987cb0cee5               |
+--------------------------------------+------------------------------------------------+</pre></div><p>
   Run openstack server list:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list</pre></div><p>
   See the VM running:
  </p><div class="verbatim-wrap"><pre class="screen">+-------------------+-----------------------------+--------+------------+-------------+--------------------+
| ID                | Name                        | Status | Task State | Power State | Networks           |
+-------------------+-----------------------------+--------+------------+-------------+--------------------+
| 3a4dc...a7c2dca2a | demo2-vm-using-demo-net-nic | ACTIVE | -          | Running     | demo-net=10.0.1.11 |
+-------------------+-----------------------------+--------+------------+-------------+--------------------+</pre></div><p>
   Run openstack port list:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstask port list --device-id 3a4dc44a-027b-45e9-acf8-054a7c2dca2a</pre></div><p>
   View the subnet:
  </p><div class="verbatim-wrap"><pre class="screen">+---------------------+------+-------------------+-------------------------------------------------------------------+
| id                  | name | mac_address       | fixed_ips                                                         |
+---------------------+------+-------------------+-------------------------------------------------------------------+
| 7d14ef8b-9...80348f |      | fa:16:3e:75:32:8e | {"subnet_id": "d9b765da-45...00a2556", "ip_address": "10.0.1.11"} |
+---------------------+------+-------------------+-------------------------------------------------------------------+</pre></div><p>
   Run openstack port show:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port show 7d14ef8b-9d48-4310-8c02-00c74d80348f</pre></div><div class="verbatim-wrap"><pre class="screen">+-----------------------+-----------------------------------------------------------------------------------------------------+
| Field                 | Value                                                                                               |
+-----------------------+-----------------------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                                |
| allowed_address_pairs |                                                                                                     |
| binding:vnic_type     | normal                                                                                              |
| device_id             | 3a4dc44a-027b-45e9-acf8-054a7c2dca2a                                                                |
| device_owner          | compute:None                                                                                        |
| dns_assignment        | {"hostname": "host-10-0-1-11", "ip_address": "10.0.1.11", "fqdn": "host-10-0-1-11.openstacklocal."} |
| dns_name              |                                                                                                     |
| extra_dhcp_opts       |                                                                                                     |
| fixed_ips             | {"subnet_id": "d9b765da-45eb-4543-be96-1b69a00a2556", "ip_address": "10.0.1.11"}                    |
| id                    | 7d14ef8b-9d48-4310-8c02-00c74d80348f                                                                |
| mac_address           | fa:16:3e:75:32:8e                                                                                   |
| name                  |                                                                                                     |
| network_id            | c3d55c21-d8c9-4ee5-944b-560b7e0ea33b                                                                |
| security_groups       | 275802d0-33cb-4796-9e57-03d8ddd29b94                                                                |
| status                | ACTIVE                                                                                              |
| tenant_id             | 5a582af8b44b422fafcd4545bd2b7eb5                                                                    |
+-----------------------+-----------------------------------------------------------------------------------------------------+</pre></div></div><div class="sect3" id="sec-network-rbac-limitation"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.10.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="#sec-network-rbac-limitation">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/network-rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>network-rbac.xml</li><li><span class="ds-label">ID: </span>sec-network-rbac-limitation</li></ul></div></div></div></div><p>
   Note the following limitations of RBAC in neutron.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     neutron network is the only supported RBAC neutron object type.
    </p></li><li class="listitem "><p>
     The "access_as_external" action is not supported – even though it is
     listed as a valid action by python-neutronclient.
    </p></li><li class="listitem "><p>
     The neutron-api server will not accept action value of
     'access_as_external'. The <code class="literal">access_as_external</code> definition
     is not found in the specs.
    </p></li><li class="listitem "><p>
     The target project users cannot create, modify, or delete subnets on
     networks that have RBAC policies.
    </p></li><li class="listitem "><p>
     The subnet of a network that has an RBAC policy cannot be added as an
     interface of a target tenant's router. For example, the command
     <code class="literal">openstack router add subnet tgt-tenant-router &lt;sb-demo-net
     uuid&gt;</code> will error out.
    </p></li><li class="listitem "><p>
     The security group rules on the network owner do not apply to other
     projects that can use the network.
    </p></li><li class="listitem "><p>
     A user in target project can boot up VMs using a VNIC using the shared
     network. The user of the target project can assign a floating IP (FIP) to
     the VM. The target project must have SG rules that allows SSH and/or ICMP
     for VM connectivity.
    </p></li><li class="listitem "><p>
     neutron RBAC creation and management are currently not supported in
     horizon. For now, the neutron CLI has to be used to manage RBAC rules.
    </p></li><li class="listitem "><p>
     A RBAC rule tells neutron whether a tenant can access a network (Allow).
     Currently there is no DENY action.
    </p></li><li class="listitem "><p>
     Port creation on a shared network fails if <code class="literal">--fixed-ip</code>
     is specified in the <code class="literal">openstack port create</code> command.
     
     
    </p></li></ul></div></div></div><div class="sect2" id="configureMTU"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Maximum Transmission Units in neutron</span> <a title="Permalink" class="permalink" href="#configureMTU">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-configure_mtu.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-configure_mtu.xml</li><li><span class="ds-label">ID: </span>configureMTU</li></ul></div></div></div></div><p>
  This topic explains how you can configure MTUs, what to look out for, and
  the results and implications of changing the default MTU settings. It is
  important to note that every network within a network group will have the
  same MTU.
 </p><div id="id-1.5.12.6.15.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
   An MTU change will not affect existing networks that have had VMs created
   on them. It will only take effect on new networks created after the
   reconfiguration process.
  </p></div><div class="sect3" id="id-1.5.12.6.15.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.11.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Overview</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.15.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-configure_mtu.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-configure_mtu.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   A Maximum Transmission Unit, or MTU is the maximum packet size (in bytes)
   that a network device can or is configured to handle. There are a number of
   places in your cloud where MTU configuration is relevant: the physical
   interfaces managed and configured by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the virtual
   interfaces created by neutron and nova for neutron networking, and the
   interfaces inside the VMs.
  </p><p>
   <span class="bold"><strong><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>-managed physical interfaces </strong></span>
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>-managed physical interfaces include the physical interfaces
   and the bonds, bridges, and VLANs created on top of them. The MTU for these
   interfaces is configured via the 'mtu' property of a network group. Because
   multiple network groups can be mapped to one physical interface, there may
   have to be some resolution of differing MTUs between the untagged and tagged
   VLANs on the same physical interface. For instance, if one untagged VLAN,
   vlan101 (with an MTU of 1500) and a tagged VLAN vlan201 (with an MTU of
   9000) are both on one interface (eth0), this means that eth0 can handle
   1500, but the VLAN interface which is created on top of eth0 (that is,
   <code class="literal">vlan201@eth0</code>) wants 9000. However, vlan201 cannot have a
   higher MTU than eth0, so vlan201 will be limited to 1500 when it is brought
   up, and fragmentation will result.
  </p><p>
   In general, a VLAN interface MTU must be lower than or equal to the base
   device MTU. If they are different, as in the case above, the MTU of eth0 can
   be overridden and raised to 9000, but in any case the discrepancy will have
   to be reconciled.
  </p><p>
   <span class="bold"><strong>neutron/nova interfaces </strong></span>
  </p><p>
   neutron/nova interfaces include the virtual devices created by neutron
   and nova during the normal process of realizing a neutron
   network/router and booting a VM on it (qr-*, qg-*, tap-*, qvo-*, qvb-*,
   etc.). There is currently no support in neutron/nova for per-network
   MTUs in which every interface along the path for a particular neutron
   network has the correct MTU for that network. There is, however, support for
   globally changing the MTU of devices created by neutron/nova (see
   network_device_mtu below). This means that if you want to enable jumbo
   frames for any set of VMs, you will have to enable it for all your VMs. You
   cannot just enable them for a particular neutron network.
  </p><p>
   <span class="bold"><strong>VM interfaces</strong></span>
  </p><p>
   VMs typically get their MTU via DHCP advertisement, which means that the
   dnsmasq processes spawned by the neutron-dhcp-agent actually advertise a
   particular MTU to the VMs. In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9, the DHCP server advertises to
   all VMS a 1400 MTU via a forced setting in dnsmasq-neutron.conf. This is
   suboptimal for every network type (vxlan, flat, vlan, etc) but it does
   prevent fragmentation of a VM's packets due to encapsulation.
  </p><p>
   For instance, if you set the new *-mtu configuration options to a default of
   1500 and create a VXLAN network, it will be given an MTU of 1450 (with the
   remaining 50 bytes used by the VXLAN encapsulation header) and will
   advertise a 1450 MTU to any VM booted on that network. If you create a
   provider VLAN network, it will have an MTU of 1500 and will advertise 1500
   to booted VMs on the network. It should be noted that this default starting
   point for MTU calculation and advertisement is also global, meaning you
   cannot have an MTU of 8950 on one VXLAN network and 1450 on
   another. However, you can have provider physical networks with different
   MTUs by using the physical_network_mtus config option, but nova still
   requires a global MTU option for the interfaces it creates, thus you cannot
   really take advantage of that configuration option.
  </p></div><div class="sect3" id="id-1.5.12.6.15.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.11.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network settings in the input model</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.15.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-configure_mtu.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-configure_mtu.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   MTU can be set as an attribute of a network group in network_groups.yml.
   Note that this applies only to KVM. That setting means that every network in
   the network group will be assigned the specified MTU. The MTU value must be
   set individually for each network group. For example:
  </p><div class="verbatim-wrap"><pre class="screen">network-groups:
        - name: GUEST
        mtu: 9000
        ...

        - name: EXTERNAL-API
        mtu: 9000
        ...

        - name: EXTERNAL-VM
        mtu: 9000
        ...</pre></div></div><div class="sect3" id="id-1.5.12.6.15.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.11.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Infrastructure support for jumbo frames</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.15.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-configure_mtu.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-configure_mtu.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If you want to use jumbo frames, or frames with an MTU of 9000 or more, the
   physical switches and routers that make up the infrastructure of the
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installation must be configured to support them. To realize
   the advantages, all devices in the same broadcast domain must have
   the same MTU.
  </p><p>
   If you want to configure jumbo frames on compute and controller nodes, then
   all switches joining the compute and controller nodes must have jumbo frames
   enabled. Similarly, the "infrastructure gateway" through which the external
   VM network flows, commonly known as the default route for the external VM
   VLAN, must also have the same MTU configured.
  </p><p>
   You can also consider anything in the same broadcast domain to be anything
   in the same VLAN or anything in the same IP subnet.
  </p></div><div class="sect3" id="id-1.5.12.6.15.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.11.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling end-to-end jumbo frames for a VM</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.15.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-configure_mtu.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-configure_mtu.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Add an <code class="literal">mtu</code> attribute to all the network groups in your
     model. Note that adding the MTU for the network groups will only affect
     the configuration for physical network interfaces.
    </p><p>
     To add the mtu attribute, find the YAML file that contains your
     network-groups entry. We will assume it is network_groups.yml, unless you
     have changed it. Whatever the file is named, it will be found in
     ~/openstack/my_cloud/definition/data/.
    </p><p>
     To edit these files, begin by checking out the
     <span class="bold"><strong>site</strong></span> branch on the Cloud Lifecycle Manager
     node. You may already be on that branch. If so, you will remain there.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git checkout site</pre></div><p>
     Then begin editing the files. In <code class="filename">network_groups.yml</code>,
     add <code class="literal">mtu: 9000</code>.
    </p><div class="verbatim-wrap"><pre class="screen">network-groups:
            - name: GUEST
            hostname-suffix: guest
            mtu: 9000
            tags:
            - neutron.networks.vxlan</pre></div><p>
     This sets the physical interface managed by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 that has the GUEST
     network group tag assigned to it. This can be found in the
     <code class="filename">interfaces_set.yml</code> file under the
     <code class="literal">interface-models</code> section.
    </p></li><li class="step "><p>
     Edit <code class="filename">neutron.conf.j2</code> found in
     <code class="filename">~/openstack/my_cloud/config/neutron/</code> to set
     <code class="literal">global_physnet_mtu</code> to <code class="literal">9000</code> under
     <code class="literal">[DEFAULT]</code>:
    </p><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
...
global_physnet_mtu = 9000</pre></div><p>
     This allows neutron to advertise the optimal MTU to instances (based on
     <code class="literal">global_physnet_mtu</code> minus the encapsulation size).
    </p></li><li class="step "><p>
     Remove the <code class="literal">dhcp-option-force=26,1400</code> line from
     <code class="filename">~/openstack/my_cloud/config/neutron/dnsmasq-neutron.conf.j2</code>.
    </p></li><li class="step "><p>
     OvS will set <code class="literal">br-int</code> to the value of the lowest physical
     interface. If you are using Jumbo frames on some of your networks,
     <code class="literal">br-int</code> on the controllers may be set to 1500 instead of
     9000. Work around this condition by running:
    </p><div class="verbatim-wrap"><pre class="screen">ovs-vsctl set int br-int mtu_request=9000</pre></div></li><li class="step "><p>
     Commit your changes
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "your commit message goes here in quotes"</pre></div></li><li class="step "><p>
     If <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> has not been deployed yet, do normal deployment and skip to
     <a class="xref" href="#enable-jumbo-normal" title="Step 8">Step 8</a>.
    </p></li><li class="step "><p>
     Assuming it has been deployed already, continue here:
    </p><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
     and ready the deployment:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div><p>
     Then run the network_interface-reconfigure.yml playbook, changing
     directories first:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts network_interface-reconfigure.yml</pre></div><p>
     Then run neutron-reconfigure.yml:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div><p>
     Then nova-reconfigure.yml:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</pre></div><p>
     Note: adding/changing network-group mtu settings will likely require a
     network restart when
     <code class="filename">network_interface-reconfigure.yml</code> is run.
    </p></li><li class="step " id="enable-jumbo-normal"><p>
     Follow the normal process for creating a neutron network and booting a VM
     or two. In this example, if a VXLAN network is created and a VM is booted
     on it, the VM will have an MTU of 8950, with the remaining 50 bytes used
     by the VXLAN encapsulation header.
    </p></li><li class="step "><p>
     Test and verify that the VM can send and receive jumbo frames without
     fragmentation. You can use <code class="command">ping</code>. For example, to test
     an MTU of 9000 using VXLAN:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ping –M do –s 8950 <em class="replaceable ">YOUR_VM_FLOATING_IP</em></pre></div><p>
     Substitute your actual floating IP address for the
     <em class="replaceable ">YOUR_VM_FLOATING_IP</em>.
    </p></li></ol></div></div></div><div class="sect3" id="optimal-mtu"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.11.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling Optimal MTU Advertisement Feature</span> <a title="Permalink" class="permalink" href="#optimal-mtu">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-configure_mtu.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-configure_mtu.xml</li><li><span class="ds-label">ID: </span>optimal-mtu</li></ul></div></div></div></div><p>
   To enable the optimal MTU feature, follow these steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Edit
     <code class="literal">~/openstack/my_cloud/config/neutron/neutron.conf.j2</code> to
     <span class="bold"><strong>remove</strong></span> <code class="literal">advertise_mtu</code>
     variable under [DEFAULT]
    </p><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
...
advertise_mtu = False #remove this</pre></div></li><li class="step "><p>
     Remove the <code class="literal">dhcp-option-force=26,1400</code> line from
     <code class="filename">~/openstack/my_cloud/config/neutron/dnsmasq-neutron.conf.j2</code>.
    </p></li><li class="step "><p>
     If <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> has already been deployed, follow the remaining steps,
     otherwise follow the normal deployment procedures.
    </p></li><li class="step "><p>
     Commit your changes
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "your commit message goes here in quotes"</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Run ready deployment:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the <code class="literal">network_interface-reconfigure.yml</code> playbook,
     changing directories first:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts network_interface-reconfigure.yml</pre></div></li><li class="step "><p>
     Run the <code class="literal">neutron-reconfigure.yml</code> playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></li></ol></div></div><div id="id-1.5.12.6.15.8.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    If you are upgrading an existing deployment, avoid creating MTU mismatch
    between network interfaces in preexisting VMs and that of VMs created after
    upgrade. If you do have an MTU mismatch, then the new VMs (having interface
    with 1500 minus the underlay protocol overhead) will not be able to have L2
    connectivity with preexisting VMs (with 1400 MTU due to
    <code class="literal">dhcp-option-force</code>).
   </p></div></div></div><div class="sect2" id="topic-shy-ksv-jw"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Improve Network Peformance with Isolated Metadata Settings</span> <a title="Permalink" class="permalink" href="#topic-shy-ksv-jw">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-isolated_metadata.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-isolated_metadata.xml</li><li><span class="ds-label">ID: </span>topic-shy-ksv-jw</li></ul></div></div></div></div><p>
  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, neutron currently sets <code class="literal">enable_isolated_metadata =
  True</code> by default in <code class="literal">dhcp_agent.ini</code> because
  several services require isolated networks (neutron networks without a
  router). It also sets <code class="literal">force_metadata = True</code> if DVR is 
  enabled to improve the scalability on large environments with a high churn 
  rate. However, this has the effect of spawning a <code class="literal">neutron-ns-metadata-proxy</code>
  process on one of the controller nodes for every active neutron network.
 </p><p>
  In environments that create many neutron networks, these extra
  <code class="literal">neutron-ns-metadata-proxy</code> processes can quickly eat up a
  lot of memory on the controllers, which does not scale up well.
 </p><p>
  For deployments that do not require isolated metadata (that is, they do not
  require the Platform Services and will always create networks with an
  attached router) and do not have a high churn rate, you can set 
  <code class="literal">enable_isolated_metadata = False</code> and <code class="literal">force_metadata = False</code>
  in <code class="literal">dhcp_agent.ini</code> to reduce neutron memory usage on controllers,
  allowing a greater number of active neutron networks.
 </p><p>
  Note that the <code class="literal">dhcp_agent.ini.j2</code> template is found in
  <code class="literal">~/openstack/my_cloud/config/neutron</code> on the Cloud Lifecycle Manager
  node. The edit can be made there and the standard deployment can be run if
  this is install time. In a deployed cloud, run the neutron reconfiguration
  procedure outlined here:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    First check out the site branch:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/config/neutron
<code class="prompt user">ardana &gt; </code>git checkout site</pre></div></li><li class="step "><p>
    Edit the <code class="literal">dhcp_agent.ini.j2</code> file to change the
    <code class="literal">enable_isolated_metadata = {{ neutron_enable_isolated_metadata }}</code>
    <code class="literal">force_metadata = {{ router_distributed }}</code>
    line in the <code class="literal">[DEFAULT]</code> section to read:
   </p><div class="verbatim-wrap"><pre class="screen">enable_isolated_metadata = False</pre></div><div class="verbatim-wrap"><pre class="screen">force_metadata = False</pre></div></li><li class="step "><p>
    Commit the file:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "your commit message goes here in quotes"</pre></div></li><li class="step "><p>
    Run the <code class="literal">ready-deployment.yml</code> playbook from
    <code class="literal">~/openstack/ardana/ansible</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
    Then run the <code class="literal">neutron-reconfigure.yml</code> playbook, changing
    directories first:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect2" id="moving-from-dvr-deployments"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Moving from DVR deployments to non_DVR</span> <a title="Permalink" class="permalink" href="#moving-from-dvr-deployments">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-moving_from_dvr_deployments.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-moving_from_dvr_deployments.xml</li><li><span class="ds-label">ID: </span>moving-from-dvr-deployments</li></ul></div></div></div></div><p>
  If you have an older deployment of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> which is using DVR as a default
  and you are attempting to move to non_DVR, follow these steps:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Remove all your existing DVR routers and their workloads. Make sure to
    remove interfaces, floating ips and gateways, if applicable.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack router remove subnet <em class="replaceable ">ROUTER-NAME</em> <em class="replaceable ">SUBNET-NAME</em>/<em class="replaceable ">SUBNET-ID</em>
<code class="prompt user">ardana &gt; </code>openstack floating ip unset –port <em class="replaceable ">FLOATINGIP-ID</em> <em class="replaceable ">PRIVATE-PORT-ID</em>
<code class="prompt user">ardana &gt; </code>openstack router unset <em class="replaceable ">ROUTER-NAME</em> <em class="replaceable ">-NET-NAME</em>/<em class="replaceable ">EXT-NET-ID</em></pre></div></li><li class="step "><p>
    Then delete the router.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack router delete <em class="replaceable ">ROUTER-NAME</em></pre></div></li><li class="step "><p>
    Before you create any non_DVR router make sure that l3-agents and
    metadata-agents are not running in any compute host. You can run the
    command <code class="literal">openstack network agent list</code> to see if there are
    any neutron-l3-agent running in any compute-host in your deployment.
   </p><p>
    You must disable <code class="literal">neutron-l3-agent</code> and
    <code class="literal">neutron-metadata-agent</code> on every compute host by running
    the following commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network agent list
+--------------------------------------+----------------------+--------------------------+-------------------+-------+----------------+---------------------------+
| id                                   | agent_type           | host                     | availability_zone | alive | admin_state_up | binary                    |
+--------------------------------------+----------------------+--------------------------+-------------------+-------+----------------+---------------------------+
| 810f0ae7-63aa-4ee3-952d-69837b4b2fe4 | L3 agent             | ardana-cp1-comp0001-mgmt | nova              | :-)   | True           | neutron-l3-agent          |
| 89ac17ba-2f43-428a-98fa-b3698646543d | Metadata agent       | ardana-cp1-comp0001-mgmt |                   | :-)   | True           | neutron-metadata-agent    |
| f602edce-1d2a-4c8a-ba56-fa41103d4e17 | Open vSwitch agent   | ardana-cp1-comp0001-mgmt |                   | :-)   | True           | neutron-openvswitch-agent |
...
+--------------------------------------+----------------------+--------------------------+-------------------+-------+----------------+---------------------------+

$ openstack network agent set --disable 810f0ae7-63aa-4ee3-952d-69837b4b2fe4
Updated agent: 810f0ae7-63aa-4ee3-952d-69837b4b2fe4

$ openstack network agent set --disable 89ac17ba-2f43-428a-98fa-b3698646543d
Updated agent: 89ac17ba-2f43-428a-98fa-b3698646543d</pre></div><div id="id-1.5.12.6.17.3.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      Only L3 and Metadata agents were disabled.
     </p></div></li><li class="step "><p>
    Once L3 and metadata neutron agents are stopped, follow steps 1 through 7
    in the document <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 12 “Alternative Configurations”, Section 12.2 “Configuring <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> without DVR”</span> and then run the
    <code class="literal">neutron-reconfigure.yml</code> playbook:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect2" id="dpdk"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">OVS-DPDK Support</span> <a title="Permalink" class="permalink" href="#dpdk">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-dpdk_ovs.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_ovs.xml</li><li><span class="ds-label">ID: </span>dpdk</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses a version of Open vSwitch (OVS) that is built with the
  Data Plane Development Kit (DPDK) and includes a QEMU hypervisor which
  supports vhost-user.
 </p><p>
  The OVS-DPDK package modifes the OVS fast path, which is normally performed
  in kernel space, and allows it to run in userspace so there is no context
  switch to the kernel for processing network packets.
 </p><p>
  The EAL component of DPDK supports mapping the Network Interface Card (NIC)
  registers directly into userspace. The DPDK provides a Poll Mode Driver
  (PMD) that can access the NIC hardware from userspace and uses polling
  instead of interrupts to avoid the user to kernel transition.
 </p><p>
  The PMD maps the shared address space of the VM that is provided by the
  vhost-user capability of QEMU. The vhost-user mode causes neutron to create
  a Unix domain socket that allows communication between the PMD and QEMU. The
  PMD uses this in order to acquire the file descriptors to the pre-allocated
  VM memory. This allows the PMD to directly access the VM memory space and
  perform a fast zero-copy of network packets directly into and out of the VMs
  virtio_net vring.
 </p><p>
  This yields performance improvements in the time it takes to process network
  packets.
 </p><div class="sect3" id="id-1.5.12.6.18.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.14.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Usage considerations</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.18.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-dpdk_ovs.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_ovs.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The target for a DPDK Open vSwitch is VM performance and VMs only run on
   compute nodes so the following considerations are compute node specific.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     In order to use DPDK with VMs, <code class="literal">hugepages</code> must be
     enabled; please see <a class="xref" href="#hugepages" title="10.4.14.3. Configuring Hugepages for DPDK in Networks">Section 10.4.14.3, “Configuring Hugepages for DPDK in Networks”</a>. The memory to be used
     must be allocated at boot time so you must know beforehand how many VMs
     will be scheduled on a node. Also, for NUMA considerations, you want those
     hugepages on the same NUMA node as the NIC.  A VM maps its entire address
     space into a hugepage.
    </p></li><li class="listitem "><p>
     For maximum performance you must reserve logical cores for DPDK poll mode
     driver (PMD) usage and for hypervisor (QEMU) usage. This keeps the Linux
     kernel from scheduling processes on those cores. The PMD threads will go
     to 100% cpu utilization since it uses polling of the hardware instead of
     interrupts. There will be at least 2 cores dedicated to PMD threads. Each
     VM will have a core dedicated to it although for less performance VMs can
     share cores.
    </p></li><li class="listitem "><p>
     VMs can use the virtio_net or the virtio_pmd drivers. There is also a PMD
     for an emulated e1000.
    </p></li><li class="listitem "><p>
     Only VMs that use hugepages can be sucessfully launched on a DPDK-enabled
     NIC. If there is a need to support both DPDK and non-DPDK-based VMs, an
     additional port managed by the Linux kernel must exist.
    </p></li></ol></div></div><div class="sect3" id="id-1.5.12.6.18.8"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.14.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For more information</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.18.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-dpdk_ovs.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_ovs.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   See the following topics for more information:
  </p></div><div class="sect3" id="hugepages"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.14.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Hugepages for DPDK in Networks</span> <a title="Permalink" class="permalink" href="#hugepages">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-hugepages.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-hugepages.xml</li><li><span class="ds-label">ID: </span>hugepages</li></ul></div></div></div></div><p>
  To take advantage of DPDK and its network
  performance enhancements, enable hugepages first.
 </p><p>
  With hugepages, physical RAM is reserved at boot time and dedicated to a
  virtual machine. Only that virtual machine and Open vSwitch can use this
  specifically allocated RAM. The host OS cannot access it. This memory is
  contiguous, and because of its larger size, reduces the number of entries in
  the memory map and number of times it must be read.
 </p><p>
  The hugepage reservation is made in <code class="literal">/etc/default/grub</code>,
  but this is handled by the Cloud Lifecycle Manager.
 </p><p>
  In addition to hugepages, to use DPDK, CPU isolation is required. This is
  achieved with the 'isolcups' command in
  <code class="literal">/etc/default/grub</code>, but this is also managed by the
  Cloud Lifecycle Manager using a new input model file.
 </p><p>
  The two new input model files introduced with this release to help you
  configure the necessary settings and persist them are:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    memory_models.yml (for hugepages)
   </p></li><li class="listitem "><p>
    cpu_models.yml (for CPU isolation)
   </p></li></ul></div><div class="sect4" id="id-1.5.12.6.18.9.8"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.14.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">memory_models.yml</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.18.9.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-hugepages.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-hugepages.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   In this file you set your huge page size along with the number of such
   huge-page allocations.
  </p><div class="verbatim-wrap"><pre class="screen"> ---
  product:
    version: 2

  memory-models:
    - name: COMPUTE-MEMORY-NUMA
      default-huge-page-size: 1G
      huge-pages:
        - size: 1G
          count: 24
          numa-node: 0
        - size: 1G
          count: 24
          numa-node: 1
        - size: 1G
          count: 48</pre></div></div><div class="sect4" id="id-1.5.12.6.18.9.9"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.14.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">cpu_models.yml</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.18.9.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-hugepages.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-hugepages.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  cpu-models:

    - name: COMPUTE-CPU
      assignments:
       - components:
           - nova-compute-kvm
         cpu:
           - processor-ids: 3-5,12-17
             role: vm

       - components:
           - openvswitch
         cpu:
           - processor-ids: 0
             role: eal
           - processor-ids: 1-2
             role: pmd</pre></div></div><div class="sect4" id="id-1.5.12.6.18.9.10"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.14.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">NUMA memory allocation</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.18.9.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-hugepages.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-hugepages.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   As mentioned above, the memory used for hugepages is locked down at boot
   time by an entry in <code class="literal">/etc/default/grub</code>. As an admin, you
   can specify in the input model how to arrange this memory on NUMA nodes. It
   can be spread across NUMA nodes or you can specify where you want it. For
   example, if you have only one NIC, you would probably want all the hugepages
   memory to be on the NUMA node closest to that NIC.


  </p><p>
   If you do not specify the <code class="literal">numa-node</code> settings in the
   <code class="literal">memory_models.yml</code> input model file and use only the last
   entry indicating "size: 1G" and "count: 48" then this memory is spread
   evenly across all NUMA nodes.
  </p><p>
   Also note that the hugepage service runs once at boot time and then goes to
   an inactive state so you should not expect to see it running. If you decide
   to make changes to the NUMA memory allocation, you will need to reboot the
   compute node for the changes to take effect.
  </p></div></div><div class="sect3" id="dpdk-setup"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.14.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DPDK Setup for Networking</span> <a title="Permalink" class="permalink" href="#dpdk-setup">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-dpdk_setup.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_setup.xml</li><li><span class="ds-label">ID: </span>dpdk-setup</li></ul></div></div></div></div><div class="sect4" id="id-1.5.12.6.18.10.2"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.14.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Hardware requirements</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.18.10.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-dpdk_setup.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_setup.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Intel-based compute node. DPDK is not available on AMD-based systems.
    </p></li><li class="listitem "><p>
     The following BIOS settings must be enabled for DL360 Gen9:
    </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
       Virtualization Technology
      </p></li><li class="listitem "><p>
       Intel(R) VT-d
      </p></li><li class="listitem "><p>
       PCI-PT (Also see <a class="xref" href="#pcipt-gen9" title="10.4.15.14. Enabling PCI-PT on HPE DL360 Gen 9 Servers">Section 10.4.15.14, “Enabling PCI-PT on HPE DL360 Gen 9 Servers”</a>)
      </p></li></ol></div></li><li class="listitem "><p>
     Need adequate host memory to allow for hugepages. The examples below use
     1G hugepages for the VMs
    </p></li></ul></div></div><div class="sect4" id="id-1.5.12.6.18.10.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.14.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.18.10.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-dpdk_setup.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_setup.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     DPDK is supported on SLES only.
    </p></li><li class="listitem "><p>
     Applies to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 only.
    </p></li><li class="listitem "><p>
     Tenant network can be untagged vlan or untagged vxlan
    </p></li><li class="listitem "><p>
     DPDK port names must be of the form 'dpdk&lt;portid&gt;' where port id is
     sequential and starts at 0
    </p></li><li class="listitem "><p>
     No support for converting DPDK ports to non DPDK ports without rebooting
     compute node.
    </p></li><li class="listitem "><p>
     No security group support, need userspace conntrack.
    </p></li><li class="listitem "><p>
     No jumbo frame support.
    </p></li></ul></div></div><div class="sect4" id="id-1.5.12.6.18.10.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.14.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setup instructions</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.18.10.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-dpdk_setup.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_setup.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   These setup instructions and example model are for a three-host system.
   There is one controller with Cloud Lifecycle Manager in cloud control plane and
   two compute hosts.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     After initial run of site.yml all compute nodes must be rebooted to pick
     up changes in grub for hugepages and isolcpus
    </p></li><li class="listitem "><p>
     Changes to non-uniform memory access (NUMA) memory, isolcpu, or network
     devices must be followed by a reboot of compute nodes
    </p></li><li class="listitem "><p>
     Run sudo reboot to pick up libvirt change and hugepage/isocpus grub
     changes
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo reboot</pre></div></li><li class="listitem "><p>
     Use the bash script below to configure nova aggregates, neutron networks,
     a new flavor, etc. And then it will spin up two VMs.
    </p></li></ol></div><p>
   <span class="bold"><strong>VM spin-up instructions</strong></span>
  </p><p>
   Before running the spin up script you need to get a copy of the cirros image
   to your Cloud Lifecycle Manager node. You can manually scp a copy of the cirros
   image to the system. You can copy it locallly with wget like so
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img</pre></div><p>
   Save the following shell script in the home directory and run it. This
   should spin up two VMs, one on each compute node.
  </p><div id="id-1.5.12.6.18.10.4.8" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
    Make sure to change all network-specific information in the script to match
    your environment.
   </p></div><div class="verbatim-wrap"><pre class="screen">#!/usr/bin/env bash

source service.osrc

######## register glance image
openstack image create --name='cirros' --container-format=bare --disk-format=qcow2 &lt; ~/cirros-0.3.4-x86_64-disk.img

####### create nova aggregate and flavor for dpdk

MI_NAME=dpdk

openstack aggregate create $MI_NAME nova
openstack aggregate add host $MI_NAME openstack-cp-comp0001-mgmt
openstack aggregate add host $MI_NAME openstack-cp-comp0002-mgmt
openstack aggregate set $MI_NAME pinned=true

openstack flavor create $MI_NAME 6 1024 20 1
openstack flavor set $MI_NAME set hw:cpu_policy=dedicated
openstack flavor set $MI_NAME set aggregate_instance_extra_specs:pinned=true
openstack flavor set $MI_NAME set hw:mem_page_size=1048576

######## sec groups NOTE: no sec groups supported on DPDK.  This is in case we do non-DPDK compute hosts.
nova secgroup-add-rule default tcp 22 22 0.0.0.0/0
nova secgroup-add-rule default icmp -1 -1 0.0.0.0/0

########  nova keys
openstack keypair create mykey &gt;mykey.pem
chmod 400 mykey.pem

######## create neutron external network
openstack network create ext-net --router:external --os-endpoint-type internalURL
openstack subnet create ext-net 10.231.0.0/19 --gateway_ip=10.231.0.1  --ip-version=4 --disable-dhcp  --allocation-pool start=10.231.17.0,end=10.231.17.255

########  neutron network
openstack network create mynet1
openstack subnet create mynet1 10.1.1.0/24 --name mysubnet1
openstack router create myrouter1
openstack router add subnet myrouter1 mysubnet1
openstack router set myrouter1 ext-net
export MYNET=$(openstack network list | grep mynet | awk '{print $2}')

######## spin up 2 VMs, 1 on each compute
openstack server create --image cirros --nic net-id=${MYNET} --key-name mykey --flavor dpdk --availability-zone nova:openstack-cp-comp0001-mgmt vm1
openstack server create --image cirros --nic net-id=${MYNET} --key-name mykey --flavor dpdk --availability-zone nova:openstack-cp-comp0002-mgmt vm2

######## create floating ip and attach to instance
export MYFIP1=$(nova floating-ip-create|grep ext-net|awk '{print $4}')
nova add-floating-ip vm1 ${MYFIP1}

export MYFIP2=$(nova floating-ip-create|grep ext-net|awk '{print $4}')
nova add-floating-ip vm2 ${MYFIP2}

openstack server list</pre></div></div></div><div class="sect3" id="dpdk-config"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.14.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DPDK Configurations</span> <a title="Permalink" class="permalink" href="#dpdk-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-dpdk_configurations.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_configurations.xml</li><li><span class="ds-label">ID: </span>dpdk-config</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="#base-config" title="10.4.14.5.1. Base configuration">Section 10.4.14.5.1, “Base configuration”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#common-perf" title="10.4.14.5.2. Performance considerations common to all NIC types">Section 10.4.14.5.2, “Performance considerations common to all NIC types”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#multiqueue-config" title="10.4.14.5.3. Multiqueue configuration">Section 10.4.14.5.3, “Multiqueue configuration”</a>
   </p></li></ul></div><div class="sect4" id="base-config"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.14.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Base configuration</span> <a title="Permalink" class="permalink" href="#base-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-dpdk_configurations.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_configurations.xml</li><li><span class="ds-label">ID: </span>base-config</li></ul></div></div></div></div><p>
   The following is specific to DL360 Gen9 and BIOS configuration as detailed
   in <a class="xref" href="#dpdk-setup" title="10.4.14.4. DPDK Setup for Networking">Section 10.4.14.4, “DPDK Setup for Networking”</a>.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     EAL cores - 1, isolate: False in cpu-models
    </p></li><li class="listitem "><p>
     PMD cores - 1 per NIC port
    </p></li><li class="listitem "><p>
     Hugepages - 1G per PMD thread
    </p></li><li class="listitem "><p>
     Memory channels - 4
    </p></li><li class="listitem "><p>
     Global rx queues - based on needs
    </p></li></ul></div></div><div class="sect4" id="common-perf"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.14.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Performance considerations common to all NIC types</span> <a title="Permalink" class="permalink" href="#common-perf">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-dpdk_configurations.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_configurations.xml</li><li><span class="ds-label">ID: </span>common-perf</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Compute host core frequency</strong></span>
  </p><p>
   Host CPUs should be running at maximum performance. The following is a
   script to set that. Note that in this case there are 24 cores. This needs to
   be modified to fit your environment. For a HP DL360 Gen9, the BIOS should be
   configured to use "OS Control Mode" which can be found on the iLO Power
   Settings page.
  </p><div class="verbatim-wrap"><pre class="screen">for i in `seq 0 23`; do echo "performance" &gt; /sys/devices/system/cpu/cpu$i/cpufreq/scaling_governor; done</pre></div><p>
   <span class="bold"><strong>IO non-posted prefetch</strong></span>
  </p><p>
   The DL360 Gen9 should have the IO non-posted prefetch disabled. Experimental
   evidence shows this yields an additional 6-8% performance boost.
  </p></div><div class="sect4" id="multiqueue-config"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.14.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Multiqueue configuration</span> <a title="Permalink" class="permalink" href="#multiqueue-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-dpdk_configurations.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-dpdk_configurations.xml</li><li><span class="ds-label">ID: </span>multiqueue-config</li></ul></div></div></div></div><p>
   In order to use multiqueue, a property must be applied to the glance image
   and a setting inside the resulting VM must be applied. In this example we
   create a 4 vCPU flavor for DPDK using 1G hugepages.
  </p><div class="verbatim-wrap"><pre class="screen">MI_NAME=dpdk

openstack aggregate create $MI_NAME nova
openstack aggregate add host $MI_NAME openstack-cp-comp0001-mgmt
openstack aggregate add host $MI_NAME openstack-cp-comp0002-mgmt
openstack aggregate set $MI_NAME pinned=true

openstack flavor create $MI_NAME 6 1024 20 4
openstack flavor set $MI_NAME set hw:cpu_policy=dedicated
openstack flavor set $MI_NAME set aggregate_instance_extra_specs:pinned=true
openstack flavor set $MI_NAME set hw:mem_page_size=1048576</pre></div><p>
   And set the hw_vif_multiqueue_enabled property on the glance image
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack image set --property hw_vif_multiqueue_enabled=true <em class="replaceable ">IMAGE UUID</em></pre></div><p>
   Once the VM is booted using the flavor above, inside the VM, choose the
   number of combined rx and tx queues to be equal to the number of vCPUs
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo ethtool -L eth0 combined 4</pre></div><p>
   On the hypervisor you can verify that multiqueue has been properly set by
   looking at the qemu process
  </p><div class="verbatim-wrap"><pre class="screen">-netdev type=vhost-user,id=hostnet0,chardev=charnet0,queues=4 -device virtio-net-pci,mq=on,vectors=10,</pre></div><p>
   Here you can see that 'mq=on' and vectors=10. The formula for vectors is
   2*num_queues+2
  </p></div></div><div class="sect3" id="dpdk-troubleshooting"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.14.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting DPDK</span> <a title="Permalink" class="permalink" href="#dpdk-troubleshooting">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-ts_dpdk.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-ts_dpdk.xml</li><li><span class="ds-label">ID: </span>dpdk-troubleshooting</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="#hardware" title="10.4.14.6.1. Hardware configuration">Section 10.4.14.6.1, “Hardware configuration”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#system" title="10.4.14.6.2. System configuration">Section 10.4.14.6.2, “System configuration”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#inputModel" title="10.4.14.6.3. Input model configuration">Section 10.4.14.6.3, “Input model configuration”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#reboot" title="10.4.14.6.4. Reboot requirements">Section 10.4.14.6.4, “Reboot requirements”</a>
   </p></li></ul></div><div class="sect4" id="hardware"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.14.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Hardware configuration</span> <a title="Permalink" class="permalink" href="#hardware">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-ts_dpdk.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-ts_dpdk.xml</li><li><span class="ds-label">ID: </span>hardware</li></ul></div></div></div></div><p>
   Because there are several variations of hardware, it is up to you to verify
   that the hardware is configured properly.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Only Intel based compute nodes are supported. There is no DPDK available
     for AMD-based CPUs.
    </p></li><li class="listitem "><p>
     PCI-PT must be enabled for the NIC that will be used with DPDK.
    </p></li><li class="listitem "><p>
     When using Intel Niantic and the igb_uio driver, the VT-d must be enabled
     in the BIOS.
    </p></li><li class="listitem "><p>
     For DL360 Gen9 systems, the BIOS shared-memory
     <a class="xref" href="#pcipt-gen9" title="10.4.15.14. Enabling PCI-PT on HPE DL360 Gen 9 Servers">Section 10.4.15.14, “Enabling PCI-PT on HPE DL360 Gen 9 Servers”</a>.
    </p></li><li class="listitem "><p>
     Adequate memory must be available for <a class="xref" href="#hugepages" title="10.4.14.3. Configuring Hugepages for DPDK in Networks">Section 10.4.14.3, “Configuring Hugepages for DPDK in Networks”</a> usage.
    </p></li><li class="listitem "><p>
     Hyper-threading can be enabled but is not required for base functionality.
    </p></li><li class="listitem "><p>
     Determine the PCI slot that the DPDK NIC(s) are installed in to
     determine the associated NUMA node.
    </p></li><li class="listitem "><p>
     Only the Intel Haswell, Broadwell, and Skylake microarchitectures are
     supported.
     Intel Sandy Bridge is not supported.
    </p></li></ul></div></div><div class="sect4" id="system"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.14.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">System configuration</span> <a title="Permalink" class="permalink" href="#system">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-ts_dpdk.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-ts_dpdk.xml</li><li><span class="ds-label">ID: </span>system</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Only SLES12-SP4 compute nodes are supported.
    </p></li><li class="listitem "><p>
     If a NIC port is used with PCI-PT, SRIOV-only, or PCI-PT+SRIOV, then it
     cannot be used with DPDK. They are mutually exclusive. This is because DPDK
     depends on an OvS bridge which does not exist if you use any combination of
     PCI-PT and SRIOV. You can use DPDK, SRIOV-only, and PCI-PT on difference
     interfaces of the same server.
    </p></li><li class="listitem "><p>
     There is an association between the PCI slot for the NIC and a NUMA node.
     Make sure to use logical CPU cores that are on the NUMA node associated to
     the NIC. Use the following to determine which CPUs are on which NUMA node.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>lscpu

Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                48
On-line CPU(s) list:   0-47
Thread(s) per core:    2
Core(s) per socket:    12
Socket(s):             2
NUMA node(s):          2
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 63
Model name:            Intel(R) Xeon(R) CPU E5-2650L v3 @ 1.80GHz
Stepping:              2
CPU MHz:               1200.000
CPU max MHz:           1800.0000
CPU min MHz:           1200.0000
BogoMIPS:              3597.06
Virtualization:        VT-x
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              30720K
NUMA node0 CPU(s):     0-11,24-35
NUMA node1 CPU(s):     12-23,36-47</pre></div></li></ul></div></div><div class="sect4" id="inputModel"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.14.6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Input model configuration</span> <a title="Permalink" class="permalink" href="#inputModel">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-ts_dpdk.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-ts_dpdk.xml</li><li><span class="ds-label">ID: </span>inputModel</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     If you do not specify a driver for a DPDK device, the igb_uio will be
     selected as default.
    </p></li><li class="listitem "><p>
     DPDK devices must be named <code class="literal">dpdk&lt;port-id&gt;</code> where
     the port-id starts at 0 and increments sequentially.
    </p></li><li class="listitem "><p>
     Tenant networks supported are untagged VXLAN and VLAN.
    </p></li><li class="listitem "><p>
     Jumbo Frames MTU is not supported with DPDK.
    </p></li><li class="listitem "><p>
     Sample VXLAN model
    </p></li><li class="listitem "><p>
     Sample VLAN model
    </p></li></ul></div></div><div class="sect4" id="reboot"><div class="titlepage"><div><div><h5 class="title"><span class="number">10.4.14.6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reboot requirements</span> <a title="Permalink" class="permalink" href="#reboot">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-ts_dpdk.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-ts_dpdk.xml</li><li><span class="ds-label">ID: </span>reboot</li></ul></div></div></div></div><p>
   A reboot of a compute node must be performed when an input model change
   causes the following:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     After the initial <code class="filename">site.yml</code> play on a new <span class="productname">OpenStack</span>
     environment
    </p></li><li class="listitem "><p>
     Changes to an existing <span class="productname">OpenStack</span> environment that modify the
     <code class="literal">/etc/default/grub</code> file, such as
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       hugepage allocations
      </p></li><li class="listitem "><p>
       CPU isolation
      </p></li><li class="listitem "><p>
       iommu changes
      </p></li></ul></div></li><li class="listitem "><p>
     Changes to a NIC port usage type, such as
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       moving from DPDK to any combination of PCI-PT and SRIOV
      </p></li><li class="listitem "><p>
       moving from DPDK to kernel based eth driver
      </p></li></ul></div></li></ol></div></div></div></div><div class="sect2" id="sr-iov"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.15 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SR-IOV and PCI Passthrough Support</span> <a title="Permalink" class="permalink" href="#sr-iov">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span>sr-iov</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> supports both single-root I/O virtualization (SR-IOV) and PCI
  passthrough (PCIPT). Both technologies provide for better network
  performance.
 </p><p>
  This improves network I/O, decreases latency, and reduces processor overhead.
 </p><div class="sect3" id="id-1.5.12.6.19.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SR-IOV</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.19.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   A PCI-SIG Single Root I/O Virtualization and Sharing (SR-IOV) Ethernet
   interface is a physical PCI Ethernet NIC that implements hardware-based
   virtualization mechanisms to expose multiple virtual network interfaces that
   can be used by one or more virtual machines simultaneously. With SR-IOV
   based NICs, the traditional virtual bridge is no longer required. Each
   SR-IOV port is associated with a virtual function (VF).
  </p><p>
   When compared with a PCI Passthtrough Ethernet interface, an SR-IOV Ethernet
   interface:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Provides benefits similar to those of a PCI Passthtrough Ethernet
     interface, including lower latency packet processing.
    </p></li><li class="listitem "><p>
     Scales up more easily in a virtualized environment by providing multiple
     VFs that can be attached to multiple virtual machine interfaces.
    </p></li><li class="listitem "><p>
     Shares the same limitations, including the lack of support for LAG, QoS,
     ACL, and live migration.
    </p></li><li class="listitem "><p>
     Has the same requirements regarding the VLAN configuration of the access
     switches.
    </p></li></ul></div><p>
   The process for configuring SR-IOV includes creating a VLAN provider network
   and subnet, then attaching VMs to that network.
  </p><p>
   With SR-IOV based NICs, the traditional virtual bridge is no longer
   required. Each SR-IOV port is associated with a virtual function (VF)
  </p></div><div class="sect3" id="id-1.5.12.6.19.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">PCI passthrough Ethernet interfaces</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.19.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   A passthrough Ethernet interface is a physical PCI Ethernet NIC on a compute
   node to which a virtual machine is granted direct access. PCI passthrough
   allows a VM to have direct access to the hardware without being brokered by
   the hypervisor. This minimizes packet processing delays but at the same time
   demands special operational considerations. For all purposes, a PCI
   passthrough interface behaves as if it were physically attached to the
   virtual machine. Therefore any potential throughput limitations coming from
   the virtualized environment, such as the ones introduced by internal copying
   of data buffers, are eliminated. However, by bypassing the virtualized
   environment, the use of PCI passthrough Ethernet devices introduces several
   restrictions that must be taken into consideration. They include:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     no support for LAG, QoS, ACL, or host interface monitoring
    </p></li><li class="listitem "><p>
     no support for live migration
    </p></li><li class="listitem "><p>
     no access to the compute node's OVS switch
    </p></li></ul></div><p>
   A passthrough interface bypasses the compute node's OVS switch completely,
   and is attached instead directly to the provider network's access switch.
   Therefore, proper routing of traffic to connect the passthrough interface to
   a particular tenant network depends entirely on the VLAN tagging options
   configured on both the passthrough interface and the access port on the
   switch (TOR).
  </p><p>
   The access switch routes incoming traffic based on a VLAN ID, which
   ultimately determines the tenant network to which the traffic belongs. The
   VLAN ID is either explicit, as found in incoming tagged packets, or
   implicit, as defined by the access port's default VLAN ID when the incoming
   packets are untagged. In both cases the access switch must be configured to
   process the proper VLAN ID, which therefore has to be known in advance
  </p></div><div class="sect3" id="pci-passthrough"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Leveraging PCI Passthrough</span> <a title="Permalink" class="permalink" href="#pci-passthrough">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span>pci-passthrough</li></ul></div></div></div></div><p>
   Two parts are necessary to leverage PCI passthrough on a SUSE <span class="productname">OpenStack</span> Cloud
   9 Compute Node: preparing the Compute Node, preparing nova and
   glance.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     <span class="bold"><strong>Preparing the Compute Node</strong></span>
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       There should be no kernel drivers or binaries with direct access to the
       PCI device. If there are kernel modules, they should be blacklisted.
      </p><p>
       For example, it is common to have a <code class="literal">nouveau</code> driver
       from when the node was installed. This driver is a graphics driver for
       Nvidia-based GPUs. It must be blacklisted as shown in this example.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>echo 'blacklist nouveau' &gt;&gt; /etc/modprobe.d/nouveau-default.conf</pre></div><p>
       The file location and its contents are important; the name of the file
       is your choice. Other drivers can be blacklisted in the same manner,
       possibly including Nvidia drivers.
      </p></li><li class="step "><p>
       On the host, <code class="literal">iommu_groups</code> is necessary and may
       already be enabled. To check if IOMMU is enabled:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>virt-host-validate
.....
QEMU: Checking if IOMMU is enabled by kernel
: WARN (IOMMU appears to be disabled in kernel. Add intel_iommu=on to kernel cmdline arguments)
.....</pre></div><p>
       To modify the kernel cmdline as suggested in the warning, edit the file
       <code class="filename">/etc/default/grub</code> and append
       <code class="literal">intel_iommu=on</code> to the
       <code class="literal">GRUB_CMDLINE_LINUX_DEFAULT</code> variable. Then run
       <code class="literal">update-bootloader</code>.
      </p><p>
       A reboot will be required for <code class="literal">iommu_groups</code> to be
       enabled.
      </p></li><li class="step "><p>
       After the reboot, check that IOMMU is enabled:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>virt-host-validate
.....
QEMU: Checking if IOMMU is enabled by kernel
: PASS
.....</pre></div></li><li class="step "><p>
       Confirm IOMMU groups are available by finding the group associated with
       your PCI device (for example Nvidia GPU):
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>lspci -nn | grep -i nvidia
08:00.0 VGA compatible controller [0300]: NVIDIA Corporation GT218 [NVS 300] [10de:10d8] (rev a2)
08:00.1 Audio device [0403]: NVIDIA Corporation High Definition Audio Controller [10de:0be3] (rev a1)</pre></div><p>
       In this example, <code class="literal">08:00.0</code> and
       <code class="literal">08:00.1</code> are addresses of the PCI device. The vendorID
       is <code class="literal">10de</code>. The productIDs are <code class="literal">10d8</code>
       and <code class="literal">0be3</code>.
      </p></li><li class="step "><p>
       Confirm that the devices are available for passthrough:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls -ld /sys/kernel/iommu_groups/*/devices/*08:00.?/
drwxr-xr-x 3 root root 0 Feb 14 13:05 /sys/kernel/iommu_groups/20/devices/0000:08:00.0/
drwxr-xr-x 3 root root 0 Feb 19 16:09 /sys/kernel/iommu_groups/20/devices/0000:08:00.1/</pre></div><div id="id-1.5.12.6.19.6.3.1.2.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
        With PCI passthrough, only an entire IOMMU group can be passed. Parts
        of the group cannot be passed. In this example, the IOMMU group is
        <code class="literal">20</code>.
       </p></div></li></ol></div></div></li><li class="listitem "><p>
     <span class="bold"><strong>Preparing nova and glance for
     passthrough</strong></span>
    </p><p>
     Information about configuring nova and glance is available in the
     documentation at
     <a class="link" href="https://docs.openstack.org/nova/rocky/admin/pci-passthrough.html" target="_blank">https://docs.openstack.org/nova/rocky/admin/pci-passthrough.html</a>.
     Both <code class="literal">nova-compute</code> and <code class="literal">nova-scheduler</code>
     must be configured.
    </p></li></ol></div></div><div class="sect3" id="id-1.5.12.6.19.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Supported Intel 82599 Devices</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.19.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="table" id="intel-82599-table"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 10.1: </span><span class="name">Intel 82599 devices supported with SRIOV and PCIPT </span><a title="Permalink" class="permalink" href="#intel-82599-table">#</a></h6></div><div class="table-contents"><table class="table" summary="Intel 82599 devices supported with SRIOV and PCIPT" border="1"><colgroup><col align="center" class="c1" /><col align="center" class="c2" /><col align="center" class="c3" /></colgroup><thead><tr><th align="center">Vendor</th><th align="center">Device</th><th align="center">Title</th></tr></thead><tbody><tr><td align="center">Intel Corporation</td><td align="center">10f8</td><td align="center">82599 10 Gigabit Dual Port Backplane Connection</td></tr><tr><td align="center">Intel Corporation</td><td align="center">10f9</td><td align="center">82599 10 Gigabit Dual Port Network Connection</td></tr><tr><td align="center">Intel Corporation</td><td align="center">10fb</td><td align="center">82599ES 10-Gigabit SFI/SFP+ Network Connection</td></tr><tr><td align="center">Intel Corporation</td><td align="center">10fc</td><td align="center">82599 10 Gigabit Dual Port Network Connection</td></tr></tbody></table></div></div></div><div class="sect3" id="id-1.5.12.6.19.8"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SRIOV PCIPT configuration</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.19.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If you plan to take advantage of SR-IOV support in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, plan in
   advance to meet the following requirements:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Use one of the supported NIC cards:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       HP Ethernet 10Gb 2-port 560FLR-SFP+ Adapter (Intel Niantic). Product
       part number: 665243-B21 -- Same part number for the following card
       options:
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         FlexLOM card
        </p></li><li class="listitem "><p>
         PCI slot adapter card
        </p></li></ul></div></li></ul></div></li><li class="listitem "><p>
     Identify the NIC ports to be used for PCI Passthrough devices and SRIOV
     devices from each compute node
    </p></li><li class="listitem "><p>
     Ensure that:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       SRIOV is enabled in the BIOS
      </p></li><li class="listitem "><p>
       HP Shared memory is disabled in the BIOS on the compute nodes.
      </p></li><li class="listitem "><p>
       The Intel boot agent is disabled on the compute
       (<a class="xref" href="#bootutil" title="10.4.15.11. Intel bootutils">Section 10.4.15.11, “Intel bootutils”</a> can be used to perform this)
      </p></li></ul></div><div id="id-1.5.12.6.19.8.3.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      Because of Intel driver limitations, you cannot use a NIC port as an
      SRIOV NIC as well as a physical NIC. Using the physical function to carry
      the normal tenant traffic through the OVS bridge at the same time as
      assigning the VFs from the same NIC device as passthrough to the guest VM
      is not supported.
     </p></div></li></ol></div><p>
   If the above prerequisites are met, then SR-IOV or PCIPT can be reconfigured
   at any time. There is no need to do it at install time.
  </p></div><div class="sect3" id="id-1.5.12.6.19.9"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment use cases</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.19.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following are typical use cases that should cover your particular needs:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     A device on the host needs to be enabled for both PCI-passthrough and
     PCI-SRIOV during deployment. At run time nova decides whether to use
     physical functions or virtual function depending on vnic_type of the port
     used for booting the VM.
    </p></li><li class="listitem "><p>
     A device on the host needs to be configured only for PCI-passthrough.
    </p></li><li class="listitem "><p>
     A device on the host needs to be configured only for PCI-SRIOV virtual
     functions.
    </p></li></ol></div></div><div class="sect3" id="id-1.5.12.6.19.10"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Input model updates</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.19.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 provides various options for the user to configure the
   network for tenant VMs. These options have been enhanced to support SRIOV
   and PCIPT.
  </p><p>
   the Cloud Lifecycle Manager input model changes to support SRIOV and PCIPT are as follows. If
   you were familiar with the configuration settings previously, you will
   notice these changes.
  </p><p>
   <span class="bold"><strong>net_interfaces.yml:</strong></span> This file defines the
   interface details of the nodes. In it, the following fields have been added
   under the compute node interface section:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /></colgroup><thead><tr><th>Key</th><th>Value</th></tr></thead><tbody><tr><td>sriov_only: </td><td>
       <p>
        Indicates that only SR-IOV be enabled on the interface. This should be
        set to true if you want to dedicate the NIC interface to support only
        SR-IOV functionality.
       </p>
      </td></tr><tr><td>pci-pt: </td><td>
       <p>
        When this value is set to true, it indicates that PCIPT should be
        enabled on the interface.
       </p>
      </td></tr><tr><td>vf-count: </td><td>
       <p>
        Indicates the number of VFs to be configured on a given interface.
       </p>
      </td></tr></tbody></table></div><p>
   In <code class="filename">control_plane.yml</code> under <code class="literal">Compute
   resource</code>, <code class="literal">neutron-sriov-nic-agent</code> has been
   added as a service component.
  </p><p>
   under resources:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /></colgroup><thead><tr><th>Key</th><th>Value</th></tr></thead><tbody><tr><td>name:</td><td> Compute</td></tr><tr><td>resource-prefix:</td><td> Comp</td></tr><tr><td>server-role:</td><td>COMPUTE-ROLE</td></tr><tr><td>allocation-policy:</td><td> Any</td></tr><tr><td>min-count:</td><td> 0</td></tr><tr><td>service-components:</td><td>ntp-client</td></tr><tr><td> </td><td>nova-compute</td></tr><tr><td> </td><td>nova-compute-kvm</td></tr><tr><td> </td><td>neutron-l3-agent</td></tr><tr><td> </td><td>neutron-metadata-agent</td></tr><tr><td> </td><td>neutron-openvswitch-agent</td></tr><tr><td> </td><td>- neutron-sriov-nic-agent*</td></tr></tbody></table></div><p>
   <span class="bold"><strong>nic_device_data.yml:</strong></span> This is the new file
   added with this release to support SRIOV and PCIPT configuration details. It
   contains information about the specifics of a nic, and is found at
   <code class="literal">/usr/share/ardana/input-model/2.0/services/osconfig/nic_device_data.yml</code>.
   The fields in this file are as follows.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     <span class="bold"><strong>nic-device-types:</strong></span> The nic-device-types
     section contains the following key-value pairs:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value</th></tr></thead><tbody><tr><td>name:</td><td>
         <p>
          The name of the nic-device-types that will be referenced in
          nic_mappings.yml
         </p>
        </td></tr><tr><td>family:</td><td>
         <p>
          The name of the nic-device-families to be used with this
          nic_device_type
         </p>
        </td></tr><tr><td>device_id:</td><td>
         <p>
          Device ID as specified by the vendor for the particular NIC
         </p>
        </td></tr><tr><td>type:</td><td>
         <p>
          The value of this field can be <code class="literal">simple-port</code> or
          <code class="literal">multi-port</code>. If a single bus address is assigned to
          more than one nic, The value will be
          <code class="literal">multi-port</code>. If there is a one-to-one mapping
          between bus address and the nic, it will be
          <code class="literal">simple-port</code>.
         </p>
        </td></tr></tbody></table></div></li><li class="listitem "><p>
     <span class="bold"><strong>nic-device-families:</strong></span> The
     nic-device-families section contains the following key-value pairs:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Key</th><th>Value</th></tr></thead><tbody><tr><td>name:</td><td>
         <p>
          The name of the device family that can be used for reference in
          nic-device-types.
         </p>
        </td></tr><tr><td>vendor-id: </td><td>
         <p>
          Vendor ID of the NIC
         </p>
        </td></tr><tr><td>config-script:</td><td>
         <p>
          A script file used to create the virtual functions (VF) on the
          Compute node.
         </p>
        </td></tr><tr><td>driver:</td><td>
         <p>
          Indicates the NIC driver that needs to be used.
         </p>
        </td></tr><tr><td>vf-count-type:</td><td>
         <p>
          This value can be either <code class="literal">port</code> or
          <code class="literal">driver</code>.
         </p>
        </td></tr><tr><td>“port”:</td><td>
         <p>
          Indicates that the device supports per-port virtual function (VF)
          counts.
         </p>
        </td></tr><tr><td>“driver:”</td><td>
         <p>
          Indicates that all ports using the same driver will be configured
          with the same number of VFs, whether or not the interface model
          specifies a vf-count attribute for the port. If two or more ports
          specify different vf-count values, the config processor errors out.
         </p>
        </td></tr><tr><td>Max-vf-count:</td><td>
         <p>
          This field indicates the maximum VFs that can be configured on an
          interface as defined by the vendor.
         </p>
        </td></tr></tbody></table></div></li></ol></div><p>
   <span class="bold"><strong>control_plane.yml:</strong></span> This file provides the
   information about the services to be run on a particular node. To support
   SR-IOV on a particular compute node, you must run
   <code class="literal">neutron-sriov-nic-agent</code> on that node.
  </p><p>
   <span class="bold"><strong>Mapping the use cases with various fields in input
   model</strong></span>
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /><col class="col6" /><col class="col7" /></colgroup><thead><tr><th> </th><th>Vf-count</th><th>SR-IOV</th><th>PCIPT</th><th>OVS bridge</th><th>Can be NIC bonded</th><th>Use case</th></tr></thead><tbody><tr><td>sriov-only: true</td><td>Mandatory</td><td>Yes</td><td>No</td><td>No</td><td>No</td><td>Dedicated to SRIOV</td></tr><tr><td>pci-pt : true</td><td>Not Specified</td><td>No</td><td>Yes</td><td>No</td><td>No</td><td>Dedicated to PCI-PT</td></tr><tr><td>pci-pt : true</td><td>Specified</td><td>Yes</td><td>Yes</td><td>No</td><td>No</td><td>PCI-PT or SRIOV</td></tr><tr><td>pci-pt and sriov-only keywords are not specified</td><td>Specified</td><td>Yes</td><td>No</td><td>Yes</td><td>No</td><td>SRIOV with PF used by host</td></tr><tr><td>pci-pt and sriov-only keywords are not specified</td><td>Not Specified</td><td>No</td><td>No</td><td>Yes</td><td>Yes</td><td>Traditional/Usual use case</td></tr></tbody></table></div></div><div class="sect3" id="id-1.5.12.6.19.11"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Mappings between <code class="filename">nic_mappings.yml</code> and <code class="filename">net_interfaces.yml</code></span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.19.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following diagram shows which fields in
   <code class="filename">nic_mappings.yml</code> map to corresponding fields in
   <code class="filename">net_interfaces.yml</code>:
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-sriov_pcpit.png" target="_blank"><img src="images/media-sriov_pcpit.png" width="" /></a></div></div></div><div class="sect3" id="id-1.5.12.6.19.12"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Example Use Cases for Intel</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.19.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     <span class="bold"><strong>Nic-device-types and nic-device-families</strong></span>
     with Intel 82559 with ixgbe as the driver.
    </p><div class="verbatim-wrap"><pre class="screen">nic-device-types:
    - name: ''8086:10fb
      family: INTEL-82599
      device-id: '10fb'
      type: simple-port
nic-device-families:
    # Niantic
    - name: INTEL-82599
      vendor-id: '8086'
      config-script: intel-82599.sh
      driver: ixgbe
      vf-count-type: port
      max-vf-count: 63</pre></div></li><li class="listitem "><p>
     <span class="bold"><strong>net_interfaces.yml</strong></span> for the SRIOV-only use
     case:
    </p><div class="verbatim-wrap"><pre class="screen">- name: COMPUTE-INTERFACES
   - name: hed1
     device:
       name: hed1
       sriov-only: true
       vf-count: 6
     network-groups:
      - GUEST1</pre></div></li><li class="listitem "><p>
     <span class="bold"><strong>net_interfaces.yml</strong></span> for the PCIPT-only use
     case:
    </p><div class="verbatim-wrap"><pre class="screen">- name: COMPUTE-INTERFACES
   - name: hed1
     device:
       name: hed1
       pci-pt: true
    network-groups:
     - GUEST1</pre></div></li><li class="listitem "><p>
     <span class="bold"><strong>net_interfaces.yml</strong></span> for the SRIOV and
     PCIPT use case
    </p><div class="verbatim-wrap"><pre class="screen"> - name: COMPUTE-INTERFACES
    - name: hed1
      device:
        name: hed1
        pci-pt: true
        vf-count: 6
      network-groups:
      - GUEST1</pre></div></li><li class="listitem "><p>
     <span class="bold"><strong>net_interfaces.yml</strong></span> for SRIOV and Normal
     Virtio use case
    </p><div class="verbatim-wrap"><pre class="screen">- name: COMPUTE-INTERFACES
   - name: hed1
     device:
        name: hed1
        vf-count: 6
      network-groups:
      - GUEST1</pre></div></li><li class="listitem "><p>
     <span class="bold"><strong>net_interfaces.yml</strong></span> for PCI-PT
     (<code class="literal">hed1</code> and <code class="literal">hed4</code> refer to the DUAL
     ports of the PCI-PT NIC)
    </p><div class="verbatim-wrap"><pre class="screen">    - name: COMPUTE-PCI-INTERFACES
      network-interfaces:
      - name: hed3
        device:
          name: hed3
        network-groups:
          - MANAGEMENT
          - EXTERNAL-VM
        forced-network-groups:
          - EXTERNAL-API
      - name: hed1
        device:
          name: hed1
          pci-pt: true
        network-groups:
          - GUEST
      - name: hed4
        device:
          name: hed4
          pci-pt: true
        network-groups:
          - GUEST</pre></div></li></ol></div></div><div class="sect3" id="id-1.5.12.6.19.13"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Launching Virtual Machines</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.19.13">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Provisioning a VM with SR-IOV NIC is a two-step process.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Create a neutron port with <code class="literal">vnic_type = direct</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port create --network $net_id --vnic-type direct sriov_port</pre></div></li><li class="step "><p>
     Boot a VM with the created <code class="literal">port-id</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server create --flavor m1.large --image opensuse --nic port-id=$port_id test-sriov</pre></div></li></ol></div></div><p>
   Provisioning a VM with PCI-PT NIC is a two-step process.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Create two neutron ports with <code class="literal">vnic_type =
     direct-physical</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port create --network net1 --vnic-type direct-physical pci-port1
<code class="prompt user">ardana &gt; </code>openstack port create --network net1 --vnic-type direct-physical pci-port2</pre></div></li><li class="step "><p>
     Boot a VM with the created ports.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server create --flavor 4 --image opensuse --nic port-id pci-port1-port-id \
--nic port-id pci-port2-port-id vm1-pci-passthrough</pre></div></li></ol></div></div><p>
   If PCI-PT VM gets stuck (hangs) at boot time when using an Intel NIC, the
   boot agent should be disabled.
  </p></div><div class="sect3" id="bootutil"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Intel bootutils</span> <a title="Permalink" class="permalink" href="#bootutil">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span>bootutil</li></ul></div></div></div></div><p>
   When Intel cards are used for PCI-PT, a tenant VM can get stuck at boot
   time. When this happens, you should download Intel bootutils and use it to
   should disable bootagent.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Download <code class="filename">Preboot.tar.gz</code> from
     <a class="link" href="https://downloadcenter.intel.com/download/19186/Intel-Ethernet-Connections-Boot-Utility-Preboot-Images-and-EFI-Drivers" target="_blank">https://downloadcenter.intel.com/download/19186/Intel-Ethernet-Connections-Boot-Utility-Preboot-Images-and-EFI-Drivers</a>
    </p></li><li class="listitem "><p>
     Untar the <code class="filename">Preboot.tar.gz</code> on the compute node where the
     PCI-PT VM is to be hosted.
    </p></li><li class="listitem "><p>
     Go to <code class="filename">~/APPS/BootUtil/Linux_x64</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/APPS/BootUtil/Linux_x64</pre></div><p>
     and run following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>./bootutil64e -BOOTENABLE disable -all</pre></div></li><li class="listitem "><p>
     Boot the PCI-PT VM; it should boot without getting stuck.
    </p><div id="id-1.5.12.6.19.14.3.4.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      Even though VM console shows VM getting stuck at PXE boot, it is not
      related to BIOS PXE settings.
     </p></div></li></ol></div></div><div class="sect3" id="id-1.5.12.6.19.15"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Making input model changes and implementing PCI PT and SR-IOV</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.19.15">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To implement the configuration you require, log into the Cloud Lifecycle Manager node and update
   the Cloud Lifecycle Manager model files to enable SR-IOV or PCIPT following the relevant use
   case explained above. You will need to edit the following:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <code class="filename">net_interfaces.yml</code>
    </p></li><li class="listitem "><p>
     <code class="filename">nic_device_data.yml</code>
    </p></li><li class="listitem "><p>
     <code class="filename">control_plane.yml</code>
    </p></li></ul></div><p>
   To make the edits,
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Check out the site branch of the local git repository and change to the
     correct directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git checkout site
<code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition/data/</pre></div></li><li class="step "><p>
     Open each file in vim or another editor and make the necessary changes.
     Save each file, then commit to the local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "your commit message goes here in quotes"</pre></div></li><li class="step "><p>
     Have the Cloud Lifecycle Manager enable your changes by running the necessary
     playbooks:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml</pre></div></li></ol></div></div><div id="id-1.5.12.6.19.15.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    After running the <code class="filename">site.yml</code> playbook above, you must
    reboot the compute nodes that are configured with Intel PCI devices.
   </p></div><div id="id-1.5.12.6.19.15.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    When a VM is running on an SRIOV port on a given compute node,
    reconfiguration is not supported.
   </p></div><p>
   You can set the number of virtual functions that must be enabled on a
   compute node at install time. You can update the number of virtual functions
   after deployment. If any VMs have been spawned before you change the number
   of virtual functions, those VMs may lose connectivity. Therefore, it is
   always recommended that if any virtual function is used by any tenant VM,
   you should not reconfigure the virtual functions. Instead, you should
   delete/migrate all the VMs on that NIC before reconfiguring the number of
   virtual functions.
  </p></div><div class="sect3" id="id-1.5.12.6.19.16"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.19.16">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-pcipt_sriov_config.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-pcipt_sriov_config.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Security groups are not applicable for PCI-PT and SRIOV ports.
    </p></li><li class="listitem "><p>
     Live migration is not supported for VMs with PCI-PT and SRIOV ports.
    </p></li><li class="listitem "><p>
     Rate limiting (QoS) is not applicable on SRIOV and PCI-PT ports.
    </p></li><li class="listitem "><p>
     SRIOV/PCIPT is not supported for VxLAN network.
    </p></li><li class="listitem "><p>
     DVR is not supported with SRIOV/PCIPT.
    </p></li><li class="listitem "><p>
     For Intel cards, the same NIC cannot be used for both SRIOV and normal VM
     boot.
    </p></li><li class="listitem "><p>
     Current upstream OpenStack code does not support this hot plugin of
     SRIOV/PCIPT interface using the nova <code class="literal">attach_interface</code>
     command. See <a class="link" href="https://review.openstack.org/#/c/139910/" target="_blank">https://review.openstack.org/#/c/139910/</a>
     for more information.
    </p></li><li class="listitem "><p>
     The <code class="command">openstack port update</code> command will not work when
     admin state is down.
    </p></li><li class="listitem "><p>
     SLES Compute Nodes with dual-port PCI-PT NICs, both ports should always be
     passed in the VM. It is not possible to split the dual port and pass
     through just a single port.
    </p></li></ul></div></div><div class="sect3" id="pcipt-gen9"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.15.14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling PCI-PT on HPE DL360 Gen 9 Servers</span> <a title="Permalink" class="permalink" href="#pcipt-gen9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-enabling_pcipt_on_gen9.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-enabling_pcipt_on_gen9.xml</li><li><span class="ds-label">ID: </span>pcipt-gen9</li></ul></div></div></div></div><p>
  The HPE DL360 Gen 9 and HPE ProLiant systems with Intel processors use a
  region of system memory for sideband communication of management
  information. The BIOS sets up Reserved Memory Region Reporting (RMRR) to
  report these memory regions and devices to the operating system. There is a
  conflict between the Linux kernel and RMRR which causes problems with PCI
  pass-through (PCI-PT). This is needed for IOMMU use by DPDK. Note that this
  does not affect SR-IOV.
 </p><p>
  In order to enable PCI-PT on the HPE DL360 Gen 9 you must have a version of
  firmware that supports setting this and you must change a BIOS setting.
 </p><p>
  To begin, get the latest firmware and install it on your compute nodes.
 </p><p>
  Once the firmware has been updated:
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Reboot the server and press <span class="keycap">F9</span> (system utilities) during POST (power on
    self test)
   </p></li><li class="listitem "><p>
    Choose <span class="guimenu ">System Configuration</span>
   </p></li><li class="listitem "><p>
    Select the NIC for which you want to enable PCI-PT
   </p></li><li class="listitem "><p>
    Choose <span class="guimenu ">Device Level Configuration</span>
   </p></li><li class="listitem "><p>
    Disable the shared memory feature in the BIOS.
   </p></li><li class="listitem "><p>
    Save the changes and reboot server
   </p></li></ol></div></div></div><div class="sect2" id="vlan-aware"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.4.16 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting up VLAN-Aware VMs</span> <a title="Permalink" class="permalink" href="#vlan-aware">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span>vlan-aware</li></ul></div></div></div></div><p>
  Creating a VM with a trunk port will allow a VM to gain connectivity to one
  or more networks over the same virtual NIC (vNIC) through the use VLAN
  interfaces in the guest VM. Connectivity to different networks can be added
  and removed dynamically through the use of subports. The network of the
  parent port will be presented to the VM as the untagged VLAN, and the
  networks of the child ports will be presented to the VM as the tagged VLANs
  (the VIDs of which can be chosen arbitrarily as long as they are unique to
  that trunk). The VM will send/receive VLAN-tagged traffic over the subports,
  and neutron will mux/demux the traffic onto the subport's corresponding
  network. This is not to be confused with VLAN transparency where a VM can
  pass VLAN-tagged traffic transparently across the network without
  interference from neutron. VLAN transparency is not supported.
 </p><div class="sect3" id="id-1.5.12.6.20.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.16.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Terminology</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.20.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>Trunk</strong></span>: a resource that logically
     represents a trunked vNIC and references a parent port.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Parent port</strong></span>: a neutron port that a Trunk
     is referenced to. Its network is presented as the untagged VLAN.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Subport</strong></span>: a resource that logically
     represents a tagged VLAN port on a Trunk. A Subport references a child
     port and consists of the
     &lt;port&gt;,&lt;segmentation-type&gt;,&lt;segmentation-id&gt; tuple.
     Currently only the <code class="literal">vlan</code> segmentation type is supported.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Child port</strong></span>: a neutron port that a Subport
     is referenced to. Its network is presented as a tagged VLAN based upon the
     segmentation-id used when creating/adding a Subport.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Legacy VM</strong></span>: a VM that does not use a trunk
     port.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Legacy port</strong></span>: a neutron port that is not
     used in a Trunk.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>VLAN-aware VM</strong></span>: a VM that uses at least
     one trunk port.
    </p></li></ul></div></div><div class="sect3" id="id-1.5.12.6.20.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.16.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Trunk CLI reference</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.20.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th> Command</th><th>Action</th></tr></thead><tbody><tr><td>network trunk create </td><td>Create a trunk.</td></tr><tr><td>network trunk delete </td><td>Delete a given trunk.</td></tr><tr><td>network trunk list </td><td>List all trunks.</td></tr><tr><td>network trunk show </td><td>Show information of a given trunk.</td></tr><tr><td>network trunk set </td><td>Add subports to a given trunk.</td></tr><tr><td>network subport list </td><td>List all subports for a given trunk.</td></tr><tr><td>network trunk unset</td><td>Remove subports from a given trunk.</td></tr><tr><td>network trunk set</td><td>Update trunk properties.</td></tr></tbody></table></div></div><div class="sect3" id="id-1.5.12.6.20.5"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.16.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling VLAN-aware VM capability</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.20.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Edit
     <code class="filename">~/openstack/my_cloud/config/neutron/neutron.conf.j2</code>
     to add the <code class="literal">trunk</code> service_plugin:
    </p><div class="verbatim-wrap"><pre class="screen">service_plugins = {{ neutron_service_plugins }},trunk</pre></div></li><li class="step "><p>
     Edit
     <code class="filename">~/openstack/my_cloud/config/neutron/ml2_conf.ini.j2</code>
     to enable the noop firewall driver:
    </p><div class="verbatim-wrap"><pre class="screen">[securitygroup]
firewall_driver = neutron.agent.firewall.NoopFirewallDriver</pre></div><div id="id-1.5.12.6.20.5.2.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      This is a manual configuration step because it must be made apparent that
      this step disables neutron security groups completely. The default
      <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> firewall_driver is
      <code class="literal">neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewall
      Driver</code> which does not implement security groups for trunk
      ports. Optionally, the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> default firewall_driver may still be used
      (this step can be skipped), which would provide security groups for legacy
      VMs but not for VLAN-aware VMs. However, this mixed environment is not
      recommended. For more information, see <a class="xref" href="#firewall" title="10.4.16.6. Firewall issues">Section 10.4.16.6, “Firewall issues”</a>.
     </p></div></li><li class="step "><p>
     Commit the configuration changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Enable vlan-aware VMs"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/</pre></div></li><li class="step "><p>
     If this is an initial deployment, continue the rest of normal deployment
     process:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml</pre></div></li><li class="step "><p>
     If the cloud has already been deployed and this is a reconfiguration:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="id-1.5.12.6.20.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.16.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Use Cases</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.20.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="bold"><strong>Creating a trunk port</strong></span>
  </p><p>
   Assume that a number of neutron networks/subnets already exist: private,
   foo-net, and bar-net. This will create a trunk with two subports allocated
   to it. The parent port will be on the "private" network, while the two child
   ports will be on "foo-net" and "bar-net", respectively:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Create a port that will function as the trunk's parent port:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port create --name trunkparent private</pre></div></li><li class="step "><p>
     Create ports that will function as the child ports to be used in subports:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port create --name subport1 foo-net
<code class="prompt user">ardana &gt; </code>openstack port create --name subport2 bar-net</pre></div></li><li class="step "><p>
     Create a trunk port using the <code class="literal">openstack network trunk
     create</code> command, passing the parent port created in step 1 and
     child ports created in step 2:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network trunk create --parent-port trunkparent --subport port=subport1,segmentation-type=vlan,segmentation-id=1 --subport port=subport2,segmentation-type=vlan,segmentation-id=2 mytrunk
+-----------------+-----------------------------------------------------------------------------------------------+
| Field           | Value                                                                                         |
+-----------------+-----------------------------------------------------------------------------------------------+
| admin_state_up  | UP                                                                                            |
| created_at      | 2017-06-02T21:49:59Z                                                                          |
| description     |                                                                                               |
| id              | bd822ebd-33d5-423e-8731-dfe16dcebac2                                                          |
| name            | mytrunk                                                                                       |
| port_id         | 239f8807-be2e-4732-9de6-c64519f46358                                                          |
| project_id      | f51610e1ac8941a9a0d08940f11ed9b9                                                              |
| revision_number | 1                                                                                             |
| status          | DOWN                                                                                          |
| sub_ports       | port_id='9d25abcf-d8a4-4272-9436-75735d2d39dc', segmentation_id='1', segmentation_type='vlan' |
|                 | port_id='e3c38cb2-0567-4501-9602-c7a78300461e', segmentation_id='2', segmentation_type='vlan' |
| tenant_id       | f51610e1ac8941a9a0d08940f11ed9b9                                                              |
| updated_at      | 2017-06-02T21:49:59Z                                                                          |
+-----------------+-----------------------------------------------------------------------------------------------+

$ openstack network subport list --trunk mytrunk
+--------------------------------------+-------------------+-----------------+
| Port                                 | Segmentation Type | Segmentation ID |
+--------------------------------------+-------------------+-----------------+
| 9d25abcf-d8a4-4272-9436-75735d2d39dc | vlan              |               1 |
| e3c38cb2-0567-4501-9602-c7a78300461e | vlan              |               2 |
+--------------------------------------+-------------------+-----------------+</pre></div><p>
     Optionally, a trunk may be created without subports (they can be added
     later):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network trunk create --parent-port trunkparent mytrunk
+-----------------+--------------------------------------+
| Field           | Value                                |
+-----------------+--------------------------------------+
| admin_state_up  | UP                                   |
| created_at      | 2017-06-02T21:45:35Z                 |
| description     |                                      |
| id              | eb8a3c7d-9f0a-42db-b26a-ca15c2b38e6e |
| name            | mytrunk                              |
| port_id         | 239f8807-be2e-4732-9de6-c64519f46358 |
| project_id      | f51610e1ac8941a9a0d08940f11ed9b9     |
| revision_number | 1                                    |
| status          | DOWN                                 |
| sub_ports       |                                      |
| tenant_id       | f51610e1ac8941a9a0d08940f11ed9b9     |
| updated_at      | 2017-06-02T21:45:35Z                 |
+-----------------+--------------------------------------+</pre></div><p>
     A port that is already bound (that is, already in use by a VM) cannot be
     upgraded to a trunk port. The port must be unbound to be eligible for
     use as a trunk's parent port. When adding subports to a trunk, the child
     ports must be unbound as well.
    </p></li></ol></div></div><p>
   <span class="bold"><strong>Checking a port's trunk details</strong></span>
  </p><p>
   Once a trunk has been created, its parent port will show the
   <code class="literal">trunk_details</code> attribute, which consists of the
   <code class="literal">trunk_id</code> and list of subport dictionaries:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port show -F trunk_details trunkparent
+---------------+-------------------------------------------------------------------------------------+
| Field         | Value                                                                               |
+---------------+-------------------------------------------------------------------------------------+
| trunk_details | {"trunk_id": "bd822ebd-33d5-423e-8731-dfe16dcebac2", "sub_ports":                   |
|               | [{"segmentation_id": 2, "port_id": "e3c38cb2-0567-4501-9602-c7a78300461e",          |
|               | "segmentation_type": "vlan", "mac_address": "fa:16:3e:11:90:d2"},                   |
|               | {"segmentation_id": 1, "port_id": "9d25abcf-d8a4-4272-9436-75735d2d39dc",           |
|               | "segmentation_type": "vlan", "mac_address": "fa:16:3e:ff:de:73"}]}                  |
+---------------+-------------------------------------------------------------------------------------+</pre></div><p>
   Ports that are not trunk parent ports will not have a
   <code class="literal">trunk_details</code> field:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port show -F trunk_details subport1
need more than 0 values to unpack</pre></div><p>
   <span class="bold"><strong>Adding subports to a trunk</strong></span>
  </p><p>
   Assuming a trunk and new child port have been created already, the
   <code class="literal">trunk-subport-add</code> command will add one or more subports
   to the trunk.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Run <code class="literal">openstack network trunk set</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network trunk set --subport port=subport3,segmentation-type=vlan,segmentation-id=3 mytrunk</pre></div></li><li class="step "><p>
     Run <code class="literal">openstack network subport list</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network subport list --trunk mytrunk
+--------------------------------------+-------------------+-----------------+
| Port                                 | Segmentation Type | Segmentation ID |
+--------------------------------------+-------------------+-----------------+
| 9d25abcf-d8a4-4272-9436-75735d2d39dc | vlan              |               1 |
| e3c38cb2-0567-4501-9602-c7a78300461e | vlan              |               2 |
| bf958742-dbf9-467f-b889-9f8f2d6414ad | vlan              |               3 |
+--------------------------------------+-------------------+-----------------+</pre></div></li></ol></div></div><div id="id-1.5.12.6.20.6.13" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    The <code class="literal">--subport</code> option may be repeated multiple times in
    order to add multiple subports at a time.
   </p></div><p>
   <span class="bold"><strong>Removing subports from a trunk</strong></span>
  </p><p>
   To remove a subport from a trunk, use <code class="literal">openstack network trunk
   unset</code> command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network trunk unset --subport subport3 mytrunk</pre></div><p>
   <span class="bold"><strong>Deleting a trunk port</strong></span>
  </p><p>
   To delete a trunk port, use the <code class="literal">openstack network trunk
   delete</code> command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network trunk delete mytrunk</pre></div><p>
   Once a trunk has been created successfully, its parent port may be passed to
   the <code class="literal">openstack server create</code> command, which will make the VM VLAN-aware:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server create --image ubuntu-server --flavor 1 --nic port-id=239f8807-be2e-4732-9de6-c64519f46358 vlan-aware-vm</pre></div><div id="id-1.5.12.6.20.6.22" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    A trunk cannot be deleted until its parent port is unbound. This means you
    must delete the VM using the trunk port before you are allowed to delete
    the trunk.
   </p></div></div><div class="sect3" id="id-1.5.12.6.20.7"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.16.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">VLAN-aware VM network configuration</span> <a title="Permalink" class="permalink" href="#id-1.5.12.6.20.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   This section illustrates how to configure the VLAN interfaces inside a
   VLAN-aware VM based upon the subports allocated to the trunk port being
   used.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Run <code class="literal">openstack network trunk subport list</code> to see the
     VLAN IDs in use on the trunk port:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network subport list --trunk mytrunk
+--------------------------------------+-------------------+-----------------+
| Port                                 | Segmentation Type | Segmentation ID |
+--------------------------------------+-------------------+-----------------+
| e3c38cb2-0567-4501-9602-c7a78300461e | vlan              |               2 |
+--------------------------------------+-------------------+-----------------+</pre></div></li><li class="step "><p>
     Run <code class="command">openstack port show</code> on the child port to get its
     mac_address:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port show -F mac_address 08848e38-50e6-4d22-900c-b21b07886fb7
+-------------+-------------------+
| Field       | Value             |
+-------------+-------------------+
| mac_address | fa:16:3e:08:24:61 |
+-------------+-------------------+</pre></div></li><li class="step "><p>
     Log into the VLAN-aware VM and run the following commands to set up the
     VLAN interface:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo ip link add link ens3 ens3.2 address fa:16:3e:11:90:d2 broadcast ff:ff:ff:ff:ff:ff type vlan id 2
$ sudo ip link set dev ens3.2 up</pre></div></li><li class="step "><p>
     Note the usage of the mac_address from step 2 and VLAN ID from step 1 in
     configuring the VLAN interface:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo ip link add link ens3 ens3.2 address fa:16:3e:11:90:d2 broadcast ff:ff:ff:ff:ff:ff type vlan id 2</pre></div></li><li class="step "><p>
     Trigger a DHCP request for the new vlan interface to verify connectivity
     and retrieve its IP address. On an Ubuntu VM, this might be:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo dhclient ens3.2
<code class="prompt user">ardana &gt; </code>sudo ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: ens3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc pfifo_fast state UP group default qlen 1000
    link/ether fa:16:3e:8d:77:39 brd ff:ff:ff:ff:ff:ff
    inet 10.10.10.5/24 brd 10.10.10.255 scope global ens3
       valid_lft forever preferred_lft forever
    inet6 fe80::f816:3eff:fe8d:7739/64 scope link
       valid_lft forever preferred_lft forever
3: ens3.2@ens3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc noqueue state UP group default qlen 1000
    link/ether fa:16:3e:11:90:d2 brd ff:ff:ff:ff:ff:ff
    inet 10.10.12.7/24 brd 10.10.12.255 scope global ens3.2
       valid_lft forever preferred_lft forever
    inet6 fe80::f816:3eff:fe11:90d2/64 scope link
       valid_lft forever preferred_lft forever</pre></div></li></ol></div></div></div><div class="sect3" id="firewall"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.4.16.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Firewall issues</span> <a title="Permalink" class="permalink" href="#firewall">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-vlan-aware_vm.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-vlan-aware_vm.xml</li><li><span class="ds-label">ID: </span>firewall</li></ul></div></div></div></div><p>
   The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> default firewall_driver is
   <code class="literal">neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver</code>.
   This default does not implement security groups for VLAN-aware VMs, but it
   does implement security groups for legacy VMs. For this reason, it is
   recommended to disable neutron security groups altogether when using
   VLAN-aware VMs. To do so, set:
  </p><div class="verbatim-wrap"><pre class="screen">firewall_driver = neutron.agent.firewall.NoopFirewallDriver</pre></div><p>
   Doing this will prevent having a mix of firewalled and non-firewalled VMs in
   the same environment, but it should be done with caution because all VMs
   would be non-firewalled.
  </p></div></div></div><div class="sect1" id="CreateHARouter"><div class="titlepage"><div><div><h2 class="title"><span class="number">10.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a Highly Available Router</span> <a title="Permalink" class="permalink" href="#CreateHARouter">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-create-ha-router.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-create-ha-router.xml</li><li><span class="ds-label">ID: </span>CreateHARouter</li></ul></div></div></div></div><div class="sect2" id="CVRDVR"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">CVR and DVR High Available Routers</span> <a title="Permalink" class="permalink" href="#CVRDVR">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-create-ha-router.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-create-ha-router.xml</li><li><span class="ds-label">ID: </span>CVRDVR</li></ul></div></div></div></div><p>
   CVR (Centralized Virtual Routing) and DVR (Distributed Virtual Routing) are
   two types of technologies which can be used to provide routing processes in
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9. You can create Highly Available (HA) versions
   of CVR and DVR routers by using the options in the table below when creating
   your router.
  </p><p>
   The neutron command for creating a router <code class="literal">openstack router create
   router_name --distributed=True|False --ha=True|False</code> requires
   administrative permissions. See the example in the next section, <a class="xref" href="#CreateRouter" title="10.5.2. Creating a High Availability Router">Section 10.5.2, “Creating a High Availability Router”</a>.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col align="center" class="c1" /><col align="center" class="c2" /><col align="center" class="c3" /><col align="left" class="c4" /></colgroup><thead><tr><th align="center">--distributed</th><th align="center">--ha</th><th align="center">Router Type</th><th align="left">Description</th></tr></thead><tbody><tr><td align="center">False</td><td align="center">False</td><td align="center">CVR</td><td align="left">Centralized Virtual Router</td></tr><tr><td align="center">False</td><td align="center">True</td><td align="center">CVRHA</td><td align="left">Centralized Virtual Router with L3 High Availablity</td></tr><tr><td align="center">True</td><td align="center">False</td><td align="center">DVR</td><td align="left">Distributed Virtual Router without SNAT High Availability</td></tr><tr><td align="center">True</td><td align="center">True</td><td align="center">DVRHA</td><td align="left">Distributed Virtual Router with SNAT High Availability</td></tr></tbody></table></div></div><div class="sect2" id="CreateRouter"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a High Availability Router</span> <a title="Permalink" class="permalink" href="#CreateRouter">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-create-ha-router.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-create-ha-router.xml</li><li><span class="ds-label">ID: </span>CreateRouter</li></ul></div></div></div></div><p>
   You can create a highly available router using the OpenStackClient.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To create the HA router, add <code class="literal">--ha=True</code> to the
     <code class="command">openstack router create</code> command. If you also want to
     make the router distributed, add <code class="literal">--distributed=True</code>. In
     this example, a DVR SNAT HA router is created with the name
     <code class="literal">routerHA</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack router create routerHA --distributed=True --ha=True</pre></div></li><li class="listitem "><p>
     Set the gateway for the external network and add interface
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack router set routerHA &lt;ext-net-id&gt;
<code class="prompt user">ardana &gt; </code>openstack router add subnet routerHA &lt;private_subnet_id&gt;</pre></div></li><li class="listitem "><p>
     When the router is created, the gateway is set, and the interface
     attached, you have a router with high availability.
    </p></li></ol></div></div><div class="sect2" id="TestRouter"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Test Router for High Availability</span> <a title="Permalink" class="permalink" href="#TestRouter">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-create-ha-router.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-create-ha-router.xml</li><li><span class="ds-label">ID: </span>TestRouter</li></ul></div></div></div></div><p>
   You can demonstrate that the router is HA by running a continuous ping from
   a VM instance that is running on the private network to an external server
   such as a public DNS. As the ping is running, list the l3 agents hosting the
   router and identify the agent that is responsible for hosting the active
   router. Induce the failover mechanism by creating a catastrophic event such as
   shutting down node hosting the l3 agent. Once the node is shut down, you
   will see that the ping from the VM to the external network continues to run
   as the backup l3 agent takes over. To verify the agent hosting the primary
   router has changed, list the agents hosting the router. You will see a
   different agent is now hosting the active router.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Boot an instance on the private network
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server create --image &lt;image_id&gt; --flavor &lt;flavor_id&gt; --nic net_id=&lt;private_net_id&gt; --key_name &lt;key&gt; VM1</pre></div></li><li class="step "><p>
     Log into the VM using the SSH keys
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ssh -i &lt;key&gt; &lt;ipaddress of VM1&gt;</pre></div></li><li class="step "><p>
     Start a <code class="command">ping</code> to X.X.X.X. While pinging, make sure there
     is no packet loss and leave the ping running.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ping X.X.X.X</pre></div></li><li class="step "><p>
     Check which agent is hosting the active router.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network agent list –routers &lt;router_id&gt;</pre></div></li><li class="step "><p>
     Shutdown the node hosting the agent.
    </p></li><li class="step "><p>
     Within 10 seconds, check again to see which L3 agent is hosting the active
     router.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network agent list –routers &lt;router_id&gt;</pre></div></li><li class="step "><p>
     You will see a different agent.
    </p></li></ol></div></div></div></div></div><div class="chapter " id="ops-managing-dashboards"><div class="titlepage"><div><div><h1 class="title"><span class="number">11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing the Dashboard</span> <a title="Permalink" class="permalink" href="#ops-managing-dashboards">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-managing_dashboards.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_dashboards.xml</li><li><span class="ds-label">ID: </span>ops-managing-dashboards</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#topic1564-1"><span class="number">11.1 </span><span class="name">Configuring the Dashboard Service</span></a></span></dt><dt><span class="section"><a href="#horizonTimeout"><span class="number">11.2 </span><span class="name">Changing the Dashboard Timeout Value</span></a></span></dt><dt><span class="section"><a href="#lbaas-dashboard"><span class="number">11.3 </span><span class="name">Creating a Load Balancer with the Dashboard</span></a></span></dt></dl></div></div><p>
  Information about managing and configuring the Dashboard service.
 </p><div class="sect1" id="topic1564-1"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Dashboard Service</span> <a title="Permalink" class="permalink" href="#topic1564-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-configure_dashboard.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-configure_dashboard.xml</li><li><span class="ds-label">ID: </span>topic1564-1</li></ul></div></div></div></div><p>
  horizon is the OpenStack service that serves as the basis for the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  dashboards.
 </p><p>
  The dashboards provide a web-based user interface to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services
  including Compute, Volume Operations, Networking, and Identity.
 </p><p>
  Along the left side of the dashboard are sections that provide access to
  Project and Identity sections. If your login credentials have been assigned
  the 'admin' role you will also see a separate Admin section that provides
  additional system-wide setting options.
 </p><p>
  Across the top are menus to switch between projects and menus where you can
  access user settings.
 </p><div class="sect2" id="horizonTLS"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Dashboard Service and TLS in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <a title="Permalink" class="permalink" href="#horizonTLS">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-configure_dashboard.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-configure_dashboard.xml</li><li><span class="ds-label">ID: </span>horizonTLS</li></ul></div></div></div></div><p>
   By default, the Dashboard service is configured with TLS in the input model
   (ardana-input-model). You should not disable TLS in the input model for the
   Dashboard service. The normal use case for users is to have all services
   behind TLS, but users are given the freedom in the input model to take a
   service off TLS for troubleshooting or debugging. TLS should always be
   enabled for production environments.
  </p><p>
   Make sure that <code class="literal">horizon_public_protocol</code> and
   <code class="literal">horizon_private_protocol</code> are both be set to use https.
  </p></div></div><div class="sect1" id="horizonTimeout"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Changing the Dashboard Timeout Value</span> <a title="Permalink" class="permalink" href="#horizonTimeout">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-change_horizon_timeout.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-change_horizon_timeout.xml</li><li><span class="ds-label">ID: </span>horizonTimeout</li></ul></div></div></div></div><p>
  The default session timeout for the dashboard is 1800 seconds or 30 minutes.
  This is the recommended default and best practice for those concerned with
  security.
 </p><p>
  As an administrator, you can change the session timeout by changing the
  value of the SESSION_TIMEOUT to anything less than or equal to 14400, which
  is equal to four hours. Values greater than 14400 should not be used due to
  keystone constraints.
 </p><div id="id-1.5.13.4.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
   Increasing the value of SESSION_TIMEOUT increases the risk of abuse.
  </p></div><div class="sect2" id="Steps"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How to Change the Dashboard Timeout Value</span> <a title="Permalink" class="permalink" href="#Steps">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-configuring-change_horizon_timeout.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-configuring-change_horizon_timeout.xml</li><li><span class="ds-label">ID: </span>Steps</li></ul></div></div></div></div><p>
   Follow these steps to change and commit the horizon timeout value.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the Dashboard config file at
     <code class="literal">~/openstack/my_cloud/config/horizon/local_settings.py</code> and,
     if it is not already present, add a line for
     <code class="literal">SESSION_TIMEOUT</code> above the line for
     <code class="literal">SESSION_ENGINE</code>.
    </p><p>
     Here is an example snippet, in bold:
    </p><div class="verbatim-wrap"><pre class="screen"><span class="bold"><strong>SESSION_TIMEOUT = &lt;timeout value&gt;</strong></span>
SESSION_ENGINE = 'django.contrib.sessions.backends.db'</pre></div><div id="id-1.5.13.4.5.3.2.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      Do not exceed the maximum value of 14400.
     </p></div></li><li class="listitem "><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen">git add -A
git commit -a -m "changed horizon timeout value"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the Dashboard reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts horizon-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect1" id="lbaas-dashboard"><div class="titlepage"><div><div><h2 class="title"><span class="number">11.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a Load Balancer with the Dashboard</span> <a title="Permalink" class="permalink" href="#lbaas-dashboard">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-lbaas-dashboard.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-lbaas-dashboard.xml</li><li><span class="ds-label">ID: </span>lbaas-dashboard</li></ul></div></div></div></div><p>
  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 you can create a Load Balancer with the Load Balancer
  Panel in the Dashboard.
 </p><p>
  Follow the steps below to create the load balancer, listener, pool, add
  members to the pool and create the health monitor.
 </p><div id="id-1.5.13.5.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   Optionally, you may add members to the load balancer pool after the load
   balancer has been created.
  </p></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    <span class="bold"><strong>Login to the Dashboard</strong></span>
   </p><p>
    Login into the Dashboard using your domain, user account and password.
   </p></li><li class="step "><p>
    <span class="bold"><strong>Navigate and Create Load Balancer</strong></span>
   </p><p>
    Once logged into the Dashboard, navigate to the <span class="guimenu ">Load
    Balancers</span> panel by selecting
    <span class="guimenu ">Project</span> › <span class="guimenu ">Network</span> › <span class="guimenu ">Load
    Balancers</span> in the navigation menu, then select
    <span class="guimenu ">Create Load Balancer</span> from the Load
    Balancers page.
   </p></li><li class="step "><p>
    <span class="bold"><strong>Create Load Balancer</strong></span>
   </p><p>
    Provide the Load Balancer details, Load Balancer Name, Description
    (optional), IP Address and Subnet. When complete, select Next.
   </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-horizon-lbaas_panel-LoadBalancers2.png" target="_blank"><img src="images/media-horizon-lbaas_panel-LoadBalancers2.png" width="" /></a></div></div></li><li class="step "><p>
    <span class="bold"><strong>Create Listener</strong></span>
   </p><p>
    Provide a Name, Description, Protocol (HTTP, TCP, TERMINATED_HTTPS) and
    Port for the Load Balancer Listener.
   </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-horizon-lbaas_panel-LoadBalancers3.png" target="_blank"><img src="images/media-horizon-lbaas_panel-LoadBalancers3.png" width="" /></a></div></div></li><li class="step "><p>
    <span class="bold"><strong>Create Pool</strong></span>
   </p><p>
    Provide the Name, Description and Method (LEAST_CONNECTIONS, ROUND_ROBIN,
    SOURCE_IP) for the Load Balancer Pool.
   </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-horizon-lbaas_panel-LoadBalancers4.png" target="_blank"><img src="images/media-horizon-lbaas_panel-LoadBalancers4.png" width="" /></a></div></div></li><li class="step "><p>
    <span class="bold"><strong>Add Pool Members</strong></span>
   </p><p>
    Add members to the Load Balancer Pool.
   </p><div id="id-1.5.13.5.5.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     Optionally, you may add members to the load balancer pool after the load
     balancer has been created.
    </p></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-horizon-lbaas_panel-LoadBalancers5.png" target="_blank"><img src="images/media-horizon-lbaas_panel-LoadBalancers5.png" width="" /></a></div></div></li><li class="step "><p>
    <span class="bold"><strong>Create Health Monitor</strong></span>
   </p><p>
    Create Health Monitor by providing the Monitor type (HTTP, PING, TCP), the
    Health check interval, Retry count, timeout, HTTP Method, Expected HTTP
    status code and the URL path. Once all fields are filled, select
    <span class="bold"><strong>Create Load Balancer</strong></span>.
   </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-horizon-lbaas_panel-LoadBalancers6.png" target="_blank"><img src="images/media-horizon-lbaas_panel-LoadBalancers6.png" width="" /></a></div></div></li><li class="step "><p>
    <span class="bold"><strong>Load Balancer Provisioning Status</strong></span>
   </p><p>
    Clicking on the Load Balancers tab again will provide the status of the
    Load Balancer. The Load Balancer will be in
    <span class="bold"><strong>Pending Create</strong></span> until the Load Balancer
    is created, at which point the Load Balancer will change to an
    <span class="bold"><strong>Active</strong></span> state.
   </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-horizon-lbaas_panel-LoadBalancers8.png" target="_blank"><img src="images/media-horizon-lbaas_panel-LoadBalancers8.png" width="" /></a></div></div></li><li class="step "><p>
    <span class="bold"><strong>Load Balancer Overview</strong></span>
   </p><p>
    Once <span class="bold"><strong>Load Balancer 1</strong></span> has been created, it
    will appear in the Load Balancers list. Click the Load Balancer 1, it will show
    the Overview. In this view, you can see the Load  Balancer Provider type,
    the Admin State, Floating IP, Load Balancer, Subnet and Port ID's.
   </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-horizon-lbaas_panel-LoadBalancers7.png" target="_blank"><img src="images/media-horizon-lbaas_panel-LoadBalancers7.png" width="" /></a></div></div></li></ol></div></div></div></div><div class="chapter " id="ops-managing-orchestration"><div class="titlepage"><div><div><h1 class="title"><span class="number">12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Orchestration</span> <a title="Permalink" class="permalink" href="#ops-managing-orchestration">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-managing_orchestration.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_orchestration.xml</li><li><span class="ds-label">ID: </span>ops-managing-orchestration</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#configure-heat"><span class="number">12.1 </span><span class="name">Configuring the Orchestration Service</span></a></span></dt><dt><span class="section"><a href="#topic-sqg-cvb-dx"><span class="number">12.2 </span><span class="name">Autoscaling using the Orchestration Service</span></a></span></dt><dt><span class="section"><a href="#LBaaSheat"><span class="number">12.3 </span><span class="name">Orchestration Service support for LBaaS v2</span></a></span></dt></dl></div></div><p>
  Information about managing and configuring the Orchestration service, based
  on OpenStack heat.
 </p><div class="sect1" id="configure-heat"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Orchestration Service</span> <a title="Permalink" class="permalink" href="#configure-heat">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-orchestration-configure_orchestration.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-orchestration-configure_orchestration.xml</li><li><span class="ds-label">ID: </span>configure-heat</li></ul></div></div></div></div><p>
  Information about configuring the Orchestration service, based on OpenStack
  heat.
 </p><p>
  The Orchestration service, based on OpenStack heat, does not need any
  additional configuration to be used. This documenent describes some
  configuration options as well as reasons you may want to use them.
 </p><p>
  <span class="bold"><strong>heat Stack Tag Feature</strong></span>
 </p><p>
  heat provides a feature called Stack Tags to allow attributing a set of
  simple string-based tags to stacks and optionally the ability to hide stacks
  with certain tags by default. This feature can be used for behind-the-scenes
  orchestration of cloud infrastructure, without exposing the cloud user to the
  resulting automatically-created stacks.
 </p><p>
  Additional details can be seen here:
  <a class="link" href="https://specs.openstack.org/openstack/heat-specs/specs/kilo/stack-tags.html" target="_blank">OpenStack
  - Stack Tags</a>.
 </p><p>
  In order to use the heat stack tag feature, you need to use the following
  steps to define the <code class="literal">hidden_stack_tags</code> setting in the heat
  configuration file and then reconfigure the service to enable the feature.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="listitem "><p>
    Edit the heat configuration file, at this location:
   </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/heat/heat.conf.j2</pre></div></li><li class="listitem "><p>
    Under the <code class="literal">[DEFAULT]</code> section, add a line for
    <code class="literal">hidden_stack_tags</code>. Example:
   </p><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
hidden_stack_tags="&lt;hidden_tag&gt;"</pre></div></li><li class="listitem "><p>
    Commit the changes to your local git:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add --all
<code class="prompt user">ardana &gt; </code>git commit -m "enabling heat Stack Tag feature"</pre></div></li><li class="listitem "><p>
    Run the configuration processor:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
    Update your deployment directory:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
    Reconfigure the Orchestration service:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts heat-reconfigure.yml</pre></div></li></ol></div><p>
  To begin using the feature, use these steps to create a heat stack using the
  defined hidden tag. You will need to use credentials that have the heat admin
  permissions. In the example steps below we are going to do this from the
  Cloud Lifecycle Manager using the <code class="literal">admin</code> credentials and a heat
  template named <code class="literal">heat.yaml</code>:
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="listitem "><p>
    Source the admin credentials:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc</pre></div></li><li class="listitem "><p>
    Create a heat stack using this feature:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack stack create -f heat.yaml hidden_stack_tags --tags hidden</pre></div></li><li class="listitem "><p>
    If you list your heat stacks, your hidden one will not show unless you use
    the <code class="literal">--hidden</code> switch.
   </p><p>
    Example, not showing hidden stacks:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack stack list</pre></div><p>
    Example, showing the hidden stacks:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack stack list --hidden</pre></div></li></ol></div></div><div class="sect1" id="topic-sqg-cvb-dx"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Autoscaling using the Orchestration Service</span> <a title="Permalink" class="permalink" href="#topic-sqg-cvb-dx">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-orchestration-autoscaling.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-orchestration-autoscaling.xml</li><li><span class="ds-label">ID: </span>topic-sqg-cvb-dx</li></ul></div></div></div></div><p>
  Autoscaling is a process that can be used to scale up and down your compute
  resources based on the load they are currently experiencing to ensure a
  balanced load.
 </p><div class="sect2" id="idg-all-operations-orchestration-autoscaling-xml-4"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">What is autoscaling?</span> <a title="Permalink" class="permalink" href="#idg-all-operations-orchestration-autoscaling-xml-4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-orchestration-autoscaling.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-orchestration-autoscaling.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-orchestration-autoscaling-xml-4</li></ul></div></div></div></div><p>
   Autoscaling is a process that can be used to scale up and down your compute
   resources based on the load they are currently experiencing to ensure a
   balanced load across your compute environment.
  </p><div id="id-1.5.14.4.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    Autoscaling is only supported for KVM.
   </p></div></div><div class="sect2" id="use"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How does autoscaling work?</span> <a title="Permalink" class="permalink" href="#use">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-orchestration-autoscaling.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-orchestration-autoscaling.xml</li><li><span class="ds-label">ID: </span>use</li></ul></div></div></div></div><p>
   The monitoring service, monasca, monitors your infrastructure resources and
   generates alarms based on their state. The Orchestration service, heat,
   talks to the monasca API and offers the capability to templatize the
   existing monasca resources, which are the monasca Notification and monasca
   Alarm definition. heat can configure certain alarms for the infrastructure
   resources (compute instances and block storage volumes) it creates and can
   expect monasca to notify continuously if a certain evaluation pattern in an
   alarm definition is met.
  </p><p>
   For example, heat can tell monasca that it needs an alarm generated if the
   average CPU utilization of the compute instance in a scaling group goes
   beyond 90%.
  </p><p>
   As monasca continuously monitors all the resources in the cloud, if it
   happens to see a compute instance spiking above 90% load as configured by
   heat, it generates an alarm and in turn sends a notification to heat. Once
   heat is notified, it will execute an action that was preconfigured in the
   template. Commonly, this action will be a scale up to increase the number of
   compute instances to balance the load that is being taken by the compute
   instance scaling group.
  </p><p>
   monasca sends a notification every 60 seconds while the alarm is in the
   ALARM state.
  </p></div><div class="sect2" id="id-1.5.14.4.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Autoscaling template example</span> <a title="Permalink" class="permalink" href="#id-1.5.14.4.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-orchestration-autoscaling.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-orchestration-autoscaling.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following monasca alarm definition template snippet is an example of
   instructing monasca to generate an alarm if the average CPU utilization in a
   group of compute instances exceeds beyond 50%. If the alarm is triggered, it
   will invoke the <code class="literal">up_notification</code> webhook once the alarm
   evaluation expression is satisfied.
  </p><div class="verbatim-wrap"><pre class="screen">cpu_alarm_high:
  type: OS::monasca::AlarmDefinition
  properties:
    name: CPU utilization beyond 50 percent
    description: CPU utilization reached beyond 50 percent
    expression:
    str_replace:
    template: avg(cpu.utilization_perc{scale_group=scale_group_id}) &gt; 50 times 3
    params:
    scale_group_id: {get_param: "OS::stack_id"}
    severity: high
    alarm_actions:
      - {get_resource: up_notification }</pre></div><p>
   The following monasca notification template snippet is an example of
   creating a monasca notification resource that will be used by the alarm
   definition snippet to notify heat.
  </p><div class="verbatim-wrap"><pre class="screen">up_notification:
  type: OS::monasca::Notification
  properties:
    type: webhook
    address: {get_attr: [scale_up_policy, alarm_url]}</pre></div></div><div class="sect2" id="id-1.5.14.4.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">monasca Agent configuration options</span> <a title="Permalink" class="permalink" href="#id-1.5.14.4.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-orchestration-autoscaling.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-orchestration-autoscaling.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   There is a monasca Agent configuration option which controls the behavior
   around compute instance creation and the measurements being received from
   the compute instance.
  </p><p>
   The variable is <code class="literal">monasca_libvirt_vm_probation</code> which is set
   in the
   <code class="literal">~/openstack/my_cloud/config/nova/libvirt-monitoring.yml</code>
   file. Here is a snippet of the file showing the description and variable:
  </p><div class="verbatim-wrap"><pre class="screen"># The period of time (in seconds) in which to suspend metrics from a
# newly-created VM. This is used to prevent creating and storing
# quickly-obsolete metrics in an environment with a high amount of instance
# churn (VMs created and destroyed in rapid succession).  Setting to 0
# disables VM probation and metrics will be recorded as soon as possible
# after a VM is created.  Decreasing this value in an environment with a high
# amount of instance churn can have a large effect on the total number of
# metrics collected and increase the amount of CPU, disk space and network
# bandwidth required for monasca. This value may need to be decreased if
# heat Autoscaling is in use so that heat knows that a new VM has been
# created and is handling some of the load.
monasca_libvirt_vm_probation: 300</pre></div><p>
   The default value is <code class="literal">300</code>. This is the time in seconds
   that a compute instance must live before the monasca libvirt agent plugin
   will send measurements for it. This is so that the monasca metrics database
   does not fill with measurements from short lived compute instances. However,
   this means that the monasca threshold engine will not see measurements from
   a newly created compute instance for at least five minutes on scale up. If
   the newly created compute instance is able to start handling the load in
   less than five minutes, then heat autoscaling may mistakenly create another
   compute instance since the alarm does not clear.
  </p><p>
   If the default <code class="literal">monasca_libvirt_vm_probation</code> turns out to
   be an issue, it can be lowered. However, that will affect all compute
   instances, not just ones used by heat autoscaling which can increase the
   number of measurements stored in monasca if there are many short lived
   compute instances. You should consider how often compute instances are
   created that live less than the new value of
   <code class="literal">monasca_libvirt_vm_probation</code>. If few, if any, compute
   instances live less than the value of
   <code class="literal">monasca_libvirt_vm_probation</code>, then this value can be
   decreased without causing issues. If many compute instances live less than
   the <code class="literal">monasca_libvirt_vm_probation</code> period, then decreasing
   <code class="literal">monasca_libvirt_vm_probation</code> can cause excessive disk,
   CPU and memory usage by monasca.
  </p><p>
   If you wish to change this value, follow these steps:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the <code class="literal">monasca_libvirt_vm_probation</code> value in this
     configuration file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/nova/libvirt-monitoring.yml</pre></div></li><li class="listitem "><p>
     Commit your changes to the local git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add --all
<code class="prompt user">ardana &gt; </code>git commit -m "changing monasca Agent configuration option"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run this playbook to reconfigure the nova service and enact your changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect1" id="LBaaSheat"><div class="titlepage"><div><div><h2 class="title"><span class="number">12.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Orchestration Service support for LBaaS v2</span> <a title="Permalink" class="permalink" href="#LBaaSheat">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-orchestration-lbaas.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-orchestration-lbaas.xml</li><li><span class="ds-label">ID: </span>LBaaSheat</li></ul></div></div></div></div><p>
   In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the Orchestration service provides support for LBaaS
   v2, which means users can create LBaaS v2 resources using Orchestration.
  </p><p>
   The OpenStack documentation for LBaaSv2 resource plugins is available at
   following locations.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     neutron LBaaS v2 LoadBalancer:
     <a class="link" href="http://docs.openstack.org/developer/heat/template_guide/openstack.html#OS::Neutron::LBaaS::LoadBalancer" target="_blank">http://docs.openstack.org/developer/heat/template_guide/openstack.html#OS::Neutron::LBaaS::LoadBalancer</a>
    </p></li><li class="listitem "><p>
     neutron LBaaS v2 Listener:
     <a class="link" href="http://docs.openstack.org/developer/heat/template_guide/openstack.html#OS::Neutron::LBaaS::Listener" target="_blank">http://docs.openstack.org/developer/heat/template_guide/openstack.html#OS::Neutron::LBaaS::Listener</a>
    </p></li><li class="listitem "><p>
     neutron LBaaS v2 Pool:
     <a class="link" href="http://docs.openstack.org/developer/heat/template_guide/openstack.html#OS::Neutron::LBaaS::Pool" target="_blank">http://docs.openstack.org/developer/heat/template_guide/openstack.html#OS::Neutron::LBaaS::Pool</a>
    </p></li><li class="listitem "><p>
     neutron LBaaS v2 Pool Member:
     <a class="link" href="http://docs.openstack.org/developer/heat/template_guide/openstack.html#OS::Neutron::LBaaS::PoolMember" target="_blank">http://docs.openstack.org/developer/heat/template_guide/openstack.html#OS::Neutron::LBaaS::PoolMember</a>
    </p></li><li class="listitem "><p>
     neutron LBaaS v2 Health Monitor:
     <a class="link" href="http://docs.openstack.org/developer/heat/template_guide/openstack.html#OS::Neutron::LBaaS::HealthMonitor" target="_blank">http://docs.openstack.org/developer/heat/template_guide/openstack.html#OS::Neutron::LBaaS::HealthMonitor</a>
    </p></li></ul></div><div class="sect2" id="loadbalancer-limitations"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="#loadbalancer-limitations">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-orchestration-lbaas.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-orchestration-lbaas.xml</li><li><span class="ds-label">ID: </span>loadbalancer-limitations</li></ul></div></div></div></div><p>
   In order to avoid stack-create timeouts when using load balancers, it is
   recommended that no more than 100 load balancers be created at a time using
   stack-create loops. Larger numbers of load balancers could reach quotas
   and/or exhaust resources resulting in the stack create-timeout.
  </p></div><div class="sect2" id="idg-all-userguide-lbaas-heat-xml-8"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">More Information</span> <a title="Permalink" class="permalink" href="#idg-all-userguide-lbaas-heat-xml-8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-orchestration-lbaas.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-orchestration-lbaas.xml</li><li><span class="ds-label">ID: </span>idg-all-userguide-lbaas-heat-xml-8</li></ul></div></div></div></div><p>
   For more information on the neutron command-line interface (CLI) and load
   balancing, see the OpenStack networking command-line client reference:
   <a class="link" href="http://docs.openstack.org/cli-reference/content/neutronclient_commands.html" target="_blank">http://docs.openstack.org/cli-reference/content/neutronclient_commands.html</a>
  </p><p>
   For more information on heat see:
   <a class="link" href="http://docs.openstack.org/developer/heat" target="_blank">http://docs.openstack.org/developer/heat</a>
  </p></div></div></div><div class="chapter " id="topic-ttn-5fg-4v"><div class="titlepage"><div><div><h1 class="title"><span class="number">13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Monitoring, Logging, and Usage Reporting</span> <a title="Permalink" class="permalink" href="#topic-ttn-5fg-4v">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-managing_telemetry.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_telemetry.xml</li><li><span class="ds-label">ID: </span>topic-ttn-5fg-4v</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#mon"><span class="number">13.1 </span><span class="name">Monitoring</span></a></span></dt><dt><span class="section"><a href="#centralized-logging"><span class="number">13.2 </span><span class="name">Centralized Logging Service</span></a></span></dt><dt><span class="section"><a href="#ceilo-metering-overview"><span class="number">13.3 </span><span class="name">Metering Service (ceilometer) Overview</span></a></span></dt></dl></div></div><p>
  Information about the monitoring, logging, and metering services included
  with your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p><div class="sect1" id="mon"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring</span> <a title="Permalink" class="permalink" href="#mon">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring.xml</li><li><span class="ds-label">ID: </span>mon</li></ul></div></div></div></div><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Monitoring service leverages OpenStack monasca, which is a
  multi-tenant, scalable, fault tolerant monitoring service.
 </p><div class="sect2" id="monitoring-service"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Getting Started with Monitoring</span> <a title="Permalink" class="permalink" href="#monitoring-service">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring_service.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring_service.xml</li><li><span class="ds-label">ID: </span>monitoring-service</li></ul></div></div></div></div><p>
  You can use the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Monitoring service to monitor the health of your
  cloud and, if necessary, to troubleshoot issues.
 </p><p>
  monasca data can be extracted and used for a variety of legitimate purposes,
  and different purposes require different forms of data sanitization or
  encoding to protect against invalid or malicious data. Any data pulled from
  monasca should be considered untrusted data, so users are advised to apply
  appropriate encoding and/or sanitization techniques to ensure safe and
  correct usage and display of data in a web browser, database scan, or any
  other use of the data.
 </p><div class="sect3" id="idg-all-operations-monitoring-monitoring-overview-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring Service Overview</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-monitoring-overview-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-monitoring_overview.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monitoring_overview.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-monitoring-overview-xml-1</li></ul></div></div></div></div><div class="sect4" id="idg-all-operations-monitoring-monitoring-overview-xml-2"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installation</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-monitoring-overview-xml-2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-monitoring_overview.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monitoring_overview.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-monitoring-overview-xml-2</li></ul></div></div></div></div><p>
   The monitoring service is automatically installed as part of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   installation.
  </p><p>
   No specific configuration is required to use monasca. However, you can
   configure the database for storing metrics as explained in
   <a class="xref" href="#configure-monitoring" title="13.1.2. Configuring the Monitoring Service">Section 13.1.2, “Configuring the Monitoring Service”</a>.
  </p></div><div class="sect4" id="differences"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Differences Between Upstream and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Implementations</span> <a title="Permalink" class="permalink" href="#differences">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-monitoring_overview.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monitoring_overview.xml</li><li><span class="ds-label">ID: </span>differences</li></ul></div></div></div></div><p>
   In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the OpenStack monitoring service, monasca,
   is included as the monitoring solution, except for the following which are
   not included:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Transform Engine
    </p></li><li class="listitem "><p>
     Events Engine
    </p></li><li class="listitem "><p>
     Anomaly and Prediction Engine
    </p></li></ul></div><div id="id-1.5.15.3.3.4.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    Icinga was supported in previous <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> versions but it has been
    deprecated in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9.
   </p></div></div><div class="sect4" id="idg-all-operations-monitoring-monitoring-overview-xml-4"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Diagram of monasca Service</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-monitoring-overview-xml-4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-monitoring_overview.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monitoring_overview.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-monitoring-overview-xml-4</li></ul></div></div></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-monasca_diagram.png" target="_blank"><img src="images/media-monasca_diagram.png" width="" /></a></div></div></div><div class="sect4" id="idg-all-operations-monitoring-monitoring-overview-xml-5"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-monitoring-overview-xml-5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-monitoring_overview.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monitoring_overview.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-monitoring-overview-xml-5</li></ul></div></div></div></div><p>
   For more details on OpenStack monasca, see
   <a class="link" href="http://monasca.io/" target="_blank">monasca.io</a>
  </p></div><div class="sect4" id="id-1.5.15.3.3.4.6"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Back-end Database</span> <a title="Permalink" class="permalink" href="#id-1.5.15.3.3.4.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-monitoring_overview.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monitoring_overview.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The monitoring service default metrics database is Cassandra, which is a
   highly-scalable analytics database and the recommended database for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  </p><p>
   You can learn more about Cassandra at <a class="link" href="http://cassandra.apache.org/" target="_blank">Apache Cassandra</a>.
  </p></div></div><div class="sect3" id="working-monitoring"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Working with Monasca</span> <a title="Permalink" class="permalink" href="#working-monitoring">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-working_monitoring.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-working_monitoring.xml</li><li><span class="ds-label">ID: </span>working-monitoring</li></ul></div></div></div></div><p>
  <span class="bold"><strong>monasca-Agent</strong></span>
 </p><p>
  The <span class="bold"><strong>monasca-agent</strong></span> is a Python program that
  runs on the control plane nodes. It runs the defined checks and then sends
  data onto the API. The checks that the agent runs include:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    System Metrics: CPU utilization, memory usage, disk I/O, network I/O, and
    filesystem utilization on the control plane and resource nodes.
   </p></li><li class="listitem "><p>
    Service Metrics: the agent supports plugins such as MySQL, RabbitMQ, Kafka,
    and many others.
   </p></li><li class="listitem "><p>
    VM Metrics: CPU utilization, disk I/O, network I/O, and memory usage of
    hosted virtual machines on compute nodes. Full details of these can be
    found
    <a class="link" href="https://github.com/openstack/monasca-agent/blob/master/docs/Plugins.md#per-instance-metrics" target="_blank">https://github.com/openstack/monasca-agent/blob/master/docs/Plugins.md#per-instance-metrics</a>.
   </p></li></ul></div><p>
  For a full list of packaged plugins that are included <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, see
  <a class="link" href="https://github.com/stackforge/monasca-agent/blob/master/docs/Plugins.md" target="_blank">monasca
  Plugins</a>
 </p><p>
  You can further customize the monasca-agent to suit your needs, see
  <a class="link" href="https://github.com/stackforge/monasca-agent/blob/master/docs/Customizations.md" target="_blank">Customizing
  the Agent</a>
 </p></div><div class="sect3" id="accessing-monitoring"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Accessing the Monitoring Service</span> <a title="Permalink" class="permalink" href="#accessing-monitoring">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-accessing_monitoring.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-accessing_monitoring.xml</li><li><span class="ds-label">ID: </span>accessing-monitoring</li></ul></div></div></div></div><p>
  Access to the Monitoring service is available through a number of different
  interfaces.
 </p><div class="sect4" id="id-1.5.15.3.3.6.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Command-Line Interface</span> <a title="Permalink" class="permalink" href="#id-1.5.15.3.3.6.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-accessing_monitoring.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-accessing_monitoring.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   For users who prefer using the command line, there is the
   python-monascaclient, which is part of the default installation on your
   Cloud Lifecycle Manager node.
  </p><p>
   For details on the CLI, including installation instructions, see
   <a class="link" href="https://github.com/stackforge/python-monascaclient/blob/master/README.rst" target="_blank">Python-monasca
   Client</a>
  </p><p>
   <span class="bold"><strong>monasca API</strong></span>
  </p><p>
   If low-level access is desired, there is the monasca REST API.
  </p><p>
   Full details of the monasca API can be found
   <a class="link" href="https://github.com/stackforge/monasca-api/blob/master/docs/monasca-api-spec.md" target="_blank">on
   GitHub</a>.
  </p></div><div class="sect4" id="idg-all-operations-monitoring-accessing-monitoring-xml-2"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Operations Console GUI</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-accessing-monitoring-xml-2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-accessing_monitoring.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-accessing_monitoring.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-accessing-monitoring-xml-2</li></ul></div></div></div></div><p>
   You can use the Operations Console (Ops Console) for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to view
   data about your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud infrastructure in a web-based graphical user
   interface (GUI) and ensure your cloud is operating correctly. By logging on
   to the console, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> administrators can manage data in the following
   ways: <span class="bold"><strong>Triage alarm notifications.</strong></span>
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Alarm Definitions and notifications now have their own screens and are
     collected under the <span class="bold"><strong>Alarm Explorer</strong></span> menu
     item which can be accessed from the Central Dashboard. Central Dashboard
     now allows you to customize the view in the following ways:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Rename or re-configure existing alarm cards to include services
       different from the defaults
      </p></li><li class="listitem "><p>
       Create a new alarm card with the services you want to select
      </p></li><li class="listitem "><p>
       Reorder alarm cards using drag and drop
      </p></li><li class="listitem "><p>
       View all alarms that have no service dimension now grouped in an
       <span class="bold"><strong>Uncategorized Alarms</strong></span> card
      </p></li><li class="listitem "><p>
       View all alarms that have a service dimension that does not match any of
       the other cards -now grouped in an <span class="bold"><strong>Other
       Alarms</strong></span> card
      </p></li></ul></div></li><li class="listitem "><p>
     You can also easily access alarm data for a specific component. On the
     Summary page for the following components, a link is provided to an alarms
     screen specifically for that component.
    </p></li></ul></div></div><div class="sect4" id="idg-all-operations-monitoring-accessing-monitoring-xml-3"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.1.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Connecting to the Operations Console</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-accessing-monitoring-xml-3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-accessing_monitoring.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-accessing_monitoring.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-accessing-monitoring-xml-3</li></ul></div></div></div></div><p>
   To connect to Operations Console, perform the following:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Ensure your login has the required access credentials.
    </p></li><li class="listitem "><p>
     Connect through a browser.
    </p></li><li class="listitem "><p>
     Optionally use a Host name OR virtual IP address to access Operations Console.
    </p></li></ul></div><p>
   Operations Console will always be accessed over port 9095.
  </p></div></div><div class="sect3" id="alarms-monitoring"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Service Alarm Definitions</span> <a title="Permalink" class="permalink" href="#alarms-monitoring">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-alarms_monitoring.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-alarms_monitoring.xml</li><li><span class="ds-label">ID: </span>alarms-monitoring</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> comes with some predefined monitoring alarms for the services
  installed.
 </p><p>
  Full details of all service alarms can be found here:
  <a class="xref" href="#alarmdefinitions" title="18.1.1. Alarm Resolution Procedures">Section 18.1.1, “Alarm Resolution Procedures”</a>.
 </p><p>
  Each alarm will have one of the following statuses:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <span class="guimenu ">Critical</span> - Open alarms, identified by red indicator.
   </p></li><li class="listitem "><p>
    <span class="guimenu ">Warning</span> - Open alarms, identified by yellow indicator.
   </p></li><li class="listitem "><p>
    <span class="guimenu ">Unknown</span> - Open alarms, identified by gray indicator.
    Unknown will be the status of an alarm that has stopped receiving a metric.
    This can be caused by the following conditions:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      An alarm exists for a service or component that is not installed in the
      environment.
     </p></li><li class="listitem "><p>
      An alarm exists for a virtual machine or node that previously existed but
      has been removed without the corresponding alarms being removed.
     </p></li><li class="listitem "><p>
      There is a gap between the last reported metric and the next metric.
     </p></li></ul></div></li><li class="listitem "><p>
    <span class="guimenu ">Open</span> - Complete list of open alarms.
   </p></li><li class="listitem "><p>
    <span class="guimenu ">Total</span> - Complete list of alarms, may include
    Acknowledged and Resolved alarms.
   </p></li></ul></div><p>
  When alarms are triggered it is helpful to review the service logs.
 </p></div></div><div class="sect2" id="configure-monitoring"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Monitoring Service</span> <a title="Permalink" class="permalink" href="#configure-monitoring">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-configure_monitoring.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-configure_monitoring.xml</li><li><span class="ds-label">ID: </span>configure-monitoring</li></ul></div></div></div></div><p>
  The monitoring service, based on monasca, allows you to configure an external
  SMTP server for email notifications when alarms trigger. You also have
  options for your alarm metrics database should you choose not to use the
  default option provided with the product.
 </p><p>
  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> you have the option to specify a SMTP server for email
  notifications and a database platform you want to use for the metrics
  database. These steps will assist in this process.
 </p><div class="sect3" id="config-mon-notemail"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Monitoring Email Notification Settings</span> <a title="Permalink" class="permalink" href="#config-mon-notemail">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-config_mon_notemail.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-config_mon_notemail.xml</li><li><span class="ds-label">ID: </span>config-mon-notemail</li></ul></div></div></div></div><p>
  The monitoring service, based on monasca, allows you to configure an external
  SMTP server for email notifications when alarms trigger. In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, you
  have the option to specify a SMTP server for email notifications. These steps
  will assist in this process.
 </p><p>
  If you are going to use the email notifiication feature of the monitoring
  service, you must set the configuration options with valid email settings
  including an SMTP server and valid email addresses. The email server is not
  provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, but must be specified in the configuration file
  described below. The email server must support SMTP.
 </p><div class="sect4" id="email"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring monitoring notification settings during initial installation</span> <a title="Permalink" class="permalink" href="#email">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-config_mon_notemail.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-config_mon_notemail.xml</li><li><span class="ds-label">ID: </span>email</li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     To change the SMTP server configuration settings edit the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/cloudConfig.yml</pre></div><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       Enter your email server settings. Here is an example snippet showing the
       configuration file contents, uncomment these lines before entering your
       environment details.
      </p><div class="verbatim-wrap"><pre class="screen">    smtp-settings:
    #  server: mailserver.examplecloud.com
    #  port: 25
    #  timeout: 15
    # These are only needed if your server requires authentication
    #  user:
    #  password:</pre></div><p>
       This table explains each of these values:
      </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Value</th><th>Description</th></tr></thead><tbody><tr><td>Server (required)</td><td>
           <p>
            The server entry must be uncommented and set to a valid hostname or
            IP Address.
           </p>
          </td></tr><tr><td>Port (optional)</td><td>
           <p>
            If your SMTP server is running on a port other than the standard
            25, then uncomment the port line and set it your port.
           </p>
          </td></tr><tr><td>Timeout (optional)</td><td>
           <p>
            If your email server is heavily loaded, the timeout parameter can
            be uncommented and set to a larger value. 15 seconds is the
            default.
           </p>
          </td></tr><tr><td>User / Password (optional)</td><td>
           <p>
            If your SMTP server requires authentication, then you can configure
            user and password. Use double quotes around the password to avoid
            issues with special characters.
           </p>
          </td></tr></tbody></table></div></li></ol></div></li><li class="listitem "><p>
     To configure the sending email addresses, edit the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/monasca-notification/defaults/main.yml</pre></div><p>
     Modify the following value to add your sending email address:
    </p><div class="verbatim-wrap"><pre class="screen">email_from_addr</pre></div><div id="id-1.5.15.3.4.4.4.2.3.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The default value in the file is <code class="literal">email_from_address:
      notification@exampleCloud.com</code> which you should edit.
     </p></div></li><li class="listitem "><p>
     [optional] To configure the receiving email addresses, edit the following
     file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/monasca-default-alarms/defaults/main.yml</pre></div><p>
     Modify the following value to configure a receiving email address:
    </p><div class="verbatim-wrap"><pre class="screen">notification_address</pre></div><div id="id-1.5.15.3.4.4.4.2.4.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      You can also set the receiving email address via the Operations Console.
      Instructions for this are in the last section.
     </p></div></li><li class="listitem "><p>
     If your environment requires a proxy address then you can add that in as
     well:
    </p><div class="verbatim-wrap"><pre class="screen"># notification_environment can be used to configure proxies if needed.
# Below is an example configuration. Note that all of the quotes are required.
# notification_environment: '"http_proxy=http://&lt;your_proxy&gt;:&lt;port&gt;" "https_proxy=http://&lt;your_proxy&gt;:&lt;port&gt;"'
<span class="bold"><strong>notification_environment: ''</strong></span></pre></div></li><li class="listitem "><p>
     Commit your configuration to the local Git repository (see
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Updated monitoring service email notification settings"</pre></div></li><li class="listitem "><p>
     Continue with your installation.
    </p></li></ol></div></div><div class="sect4" id="apache-commons-validate"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monasca and Apache Commons Validator</span> <a title="Permalink" class="permalink" href="#apache-commons-validate">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-config_mon_notemail.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-config_mon_notemail.xml</li><li><span class="ds-label">ID: </span>apache-commons-validate</li></ul></div></div></div></div><p>
   The monasca notification uses a standard Apache Commons validator to
   validate the configured <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> domain names before sending the
   notification over webhook. monasca notification supports some non-standard
   domain names, but not all. See the Domain Validator documentation for more
   information:
   <a class="link" href="https://commons.apache.org/proper/commons-validator/apidocs/org/apache/commons/validator/routines/DomainValidator.html" target="_blank">https://commons.apache.org/proper/commons-validator/apidocs/org/apache/commons/validator/routines/DomainValidator.html</a>
  </p><p>
   You should ensure that any domains that you use are supported by IETF and
   IANA. As an example, <span class="bold"><strong>.local</strong></span> is not listed
   by IANA and is invalid but <span class="bold"><strong>.gov</strong></span> and
   <span class="bold"><strong>.edu</strong></span> are valid.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Internet Assigned Numbers Authority (IANA):
     <a class="link" href="https://www.iana.org/domains/root/db" target="_blank">https://www.iana.org/domains/root/db</a>
    </p></li></ul></div><p>
   Failure to use supported domains will generate an unprocessable exception in
   monasca notification create:
  </p><div class="verbatim-wrap"><pre class="screen">HTTPException code=422 message={"unprocessable_entity":
{"code":422,"message":"Address https://myopenstack.sample:8000/v1/signal/test is not of correct format","details":"","internal_code":"c6cf9d9eb79c3fc4"}</pre></div></div><div class="sect4" id="id-1.5.15.3.4.4.6"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring monitoring notification settings after the initial installation</span> <a title="Permalink" class="permalink" href="#id-1.5.15.3.4.4.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-config_mon_notemail.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-config_mon_notemail.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If you need to make changes to the email notification settings after your
   initial deployment, you can change the "From" address using the
   configuration files but the "To" address will need to be changed in the
   Operations Console. The following section will describe both of these
   processes.
  </p><p>
   <span class="bold"><strong>To change the sending email address:</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     To configure the sending email addresses, edit the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/monasca-notification/defaults/main.yml</pre></div><p>
     Modify the following value to add your sending email address:
    </p><div class="verbatim-wrap"><pre class="screen">email_from_addr</pre></div><div id="id-1.5.15.3.4.4.6.4.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The default value in the file is <code class="literal">email_from_address:
      notification@exampleCloud.com</code> which you should edit.
     </p></div></li><li class="listitem "><p>
     Commit your configuration to the local Git repository
     (<span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Updated monitoring service email notification settings"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the monasca reconfigure playbook to deploy the changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-reconfigure.yml --tags notification</pre></div><div id="id-1.5.15.3.4.4.6.4.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      You may need to use the <code class="literal">--ask-vault-pass</code> switch if you
      opted for encryption during the initial deployment.
     </p></div></li></ol></div><p>
   <span class="bold"><strong>To change the receiving email address via the
   Operations Console:</strong></span>
  </p><p>
   To configure the "To" email address, after installation,
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Connect to and log in to the Operations Console.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Home</strong></span> screen, click the menu
     represented by 3 horizontal lines (<span class="inlinemediaobject"><img xmlns="" src="images/media-opsconsole-OpsConsoleBurgerMenu.png" width="" alt="Three-Line Icon" /></span>).
    </p></li><li class="listitem "><p>
     From the menu that slides in on the left side, click
     <span class="bold"><strong>Home</strong></span>, and then
     <span class="bold"><strong>Alarm Explorer</strong></span>.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Alarm Explorer</strong></span> page, at the top,
     click the <span class="bold"><strong>Notification Methods</strong></span> text.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Notification Methods</strong></span> page, find
     the row with the <span class="bold"><strong>Default Email</strong></span>
     notification.
    </p></li><li class="listitem "><p>
     In the <span class="bold"><strong>Default Email</strong></span> row, click the
     details icon (<span class="inlinemediaobject"><img xmlns="" src="images/media-opsconsole-DetailDots.png" width="" alt="Ellipsis Icon" /></span>), then click
     <span class="bold"><strong>Edit</strong></span>.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Edit Notification Method: Default
     Email</strong></span> page, in <span class="bold"><strong>Name</strong></span>,
     <span class="bold"><strong>Type</strong></span>, and
     <span class="bold"><strong>Address/Key</strong></span>, type in the values you want
     to use.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Edit Notification Method: Default
     Email</strong></span> page, click <span class="bold"><strong>Update
     Notification</strong></span>.
    </p></li></ol></div><div id="id-1.5.15.3.4.4.6.8" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    Once the notification has been added, using the procedures using the
    Ansible playbooks will not change it.
   </p></div></div></div><div class="sect3" id="manage-note-methods"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Notification Methods for Alarms</span> <a title="Permalink" class="permalink" href="#manage-note-methods">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-managing_notificationmethods.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_notificationmethods.xml</li><li><span class="ds-label">ID: </span>manage-note-methods</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="#proxy" title="13.1.2.2.1. Enabling a Proxy for Webhook or Pager Duty Notifications">Section 13.1.2.2.1, “Enabling a Proxy for Webhook or Pager Duty Notifications”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#create" title="13.1.2.2.2. Creating a New Notification Method">Section 13.1.2.2.2, “Creating a New Notification Method”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#apply" title="13.1.2.2.3. Applying a Notification Method to an Alarm Definition">Section 13.1.2.2.3, “Applying a Notification Method to an Alarm Definition”</a>
   </p></li></ul></div><div class="sect4" id="proxy"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling a Proxy for Webhook or Pager Duty Notifications</span> <a title="Permalink" class="permalink" href="#proxy">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-managing_notificationmethods.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_notificationmethods.xml</li><li><span class="ds-label">ID: </span>proxy</li></ul></div></div></div></div><p>
   If your environment requires a proxy in order for communications to function
   then these steps will show you how you can enable one. These steps will only
   be needed if you are utilizing the webhook or pager duty notification
   methods.
  </p><p>
   These steps will require access to the Cloud Lifecycle Manager in your cloud
   deployment so you may need to contact your Administrator. You can make these
   changes during the initial configuration phase prior to the first
   installation or you can modify your existing environment, the only
   difference being the last step.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Edit the
     <code class="literal">~/openstack/ardana/ansible/roles/monasca-notification/defaults/main.yml</code>
     file and edit the line below with your proxy address values:
    </p><div class="verbatim-wrap"><pre class="screen">notification_environment: '"http_proxy=http://&lt;proxy_address&gt;:&lt;port&gt;" "https_proxy=&lt;http://proxy_address&gt;:&lt;port&gt;"'</pre></div><div id="id-1.5.15.3.4.5.3.4.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      There are single quotation marks around the entire value of this entry and
      then double quotation marks around the individual proxy entries. This
      formatting must exist when you enter these values into your configuration
      file.
     </p></div></li><li class="step "><p>
     If you are making these changes prior to your initial installation then
     you are done and can continue on with the installation. However, if you
     are modifying an existing environment, you will need to continue on with
     the remaining steps below.
    </p></li><li class="step "><p>
     Commit your configuration to the local Git repository (see
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Generate an updated deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the monasca reconfigure playbook to enable these changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-reconfigure.yml --tags notification</pre></div></li></ol></div></div></div><div class="sect4" id="create"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a New Notification Method</span> <a title="Permalink" class="permalink" href="#create">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-managing_notificationmethods.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_notificationmethods.xml</li><li><span class="ds-label">ID: </span>create</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Operations Console.
    </p></li><li class="step "><p>
     Use the navigation menu to go to the <span class="bold"><strong>Alarm
     Explorer</strong></span> page:
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-opsconsole_createnotificationmethod.png" target="_blank"><img src="images/media-hos.docs-opsconsole_createnotificationmethod.png" width="" /></a></div></div></li><li class="step "><p>
     Select the <span class="bold"><strong>Notification Methods</strong></span> menu and
     then click the <span class="bold"><strong>Create Notification Method</strong></span>
     button:
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-opsconsole_createnotificationmethod1.png" target="_blank"><img src="images/media-hos.docs-opsconsole_createnotificationmethod1.png" width="" /></a></div></div></li><li class="step "><p>
     On the <span class="bold"><strong>Create Notification Method</strong></span> window
     you will select your options and then click the
     <span class="bold"><strong>Create Notification</strong></span> button.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-opsconsole_createnotificationmethod3.png" target="_blank"><img src="images/media-hos.docs-opsconsole_createnotificationmethod3.png" width="" /></a></div></div><p>
     A description of each of the fields you use for each notification method:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td>Name</td><td><p>Enter a unique name value for the notification method you are creating.</p></td></tr><tr><td>Type</td><td><p>Choose a type. Available values are <span class="bold"><strong>Webhook</strong></span>, <span class="bold"><strong>Email</strong></span>, or <span class="bold"><strong>Pager Duty</strong></span>.</p></td></tr><tr><td>Address/Key</td><td>Enter the value corresponding to the type you chose.</td></tr></tbody></table></div></li></ol></div></div></div><div class="sect4" id="apply"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Applying a Notification Method to an Alarm Definition</span> <a title="Permalink" class="permalink" href="#apply">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-managing_notificationmethods.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_notificationmethods.xml</li><li><span class="ds-label">ID: </span>apply</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Operations Console.
    </p></li><li class="step "><p>
     Use the navigation menu to go to the <span class="bold"><strong>Alarm
     Explorer</strong></span> page:
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-opsconsole_createnotificationmethod.png" target="_blank"><img src="images/media-hos.docs-opsconsole_createnotificationmethod.png" width="" /></a></div></div></li><li class="step "><p>
     Select the <span class="bold"><strong>Alarm Definition</strong></span> menu which
     will give you a list of each of the alarm definitions in your environment.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-opsconsole_createnotificationmethod4.png" target="_blank"><img src="images/media-hos.docs-opsconsole_createnotificationmethod4.png" width="" /></a></div></div></li><li class="step "><p>
     Locate the alarm you want to change the notification method for and click
     on its name to bring up the edit menu. You can use the sorting methods for
     assistance.
    </p></li><li class="step "><p>
     In the edit menu, scroll down to the <span class="bold"><strong>Notifications
     and Severity</strong></span> section where you will select one or more
     <span class="bold"><strong>Notification Methods</strong></span> before selecting the
     <span class="bold"><strong>Update Alarm Definition</strong></span> button:
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-opsconsole_createnotificationmethod6.png" target="_blank"><img src="images/media-hos.docs-opsconsole_createnotificationmethod6.png" width="" /></a></div></div></li><li class="step "><p>
     Repeat as needed until all of your alarms have the notification methods
     you desire.
    </p></li></ol></div></div></div></div><div class="sect3" id="enable-rabbitmq-admin-console"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling the RabbitMQ Admin Console</span> <a title="Permalink" class="permalink" href="#enable-rabbitmq-admin-console">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-enable_mon_console.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-enable_mon_console.xml</li><li><span class="ds-label">ID: </span>enable-rabbitmq-admin-console</li></ul></div></div></div></div><p>
  The RabbitMQ Admin Console is off by default in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. You can turn on the
  console by following these steps:
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="listitem "><p>
    Edit the <code class="filename">~/openstack/my_cloud/config/rabbitmq/main.yml</code>
    file. Under the <code class="literal">rabbit_plugins:</code>line, uncomment
   </p><div class="verbatim-wrap"><pre class="screen">- rabbitmq_management</pre></div></li><li class="listitem "><p>
    Commit your configuration to the local Git repository (see
    <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>), as follows:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Enabled RabbitMQ Admin Console"</pre></div></li><li class="listitem "><p>
    Run the configuration processor:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
    Update your deployment directory:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
    Run the RabbitMQ reconfigure playbook to deploy the changes:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts rabbitmq-reconfigure.yml</pre></div></li></ol></div><p>
  To turn the RabbitMQ Admin Console off again, add the comment back and repeat
  steps 3 through 6.
  
 </p></div><div class="sect3" id="capacity-reporting-transform"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Capacity Reporting and Monasca Transform</span> <a title="Permalink" class="permalink" href="#capacity-reporting-transform">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-capacity_reporting_transform.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-capacity_reporting_transform.xml</li><li><span class="ds-label">ID: </span>capacity-reporting-transform</li></ul></div></div></div></div><p>
  Capacity reporting is a new feature in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> which will provide cloud
  operators overall capacity (available, used, and remaining) information via
  the Operations Console so that the cloud operator can ensure that cloud resource pools
  have sufficient capacity to meet the demands of users.  The cloud operator is
  also able to set thresholds and set alarms to be notified when the thresholds
  are reached.
 </p><p>
  <span class="bold"><strong>For Compute</strong></span>
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Host Capacity - CPU/Disk/Memory: Used, Available and Remaining Capacity -
    for the entire cloud installation or by host
   </p></li><li class="listitem "><p>
    VM Capacity - CPU/Disk/Memory: Allocated, Available and Remaining - for
    the entire cloud installation, by host or by project
   </p></li></ul></div><p>
  <span class="bold"><strong>For Object Storage</strong></span>
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Disk Capacity - Used, Available and Remaining Capacity - for the entire
    cloud installation or by project
   </p></li></ul></div><p>
  In addition to overall capacity, roll up views with appropriate slices provide
  views by a particular project, or compute node. Graphs also show trends and
  the change in capacity over time.
 </p><div class="sect4" id="crt-features"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">monasca Transform Features</span> <a title="Permalink" class="permalink" href="#crt-features">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-crt_features.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_features.xml</li><li><span class="ds-label">ID: </span>crt-features</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    monasca Transform is a new component in monasca which transforms and
    aggregates metrics using Apache Spark
   </p></li><li class="listitem "><p>
    Aggregated metrics are published to Kafka and are available for other
    monasca components like monasca-threshold and are stored in monasca
    datastore
   </p></li><li class="listitem "><p>
    Cloud operators can set thresholds and set alarms to receive notifications
    when thresholds are met.
   </p></li><li class="listitem "><p>
    These aggregated metrics are made available to the cloud operators via
    Operations Console's new Capacity Summary (reporting) UI
   </p></li><li class="listitem "><p>
    Capacity reporting is a new feature in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> which will provides
    cloud operators an overall capacity (available, used and remaining) for
    Compute and Object Storage
   </p></li><li class="listitem "><p>
    Cloud operators can look at Capacity reporting via Operations Console's Compute
    Capacity Summary and Object Storage Capacity Summary UI
   </p></li><li class="listitem "><p>
    Capacity reporting allows the cloud operators the ability to ensure that
    cloud resource pools have sufficient capacity to meet demands of users. See
    table below for Service and Capacity Types.
   </p></li><li class="listitem "><p>
    A list of aggregated metrics is provided in
    <a class="xref" href="#crt-aggregated-metrics" title="13.1.2.4.4. New Aggregated Metrics">Section 13.1.2.4.4, “New Aggregated Metrics”</a>.
   </p></li><li class="listitem "><p>
    Capacity reporting aggregated metrics are aggregated and published every
    hour
   </p></li><li class="listitem "><p>
    In addition to the overall capacity, there are graphs which show the
    capacity trends over time range (for 1 day, for 7 days, for 30 days or for
    45 days)
   </p></li><li class="listitem "><p>
    Graphs showing the capacity trends by a particular project or compute host
    are also provided.
   </p></li><li class="listitem "><p>
    monasca Transform is integrated with centralized monitoring (monasca) and
    centralized logging
   </p></li><li class="listitem "><p>
    Flexible Deployment
   </p></li><li class="listitem "><p>
    Upgrade &amp; Patch Support
   </p></li></ul></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Type of Capacity</th><th>Description</th></tr></thead><tbody><tr><td>Compute</td><td>Host Capacity</td><td>
      <p>
       CPU/Disk/Memory: Used, Available and Remaining Capacity - for entire
       cloud installation or by compute host
      </p>
     </td></tr><tr><td> </td><td>VM Capacity</td><td>
      <p>
       CPU/Disk/Memory: Allocated, Available and Remaining - for entire cloud
       installation, by host or by project
      </p>
     </td></tr><tr><td>Object Storage</td><td>Disk Capacity</td><td>
      <p>
       Used, Available and Remaining Disk Capacity - for entire cloud
       installation or by project
      </p>
     </td></tr><tr><td> </td><td>Storage Capacity</td><td>
      <p>
       Utilized Storage Capacity - for entire cloud installation or by project
      </p>
     </td></tr></tbody></table></div></div><div class="sect4" id="crt-arch-transform-spark"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Architecture for Monasca Transform and Spark</span> <a title="Permalink" class="permalink" href="#crt-arch-transform-spark">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-crt_arch_transform_spark.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_arch_transform_spark.xml</li><li><span class="ds-label">ID: </span>crt-arch-transform-spark</li></ul></div></div></div></div><p>
  monasca Transform is a new component in monasca. monasca Transform uses
  Spark for data aggregation. Both monasca Transform and Spark are depicted in
  the example diagram below.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-monasca-Monasca_Service_Arch_Diagram.png" target="_blank"><img src="images/media-monasca-Monasca_Service_Arch_Diagram.png" width="" /></a></div></div><p>
  You can see that the monasca components run on the Cloud Controller nodes,
  and the monasca agents run on all nodes in the Mid-scale Example
  configuration.
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-networkImages-Mid-Scale-AllNetworks.png" target="_blank"><img src="images/media-networkImages-Mid-Scale-AllNetworks.png" width="" /></a></div></div></div><div class="sect4" id="crt-components"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Components for Capacity Reporting</span> <a title="Permalink" class="permalink" href="#crt-components">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-crt_components.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_components.xml</li><li><span class="ds-label">ID: </span>crt-components</li></ul></div></div></div></div><div class="sect5" id="id-1.5.15.3.4.7.10.2"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.2.4.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">monasca Transform: Data Aggregation Reporting</span> <a title="Permalink" class="permalink" href="#id-1.5.15.3.4.7.10.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-crt_components.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_components.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   monasca-transform is a new component which provides mechanism to aggregate
   or transform metrics and publish new aggregated metrics to monasca.
  </p><p>
   monasca Transform is a data driven Apache Spark based data aggregation
   engine which collects, groups and aggregates existing individual monasca
   metrics according to business requirements and publishes new transformed
   (derived) metrics to the monasca Kafka queue.
  </p><p>
   Since the new transformed metrics are published as any other metric in
   monasca, alarms can be set and triggered on the transformed metric, just
   like any other metric.
  </p></div><div class="sect5" id="id-1.5.15.3.4.7.10.3"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.2.4.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Object Storage and Compute Capacity Summary Operations Console UI</span> <a title="Permalink" class="permalink" href="#id-1.5.15.3.4.7.10.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-crt_components.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_components.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   A new "Capacity Summary" tab for Compute and Object Storage will displays
   all the aggregated metrics under the "Compute" and "Object Storage"
   sections.
  </p><p>
   Operations Console UI makes calls to monasca API to retrieve and display various
   tiles and graphs on Capacity Summary tab in Compute and Object Storage
   Summary UI pages.
  </p></div><div class="sect5" id="id-1.5.15.3.4.7.10.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.2.4.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Persist new metrics and Trigger Alarms</span> <a title="Permalink" class="permalink" href="#id-1.5.15.3.4.7.10.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-crt_components.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_components.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   New aggregated metrics will be published to monasca's Kafka queue and will
   be ingested by monasca-persister. If thresholds and alarms have been set on
   the aggregated metrics, monasca will generate and trigger alarms as it
   currently does with any other metric. No new/additional change is expected
   with persisting of new aggregated metrics or setting threshold/alarms.
  </p></div></div><div class="sect4" id="crt-aggregated-metrics"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">New Aggregated Metrics</span> <a title="Permalink" class="permalink" href="#crt-aggregated-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-crt_aggregated_metrics.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_aggregated_metrics.xml</li><li><span class="ds-label">ID: </span>crt-aggregated-metrics</li></ul></div></div></div></div><p>
  Following is the list of aggregated metrics produced by monasca transform in
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
 </p><div class="table" id="table-ztc-yn5-3y"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 13.1: </span><span class="name">Aggregated Metrics </span><a title="Permalink" class="permalink" href="#table-ztc-yn5-3y">#</a></h6></div><div class="table-contents"><table class="table" summary="Aggregated Metrics" border="1"><colgroup><col align="left" class="c1" /><col align="left" class="c2" /><col align="left" class="c3" /><col align="left" class="c4" /><col align="left" class="c5" /><col align="left" class="c6" /></colgroup><thead><tr><th align="left"> </th><th align="left">Metric Name</th><th align="left">For</th><th align="left">Description</th><th align="left">Dimensions</th><th align="left">Notes</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">
      <p>
       cpu.utilized_logical_cores_agg
      </p>
     </td><td align="left">compute summary </td><td align="left">
      <p>
       utilized physical host cpu core capacity for one or all hosts by time
       interval (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all or &lt;host name&gt;
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left">Available as total or per host</td></tr><tr><td align="left">2</td><td align="left">cpu.total_logical_cores_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       total physical host cpu core capacity for one or all hosts by time
       interval (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all or &lt;host name&gt;
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left">Available as total or per host</td></tr><tr><td align="left">3</td><td align="left">mem.total_mb_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       total physical host memory capacity by time interval (defaults to a
       hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left"> </td></tr><tr><td align="left">4</td><td align="left">mem.usable_mb_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       usable physical host memory capacity by time interval (defaults to a
       hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left"> </td></tr><tr><td align="left">5</td><td align="left">disk.total_used_space_mb_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       utilized physical host disk capacity by time interval (defaults to a
       hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left"> </td></tr><tr><td align="left">6</td><td align="left">disk.total_space_mb_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       total physical host disk capacity by time interval (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left"> </td></tr><tr><td align="left">7</td><td align="left">nova.vm.cpu.total_allocated_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       cpus allocated across all VMs by time interval (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left"> </td></tr><tr><td align="left">8</td><td align="left">vcpus_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       virtual cpus allocated capacity for VMs of one or all projects by time
       interval (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all or &lt;project ID&gt;
      </p>
     </td><td align="left">Available as total or per project</td></tr><tr><td align="left">9</td><td align="left">nova.vm.mem.total_allocated_mb_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       memory allocated to all VMs by time interval (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left"> </td></tr><tr><td align="left">10</td><td align="left">vm.mem.used_mb_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       memory utilized by VMs of one or all projects by time interval (defaults
       to an hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: &lt;project ID&gt;
      </p>
     </td><td align="left">Available as total or per project</td></tr><tr><td align="left">11</td><td align="left">vm.mem.total_mb_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       memory allocated to VMs of one or all projects by time interval
       (defaults to an hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: &lt;project ID&gt;
      </p>
     </td><td align="left">Available as total or per project</td></tr><tr><td align="left">12</td><td align="left">vm.cpu.utilization_perc_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       cpu utilized by all VMs by project by time interval (defaults to an
       hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: &lt;project ID&gt;
      </p>
     </td><td align="left"> </td></tr><tr><td align="left">13</td><td align="left">nova.vm.disk.total_allocated_gb_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       disk space allocated to all VMs by time interval (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left"> </td></tr><tr><td align="left">14</td><td align="left">vm.disk.allocation_agg</td><td align="left">compute summary </td><td align="left">
      <p>
       disk allocation for VMs of one or all projects by time interval
       (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all or &lt;project ID&gt;
      </p>
     </td><td align="left">Available as total or per project</td></tr><tr><td align="left">15</td><td align="left">swiftlm.diskusage.val.size_agg</td><td align="left">object storage summary</td><td align="left">
      <p>
       total available object storage capacity by time interval (defaults to a
       hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all or &lt;host name&gt;
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left">Available as total or per host</td></tr><tr><td align="left">16</td><td align="left">swiftlm.diskusage.val.avail_agg</td><td align="left">object storage summary</td><td align="left">
      <p>
       remaining object storage capacity by time interval (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all or &lt;host name&gt;
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left">Available as total or per host</td></tr><tr><td align="left">17</td><td align="left">swiftlm.diskusage.rate_agg</td><td align="left">object storage summary</td><td align="left">
      <p>
       rate of change of object storage usage by time interval (defaults to a
       hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left"> </td></tr><tr><td align="left">18</td><td align="left">storage.objects.size_agg</td><td align="left">object storage summary</td><td align="left">
      <p>
       used object storage capacity by time interval (defaults to a hour)
      </p>
     </td><td align="left">
      <p>
       aggregation_period: hourly
      </p>
      <p>
       host: all
      </p>
      <p>
       project_id: all
      </p>
     </td><td align="left"> </td></tr></tbody></table></div></div></div><div class="sect4" id="crt-deployment"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment</span> <a title="Permalink" class="permalink" href="#crt-deployment">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-crt_deployment.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_deployment.xml</li><li><span class="ds-label">ID: </span>crt-deployment</li></ul></div></div></div></div><p>
  monasca Transform and Spark will be deployed on the same control plane nodes
  along with Logging and Monitoring Service (monasca).
 </p><p>
  <span class="bold"><strong>Security Consideration during deployment of monasca
  Transform and Spark</strong></span>
 </p><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Monitoring system connects internally to the Kafka and Spark
  technologies without authentication. If you choose to deploy Monitoring,
  configure it to use only trusted networks such as the Management network, as
  illustrated on the network diagrams below for Entry Scale Deployment and Mid
  Scale Deployment.
 </p><p>
  <span class="bold"><strong>Entry Scale Deployment</strong></span>
 </p><p>
  In Entry Scale Deployment monasca Transform and Spark will be deployed on
  Shared Control Plane along with other Openstack Services along with
  Monitoring and Logging
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-entryScale-Entry-ScaleAllNetworks.png" target="_blank"><img src="images/media-entryScale-Entry-ScaleAllNetworks.png" width="" /></a></div></div><p>
  <span class="bold"><strong>Mid scale Deployment</strong></span>
 </p><p>
  In a Mid Scale Deployment monasca Transform and Spark will be deployed on
  dedicated Metering Monitoring and Logging (MML) control plane along with
  other data processing intensive services like Metering, Monitoring and
  Logging
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-networkImages-Mid-Scale-AllNetworks.png" target="_blank"><img src="images/media-networkImages-Mid-Scale-AllNetworks.png" width="" /></a></div></div><p>
  <span class="bold"><strong>Multi Control Plane Deployment</strong></span>
 </p><p>
  In a Multi Control Plane Deployment, monasca Transform and Spark will be
  deployed on the Shared Control plane along with rest of monasca Components.
 </p><p>
  <span class="bold"><strong>Start, Stop and Status for monasca Transform and Spark
  processes</strong></span>
 </p><p>
  The service management methods for monasca-transform and spark follow the
  convention for services in the <span class="productname">OpenStack</span> platform. When executing from the
  deployer node, the commands are as follows:
 </p><p>
  <span class="bold"><strong>Status</strong></span>
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts spark-status.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-transform-status.yml</pre></div><p>
  <span class="bold"><strong>Start</strong></span>
 </p><p>
  As monasca-transform depends on spark for the processing of the metrics spark
  will need to be started before monasca-transform.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts spark-start.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-transform-start.yml</pre></div><p>
  <span class="bold"><strong>Stop</strong></span>
 </p><p>
  As a precaution, stop the monasca-transform service before
  taking spark down. Interruption to the spark service altogether while
  monasca-transform is still running can result in a monasca-transform process
  that is unresponsive and needing to be tidied up.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-transform-stop.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts spark-stop.yml</pre></div></div><div class="sect4" id="crt-reconfigure"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reconfigure</span> <a title="Permalink" class="permalink" href="#crt-reconfigure">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-crt_reconfigure.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_reconfigure.xml</li><li><span class="ds-label">ID: </span>crt-reconfigure</li></ul></div></div></div></div><p>
  The reconfigure process can be triggered again from the deployer. Presuming
  that changes have been made to the variables in the appropriate places
  execution of the respective ansible scripts will be enough to update the
  configuration. The spark reconfigure process alters the nodes serially
  meaning that spark is never down altogether, each node is stopped in turn
  and zookeeper manages the leaders accordingly. This means that
  monasca-transform may be left running even while spark is upgraded.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts spark-reconfigure.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-transform-reconfigure.yml</pre></div></div><div class="sect4" id="crt-adding-transform-spark"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.4.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding monasca Transform and Spark to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Deployment</span> <a title="Permalink" class="permalink" href="#crt-adding-transform-spark">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-crt_adding_transform_spark.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_adding_transform_spark.xml</li><li><span class="ds-label">ID: </span>crt-adding-transform-spark</li></ul></div></div></div></div><p>
  Since monasca Transform and Spark are optional components, the users might
  elect to not install these two components during their initial <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  install. The following instructions provide a way the users can add monasca
  Transform and Spark to their existing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> deployment.
 </p><p>
  <span class="bold"><strong>Steps</strong></span>
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Add monasca Transform and Spark to the input model. monasca Transform and
    Spark on a entry level cloud would be installed on the common control
    plane, for mid scale cloud which has a MML (Metering, Monitoring and
    Logging) cluster, monasca Transform and Spark will should be added to
    MML cluster.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition/data/</pre></div><p>
    Add spark and monasca-transform to input model, control_plane.yml
   </p><div class="verbatim-wrap"><pre class="screen">clusters
       - name: core
         cluster-prefix: c1
         server-role: CONTROLLER-ROLE
         member-count: 3
         allocation-policy: strict
         service-components:

           [...]

           - zookeeper
           - kafka
           - cassandra
           - storm
           - spark
           - monasca-api
           - monasca-persister
           - monasca-notifier
           - monasca-threshold
           - monasca-client
           - monasca-transform

           [...]</pre></div></li><li class="listitem "><p>
    Run the Configuration Processor
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Adding monasca Transform and Spark"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
    Run Ready Deployment
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
    Run Cloud Lifecycle Manager Deploy
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-deploy.yml</pre></div></li></ol></div><p>
  <span class="bold"><strong>Verify Deployment</strong></span>
 </p><p>
  Login to each controller node and run
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo service monasca-transform status
<code class="prompt user">tux &gt; </code>sudo service spark-master status
<code class="prompt user">tux &gt; </code>sudo service spark-worker status</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo service monasca-transform status
● monasca-transform.service - monasca Transform Daemon
  Loaded: loaded (/etc/systemd/system/monasca-transform.service; disabled)
  Active: active (running) since Wed 2016-08-24 00:47:56 UTC; 2 days ago
Main PID: 7351 (bash)
  CGroup: /system.slice/monasca-transform.service
          ├─ 7351 bash /etc/monasca/transform/init/start-monasca-transform.sh
          ├─ 7352 /opt/stack/service/monasca-transform/venv//bin/python /opt/monasca/monasca-transform/lib/service_runner.py
          ├─27904 /bin/sh -c export SPARK_HOME=/opt/stack/service/spark/venv/bin/../current &amp;&amp; spark-submit --supervise --master spark://omega-cp1-c1-m1-mgmt:7077,omega-cp1-c1-m2-mgmt:7077,omega-cp1-c1...
          ├─27905 /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -cp /opt/stack/service/spark/venv/lib/drizzle-jdbc-1.3.jar:/opt/stack/service/spark/venv/bin/../current/conf/:/opt/stack/service/spark/v...
          └─28355 python /opt/monasca/monasca-transform/lib/driver.py
Warning: Journal has been rotated since unit was started. Log output is incomplete or unavailable.


<code class="prompt user">tux &gt; </code>sudo service spark-worker status
● spark-worker.service - Spark Worker Daemon
  Loaded: loaded (/etc/systemd/system/spark-worker.service; disabled)
  Active: active (running) since Wed 2016-08-24 00:46:05 UTC; 2 days ago
Main PID: 63513 (bash)
  CGroup: /system.slice/spark-worker.service
          ├─ 7671 python -m pyspark.daemon
          ├─28948 /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -cp /opt/stack/service/spark/venv/bin/../current/conf/:/opt/stack/service/spark/venv/bin/../current/lib/spark-assembly-1.6.1-hadoop2.6.0...
          ├─63513 bash /etc/spark/init/start-spark-worker.sh &amp;
          └─63514 /usr/bin/java -cp /opt/stack/service/spark/venv/bin/../current/conf/:/opt/stack/service/spark/venv/bin/../current/lib/spark-assembly-1.6.1-hadoop2.6.0.jar:/opt/stack/service/spark/ven...
Warning: Journal has been rotated since unit was started. Log output is incomplete or unavailable.



<code class="prompt user">tux &gt; </code>sudo service spark-master status
● spark-master.service - Spark Master Daemon
  Loaded: loaded (/etc/systemd/system/spark-master.service; disabled)
  Active: active (running) since Wed 2016-08-24 00:44:24 UTC; 2 days ago
Main PID: 55572 (bash)
  CGroup: /system.slice/spark-master.service
          ├─55572 bash /etc/spark/init/start-spark-master.sh &amp;
          └─55573 /usr/bin/java -cp /opt/stack/service/spark/venv/bin/../current/conf/:/opt/stack/service/spark/venv/bin/../current/lib/spark-assembly-1.6.1-hadoop2.6.0.jar:/opt/stack/service/spark/ven...
Warning: Journal has been rotated since unit was started. Log output is incomplete or unavailable.</pre></div></div><div class="sect4" id="crt-increasing-scale"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.4.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Increase monasca Transform Scale</span> <a title="Permalink" class="permalink" href="#crt-increasing-scale">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-crt_increasing_scale.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_increasing_scale.xml</li><li><span class="ds-label">ID: </span>crt-increasing-scale</li></ul></div></div></div></div><p>
  monasca Transform in the default configuration can scale up to estimated
  data for 100 node cloud deployment. Estimated maximum rate of metrics from a
  100 node cloud deployment is 120M/hour.
 </p><p>
  You can further increase the processing rate to 180M/hour. Making
  the Spark configuration change will increase the CPU's being used by Spark
  and monasca Transform from average of around 3.5 to 5.5 CPU's per control
  node over a 10 minute batch processing interval.
 </p><p>
  To increase the processing rate to 180M/hour the customer will have to make
  following spark configuration change.
 </p><p>
  <span class="bold"><strong>Steps</strong></span>
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Edit /var/lib/ardana/openstack/my_cloud/config/spark/spark-defaults.conf.j2 and
    set spark.cores.max to 6 and spark.executor.cores 2
   </p><p>
    <span class="bold"><strong>Set spark.cores.max to 6</strong></span>
   </p><div class="verbatim-wrap"><pre class="screen">spark.cores.max {{ spark_cores_max }}</pre></div><p>
    to
   </p><div class="verbatim-wrap"><pre class="screen">spark.cores.max 6</pre></div><p>
    <span class="bold"><strong>Set spark.executor.cores to 2</strong></span>
   </p><div class="verbatim-wrap"><pre class="screen">spark.executor.cores {{ spark_executor_cores }}</pre></div><p>
    to
   </p><div class="verbatim-wrap"><pre class="screen">spark.executor.cores 2</pre></div></li><li class="listitem "><p>
    Edit ~/openstack/my_cloud/config/spark/spark-env.sh.j2
   </p><p>
    <span class="bold"><strong>Set SPARK_WORKER_CORES to 2</strong></span>
   </p><div class="verbatim-wrap"><pre class="screen">export SPARK_WORKER_CORES={{ spark_worker_cores }}</pre></div><p>
    to
   </p><div class="verbatim-wrap"><pre class="screen">export SPARK_WORKER_CORES=2</pre></div></li><li class="listitem "><p>
    Edit ~/openstack/my_cloud/config/spark/spark-worker-env.sh.j2
   </p><p>
    <span class="bold"><strong>Set SPARK_WORKER_CORES to 2</strong></span>
   </p><div class="verbatim-wrap"><pre class="screen">export SPARK_WORKER_CORES={{ spark_worker_cores }}</pre></div><p>
    to
   </p><div class="verbatim-wrap"><pre class="screen">export SPARK_WORKER_CORES=2</pre></div></li><li class="listitem "><p>
    Run Configuration Processor
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Changing Spark Config increase scale"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
    Run Ready Deployment
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
    Run spark-reconfigure.yml and monasca-transform-reconfigure.yml
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts spark-reconfigure.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-transform-reconfigure.yml</pre></div></li></ol></div></div><div class="sect4" id="crt-change-compute-host"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.4.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Change Compute Host Pattern Filter in Monasca Transform</span> <a title="Permalink" class="permalink" href="#crt-change-compute-host">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-crt_change_compute_host.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crt_change_compute_host.xml</li><li><span class="ds-label">ID: </span>crt-change-compute-host</li></ul></div></div></div></div><p>
  monasca Transform identifies compute host metrics by pattern matching on
  hostname dimension in the incoming monasca metrics. The default pattern is of
  the form <code class="literal">comp<em class="replaceable ">NNN</em></code>. For example,
  <code class="literal">comp001</code>, <code class="literal">comp002</code>, etc. To filter for it
  in the transformation specs, use the expression
  <code class="literal">-comp[0-9]+-</code>. In case the compute
  host names follow a different pattern other than the standard pattern above,
  the filter by expression when aggregating metrics will have to be changed.
 </p><p>
  <span class="bold"><strong>Steps</strong></span>
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     On the deployer: Edit
     <code class="filename">~/openstack/my_cloud/config/monasca-transform/transform_specs.json.j2</code>
   </p></li><li class="step " id="st-monasca-check-comp-reference"><p>
    Look for all references of <code class="literal">-comp[0-9]+-</code> and change the
    regular expression to the desired pattern say for example
    <code class="literal">-compute[0-9]+-</code>.
   </p><div class="verbatim-wrap"><pre class="screen">{"aggregation_params_map":{"aggregation_pipeline":{"source":"streaming", "usage":"fetch_quantity", "setters":["rollup_quantity", "set_aggregated_metric_name", "set_aggregated_period"], "insert":["prepare_data","insert_data_pre_hourly"]}, "aggregated_metric_name":"mem.total_mb_agg", "aggregation_period":"hourly", "aggregation_group_by_list": ["host", "metric_id", "tenant_id"], "usage_fetch_operation": "avg", "filter_by_list": [{"field_to_filter": "host", "filter_expression": "-comp[0-9]+", "filter_operation": "include"}], "setter_rollup_group_by_list":[], "setter_rollup_operation": "sum", "dimension_list":["aggregation_period", "host", "project_id"], "pre_hourly_operation":"avg", "pre_hourly_group_by_list":["default"]}, "metric_group":"mem_total_all", "metric_id":"mem_total_all"}</pre></div><p>
    to
   </p><div class="verbatim-wrap"><pre class="screen">{"aggregation_params_map":{"aggregation_pipeline":{"source":"streaming", "usage":"fetch_quantity", "setters":["rollup_quantity", "set_aggregated_metric_name", "set_aggregated_period"], "insert":["prepare_data", "insert_data_pre_hourly"]}, "aggregated_metric_name":"mem.total_mb_agg", "aggregation_period":"hourly", "aggregation_group_by_list": ["host", "metric_id", "tenant_id"],"usage_fetch_operation": "avg","filter_by_list": [{"field_to_filter": "host","filter_expression": "-compute[0-9]+", "filter_operation": "include"}], "setter_rollup_group_by_list":[], "setter_rollup_operation": "sum", "dimension_list":["aggregation_period", "host", "project_id"], "pre_hourly_operation":"avg", "pre_hourly_group_by_list":["default"]}, "metric_group":"mem_total_all", "metric_id":"mem_total_all"}</pre></div><div id="id-1.5.15.3.4.7.16.4.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     The filter_expression has been changed to the <span class="emphasis"><em>new</em></span>
     pattern.
    </p></div></li><li class="step "><p>
    To change all host metric transformation specs in the same
    JSON file, repeat <a class="xref" href="#st-monasca-check-comp-reference" title="Step 2">Step 2</a>.
   </p><p>
    Transformation specs will have to be changed for following metric_ids
    namely "mem_total_all", "mem_usable_all", "disk_total_all",
    "disk_usable_all", "cpu_total_all", "cpu_total_host", "cpu_util_all",
    "cpu_util_host"
   </p></li><li class="step "><p>
     Run the Configuration Processor:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "Changing monasca Transform specs"
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Run Ready Deployment:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run monasca Transform Reconfigure:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-transform-reconfigure.yml</pre></div></li></ol></div></div></div></div><div class="sect3" id="config-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Availability of Alarm Metrics</span> <a title="Permalink" class="permalink" href="#config-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-config_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-config_metrics.xml</li><li><span class="ds-label">ID: </span>config-metrics</li></ul></div></div></div></div><p>
  Using the monasca agent tuning knobs, you can choose which alarm metrics are
  available in your environment.
 </p><p>
  The addition of the libvirt and OVS plugins to the monasca agent provides a
  number of additional metrics that can be used. Most of these metrics are
  included by default, but others are not. You have the ability to use tuning
  knobs to add or remove these metrics to your environment based on your
  individual needs in your cloud.
 </p><p>
  We will list these metrics along with the tuning knob name and instructions
  for how to adjust these.
 </p><div class="sect4" id="libvirt-tuningknobs"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Libvirt plugin metric tuning knobs</span> <a title="Permalink" class="permalink" href="#libvirt-tuningknobs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-libvirt_tuningknobs.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-libvirt_tuningknobs.xml</li><li><span class="ds-label">ID: </span>libvirt-tuningknobs</li></ul></div></div></div></div><p>
  The following metrics are added as part of the libvirt plugin:
 </p><div id="id-1.5.15.3.4.8.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   For a description of each of these metrics, see
   <a class="xref" href="#libvirt-metrics" title="13.1.4.11. Libvirt Metrics">Section 13.1.4.11, “Libvirt Metrics”</a>.
  </p></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="newCol2" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Tuning Knob</th><th>Default Setting</th><th>Admin Metric Name</th><th>Project Metric Name</th></tr></thead><tbody><tr><td rowspan="3">vm_cpu_check_enable</td><td rowspan="3">True</td><td>vm.cpu.time_ns</td><td>cpu.time_ns</td></tr><tr><td>vm.cpu.utilization_norm_perc</td><td>cpu.utilization_norm_perc</td></tr><tr><td>vm.cpu.utilization_perc</td><td>cpu.utilization_perc</td></tr><tr><td rowspan="10">vm_disks_check_enable</td><td rowspan="10">
      <p>
       True
      </p>
      <p>
       Creates 20 disk metrics per disk device per virtual machine.
      </p>
     </td><td>vm.io.errors</td><td>io.errors</td></tr><tr><td>vm.io.errors_sec</td><td>io.errors_sec</td></tr><tr><td>vm.io.read_bytes</td><td>io.read_bytes</td></tr><tr><td>vm.io.read_bytes_sec</td><td>io.read_bytes_sec</td></tr><tr><td>vm.io.read_ops</td><td>io.read_ops</td></tr><tr><td>vm.io.read_ops_sec</td><td>io.read_ops_sec</td></tr><tr><td>vm.io.write_bytes</td><td>io.write_bytes</td></tr><tr><td>vm.io.write_bytes_sec</td><td>io.write_bytes_sec</td></tr><tr><td>vm.io.write_ops</td><td>io.write_ops</td></tr><tr><td>vm.io.write_ops_sec</td><td> io.write_ops_sec</td></tr><tr><td rowspan="8">vm_network_check_enable</td><td rowspan="8">
      <p>
       True
      </p>
      <p>
       Creates 16 network metrics per NIC per virtual machine.
      </p>
     </td><td>vm.net.in_bytes</td><td>net.in_bytes</td></tr><tr><td>vm.net.in_bytes_sec</td><td>net.in_bytes_sec</td></tr><tr><td>vm.net.in_packets</td><td>net.in_packets</td></tr><tr><td>vm.net.in_packets_sec</td><td>net.in_packets_sec</td></tr><tr><td>vm.net.out_bytes</td><td>net.out_bytes</td></tr><tr><td>vm.net.out_bytes_sec</td><td>net.out_bytes_sec</td></tr><tr><td>vm.net.out_packets</td><td>net.out_packets</td></tr><tr><td>vm.net.out_packets_sec</td><td>net.out_packets_sec</td></tr><tr><td>vm_ping_check_enable</td><td>True</td><td>vm.ping_status</td><td>ping_status</td></tr><tr><td rowspan="6">vm_extended_disks_check_enable</td><td rowspan="3">
      <p>
       True
      </p>
      <p>
       Creates 6 metrics per device per virtual machine.
      </p>
     </td><td>vm.disk.allocation</td><td>disk.allocation</td></tr><tr><td>vm.disk.capacity</td><td>disk.capacity</td></tr><tr><td>vm.disk.physical</td><td>disk.physical</td></tr><tr><td rowspan="3">
      <p>
       True
      </p>
      <p>
       Creates 6 aggregate metrics per virtual machine.
      </p>
     </td><td>vm.disk.allocation_total</td><td>disk.allocation_total</td></tr><tr><td>vm.disk.capacity_total</td><td>disk.capacity.total</td></tr><tr><td>vm.disk.physical_total</td><td>disk.physical_total</td></tr><tr><td rowspan="10">vm_disks_check_enable vm_extended_disks_check_enable</td><td rowspan="10">
      <p>
       True
      </p>
      <p>
       Creates 20 aggregate metrics per virtual machine.
      </p>
     </td><td>vm.io.errors_total</td><td>io.errors_total</td></tr><tr><td>vm.io.errors_total_sec</td><td>io.errors_total_sec</td></tr><tr><td>vm.io.read_bytes_total</td><td>io.read_bytes_total</td></tr><tr><td>vm.io.read_bytes_total_sec</td><td>io.read_bytes_total_sec</td></tr><tr><td>vm.io.read_ops_total</td><td>io.read_ops_total</td></tr><tr><td>vm.io.read_ops_total_sec</td><td>io.read_ops_total_sec</td></tr><tr><td>vm.io.write_bytes_total</td><td>io.write_bytes_total</td></tr><tr><td>vm.io.write_bytes_total_sec</td><td>io.write_bytes_total_sec</td></tr><tr><td>vm.io.write_ops_total</td><td>io.write_ops_total</td></tr><tr><td>vm.io.write_ops_total_sec</td><td>io.write_ops_total_sec</td></tr></tbody></table></div><div class="sect5" id="configuring-libvirt-tuning-knobs"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.2.5.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the libvirt metrics using the tuning knobs</span> <a title="Permalink" class="permalink" href="#configuring-libvirt-tuning-knobs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-libvirt_tuningknobs.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-libvirt_tuningknobs.xml</li><li><span class="ds-label">ID: </span>configuring-libvirt-tuning-knobs</li></ul></div></div></div></div><p>
   Use the following steps to configure the tuning knobs for the libvirt plugin
   metrics.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/nova/libvirt-monitoring.yml</pre></div></li><li class="listitem "><p>
     Change the value for each tuning knob to the desired setting,
     <code class="literal">True</code> if you want the metrics created and
     <code class="literal">False</code> if you want them removed. Refer to the table
     above for which metrics are controlled by each tuning knob.
    </p><div class="verbatim-wrap"><pre class="screen">vm_cpu_check_enable: <span class="emphasis"><em>&lt;true or false&gt;</em></span>
vm_disks_check_enable: <span class="emphasis"><em>&lt;true or false&gt;</em></span>
vm_extended_disks_check_enable: <span class="emphasis"><em>&lt;true or false&gt;</em></span>
vm_network_check_enable: <span class="emphasis"><em>&lt;true or false&gt;</em></span>
vm_ping_check_enable: <span class="emphasis"><em>&lt;true or false&gt;</em></span></pre></div></li><li class="listitem "><p>
     Commit your configuration to the local Git repository (see
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "configuring libvirt plugin tuning knobs"</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the nova reconfigure playbook to implement the changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</pre></div></li></ol></div><div id="id-1.5.15.3.4.8.5.5.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   If you modify either of the following files, then the monasca tuning
   parameters should be adjusted to handle a higher load on the system.
  </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/nova/libvirt-monitoring.yml
~/openstack/my_cloud/config/neutron/monasca_ovs_plugin.yaml.j2</pre></div><p>
   Tuning parameters are located in
   <code class="filename">~/openstack/my_cloud/config/monasca/configuration.yml</code>.
   The parameter <code class="literal">monasca_tuning_selector_override</code> should be
   changed to the <code class="literal">extra-large</code> setting.
  </p></div></div></div><div class="sect4" id="ovs-tuningknobs"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.2.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">OVS plugin metric tuning knobs</span> <a title="Permalink" class="permalink" href="#ovs-tuningknobs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-ovs_tuningknobs.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-ovs_tuningknobs.xml</li><li><span class="ds-label">ID: </span>ovs-tuningknobs</li></ul></div></div></div></div><p>
  The following metrics are added as part of the OVS plugin:
 </p><div id="id-1.5.15.3.4.8.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   For a description of each of these metrics, see
   <a class="xref" href="#sec-metric-ovs" title="13.1.4.16. Open vSwitch (OVS) Metrics">Section 13.1.4.16, “Open vSwitch (OVS) Metrics”</a>.
  </p></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="newCol2" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Tuning Knob</th><th>Default Setting</th><th>Admin Metric Name</th><th>Project Metric Name</th></tr></thead><tbody><tr><td rowspan="4">use_rate_metrics</td><td rowspan="4">False</td><td>ovs.vrouter.in_bytes_sec</td><td>vrouter.in_bytes_sec</td></tr><tr><td>ovs.vrouter.in_packets_sec</td><td>vrouter.in_packets_sec</td></tr><tr><td>ovs.vrouter.out_bytes_sec</td><td>vrouter.out_bytes_sec</td></tr><tr><td>ovs.vrouter.out_packets_sec</td><td>vrouter.out_packets_sec</td></tr><tr><td rowspan="4">use_absolute_metrics</td><td rowspan="4">True</td><td>ovs.vrouter.in_bytes</td><td>vrouter.in_bytes</td></tr><tr><td>ovs.vrouter.in_packets</td><td>vrouter.in_packets</td></tr><tr><td>ovs.vrouter.out_bytes</td><td>vrouter.out_bytes</td></tr><tr><td>ovs.vrouter.out_packets</td><td>vrouter.out_packets</td></tr><tr><td rowspan="4">use_health_metrics with use_rate_metrics</td><td rowspan="4">False</td><td>ovs.vrouter.in_dropped_sec</td><td>vrouter.in_dropped_sec</td></tr><tr><td>ovs.vrouter.in_errors_sec</td><td>vrouter.in_errors_sec</td></tr><tr><td>ovs.vrouter.out_dropped_sec</td><td>vrouter.out_dropped_sec</td></tr><tr><td>ovs.vrouter.out_errors_sec</td><td>vrouter.out_errors_sec</td></tr><tr><td rowspan="4">use_health_metrics with use_absolute_metrics</td><td rowspan="4">False</td><td>ovs.vrouter.in_dropped</td><td>vrouter.in_dropped</td></tr><tr><td>ovs.vrouter.in_errors</td><td>vrouter.in_errors</td></tr><tr><td>ovs.vrouter.out_dropped</td><td>vrouter.out_dropped</td></tr><tr><td>ovs.vrouter.out_errors</td><td>vrouter.out_errors</td></tr></tbody></table></div><div class="sect5" id="id-1.5.15.3.4.8.6.5"><div class="titlepage"><div><div><h6 class="title"><span class="number">13.1.2.5.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the OVS metrics using the tuning knobs</span> <a title="Permalink" class="permalink" href="#id-1.5.15.3.4.8.6.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-ovs_tuningknobs.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-ovs_tuningknobs.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Use the following steps to configure the tuning knobs for the libvirt plugin
   metrics.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/neutron/monasca_ovs_plugin.yaml.j2</pre></div></li><li class="listitem "><p>
     Change the value for each tuning knob to the desired setting,
     <code class="literal">True</code> if you want the metrics created and
     <code class="literal">False</code> if you want them removed. Refer to the table
     above for which metrics are controlled by each tuning knob.
    </p><div class="verbatim-wrap"><pre class="screen">init_config:
   use_absolute_metrics: <span class="emphasis"><em>&lt;true or false&gt;</em></span>
   use_rate_metrics: <span class="emphasis"><em>&lt;true or false&gt;</em></span>
   use_health_metrics: <span class="emphasis"><em>&lt;true or false&gt;</em></span></pre></div></li><li class="listitem "><p>
     Commit your configuration to the local Git repository (see
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "configuring OVS plugin tuning knobs"</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the neutron reconfigure playbook to implement the changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></li></ol></div></div></div></div></div><div class="sect2" id="monasca-notification-plugins"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Integrating HipChat, Slack, and JIRA</span> <a title="Permalink" class="permalink" href="#monasca-notification-plugins">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-monasca_plugins_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monasca_plugins_overview.xml</li><li><span class="ds-label">ID: </span>monasca-notification-plugins</li></ul></div></div></div></div><p>
  monasca, the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> monitoring and notification service, includes
  three default notification methods, <span class="bold"><strong>email</strong></span>,
  <span class="bold"><strong>PagerDuty</strong></span>, and
  <span class="bold"><strong>webhook</strong></span>. monasca also supports three other
  notification plugins which allow you to send notifications to
  <span class="bold"><strong>HipChat</strong></span>,
  <span class="bold"><strong>Slack</strong></span>, and
  <span class="bold"><strong>JIRA</strong></span>. Unlike the default notification
  methods, the additional notification plugins must be manually configured.
 </p><p>
  This guide details the steps to configure each of the three non-default
  notification plugins. This guide also assumes that your cloud
  is fully deployed and functional.
 </p><div class="sect3" id="hipchat-plugin"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the HipChat Plugin</span> <a title="Permalink" class="permalink" href="#hipchat-plugin">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-monasca_hipchat_plugin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monasca_hipchat_plugin.xml</li><li><span class="ds-label">ID: </span>hipchat-plugin</li></ul></div></div></div></div><p>
  To configure the HipChat plugin you will need the following four pieces of
  information from your HipChat system.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    The URL of your HipChat system.
   </p></li><li class="listitem "><p>
    A token providing permission to send notifications to your HipChat system.
   </p></li><li class="listitem "><p>
    The ID of the HipChat room you wish to send notifications to.
   </p></li><li class="listitem "><p>
    A HipChat user account. This account will be used to authenticate any
    incoming notifications from your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud.
   </p></li></ul></div><p>
  <span class="bold"><strong>Obtain a token</strong></span>
 </p><p>
  Use the following instructions to obtain a token from your Hipchat system.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Log in to HipChat as the user account that will be used to authenticate the
    notifications.
   </p></li><li class="listitem "><p>
    Navigate to the following URL:
    <code class="literal">https://&lt;your_hipchat_system&gt;/account/api</code>. Replace
    <code class="literal">&lt;your_hipchat_system&gt;</code> with the
    fully-qualified-domain-name of your HipChat system.
   </p></li><li class="listitem "><p>
    Select the <span class="bold"><strong>Create token</strong></span> option. Ensure
    that the token has the "SendNotification" attribute.
   </p></li></ol></div><p>
  <span class="bold"><strong>Obtain a room ID</strong></span>
 </p><p>
  Use the following instructions to obtain the ID of a HipChat room.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Log in to HipChat as the user account that will be used to authenticate the
    notifications.
   </p></li><li class="listitem "><p>
    Select <span class="bold"><strong>My account</strong></span> from the application
    menu.
   </p></li><li class="listitem "><p>
    Select the <span class="bold"><strong>Rooms</strong></span> tab.
   </p></li><li class="listitem "><p>
    Select the room that you want your notifications sent to.
   </p></li><li class="listitem "><p>
    Look for the API ID field in the room information. This is the room ID.
   </p></li></ol></div><p>
  <span class="bold"><strong>Create HipChat notification type</strong></span>
 </p><p>
  Use the following instructions to create a HipChat notification type.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Begin by obtaining the API URL for the HipChat room that you wish to send
    notifications to. The format for a URL used to send notifications to a room
    is as follows:
   </p><p>
    <code class="literal">/v2/room/{room_id_or_name}/notification</code>
   </p></li><li class="listitem "><p>
    Use the monasca API to create a new notification method. The following
    example demonstrates how to create a HipChat notification type named
    <span class="bold"><strong>MyHipChatNotification</strong></span>, for room
    <span class="bold"><strong>ID 13</strong></span>, using an example API URL and auth
    token.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>monasca notification-create  <em class="replaceable ">NAME</em> <em class="replaceable ">TYPE</em> <em class="replaceable ">ADDRESS</em>
<code class="prompt user">ardana &gt; </code>monasca notification-create  MyHipChatNotification HIPCHAT https://hipchat.hpe.net/v2/room/13/notification?auth_token=1234567890</pre></div><p>
    The preceding example creates a notification type with the following
    characteristics
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      NAME: MyHipChatNotification
     </p></li><li class="listitem "><p>
      TYPE: HIPCHAT
     </p></li><li class="listitem "><p>
      ADDRESS: https://hipchat.hpe.net/v2/room/13/notification
     </p></li><li class="listitem "><p>
      auth_token: 1234567890
     </p></li></ul></div></li></ol></div><div id="id-1.5.15.3.5.4.13" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   The horizon dashboard can also be used to create a HipChat notification
   type.
  </p></div></div><div class="sect3" id="monasca-slack-plugin"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Slack Plugin</span> <a title="Permalink" class="permalink" href="#monasca-slack-plugin">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-monasca_slack_plugin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monasca_slack_plugin.xml</li><li><span class="ds-label">ID: </span>monasca-slack-plugin</li></ul></div></div></div></div><p>
  Configuring a Slack notification type requires four pieces of information
  from your Slack system.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Slack server URL
   </p></li><li class="listitem "><p>
    Authentication token
   </p></li><li class="listitem "><p>
    Slack channel
   </p></li><li class="listitem "><p>
    A Slack user account. This account will be used to authenticate incoming
    notifications to Slack.
   </p></li></ul></div><p>
  <span class="bold"><strong>Identify a Slack channel</strong></span>
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Log in to your Slack system as the user account that will be used to
    authenticate the notifications to Slack.
   </p></li><li class="listitem "><p>
    In the left navigation panel, under the
    <span class="bold"><strong>CHANNELS</strong></span> section locate the channel that
    you wish to receive the notifications. The instructions that follow will
    use the example channel <span class="bold"><strong>#general</strong></span>.
   </p></li></ol></div><p>
  <span class="bold"><strong>Create a Slack token</strong></span>
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Log in to your Slack system as the user account that will be used to
    authenticate the notifications to Slack
   </p></li><li class="listitem "><p>
    Navigate to the following URL:
    <a class="link" href="https://api.slack.com/docs/oauth-test-tokens" target="_blank">https://api.slack.com/docs/oauth-test-tokens</a>
   </p></li><li class="listitem "><p>
    Select the <span class="bold"><strong>Create token</strong></span> button.
   </p></li></ol></div><p>
  <span class="bold"><strong>Create a Slack notification type</strong></span>
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Begin by identifying the structure of the API call to be used by your
    notification method. The format for a call to the Slack Web API is as
    follows:
   </p><p>
    <code class="literal">https://slack.com/api/METHOD</code>
   </p><p>
    You can authenticate a Web API request by using the token that you created
    in the previous <span class="bold"><strong>Create a Slack
    Token</strong></span>section. Doing so will result in an API call that looks
    like the following.
   </p><p>
    <code class="literal">https://slack.com/api/METHOD?token=auth_token</code>
   </p><p>
    You can further refine your call by specifying the channel that the message
    will be posted to. Doing so will result in an API call that looks like the
    following.
   </p><p>
    <code class="literal">https://slack.com/api/<em class="replaceable ">METHOD</em>?token=<em class="replaceable ">AUTH_TOKEN</em>&amp;channel=<em class="replaceable ">#channel</em></code>
   </p><p>
    The following example uses the <code class="literal">chat.postMessage</code> method,
    the token <code class="literal">1234567890</code>, and the channel
    <code class="literal">#general</code>.
   </p><div class="verbatim-wrap"><pre class="screen">https://slack.com/api/chat.postMessage?token=1234567890&amp;channel=#general</pre></div><p>
    Find more information on the Slack Web API here:
    <a class="link" href="https://api.slack.com/web" target="_blank">https://api.slack.com/web</a>
   </p></li><li class="listitem "><p>
    Use the CLI on your Cloud Lifecycle Manager to create a new Slack notification
    type, using the API call that you created in the preceding step. The
    following example creates a notification type named
    <span class="bold"><strong>MySlackNotification</strong></span>, using token
    <span class="bold"><strong>1234567890</strong></span>, and posting to channel
    <span class="bold"><strong>#general</strong></span>.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>monasca notification-create  MySlackNotification SLACK https://slack.com/api/chat.postMessage?token=1234567890&amp;channel=#general</pre></div></li></ol></div><div id="id-1.5.15.3.5.5.10" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   Notification types can also be created in the horizon dashboard.
  </p></div></div><div class="sect3" id="monasca-jira-plugin"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the JIRA Plugin</span> <a title="Permalink" class="permalink" href="#monasca-jira-plugin">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-monasca_jira_plugin.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monasca_jira_plugin.xml</li><li><span class="ds-label">ID: </span>monasca-jira-plugin</li></ul></div></div></div></div><p>
  Configuring the JIRA plugin requires three pieces of information from your
  JIRA system.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    The URL of your JIRA system.
   </p></li><li class="listitem "><p>
    Username and password of a JIRA account that will be used to authenticate
    the notifications.
   </p></li><li class="listitem "><p>
    The name of the JIRA project that the notifications will be sent to.
   </p></li></ul></div><p>
  <span class="bold"><strong>Create JIRA notification type</strong></span>
 </p><p>
  You will configure the monasca service to send notifications to a particular
  JIRA project. You must also configure JIRA to create new issues for each
  notification it receives to this project, however, that configuration is
  outside the scope of this document.
 </p><p>
  The monasca JIRA notification plugin supports only the following two JIRA
  issue fields.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <em class="replaceable ">PROJECT</em>. This is the only supported
    <span class="quote">“<span class="quote ">mandatory</span>”</span> JIRA issue field.
   </p></li><li class="listitem "><p>
    <em class="replaceable ">COMPONENT</em>. This is the only supported
    <span class="quote">“<span class="quote ">optional</span>”</span> JIRA issue field.
   </p></li></ul></div><p>
  The JIRA issue type that your notifications will create may only be
  configured with the "Project" field as mandatory. If your JIRA issue type has
  any other mandatory fields, the monasca plugin will not function correctly.
  Currently, the monasca plugin only supports the single optional "component"
  field.
 </p><p>
  Creating the JIRA notification type requires a few more steps than other
  notification types covered in this guide. Because the Python and YAML files
  for this notification type are not yet included in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9, you must
  perform the following steps to manually retrieve and place them on your
  Cloud Lifecycle Manager.
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Configure the JIRA plugin by adding the following block to the
    <code class="filename">/etc/monasca/notification.yaml</code> file, under the
    <code class="literal">notification_types</code> section, and adding the username and
    password of the JIRA account used for the notifications to the respective
    sections.
   </p><div class="verbatim-wrap"><pre class="screen">    plugins:

     - monasca_notification.plugins.jira_notifier:JiraNotifier

    jira:
        user:

        password:

        timeout: 60</pre></div><p>
    After adding the necessary block, the <code class="literal">notification_types</code>
    section should look like the following example. Note that you must also add
    the username and password for the JIRA user related to the notification
    type.
   </p><div class="verbatim-wrap"><pre class="screen">notification_types:
    plugins:

     - monasca_notification.plugins.jira_notifier:JiraNotifier

    jira:
        user:

        password:

        timeout: 60

    webhook:
        timeout: 5

    pagerduty:
        timeout: 5

        url: "https://events.pagerduty.com/generic/2010-04-15/create_event.json"</pre></div></li><li class="listitem "><p>
    Create the JIRA notification type. The following command example creates a
    JIRA notification type named
    <code class="literal">MyJiraNotification</code>, in the JIRA project
    <code class="literal">HISO</code>.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>monasca notification-create  MyJiraNotification JIRA https://jira.hpcloud.net/?project=HISO</pre></div><p>
    The following command example creates a JIRA notification type named
    <code class="literal">MyJiraNotification</code>, in the JIRA project
    <code class="literal">HISO</code>, and adds the optional
    <em class="replaceable ">component</em> field with a value of
    <code class="literal">keystone</code>.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>monasca notification-create MyJiraNotification JIRA https://jira.hpcloud.net/?project=HISO&amp;component=keystone</pre></div><div id="id-1.5.15.3.5.6.10.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     There is a slash (<code class="literal">/</code>) separating the URL path and the
     query string. The
     slash is required if you have a query parameter without a path parameter.
    </p></div><div id="id-1.5.15.3.5.6.10.2.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     Notification types may also be created in the horizon dashboard.
    </p></div></li></ol></div></div></div><div class="sect2" id="alarm-metrics"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Alarm Metrics</span> <a title="Permalink" class="permalink" href="#alarm-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-alarm_metrics.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-alarm_metrics.xml</li><li><span class="ds-label">ID: </span>alarm-metrics</li></ul></div></div></div></div><p>
  You can use the available metrics to create custom alarms to further monitor
  your cloud infrastructure and facilitate autoscaling features.
 </p><p>
  For details on how to create customer alarms using the Operations Console,
  see <a class="xref" href="#opsconsole-alarm-definitions" title="16.2. Alarm Definition">Section 16.2, “Alarm Definition”</a>.
 </p><div class="sect3" id="apache-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Apache Metrics</span> <a title="Permalink" class="permalink" href="#apache-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-apache_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-apache_metrics.xml</li><li><span class="ds-label">ID: </span>apache-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with the Apache service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>apache.net.hits</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=apache
component=apache</pre></div>
     </td><td>Total accesses</td></tr><tr><td>apache.net.kbytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=apache
component=apache</pre></div>
     </td><td>Total Kbytes per second</td></tr><tr><td>apache.net.requests_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=apache
component=apache</pre></div>
     </td><td>Total accesses per second</td></tr><tr><td>apache.net.total_kbytes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=apache
component=apache</pre></div>
     </td><td>Total Kbytes</td></tr><tr><td>apache.performance.busy_worker_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=apache
component=apache</pre></div>
     </td><td>The number of workers serving requests</td></tr><tr><td>apache.performance.cpu_load_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=apache
component=apache</pre></div>
     </td><td>
      <p>
       The current percentage of CPU used by each worker and in total by all
       workers combined
      </p>
     </td></tr><tr><td>apache.performance.idle_worker_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=apache
component=apache</pre></div>
     </td><td>The number of idle workers</td></tr><tr><td>apache.status</td><td>
<div class="verbatim-wrap"><pre class="screen">apache_port
hostname
service=apache
component=apache</pre></div>
     </td><td>Status of Apache port</td></tr></tbody></table></div></div><div class="sect3" id="ceilometer-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">ceilometer Metrics</span> <a title="Permalink" class="permalink" href="#ceilometer-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-ceilometer_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-ceilometer_metrics.xml</li><li><span class="ds-label">ID: </span>ceilometer-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with the ceilometer service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>disk.total_space_mb_agg</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period=hourly,
host=all,
project_id=all</pre></div>
     </td><td>Total space of disk</td></tr><tr><td>disk.total_used_space_mb_agg</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period=hourly,
host=all,
project_id=all</pre></div>
     </td><td>Total used space of disk</td></tr><tr><td>swiftlm.diskusage.rate_agg</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period=hourly,
host=all,
project_id=all</pre></div>
     </td><td> </td></tr><tr><td>swiftlm.diskusage.val.avail_agg</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period=hourly,
host,
project_id=all</pre></div>
     </td><td> </td></tr><tr><td>swiftlm.diskusage.val.size_agg</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period=hourly,
host,
project_id=all</pre></div>
     </td><td> </td></tr><tr><td>image</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=image,
source=openstack</pre></div>
     </td><td>Existence of the image</td></tr><tr><td>image.delete</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=image,
source=openstack</pre></div>
     </td><td>Delete operation on this image</td></tr><tr><td>image.size</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=B,
source=openstack</pre></div>
     </td><td>Size of the uploaded image</td></tr><tr><td>image.update</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=image,
source=openstack</pre></div>
     </td><td>Update operation on this image</td></tr><tr><td>image.upload</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=image,
source=openstack</pre></div>
     </td><td>Upload operation on this image</td></tr><tr><td>instance</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=instance,
source=openstack</pre></div>
     </td><td>Existence of instance</td></tr><tr><td>disk.ephemeral.size</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=GB,
source=openstack</pre></div>
     </td><td>Size of ephemeral disk on this instance</td></tr><tr><td>disk.root.size</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=GB,
source=openstack</pre></div>
     </td><td>Size of root disk on this instance</td></tr><tr><td>memory</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=MB,
source=openstack</pre></div>
     </td><td>Size of memory on this instance</td></tr><tr><td>ip.floating</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=ip,
source=openstack</pre></div>
     </td><td>Existence of IP</td></tr><tr><td>ip.floating.create</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=ip,
source=openstack</pre></div>
     </td><td>Create operation on this fip</td></tr><tr><td>ip.floating.update</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=ip,
source=openstack</pre></div>
     </td><td>Update operation on this fip</td></tr><tr><td>mem.total_mb_agg</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period=hourly,
host=all,
project_id=all</pre></div>
     </td><td>Total space of memory</td></tr><tr><td>mem.usable_mb_agg</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period=hourly,
host=all,
project_id=all</pre></div>
     </td><td>Available space of memory</td></tr><tr><td>network</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=network,
source=openstack</pre></div>
     </td><td>Existence of network</td></tr><tr><td>network.create</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=network,
source=openstack</pre></div>
     </td><td>Create operation on this network</td></tr><tr><td>network.update</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=network,
source=openstack</pre></div>
     </td><td>Update operation on this network</td></tr><tr><td>network.delete</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=network,
source=openstack</pre></div>
     </td><td>Delete operation on this network</td></tr><tr><td>port</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=port,
source=openstack</pre></div>
     </td><td>Existence of port</td></tr><tr><td>port.create</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=port,
source=openstack</pre></div>
     </td><td>Create operation on this port</td></tr><tr><td>port.delete</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=port,
source=openstack</pre></div>
     </td><td>Delete operation on this port</td></tr><tr><td>port.update</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=port,
source=openstack</pre></div>
     </td><td>Update operation on this port</td></tr><tr><td>router</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=router,
source=openstack</pre></div>
     </td><td>Existence of router</td></tr><tr><td>router.create</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=router,
source=openstack</pre></div>
     </td><td>Create operation on this router</td></tr><tr><td>router.delete</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=router,
source=openstack</pre></div>
     </td><td>Delete operation on this router</td></tr><tr><td>router.update</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=router,
source=openstack</pre></div>
     </td><td>Update operation on this router</td></tr><tr><td>snapshot</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=snapshot,
source=openstack</pre></div>
     </td><td>Existence of the snapshot</td></tr><tr><td>snapshot.create.end</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=snapshot,
source=openstack</pre></div>
     </td><td>Create operation on this snapshot</td></tr><tr><td>snapshot.delete.end</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=snapshot,
source=openstack</pre></div>
     </td><td>Delete operation on this snapshot</td></tr><tr><td>snapshot.size</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=GB,
source=openstack</pre></div>
     </td><td>Size of this snapshot</td></tr><tr><td>subnet</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=subnet,
source=openstack</pre></div>
     </td><td>Existence of the subnet</td></tr><tr><td>subnet.create</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=subnet,
source=openstack</pre></div>
     </td><td>Create operation on this subnet</td></tr><tr><td>subnet.delete</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=subnet,
source=openstack</pre></div>
     </td><td>Delete operation on this subnet</td></tr><tr><td>subnet.update</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=subnet,
source=openstack</pre></div>
     </td><td>Update operation on this subnet</td></tr><tr><td>vcpus</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=vcpus,
source=openstack</pre></div>
     </td><td>Number of virtual CPUs allocated to the instance</td></tr><tr><td>vcpus_agg</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period=hourly,
host=all,
project_id</pre></div>
     </td><td>Number of vcpus used by a project</td></tr><tr><td>volume</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=volume,
source=openstack</pre></div>
     </td><td>Existence of the volume</td></tr><tr><td>volume.create.end</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=volume,
source=openstack</pre></div>
     </td><td>Create operation on this volume</td></tr><tr><td>volume.delete.end</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=volume,
source=openstack</pre></div>
     </td><td>Delete operation on this volume</td></tr><tr><td>volume.resize.end</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=volume,
source=openstack</pre></div>
     </td><td>Resize operation on this volume</td></tr><tr><td>volume.size</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=GB,
source=openstack</pre></div>
     </td><td>Size of this volume</td></tr><tr><td>volume.update.end</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=delta,
unit=volume,
source=openstack</pre></div>
     </td><td>Update operation on this volume</td></tr><tr><td>storage.objects</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=object,
source=openstack</pre></div>
     </td><td>Number of objects</td></tr><tr><td>storage.objects.size</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=B,
source=openstack</pre></div>
     </td><td>Total size of stored objects</td></tr><tr><td>storage.objects.containers</td><td>
<div class="verbatim-wrap"><pre class="screen">user_id,
region,
resource_id,
datasource=ceilometer,
project_id,
type=gauge,
unit=container,
source=openstack</pre></div>
     </td><td>Number of containers</td></tr></tbody></table></div></div><div class="sect3" id="idg-all-operations-monitoring-cinder-metrics-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">cinder Metrics</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-cinder-metrics-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-cinder_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-cinder_metrics.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-cinder-metrics-xml-1</li></ul></div></div></div></div><p>
  A list of metrics associated with the cinder service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>cinderlm.cinder.backend.physical.list</td><td>
      <p>
       service=block-storage, hostname, cluster, cloud_name, control_plane,
       component, backends
      </p>
     </td><td> List of physical backends</td></tr><tr><td>cinderlm.cinder.backend.total.avail</td><td>
      <p>
       service=block-storage, hostname, cluster, cloud_name, control_plane,
       component, backendname
      </p>
     </td><td>Total available capacity metric per backend</td></tr><tr><td>cinderlm.cinder.backend.total.size</td><td>
      <p>
       service=block-storage, hostname, cluster, cloud_name, control_plane,
       component, backendname
      </p>
     </td><td>Total capacity metric per backend</td></tr><tr><td>cinderlm.cinder.cinder_services</td><td>
      <p>
       service=block-storage, hostname, cluster, cloud_name, control_plane,
       component
      </p>
     </td><td>Status of a cinder-volume service</td></tr><tr><td>cinderlm.hp_hardware.hpssacli.logical_drive</td><td>
      <p>
       service=block-storage, hostname, cluster, cloud_name, control_plane,
       component, sub_component, logical_drive, controller_slot, array
      </p>
      <p>
       The HPE Smart Storage Administrator (HPE SSA) CLI component will have to be
       installed for SSACLI status to be reported. To download and install the
       SSACLI utility to enable management of disk controllers, please refer
       to: <a class="link" href="https://support.hpe.com/hpsc/swd/public/detail?swItemId=MTX_3d16386b418a443388c18da82f" target="_blank">https://support.hpe.com/hpsc/swd/public/detail?swItemId=MTX_3d16386b418a443388c18da82f</a>
      </p>
     </td><td>Status of a logical drive</td></tr><tr><td>cinderlm.hp_hardware.hpssacli.physical_drive</td><td>
      <p>
       service=block-storage, hostname, cluster, cloud_name, control_plane,
       component, box, bay, controller_slot
      </p>
     </td><td>Status of a logical drive</td></tr><tr><td>cinderlm.hp_hardware.hpssacli.smart_array</td><td>
      <p>
       service=block-storage, hostname, cluster, cloud_name, control_plane,
       component, sub_component, model
      </p>
     </td><td>Status of smart array</td></tr><tr><td>cinderlm.hp_hardware.hpssacli.smart_array.firmware</td><td>
      <p>
       service=block-storage, hostname, cluster, cloud_name, control_plane,
       component, model
      </p>
     </td><td>Checks firmware version</td></tr></tbody></table></div></div><div class="sect3" id="idg-all-operations-monitoring-compute-metrics-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute Metrics</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-compute-metrics-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-compute_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-compute_metrics.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-compute-metrics-xml-1</li></ul></div></div></div></div><div id="id-1.5.15.3.6.7.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   Compute instance metrics are listed in <a class="xref" href="#libvirt-metrics" title="13.1.4.11. Libvirt Metrics">Section 13.1.4.11, “Libvirt Metrics”</a>.
  </p></div><p>
  A list of metrics associated with the Compute service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>nova.heartbeat</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
cloud_name
hostname
component
control_plane
cluster</pre></div>
     </td><td>
      <p>
       Checks that all services are running heartbeats (uses nova user and to
       list services then sets up checks for each. For example, nova-scheduler,
       nova-conductor, nova-compute)
      </p>
     </td></tr><tr><td>nova.vm.cpu.total_allocated</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
hostname
component
control_plane
cluster</pre></div>
     </td><td>Total CPUs allocated across all VMs</td></tr><tr><td>nova.vm.disk.total_allocated_gb</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
hostname
component
control_plane
cluster</pre></div>
     </td><td>Total Gbytes of disk space allocated to all VMs</td></tr><tr><td>nova.vm.mem.total_allocated_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
hostname
component
control_plane
cluster</pre></div>
     </td><td>Total Mbytes of memory allocated to all VMs</td></tr></tbody></table></div></div><div class="sect3" id="crash-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Crash Metrics</span> <a title="Permalink" class="permalink" href="#crash-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-crash_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-crash_metrics.xml</li><li><span class="ds-label">ID: </span>crash-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with the Crash service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>crash.dump_count</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>Number of crash dumps found</td></tr></tbody></table></div></div><div class="sect3" id="idg-all-operations-monitoring-directory-metrics-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Directory Metrics</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-directory-metrics-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-directory_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-directory_metrics.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-directory-metrics-xml-1</li></ul></div></div></div></div><p>
  A list of metrics associated with the Directory service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>directory.files_count</td><td>
<div class="verbatim-wrap"><pre class="screen">service
hostname
path</pre></div>
     </td><td>Total number of files under a specific directory path</td></tr><tr><td>directory.size_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">service
hostname
path</pre></div>
     </td><td>Total size of a specific directory path</td></tr></tbody></table></div></div><div class="sect3" id="idg-all-operations-monitoring-elasticsearch-metrics-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Elasticsearch Metrics</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-elasticsearch-metrics-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-elasticsearch_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-elasticsearch_metrics.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-elasticsearch-metrics-xml-1</li></ul></div></div></div></div><p>
  A list of metrics associated with the Elasticsearch service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>elasticsearch.active_primary_shards</td><td>
<div class="verbatim-wrap"><pre class="screen">service=logging
url
hostname</pre></div>
     </td><td>
      <p>
       Indicates the number of primary shards in your cluster. This is an
       aggregate total across all indices.
      </p>
     </td></tr><tr><td>elasticsearch.active_shards</td><td>
<div class="verbatim-wrap"><pre class="screen">service=logging
url
hostname</pre></div>
     </td><td>
      <p>
       Aggregate total of all shards across all indices, which includes replica
       shards.
      </p>
     </td></tr><tr><td>elasticsearch.cluster_status</td><td>
<div class="verbatim-wrap"><pre class="screen">service=logging
url
hostname</pre></div>
     </td><td>
      <p>
       Cluster health status.
      </p>
     </td></tr><tr><td>elasticsearch.initializing_shards</td><td>
<div class="verbatim-wrap"><pre class="screen">service=logging
url
hostname</pre></div>
     </td><td>
      <p>
       The count of shards that are being freshly created.
      </p>
     </td></tr><tr><td>elasticsearch.number_of_data_nodes</td><td>
<div class="verbatim-wrap"><pre class="screen">service=logging
url
hostname</pre></div>
     </td><td>
      <p>
       Number of data nodes.
      </p>
     </td></tr><tr><td>elasticsearch.number_of_nodes</td><td>
<div class="verbatim-wrap"><pre class="screen">service=logging
url
hostname</pre></div>
     </td><td>
      <p>
       Number of nodes.
      </p>
     </td></tr><tr><td>elasticsearch.relocating_shards</td><td>
<div class="verbatim-wrap"><pre class="screen">service=logging
url
hostname</pre></div>
     </td><td>
      <p>
       Shows the number of shards that are currently moving from one node to
       another node.
      </p>
     </td></tr><tr><td>elasticsearch.unassigned_shards</td><td>
<div class="verbatim-wrap"><pre class="screen">service=logging
url
hostname</pre></div>
     </td><td>
      <p>
       The number of unassigned shards from the master node.
      </p>
     </td></tr></tbody></table></div></div><div class="sect3" id="idg-all-operations-monitoring-haproxy-metrics-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HAProxy Metrics</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-haproxy-metrics-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-haproxy_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-haproxy_metrics.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-haproxy-metrics-xml-1</li></ul></div></div></div></div><p>
  A list of metrics associated with the HAProxy service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>haproxy.backend.bytes.in_rate</td><td> </td><td> </td></tr><tr><td>haproxy.backend.bytes.out_rate</td><td> </td><td> </td></tr><tr><td>haproxy.backend.denied.req_rate</td><td> </td><td> </td></tr><tr><td>haproxy.backend.denied.resp_rate</td><td> </td><td> </td></tr><tr><td>haproxy.backend.errors.con_rate</td><td> </td><td> </td></tr><tr><td>haproxy.backend.errors.resp_rate</td><td> </td><td> </td></tr><tr><td>haproxy.backend.queue.current</td><td> </td><td> </td></tr><tr><td>haproxy.backend.response.1xx</td><td> </td><td> </td></tr><tr><td>haproxy.backend.response.2xx</td><td> </td><td> </td></tr><tr><td>haproxy.backend.response.3xx</td><td> </td><td> </td></tr><tr><td>haproxy.backend.response.4xx</td><td> </td><td> </td></tr><tr><td>haproxy.backend.response.5xx</td><td> </td><td> </td></tr><tr><td>haproxy.backend.response.other</td><td> </td><td> </td></tr><tr><td>haproxy.backend.session.current</td><td> </td><td> </td></tr><tr><td>haproxy.backend.session.limit</td><td> </td><td> </td></tr><tr><td>haproxy.backend.session.pct</td><td> </td><td> </td></tr><tr><td>haproxy.backend.session.rate</td><td> </td><td> </td></tr><tr><td>haproxy.backend.warnings.redis_rate</td><td> </td><td> </td></tr><tr><td>haproxy.backend.warnings.retr_rate</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.bytes.in_rate</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.bytes.out_rate</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.denied.req_rate</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.denied.resp_rate</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.errors.req_rate</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.requests.rate</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.response.1xx</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.response.2xx</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.response.3xx</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.response.4xx</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.response.5xx</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.response.other</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.session.current</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.session.limit</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.session.pct</td><td> </td><td> </td></tr><tr><td>haproxy.frontend.session.rate</td><td> </td><td> </td></tr></tbody></table></div></div><div class="sect3" id="httpcheck-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HTTP Check Metrics</span> <a title="Permalink" class="permalink" href="#httpcheck-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-httpcheck_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-httpcheck_metrics.xml</li><li><span class="ds-label">ID: </span>httpcheck-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with the HTTP Check service:
 </p><div class="table" id="id-1.5.15.3.6.12.3"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 13.2: </span><span class="name">HTTP Check Metrics </span><a title="Permalink" class="permalink" href="#id-1.5.15.3.6.12.3">#</a></h6></div><div class="table-contents"><table class="table" summary="HTTP Check Metrics" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>http_response_time</td><td>
<div class="verbatim-wrap"><pre class="screen">url
hostname
service
component</pre></div>
     </td><td>The response time in seconds of the http endpoint call.</td></tr><tr><td>http_status</td><td>
<div class="verbatim-wrap"><pre class="screen">url
hostname
service</pre></div>
     </td><td>The status of the http endpoint call (0 = success, 1 = failure).</td></tr></tbody></table></div></div><p>
  For each component and HTTP metric name there are two separate metrics
  reported, one for the local URL and another for the virtual IP (VIP) URL:
 </p><div class="table" id="id-1.5.15.3.6.12.5"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 13.3: </span><span class="name">HTTP Metric Components </span><a title="Permalink" class="permalink" href="#id-1.5.15.3.6.12.5">#</a></h6></div><div class="table-contents"><table class="table" summary="HTTP Metric Components" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Component</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>account-server</td><td>
<div class="verbatim-wrap"><pre class="screen">service=object-storage
component=account-server
url</pre></div>
     </td><td>swift account-server http endpoint status and response time</td></tr><tr><td>barbican-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=key-manager
component=barbican-api
url</pre></div>
     </td><td>barbican-api http endpoint status and response time</td></tr><tr><td>cinder-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=block-storage
component=cinder-api
url</pre></div>
     </td><td>cinder-api http endpoint status and response time</td></tr><tr><td>container-server</td><td>
<div class="verbatim-wrap"><pre class="screen">service=object-storage
component=container-server
url</pre></div>
     </td><td>swift container-server http endpoint status and response time</td></tr><tr><td>designate-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=dns
component=designate-api
url</pre></div>
     </td><td>designate-api http endpoint status and response time</td></tr><tr><td>glance-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=image-service
component=glance-api
url</pre></div>
     </td><td>glance-api http endpoint status and response time</td></tr><tr><td>glance-registry</td><td>
<div class="verbatim-wrap"><pre class="screen">service=image-service
component=glance-registry
url</pre></div>
     </td><td>glance-registry http endpoint status and response time</td></tr><tr><td>heat-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=orchestration
component=heat-api
url</pre></div>
     </td><td>heat-api http endpoint status and response time</td></tr><tr><td>heat-api-cfn</td><td>
<div class="verbatim-wrap"><pre class="screen">service=orchestration
component=heat-api-cfn
url</pre></div>
     </td><td>heat-api-cfn http endpoint status and response time</td></tr><tr><td>heat-api-cloudwatch</td><td>
<div class="verbatim-wrap"><pre class="screen">service=orchestration
component=heat-api-cloudwatch
url</pre></div>
     </td><td>heat-api-cloudwatch http endpoint status and response time</td></tr><tr><td>ardana-ux-services</td><td>
<div class="verbatim-wrap"><pre class="screen">service=ardana-ux-services
component=ardana-ux-services
url</pre></div>
     </td><td>ardana-ux-services http endpoint status and response time</td></tr><tr><td>horizon</td><td>
<div class="verbatim-wrap"><pre class="screen">service=web-ui
component=horizon
url</pre></div>
     </td><td>horizon http endpoint status and response time</td></tr><tr><td>keystone-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=identity-service
component=keystone-api
url</pre></div>
     </td><td>keystone-api http endpoint status and response time</td></tr><tr><td>monasca-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
component=monasca-api
url</pre></div>
     </td><td>monasca-api http endpoint status</td></tr><tr><td>monasca-persister</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
component=monasca-persister
url</pre></div>
     </td><td>monasca-persister http endpoint status</td></tr><tr><td>neutron-server</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
component=neutron-server
url</pre></div>
     </td><td>neutron-server http endpoint status and response time</td></tr><tr><td>neutron-server-vip</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
component=neutron-server-vip
url</pre></div>
     </td><td>neutron-server-vip http endpoint status and response time</td></tr><tr><td>nova-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
component=nova-api
url</pre></div>
     </td><td>nova-api http endpoint status and response time</td></tr><tr><td>nova-vnc</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
component=nova-vnc
url</pre></div>
     </td><td>nova-vnc http endpoint status and response time</td></tr><tr><td>object-server</td><td>
<div class="verbatim-wrap"><pre class="screen">service=object-storage
component=object-server
url</pre></div>
     </td><td>object-server http endpoint status and response time</td></tr><tr><td>object-storage-vip</td><td>
<div class="verbatim-wrap"><pre class="screen">service=object-storage
component=object-storage-vip
url</pre></div>
     </td><td>object-storage-vip http endpoint status and response time</td></tr><tr><td>octavia-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=octavia
component=octavia-api
url</pre></div>
     </td><td>octavia-api http endpoint status and response time</td></tr><tr><td>ops-console-web</td><td>
<div class="verbatim-wrap"><pre class="screen">service=ops-console
component=ops-console-web
url</pre></div>
     </td><td>ops-console-web http endpoint status and response time</td></tr><tr><td>proxy-server</td><td>
<div class="verbatim-wrap"><pre class="screen">service=object-storage
component=proxy-server
url</pre></div>
     </td><td>proxy-server http endpoint status and response time</td></tr></tbody></table></div></div></div><div class="sect3" id="kafka-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Kafka Metrics</span> <a title="Permalink" class="permalink" href="#kafka-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-kafka_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-kafka_metrics.xml</li><li><span class="ds-label">ID: </span>kafka-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with the Kafka service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>kafka.consumer_lag</td><td>
<div class="verbatim-wrap"><pre class="screen">topic
service
component=kafka
consumer_group
hostname</pre></div>
     </td><td>Hostname consumer offset lag from broker offset</td></tr></tbody></table></div></div><div class="sect3" id="libvirt-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Libvirt Metrics</span> <a title="Permalink" class="permalink" href="#libvirt-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-libvirt_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-libvirt_metrics.xml</li><li><span class="ds-label">ID: </span>libvirt-metrics</li></ul></div></div></div></div><div id="id-1.5.15.3.6.14.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   For information on how to turn these metrics on and off using the tuning
   knobs, see <a class="xref" href="#libvirt-tuningknobs" title="13.1.2.5.1. Libvirt plugin metric tuning knobs">Section 13.1.2.5.1, “Libvirt plugin metric tuning knobs”</a>.
  </p></div><p>
  A list of metrics associated with the Libvirt service.
 </p><div class="table" id="id-1.5.15.3.6.14.4"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 13.4: </span><span class="name">Tunable Libvirt Metrics </span><a title="Permalink" class="permalink" href="#id-1.5.15.3.6.14.4">#</a></h6></div><div class="table-contents"><table class="table" summary="Tunable Libvirt Metrics" border="1"><colgroup><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /></colgroup><thead><tr><th>Admin Metric Name</th><th>Project Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>vm.cpu.time_ns</td><td>cpu.time_ns</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Cumulative CPU time (in ns)</td></tr><tr><td>vm.cpu.utilization_norm_perc</td><td>cpu.utilization_norm_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Normalized CPU utilization (percentage)</td></tr><tr><td>vm.cpu.utilization_perc</td><td>cpu.utilization_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Overall CPU utilization (percentage)</td></tr><tr><td>vm.io.errors</td><td>io.errors</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Overall disk I/O errors</td></tr><tr><td>vm.io.errors_sec</td><td>io.errors_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Disk I/O errors per second</td></tr><tr><td>vm.io.read_bytes</td><td>io.read_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Disk I/O read bytes value</td></tr><tr><td>vm.io.read_bytes_sec</td><td>io.read_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Disk I/O read bytes per second</td></tr><tr><td>vm.io.read_ops</td><td>io.read_ops</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Disk I/O read operations value</td></tr><tr><td>vm.io.read_ops_sec</td><td>io.read_ops_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Disk I/O write operations per second</td></tr><tr><td>vm.io.write_bytes</td><td>io.write_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Disk I/O write bytes value</td></tr><tr><td>vm.io.write_bytes_sec</td><td>io.write_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Disk I/O write bytes per second</td></tr><tr><td>vm.io.write_ops</td><td>io.write_ops</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Disk I/O write operations value</td></tr><tr><td>vm.io.write_ops_sec</td><td> io.write_ops_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Disk I/O write operations per second</td></tr><tr><td>vm.net.in_bytes</td><td>net.in_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component
device
port_id</pre></div>
     </td><td>Network received total bytes</td></tr><tr><td>vm.net.in_bytes_sec</td><td>net.in_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component
device
port_id</pre></div>
     </td><td>Network received bytes per second</td></tr><tr><td>vm.net.in_packets</td><td>net.in_packets</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component
device
port_id</pre></div>
     </td><td>Network received total packets</td></tr><tr><td>vm.net.in_packets_sec</td><td>net.in_packets_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component
device
port_id</pre></div>
     </td><td>Network received packets per second</td></tr><tr><td>vm.net.out_bytes</td><td>net.out_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component
device
port_id</pre></div>
     </td><td>Network transmitted total bytes</td></tr><tr><td>vm.net.out_bytes_sec</td><td>net.out_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component
device
port_id</pre></div>
     </td><td>Network transmitted bytes per second</td></tr><tr><td>vm.net.out_packets</td><td>net.out_packets</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component
device
port_id</pre></div>
     </td><td>Network transmitted total packets</td></tr><tr><td>vm.net.out_packets_sec</td><td>net.out_packets_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component
device
port_id</pre></div>
     </td><td>Network transmitted packets per second</td></tr><tr><td>vm.ping_status</td><td>ping_status</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>0 for ping success, 1 for ping failure</td></tr><tr><td>vm.disk.allocation</td><td>disk.allocation</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk allocation for a device</td></tr><tr><td>vm.disk.allocation_total</td><td>disk.allocation_total</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk allocation across devices for instances</td></tr><tr><td>vm.disk.capacity</td><td>disk.capacity</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk capacity for a device</td></tr><tr><td>vm.disk.capacity_total</td><td>disk.capacity_total</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk capacity across devices for instances</td></tr><tr><td>vm.disk.physical</td><td>disk.physical</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk usage for a device</td></tr><tr><td>vm.disk.physical_total</td><td>disk.physical_total</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk usage across devices for instances</td></tr><tr><td>vm.io.errors_total</td><td>io.errors_total</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O errors across all devices</td></tr><tr><td>vm.io.errors_total_sec</td><td>io.errors_total_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O errors per second across all devices</td></tr><tr><td>vm.io.read_bytes_total</td><td>io.read_bytes_total</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O read bytes across all devices</td></tr><tr><td>vm.io.read_bytes_total_sec</td><td>io.read_bytes_total_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O read bytes per second across devices</td></tr><tr><td>vm.io.read_ops_total</td><td>io.read_ops_total</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O read operations across all devices</td></tr><tr><td>vm.io.read_ops_total_sec</td><td>io.read_ops_total_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O read operations across all devices per sec</td></tr><tr><td>vm.io.write_bytes_total</td><td>io.write_bytes_total</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O write bytes across all devices</td></tr><tr><td>vm.io.write_bytes_total_sec</td><td>io.write_bytes_total_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O Write bytes per second across devices</td></tr><tr><td>vm.io.write_ops_total</td><td>io.write_ops_total</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O write operations across all devices</td></tr><tr><td>vm.io.write_ops_total_sec</td><td>io.write_ops_total_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>Total Disk I/O write operations across all devices per sec</td></tr></tbody></table></div></div><p>
  These metrics in libvirt are always enabled and cannot be disabled using the
  tuning knobs.
 </p><div class="table" id="id-1.5.15.3.6.14.6"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 13.5: </span><span class="name">Untunable Libvirt Metrics </span><a title="Permalink" class="permalink" href="#id-1.5.15.3.6.14.6">#</a></h6></div><div class="table-contents"><table class="table" summary="Untunable Libvirt Metrics" border="1"><colgroup><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /></colgroup><thead><tr><th>Admin Metric Name</th><th>Project Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>vm.host_alive_status</td><td>host_alive_status</td><td>
<div class="verbatim-wrap"><pre class="screen">zone
service
resource_id
hostname
component</pre></div>
     </td><td>
      <p>
       -1 for no status, 0 for Running / OK, 1 for Idle / blocked, 2 for
       Paused,
      </p>
      <p>
       3 for Shutting down, 4 for Shut off or nova suspend 5 for Crashed,
      </p>
      <p>
       6 for Power management suspend (S3 state)
      </p>
     </td></tr><tr><td>vm.mem.free_mb</td><td>mem.free_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
service
hostname</pre></div>
     </td><td>Free memory in Mbytes</td></tr><tr><td>vm.mem.free_perc</td><td>mem.free_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
service
hostname</pre></div>
     </td><td>Percent of memory free</td></tr><tr><td>vm.mem.resident_mb</td><td> </td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
service
hostname</pre></div>
     </td><td>Total memory used on host, an Operations-only metric</td></tr><tr><td>vm.mem.swap_used_mb</td><td>mem.swap_used_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
service
hostname</pre></div>
     </td><td>Used swap space in Mbytes</td></tr><tr><td>vm.mem.total_mb</td><td>mem.total_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
service
hostname</pre></div>
     </td><td>Total memory in Mbytes</td></tr><tr><td>vm.mem.used_mb</td><td>mem.used_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
service
hostname</pre></div>
     </td><td>Used memory in Mbytes</td></tr></tbody></table></div></div></div><div class="sect3" id="idg-all-operations-monitoring-monasca-metrics-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring Metrics</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-monasca-metrics-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-monasca_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monasca_metrics.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-monasca-metrics-xml-1</li></ul></div></div></div></div><p>
  A list of metrics associated with the Monitoring service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>alarm-state-transitions-added-to-batch-counter</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
url
hostname
component=monasca-persister</pre></div>
     </td><td> </td></tr><tr><td>jvm.memory.total.max</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
url
hostname
component</pre></div>
     </td><td>Maximum JVM overall memory</td></tr><tr><td>jvm.memory.total.used</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
url
hostname
component</pre></div>
     </td><td>Used JVM overall memory</td></tr><tr><td>metrics-added-to-batch-counter</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
url
hostname
component=monasca-persister</pre></div>
     </td><td> </td></tr><tr><td>metrics.published</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
url
hostname
component=monasca-api</pre></div>
     </td><td>Total number of published metrics</td></tr><tr><td>monasca.alarms_finished_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-notification
service=monitoring</pre></div>
     </td><td>Total number of alarms received</td></tr><tr><td>monasca.checks_running_too_long</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-agent
service=monitoring
cluster</pre></div>
     </td><td>Only emitted when collection time for a check is too long</td></tr><tr><td>monasca.collection_time_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-agent
service=monitoring
cluster</pre></div>
     </td><td>Collection time in monasca-agent</td></tr><tr><td>monasca.config_db_time</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-notification
service=monitoring</pre></div>
     </td><td> </td></tr><tr><td>monasca.created_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-notification
service=monitoring</pre></div>
     </td><td>Number of notifications created</td></tr><tr><td>monasca.invalid_type_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-notification
service=monitoring</pre></div>
     </td><td>Number of notifications with invalid type</td></tr><tr><td>monasca.log.in_bulks_rejected</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-log-api
service=monitoring
version</pre></div>
     </td><td> </td></tr><tr><td>monasca.log.in_logs</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-log-api
service=monitoring
version</pre></div>
     </td><td> </td></tr><tr><td>monasca.log.in_logs_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-log-api
service=monitoring
version</pre></div>
     </td><td> </td></tr><tr><td>monasca.log.in_logs_rejected</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-log-api
service=monitoring
version</pre></div>
     </td><td> </td></tr><tr><td>monasca.log.out_logs</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-log-api
service=monitoring</pre></div>
     </td><td> </td></tr><tr><td>monasca.log.out_logs_lost</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-log-api
service=monitoring</pre></div>
     </td><td> </td></tr><tr><td>monasca.log.out_logs_truncated_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-log-api
service=monitoring</pre></div>
     </td><td> </td></tr><tr><td>monasca.log.processing_time_ms</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-log-api
service=monitoring</pre></div>
     </td><td> </td></tr><tr><td>monasca.log.publish_time_ms</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
component=monasca-log-api
service=monitoring</pre></div>
     </td><td> </td></tr><tr><td>monasca.thread_count</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
process_name
hostname
component</pre></div>
     </td><td>Number of threads monasca is using</td></tr><tr><td>raw-sql.time.avg</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
url
hostname
component</pre></div>
     </td><td>Average raw sql query time</td></tr><tr><td>raw-sql.time.max</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
url
hostname
component</pre></div>
     </td><td>Max raw sql query time</td></tr></tbody></table></div></div><div class="sect3" id="idg-all-operations-monitoring-monasca-agg-metrics-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monasca Aggregated Metrics</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-monasca-agg-metrics-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-monasca_agg_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-monasca_agg_metrics.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-monasca-agg-metrics-xml-1</li></ul></div></div></div></div><p>
  A list of the aggregated metrics associated with the monasca Transform
  feature.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="newCol2" /><col class="newCol3" /><col class="c3" /><col class="c2" /></colgroup><thead><tr><th>Metric Name</th><th>For</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>cpu.utilized_logical_cores_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all or &lt;hostname&gt;
project_id: all</pre></div>
     </td><td>
      <p>
       Utilized physical host cpu core capacity for one or all hosts by time
       interval (defaults to a hour).
      </p>
      <p>
       Available as total or per host
      </p>
     </td></tr><tr><td>cpu.total_logical_cores_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all or &lt;hostname&gt;
project_id: all</pre></div>
     </td><td>
      <p>
       Total physical host cpu core capacity for one or all hosts by time
       interval (defaults to a hour)
      </p>
      <p>
       Available as total or per host
      </p>
     </td></tr><tr><td>mem.total_mb_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all</pre></div>
     </td><td>
      <p>
       Total physical host memory capacity by time interval (defaults to a
       hour)
      </p>
     </td></tr><tr><td>mem.usable_mb_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all</pre></div>
     </td><td>Usable physical host memory capacity by time interval (defaults to a
                hour)</td></tr><tr><td>disk.total_used_space_mb_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all</pre></div>
     </td><td>
      <p>
       Utilized physical host disk capacity by time interval (defaults to a
       hour)
      </p>
     </td></tr><tr><td>disk.total_space_mb_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all</pre></div>
     </td><td>Total physical host disk capacity by time interval (defaults to a hour)</td></tr><tr><td>nova.vm.cpu.total_allocated_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all</pre></div>
     </td><td>
      <p>
       CPUs allocated across all virtual machines by time interval (defaults to
       a hour)
      </p>
     </td></tr><tr><td>vcpus_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all or &lt;project ID&gt;</pre></div>
     </td><td>
      <p>
       Virtual CPUs allocated capacity for virtual machines of one or all
       projects by time interval (defaults to a hour)
      </p>
      <p>
       Available as total or per host
      </p>
     </td></tr><tr><td>nova.vm.mem.total_allocated_mb_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all</pre></div>
     </td><td>
      <p>
       Memory allocated to all virtual machines by time interval (defaults to a
       hour)
      </p>
     </td></tr><tr><td>vm.mem.used_mb_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all or &lt;project ID&gt;</pre></div>
     </td><td>
      <p>
       Memory utilized by virtual machines of one or all projects by time
       interval (defaults to an hour)
      </p>
      <p>
       Available as total or per host
      </p>
     </td></tr><tr><td>vm.mem.total_mb_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all or &lt;project ID&gt;</pre></div>
     </td><td>
      <p>
       Memory allocated to virtual machines of one or all projects by time
       interval (defaults to an hour)
      </p>
      <p>
       Available as total or per host
      </p>
     </td></tr><tr><td>vm.cpu.utilization_perc_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all or &lt;project ID&gt;</pre></div>
     </td><td>
      <p>
       CPU utilized by all virtual machines by project by time interval
       (defaults to an hour)
      </p>
     </td></tr><tr><td>nova.vm.disk.total_allocated_gb_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all</pre></div>
     </td><td>
      <p>
       Disk space allocated to all virtual machines by time interval (defaults
       to an hour)
      </p>
     </td></tr><tr><td>vm.disk.allocation_agg</td><td>Compute summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all or &lt;project ID&gt;</pre></div>
     </td><td>
      <p>
       Disk allocation for virtual machines of one or all projects by time
       interval (defaults to a hour)
      </p>
      <p>
       Available as total or per host
      </p>
     </td></tr><tr><td>swiftlm.diskusage.val.size_agg</td><td>Object Storage summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all or &lt;hostname&gt;
project_id: all</pre></div>
     </td><td>
      <p>
       Total available object storage capacity by time interval (defaults to a
       hour)
      </p>
      <p>
       Available as total or per host
      </p>
     </td></tr><tr><td>swiftlm.diskusage.val.avail_agg</td><td>Object Storage summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all or &lt;hostname&gt;
project_id: all</pre></div>
     </td><td>
      <p>
       Remaining object storage capacity by time interval (defaults to a hour)
      </p>
      <p>
       Available as total or per host
      </p>
     </td></tr><tr><td>swiftlm.diskusage.rate_agg</td><td>Object Storage summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all</pre></div>
     </td><td>
      <p>
       Rate of change of object storage usage by time interval (defaults to a
       hour)
      </p>
     </td></tr><tr><td>storage.objects.size_agg</td><td>Object Storage summary</td><td>
<div class="verbatim-wrap"><pre class="screen">aggregation_period: hourly
host: all
project_id: all</pre></div>
     </td><td>
      <p>
       Used object storage capacity by time interval (defaults to a hour)
      </p>
     </td></tr></tbody></table></div></div><div class="sect3" id="mysql-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">MySQL Metrics</span> <a title="Permalink" class="permalink" href="#mysql-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-mysql_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-mysql_metrics.xml</li><li><span class="ds-label">ID: </span>mysql-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with the MySQL service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>mysql.innodb.buffer_pool_free</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       The number of free pages, in bytes. This value is calculated by
       multiplying <code class="literal">Innodb_buffer_pool_pages_free</code> and
       <code class="literal">Innodb_page_size</code> of the server status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.buffer_pool_total</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       The total size of buffer pool, in bytes. This value is calculated by
       multiplying <code class="literal">Innodb_buffer_pool_pages_total</code> and
       <code class="literal">Innodb_page_size</code> of the server status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.buffer_pool_used</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       The number of used pages, in bytes. This value is calculated by
       subtracting <code class="literal">Innodb_buffer_pool_pages_total</code> away from
       <code class="literal">Innodb_buffer_pool_pages_free</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.innodb.current_row_locks</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to current row locks of the server status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.data_reads</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Innodb_data_reads</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.data_writes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Innodb_data_writes</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.mutex_os_waits</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to the OS waits of the server status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.mutex_spin_rounds</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to spinlock rounds of the server status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.mutex_spin_waits</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to the spin waits of the server status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.os_log_fsyncs</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Innodb_os_log_fsyncs</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.row_lock_time</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Innodb_row_lock_time</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.innodb.row_lock_waits</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Innodb_row_lock_waits</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.net.connections</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Connections</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.net.max_connections</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Max_used_connections</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.performance.com_delete</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Com_delete</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.performance.com_delete_multi</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Com_delete_multi</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.performance.com_insert</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Com_insert</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.performance.com_insert_select</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Com_insert_select</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.performance.com_replace_select</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Com_replace_select</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.performance.com_select</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Com_select</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.performance.com_update</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Com_update</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.performance.com_update_multi</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Com_update_multi</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.performance.created_tmp_disk_tables</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Created_tmp_disk_tables</code> of the
       server status variable.
      </p>
     </td></tr><tr><td>mysql.performance.created_tmp_files</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Created_tmp_files</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.performance.created_tmp_tables</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Created_tmp_tables</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.performance.kernel_time</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       The kernel time for the databases performance, in seconds.
      </p>
     </td></tr><tr><td>mysql.performance.open_files</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Open_files</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.performance.qcache_hits</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Qcache_hits</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.performance.queries</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Queries</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.performance.questions</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Question</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.performance.slow_queries</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Slow_queries</code> of the server status
       variable.
      </p>
     </td></tr><tr><td>mysql.performance.table_locks_waited</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Table_locks_waited</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.performance.threads_connected</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       Corresponding to <code class="literal">Threads_connected</code> of the server
       status variable.
      </p>
     </td></tr><tr><td>mysql.performance.user_time</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=mysql</pre></div>
     </td><td>
      <p>
       The CPU user time for the databases performance, in seconds.
      </p>
     </td></tr></tbody></table></div></div><div class="sect3" id="idg-all-operations-monitoring-ntp-metrics-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.15 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">NTP Metrics</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-ntp-metrics-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-ntp_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-ntp_metrics.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-ntp-metrics-xml-1</li></ul></div></div></div></div><p>
  A list of metrics associated with the NTP service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>ntp.connection_status</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
ntp_server</pre></div>
     </td><td>Value of ntp server connection status (0=Healthy)</td></tr><tr><td>ntp.offset</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
ntp_server</pre></div>
     </td><td>Time offset in seconds</td></tr></tbody></table></div></div><div class="sect3" id="sec-metric-ovs"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.16 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Open vSwitch (OVS) Metrics</span> <a title="Permalink" class="permalink" href="#sec-metric-ovs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-ovs_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-ovs_metrics.xml</li><li><span class="ds-label">ID: </span>sec-metric-ovs</li></ul></div></div></div></div><p>
  A list of metrics associated with the OVS service.
 </p><div id="id-1.5.15.3.6.19.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   For information on how to turn these metrics on and off using the tuning
   knobs, see <a class="xref" href="#ovs-tuningknobs" title="13.1.2.5.2. OVS plugin metric tuning knobs">Section 13.1.2.5.2, “OVS plugin metric tuning knobs”</a>.
  </p></div><div class="table" id="id-1.5.15.3.6.19.4"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 13.6: </span><span class="name">Per-router metrics </span><a title="Permalink" class="permalink" href="#id-1.5.15.3.6.19.4">#</a></h6></div><div class="table-contents"><table class="table" summary="Per-router metrics" border="1"><colgroup><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /></colgroup><thead><tr><th>Admin Metric Name</th><th>Project Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>ovs.vrouter.in_bytes_sec</td><td>vrouter.in_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Inbound bytes per second for the router (if
       <code class="literal">network_use_bits</code> is false)
      </p>
     </td></tr><tr><td>ovs.vrouter.in_packets_sec</td><td>vrouter.in_packets_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming packets per second for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.out_bytes_sec</td><td>vrouter.out_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing bytes per second for the router (if
       <code class="literal">network_use_bits</code> is false)
      </p>
     </td></tr><tr><td>ovs.vrouter.out_packets_sec</td><td>vrouter.out_packets_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing packets per second for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.in_bytes</td><td>vrouter.in_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Inbound bytes for the router (if <code class="literal">network_use_bits</code> is
       false)
      </p>
     </td></tr><tr><td>ovs.vrouter.in_packets</td><td>vrouter.in_packets</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming packets for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.out_bytes</td><td>vrouter.out_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing bytes for the router (if <code class="literal">network_use_bits</code> is
       false)
      </p>
     </td></tr><tr><td>ovs.vrouter.out_packets</td><td>vrouter.out_packets</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing packets for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.in_dropped_sec</td><td>vrouter.in_dropped_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming dropped packets per second for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.in_errors_sec</td><td>vrouter.in_errors_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Number of incoming errors per second for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.out_dropped_sec</td><td>vrouter.out_dropped_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing dropped packets per second for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.out_errors_sec</td><td>vrouter.out_errors_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Number of outgoing errors per second for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.in_dropped</td><td>vrouter.in_dropped</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming dropped packets for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.in_errors</td><td>vrouter.in_errors</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Number of incoming errors for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.out_dropped</td><td>vrouter.out_dropped</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing dropped packets for the router
      </p>
     </td></tr><tr><td>ovs.vrouter.out_errors</td><td>vrouter.out_errors</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Number of outgoing errors for the router
      </p>
     </td></tr></tbody></table></div></div><div class="table" id="id-1.5.15.3.6.19.5"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 13.7: </span><span class="name">Per-DHCP port and rate metrics </span><a title="Permalink" class="permalink" href="#id-1.5.15.3.6.19.5">#</a></h6></div><div class="table-contents"><table class="table" summary="Per-DHCP port and rate metrics" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /></colgroup><thead><tr><th>Admin Metric Name</th><th>Tenant Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>ovs.vswitch.in_bytes_sec</td><td>vswitch.in_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming Bytes per second on DHCP
       port(if<code class="literal">network_use_bits</code> is false)
      </p>
     </td></tr><tr><td>ovs.vswitch.in_packets_sec</td><td>vswitch.in_packets_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming packets per second for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.out_bytes_sec</td><td>vswitch.out_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing Bytes per second on DHCP
       port(if<code class="literal">network_use_bits</code> is false)
      </p>
     </td></tr><tr><td>ovs.vswitch.out_packets_sec</td><td>vswitch.out_packets_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing packets per second for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.in_bytes</td><td>vswitch.in_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Inbound bytes for the DHCP port (if <code class="literal">network_use_bits</code>
       is false)
      </p>
     </td></tr><tr><td>ovs.vswitch.in_packets</td><td>vswitch.in_packets</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming packets for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.out_bytes</td><td>vswitch.out_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing bytes for the DHCP port (if <code class="literal">network_use_bits</code>
       is false)
      </p>
     </td></tr><tr><td>ovs.vswitch.out_packets</td><td>vswitch.out_packets</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing packets for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.in_dropped_sec</td><td>vswitch.in_dropped_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming dropped per second for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.in_errors_sec</td><td>vswitch.in_errors_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming errors per second for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.out_dropped_sec</td><td>vswitch.out_dropped_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing dropped packets per second for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.out_errors_sec</td><td>vswitch.out_errors_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing errors per second for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.in_dropped</td><td>vswitch.in_dropped</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Incoming dropped packets for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.in_errors</td><td>vswitch.in_errors</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Errors received for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.out_dropped</td><td>vswitch.out_dropped</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Outgoing dropped packets for the DHCP port
      </p>
     </td></tr><tr><td>ovs.vswitch.out_errors</td><td>vswitch.out_errors</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
resource_id
tenant_id
component=ovs
router_name
port_id</pre></div>
     </td><td>
      <p>
       Errors transmitted for the DHCP port
      </p>
     </td></tr></tbody></table></div></div></div><div class="sect3" id="process-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.17 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Process Metrics</span> <a title="Permalink" class="permalink" href="#process-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-process_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-process_metrics.xml</li><li><span class="ds-label">ID: </span>process-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with processes.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>process.cpu_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service
process_name
component</pre></div>
     </td><td>Percentage of cpu being consumed by a process</td></tr><tr><td>process.io.read_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service
process_name
component</pre></div>
     </td><td>Number of reads by a process</td></tr><tr><td>process.io.read_kbytes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service
process_name
component</pre></div>
     </td><td>Kbytes read by a process</td></tr><tr><td>process.io.write_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service
process_name
component</pre></div>
     </td><td>Number of writes by a process</td></tr><tr><td>process.io.write_kbytes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service
process_name
component</pre></div>
     </td><td>Kbytes written by a process</td></tr><tr><td>process.mem.rss_mbytes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service
process_name
component</pre></div>
     </td><td>Amount of physical memory allocated to a process, including memory from shared
                libraries in Mbytes</td></tr><tr><td>process.open_file_descriptors</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service
process_name
component</pre></div>
     </td><td>Number of files being used by a process</td></tr><tr><td>process.pid_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service
process_name
component</pre></div>
     </td><td>Number of processes that exist with this process name</td></tr><tr><td>process.thread_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service
process_name
component</pre></div>
     </td><td>Number of threads a process is using</td></tr></tbody></table></div><div class="sect4" id="id-1.5.15.3.6.20.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.4.17.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">process.cpu_perc, process.mem.rss_mbytes, process.pid_count and process.thread_count metrics</span> <a title="Permalink" class="permalink" href="#id-1.5.15.3.6.20.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-process_metrics.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-process_metrics.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Component Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>apache-storm</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
process_name=monasca-thresh
process_user=storm</pre></div>
      </td><td>apache-storm process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>barbican-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=key-manager
process_name=barbican-api</pre></div>
      </td><td>barbican-api process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>ceilometer-agent-notification</td><td>
<div class="verbatim-wrap"><pre class="screen">service=telemetry
process_name=ceilometer-agent-notification</pre></div>
      </td><td>ceilometer-agent-notification process info: cpu percent, momory, pid count
                  and thread count</td></tr><tr><td>ceilometer-polling</td><td>
<div class="verbatim-wrap"><pre class="screen">service=telemetry
process_name=ceilometer-polling</pre></div>
      </td><td>ceilometer-polling process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>cinder-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=block-storage
process_name=cinder-api</pre></div>
      </td><td>cinder-api process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>cinder-scheduler</td><td>
<div class="verbatim-wrap"><pre class="screen">service=block-storage
process_name=cinder-scheduler</pre></div>
      </td><td>cinder-scheduler process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>designate-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=dns
process_name=designate-api</pre></div>
      </td><td>designate-api process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>designate-central</td><td>
<div class="verbatim-wrap"><pre class="screen">service=dns
process_name=designate-central</pre></div>
      </td><td>designate-central process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>designate-mdns</td><td>
<div class="verbatim-wrap"><pre class="screen">service=dns
process_name=designate-mdns</pre></div>
      </td><td>designate-mdns process cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>designate-pool-manager</td><td>
<div class="verbatim-wrap"><pre class="screen">service=dns
process_name=designate-pool-manager</pre></div>
      </td><td>designate-pool-manager process info: cpu percent, momory, pid count and
                  thread count</td></tr><tr><td>heat-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=orchestration
process_name=heat-api</pre></div>
      </td><td>heat-api process cpu percent, momory, pid count and thread count</td></tr><tr><td>heat-api-cfn</td><td>
<div class="verbatim-wrap"><pre class="screen">service=orchestration
process_name=heat-api-cfn</pre></div>
      </td><td>heat-api-cfn process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>heat-api-cloudwatch</td><td>
<div class="verbatim-wrap"><pre class="screen">service=orchestration
process_name=heat-api-cloudwatch</pre></div>
      </td><td>heat-api-cloudwatch process cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>heat-engine</td><td>
<div class="verbatim-wrap"><pre class="screen">service=orchestration
process_name=heat-engine</pre></div>
      </td><td>heat-engine process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>ipsec/charon</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
process_name=ipsec/charon</pre></div>
      </td><td>ipsec/charon process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>keystone-admin</td><td>
<div class="verbatim-wrap"><pre class="screen">service=identity-service
process_name=keystone-admin</pre></div>
      </td><td>keystone-admin process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>keystone-main</td><td>
<div class="verbatim-wrap"><pre class="screen">service=identity-service
process_name=keystone-main</pre></div>
      </td><td>keystone-main process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>monasca-agent</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
process_name=monasca-agent</pre></div>
      </td><td>monasca-agent process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>monasca-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
process_name=monasca-api</pre></div>
      </td><td>monasca-api process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>monasca-notification</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
process_name=monasca-notification</pre></div>
      </td><td>monasca-notification process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>monasca-persister</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
process_name=monasca-persister</pre></div>
      </td><td>monasca-persister process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>monasca-transform</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monasca-transform
process_name=monasca-transform</pre></div>
      </td><td>monasca-transform process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>neutron-dhcp-agent</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
process_name=neutron-dhcp-agent</pre></div>
      </td><td>neutron-dhcp-agent process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>neutron-l3-agent</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
process_name=neutron-l3-agent</pre></div>
      </td><td>neutron-l3-agent process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>neutron-metadata-agent</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
process_name=neutron-metadata-agent</pre></div>
      </td><td>neutron-metadata-agent process info: cpu percent, momory, pid count and
                  thread count</td></tr><tr><td>neutron-openvswitch-agent</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
process_name=neutron-openvswitch-agent</pre></div>
      </td><td>neutron-openvswitch-agent process info: cpu percent, momory, pid count and
                  thread count</td></tr><tr><td>neutron-rootwrap</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
process_name=neutron-rootwrap</pre></div>
      </td><td>neutron-rootwrap process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>neutron-server</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
process_name=neutron-server</pre></div>
      </td><td>neutron-server process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>neutron-vpn-agent</td><td>
<div class="verbatim-wrap"><pre class="screen">service=networking
process_name=neutron-vpn-agent</pre></div>
      </td><td>neutron-vpn-agent process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>nova-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
process_name=nova-api</pre></div>
      </td><td>nova-api process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>nova-compute</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
process_name=nova-compute</pre></div>
      </td><td>nova-compute process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>nova-conductor</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
process_name=nova-conductor</pre></div>
      </td><td>nova-conductor process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>nova-novncproxy</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
process_name=nova-novncproxy</pre></div>
      </td><td>nova-novncproxy process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>nova-scheduler</td><td>
<div class="verbatim-wrap"><pre class="screen">service=compute
process_name=nova-scheduler</pre></div>
      </td><td>nova-scheduler process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>octavia-api</td><td>
<div class="verbatim-wrap"><pre class="screen">service=octavia
process_name=octavia-api</pre></div>
      </td><td>octavia-api process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>octavia-health-manager</td><td>
<div class="verbatim-wrap"><pre class="screen">service=octavia
process_name=octavia-health-manager</pre></div>
      </td><td>octavia-health-manager process info: cpu percent, momory, pid count and
                  thread count</td></tr><tr><td>octavia-housekeeping</td><td>
<div class="verbatim-wrap"><pre class="screen">service=octavia
process_name=octavia-housekeeping</pre></div>
      </td><td>octavia-housekeeping process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>octavia-worker</td><td>
<div class="verbatim-wrap"><pre class="screen">service=octavia
process_name=octavia-worker</pre></div>
      </td><td>octavia-worker process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>org.apache.spark.deploy.master.Master</td><td>
<div class="verbatim-wrap"><pre class="screen">service=spark
process_name=org.apache.spark.deploy.master.Master</pre></div>
      </td><td>org.apache.spark.deploy.master.Master process info: cpu percent, momory, pid
                  count and thread count</td></tr><tr><td>org.apache.spark.executor.CoarseGrainedExecutorBackend</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monasca-transform
process_name=org.apache.spark.executor.CoarseGrainedExecutorBackend</pre></div>
      </td><td>org.apache.spark.executor.CoarseGrainedExecutorBackend process info: cpu
                  percent, momory, pid count and thread count</td></tr><tr><td> pyspark</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monasca-transform
process_name=pyspark</pre></div>
      </td><td>pyspark process info: cpu percent, momory, pid count and thread count</td></tr><tr><td>transform/lib/driver</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monasca-transform
process_name=transform/lib/driver</pre></div>
      </td><td>transform/lib/driver process info: cpu percent, momory, pid count and thread
                  count</td></tr><tr><td>cassandra</td><td>
<div class="verbatim-wrap"><pre class="screen">service=cassandra
process_name=cassandra</pre></div>
      </td><td>cassandra process info: cpu percent, momory, pid count and thread count</td></tr></tbody></table></div></div><div class="sect4" id="id-1.5.15.3.6.20.5"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.1.4.17.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">process.io.*, process.open_file_descriptors metrics</span> <a title="Permalink" class="permalink" href="#id-1.5.15.3.6.20.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-process_metrics.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-process_metrics.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Component Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>monasca-agent</td><td>
<div class="verbatim-wrap"><pre class="screen">service=monitoring
process_name=monasca-agent
process_user=mon-agent</pre></div>
      </td><td>monasca-agent process info: number of reads, number of writes,number of files
                  being used</td></tr></tbody></table></div></div></div><div class="sect3" id="rabbitmq-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.18 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">RabbitMQ Metrics</span> <a title="Permalink" class="permalink" href="#rabbitmq-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-rabbitmq_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-rabbitmq_metrics.xml</li><li><span class="ds-label">ID: </span>rabbitmq-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with the RabbitMQ service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>rabbitmq.exchange.messages.published_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
exchange
vhost
type
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "publish_out" field of "message_stats" object
      </p>
     </td></tr><tr><td>rabbitmq.exchange.messages.published_rate</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
exchange
vhost
type
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "rate" field of "message_stats/publish_out_details" object
      </p>
     </td></tr><tr><td>rabbitmq.exchange.messages.received_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
exchange
vhost
type
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "publish_in" field of "message_stats" object
      </p>
     </td></tr><tr><td>rabbitmq.exchange.messages.received_rate</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
exchange
vhost
type
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "rate" field of "message_stats/publish_in_details" object
      </p>
     </td></tr><tr><td>rabbitmq.node.fd_used</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
node
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "fd_used" field in the response of /api/nodes
      </p>
     </td></tr><tr><td>rabbitmq.node.mem_used</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
node
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "mem_used" field in the response of /api/nodes
      </p>
     </td></tr><tr><td>rabbitmq.node.run_queue</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
node
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "run_queue" field in the response of /api/nodes
      </p>
     </td></tr><tr><td>rabbitmq.node.sockets_used</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
node
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "sockets_used" field in the response of /api/nodes
      </p>
     </td></tr><tr><td>rabbitmq.queue.messages</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
queue
vhost
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Sum of ready and unacknowledged messages (queue depth)
      </p>
     </td></tr><tr><td>rabbitmq.queue.messages.deliver_rate</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
queue
vhost
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "rate" field of "message_stats/deliver_details" object
      </p>
     </td></tr><tr><td>rabbitmq.queue.messages.publish_rate</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
queue
vhost
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "rate" field of "message_stats/publish_details" object
      </p>
     </td></tr><tr><td>rabbitmq.queue.messages.redeliver_rate</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
queue
vhost
service=rabbitmq</pre></div>
     </td><td>
      <p>
       Value of the "rate" field of "message_stats/redeliver_details" object
      </p>
     </td></tr></tbody></table></div></div><div class="sect3" id="swift-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.19 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Swift Metrics</span> <a title="Permalink" class="permalink" href="#swift-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-swift_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-swift_metrics.xml</li><li><span class="ds-label">ID: </span>swift-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with the swift service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>swiftlm.access.host.operation.get.bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">service=object-storage</pre></div>
     </td><td>
      <p>
       This metric is the number of bytes read from objects in GET requests
       processed by this host during the last minute. Only successful GET
       requests to objects are counted. GET requests to the account or
       container is not included.
      </p>
     </td></tr><tr><td>swiftlm.access.host.operation.ops</td><td>
<div class="verbatim-wrap"><pre class="screen">service=object-storage</pre></div>
     </td><td>
      <p>
       This metric is a count of the all the API requests made to swift that
       were processed by this host during the last minute.
      </p>
     </td></tr><tr><td>swiftlm.access.host.operation.project.get.bytes</td><td> </td><td> </td></tr><tr><td>swiftlm.access.host.operation.project.ops</td><td> </td><td> </td></tr><tr><td>swiftlm.access.host.operation.project.put.bytes</td><td> </td><td> </td></tr><tr><td>swiftlm.access.host.operation.put.bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">service=object-storage</pre></div>
     </td><td>
      <p>
       This metric is the number of bytes written to objects in PUT or POST
       requests processed by this host during the last minute. Only successful
       requests to objects are counted. Requests to the account or container is
       not included.
      </p>
     </td></tr><tr><td>swiftlm.access.host.operation.status</td><td> </td><td> </td></tr><tr><td>swiftlm.access.project.operation.status</td><td>
<div class="verbatim-wrap"><pre class="screen">service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports whether the swiftlm-access-log-tailer program is
       running normally.
      </p>
     </td></tr><tr><td>swiftlm.access.project.operation.ops</td><td>
<div class="verbatim-wrap"><pre class="screen">tenant_id
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric is a count of the all the API requests made to swift that
       were processed by this host during the last minute to a given project
       id.
      </p>
     </td></tr><tr><td>swiftlm.access.project.operation.get.bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">tenant_id
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric is the number of bytes read from objects in GET requests
       processed by this host for a given project during the last minute. Only
       successful GET requests to objects are counted. GET requests to the
       account or container is not included.
      </p>
     </td></tr><tr><td>swiftlm.access.project.operation.put.bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">tenant_id
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric is the number of bytes written to objects in PUT or POST
       requests processed by this host for a given project during the last
       minute. Only successful requests to objects are counted. Requests to the
       account or container is not included.
      </p>
     </td></tr><tr><td>swiftlm.async_pending.cp.total.queue_length</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports the total length of all async pending queues in the
       system.
      </p>
      <p>
       When a container update fails, the update is placed on the async pending
       queue. An update may fail becuase the container server is too busy or
       because the server is down or failed. Later the system will “replay”
       updates from the queue – so eventually, the container listings will
       show all objects known to the system.
      </p>
      <p>
       If you know that container servers are down, it is normal to see the
       value of async pending increase. Once the server is restored, the value
       should return to zero.
      </p>
      <p>
       A non-zero value may also indicate that containers are too large. Look
       for “lock timeout” messages in /var/log/swift/swift.log. If you find
       such messages consider reducing the container size or enable rate
       limiting.
      </p>
     </td></tr><tr><td>swiftlm.check.failure</td><td>
<div class="verbatim-wrap"><pre class="screen">check
error
component
service=object-storage</pre></div>
     </td><td>
      <p>
       The total exception string is truncated if longer than 1919 characters
       and an ellipsis is prepended in the first three characters of the
       message. If there is more than one error reported, the list of errors is
       paired to the last reported error and the operator is expected to
       resolve failures until no more are reported. Where there are no further
       reported errors, the Value Class is emitted as ‘Ok’.
      </p>
     </td></tr><tr><td>swiftlm.diskusage.cp.avg.usage</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       Is the average utilization of all drives in the system. The value is a
       percentage (example: 30.0 means 30% of the total space is used).
      </p>
     </td></tr><tr><td>swiftlm.diskusage.cp.max.usage</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       Is the highest utilization of all drives in the system. The value is a
       percentage (example: 80.0 means at least one drive is 80% utilized). The
       value is just as important as swiftlm.diskusage.usage.avg. For example,
       if swiftlm.diskusage.usage.avg is 70% you might think that there is
       plenty of space available. However, if swiftlm.diskusage.usage.max is
       100%, this means that some objects cannot be stored on that drive. swift
       will store replicas on other drives. However, this will create extra
       overhead.
      </p>
     </td></tr><tr><td>swiftlm.diskusage.cp.min.usage</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       Is the lowest utilization of all drives in the system. The value is a
       percentage (example: 10.0 means at least one drive is 10% utilized)
      </p>
     </td></tr><tr><td>swiftlm.diskusage.cp.total.avail</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       Is the size in bytes of available (unused) space of all drives in the
       system. Only drives used by swift are included.
      </p>
     </td></tr><tr><td>swiftlm.diskusage.cp.total.size</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       Is the size in bytes of raw size of all drives in the system.
      </p>
     </td></tr><tr><td>swiftlm.diskusage.cp.total.used</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       Is the size in bytes of used space of all drives in the system. Only
       drives used by swift are included.
      </p>
     </td></tr><tr><td>swiftlm.diskusage.host.avg.usage</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports the average percent usage of all swift filesystems
       on a host.
      </p>
     </td></tr><tr><td>swiftlm.diskusage.host.max.usage</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports the percent usage of a swift filesystem that is most
       used (full) on a host. The value is the max of the percentage used of
       all swift filesystems.
      </p>
     </td></tr><tr><td>swiftlm.diskusage.host.min.usage</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports the percent usage of a swift filesystem that is
       least used (has free space) on a host. The value is the min of the
       percentage used of all swift filesystems.
      </p>
     </td></tr><tr><td>swiftlm.diskusage.host.val.avail</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage
mount
device
label</pre></div>
     </td><td>
      <p>
       This metric reports the number of bytes available (free) in a swift
       filesystem. The value is an integer (units: Bytes)
      </p>
     </td></tr><tr><td>swiftlm.diskusage.host.val.size</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage
mount
device
label</pre></div>
     </td><td>
      <p>
       This metric reports the size in bytes of a swift filesystem. The
       value is an integer (units: Bytes)
      </p>
     </td></tr><tr><td>swiftlm.diskusage.host.val.usage</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage
mount
device
label</pre></div>
     </td><td>
      <p>
       This metric reports the percent usage of a swift filesystem. The value
       is a floating point number in range 0.0 to 100.0
      </p>
     </td></tr><tr><td>swiftlm.diskusage.host.val.used</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage
mount
device
label</pre></div>
     </td><td>
      <p>
       This metric reports the number of used bytes in a swift filesystem.
       The value is an integer (units: Bytes)
      </p>
     </td></tr><tr><td>swiftlm.load.cp.avg.five</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       This is the averaged value of the five minutes system load average of
       all nodes in the swift system.
      </p>
     </td></tr><tr><td>swiftlm.load.cp.max.five</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       This is the five minute load average of the busiest host in the swift
       system.
      </p>
     </td></tr><tr><td>swiftlm.load.cp.min.five</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       This is the five minute load average of the least loaded host in the
       swift system.
      </p>
     </td></tr><tr><td>swiftlm.load.host.val.five</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports the 5 minute load average of a host. The value is
       derived from <code class="literal">/proc/loadavg</code>.
      </p>
     </td></tr><tr><td>swiftlm.md5sum.cp.check.ring_checksums</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       If you are in the middle of deploying new rings, it is normal for this
       to be in the failed state.
      </p>
      <p>
       However, if you are not in the middle of a deployment, you need to
       investigate the cause. Use “swift-recon –md5 -v” to identify the
       problem hosts.
      </p>
     </td></tr><tr><td>swiftlm.replication.cp.avg.account_duration</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       This is the average across all servers for the account replicator to
       complete a cycle. As the system becomes busy, the time to complete a
       cycle increases. The value is in seconds.
      </p>
     </td></tr><tr><td>swiftlm.replication.cp.avg.container_duration</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       This is the average across all servers for the container replicator to
       complete a cycle. As the system becomes busy, the time to complete a
       cycle increases. The value is in seconds.
      </p>
     </td></tr><tr><td>swiftlm.replication.cp.avg.object_duration</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
service=object-storage</pre></div>
     </td><td>
      <p>
       This is the average across all servers for the object replicator to
       complete a cycle. As the system becomes busy, the time to complete a
       cycle increases. The value is in seconds.
      </p>
     </td></tr><tr><td>swiftlm.replication.cp.max.account_last</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
path
service=object-storage</pre></div>
     </td><td>
      <p>
       This is the number of seconds since the account replicator last
       completed a scan on the host that has the oldest completion time.
       Normally the replicators runs periodically and hence this value will
       decrease whenever a replicator completes. However, if a replicator is
       not completing a cycle, this value increases (by one second for each
       second that the replicator is not completing). If the value remains high
       and increasing for a long period of time, it indicates that one of the
       hosts is not completing the replication cycle.
      </p>
     </td></tr><tr><td>swiftlm.replication.cp.max.container_last</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
path
service=object-storage</pre></div>
     </td><td>
      <p>
       This is the number of seconds since the container replicator last
       completed a scan on the host that has the oldest completion time.
       Normally the replicators runs periodically and hence this value will
       decrease whenever a replicator completes. However, if a replicator is
       not completing a cycle, this value increases (by one second for each
       second that the replicator is not completing). If the value remains high
       and increasing for a long period of time, it indicates that one of the
       hosts is not completing the replication cycle.
      </p>
     </td></tr><tr><td>swiftlm.replication.cp.max.object_last</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
path
service=object-storage</pre></div>
     </td><td>
      <p>
       This is the number of seconds since the object replicator last completed
       a scan on the host that has the oldest completion time. Normally the
       replicators runs periodically and hence this value will decrease
       whenever a replicator completes. However, if a replicator is not
       completing a cycle, this value increases (by one second for each second
       that the replicator is not completing). If the value remains high and
       increasing for a long period of time, it indicates that one of the hosts
       is not completing the replication cycle.
      </p>
     </td></tr><tr><td>swiftlm.swift.drive_audit</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage
mount_point
kernel_device</pre></div>
     </td><td>
      <p>
       If an unrecoverable read error (URE) occurs on a filesystem, the error
       is logged in the kernel log. The swift-drive-audit program scans the
       kernel log looking for patterns indicating possible UREs.
      </p>
      <p>
       To get more information, log onto the node in question and run:
      </p>
<div class="verbatim-wrap"><pre class="screen">sudoswift-drive-audit/etc/swift/drive-audit.conf</pre></div>
      <p>
       UREs are common on large disk drives. They do not necessarily indicate
       that the drive is failed. You can use the xfs_repair command to attempt
       to repair the filesystem. Failing this, you may need to wipe the
       filesystem.
      </p>
      <p>
       If UREs occur very often on a specific drive, this may indicate that the
       drive is about to fail and should be replaced.
      </p>
     </td></tr><tr><td>swiftlm.swift.file_ownership.config</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
path
service</pre></div>
     </td><td>
      <p>
       This metric reports if a directory or file has the appropriate owner.
       The check looks at swift configuration directories and files. It also
       looks at the top-level directories of mounted file systems (for example,
       /srv/node/disk0 and /srv/node/disk0/objects).
      </p>
     </td></tr><tr><td>swiftlm.swift.file_ownership.data</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
path
service</pre></div>
     </td><td>
      <p>
       This metric reports if a directory or file has the appropriate owner.
       The check looks at swift configuration directories and files. It also
       looks at the top-level directories of mounted file systems (for example,
       /srv/node/disk0 and /srv/node/disk0/objects).
      </p>
     </td></tr><tr><td>swiftlm.swiftlm_check</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage</pre></div>
     </td><td>
      <p>
       This indicates of the swiftlm <code class="literal">monasca-agent</code> Plug-in is running normally.
       If the status is failed, it probable that some or all metrics are no
       longer being reported.
      </p>
     </td></tr><tr><td>swiftlm.swift.replication.account.last_replication</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage</pre></div>
     </td><td>
      <p>
       This reports how long (in seconds) since the replicator process last
       finished a replication run. If the replicator is stuck, the time will
       keep increasing forever. The time a replicator normally takes depends on
       disk sizes and how much data needs to be replicated. However, a value
       over 24 hours is generally bad.
      </p>
     </td></tr><tr><td>swiftlm.swift.replication.container.last_replication</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage</pre></div>
     </td><td>
      <p>
       This reports how long (in seconds) since the replicator process last
       finished a replication run. If the replicator is stuck, the time will
       keep increasing forever. The time a replicator normally takes depends on
       disk sizes and how much data needs to be replicated. However, a value
       over 24 hours is generally bad.
      </p>
     </td></tr><tr><td>swiftlm.swift.replication.object.last_replication</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage</pre></div>
     </td><td>
      <p>
       This reports how long (in seconds) since the replicator process last
       finished a replication run. If the replicator is stuck, the time will
       keep increasing forever. The time a replicator normally takes depends on
       disk sizes and how much data needs to be replicated. However, a value
       over 24 hours is generally bad.
      </p>
     </td></tr><tr><td>swiftlm.swift.swift_services</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports of the process as named in the component dimension
       and the msg value_meta is running or not.
      </p>
      <p>
       Use the <code class="literal">swift-start.yml</code> playbook to attempt to
       restart the stopped process (it will start any process that has stopped
       – you do not need to specifically name the process).
      </p>
     </td></tr><tr><td>swiftlm.swift.swift_services.check_ip_port</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage
component</pre></div>
     </td><td>Reports if a service is listening to the correct ip and port.</td></tr><tr><td>swiftlm.systems.check_mounts</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
service=object-storage
mount
device
label</pre></div>
     </td><td>
      <p>
       This metric reports the mount state of each drive that should be mounted
       on this node.
      </p>
     </td></tr><tr><td>swiftlm.systems.connectivity.connect_check</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
url
target_port
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports if a server can connect to a VIPs. Currently the
       following VIPs are checked:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         The keystone VIP used to validate tokens (normally port 5000)
        </p></li></ul></div>
     </td></tr><tr><td>swiftlm.systems.connectivity.memcache_check</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
hostname
target_port
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports if memcached on the host as specified by the
       hostname dimension is accepting connections from the host running the
       check. The following value_meta.msg are used:
      </p>
      <p>
       We successfully connected to &lt;hostname&gt; on port
       &lt;target_port&gt;
      </p>
<div class="verbatim-wrap"><pre class="screen">{
  "dimensions": {
    "hostname": "ardana-ccp-c1-m1-mgmt",
    "observer_host": "ardana-ccp-c1-m1-mgmt",
    "service": "object-storage",
    "target_port": "11211"
  },
  "metric": "swiftlm.systems.connectivity.memcache_check",
  "timestamp": 1449084058,
  "value": 0,
  "value_meta": {
    "msg": "ardana-ccp-c1-m1-mgmt:11211 ok"
  }
}</pre></div>
      <p>
       We failed to connect to &lt;hostname&gt; on port &lt;target_port&gt;
      </p>
<div class="verbatim-wrap"><pre class="screen">{
  "dimensions": {
    "fail_message": "[Errno 111] Connection refused",
    "hostname": "ardana-ccp-c1-m1-mgmt",
    "observer_host": "ardana-ccp-c1-m1-mgmt",
    "service": "object-storage",
    "target_port": "11211"
  },
  "metric": "swiftlm.systems.connectivity.memcache_check",
  "timestamp": 1449084150,
  "value": 2,
  "value_meta": {
    "msg": "ardana-ccp-c1-m1-mgmt:11211 [Errno 111] Connection refused"
  }
}</pre></div>
     </td></tr><tr><td>swiftlm.systems.connectivity.rsync_check</td><td>
<div class="verbatim-wrap"><pre class="screen">observer_host
hostname
target_port
service=object-storage</pre></div>
     </td><td>
      <p>
       This metric reports if rsyncd on the host as specified by the hostname
       dimension is accepting connections from the host running the check. The
       following value_meta.msg are used:
      </p>
      <p>
       We successfully connected to &lt;hostname&gt; on port
       &lt;target_port&gt;:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
  "dimensions": {
    "hostname": "ardana-ccp-c1-m1-mgmt",
    "observer_host": "ardana-ccp-c1-m1-mgmt",
    "service": "object-storage",
    "target_port": "873"
  },
  "metric": "swiftlm.systems.connectivity.rsync_check",
  "timestamp": 1449082663,
  "value": 0,
  "value_meta": {
    "msg": "ardana-ccp-c1-m1-mgmt:873 ok"
  }
}</pre></div>
      <p>
       We failed to connect to &lt;hostname&gt; on port &lt;target_port&gt;:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
  "dimensions": {
    "fail_message": "[Errno 111] Connection refused",
    "hostname": "ardana-ccp-c1-m1-mgmt",
    "observer_host": "ardana-ccp-c1-m1-mgmt",
    "service": "object-storage",
    "target_port": "873"
  },
  "metric": "swiftlm.systems.connectivity.rsync_check",
  "timestamp": 1449082860,
  "value": 2,
  "value_meta": {
    "msg": "ardana-ccp-c1-m1-mgmt:873 [Errno 111] Connection refused"
  }
}</pre></div>
     </td></tr><tr><td>swiftlm.umon.target.avg.latency_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
observer_host
service=object-storage
url</pre></div>
     </td><td>
      <p>
       Reports the average value of N-iterations of the latency values recorded
       for a component.
      </p>
     </td></tr><tr><td>swiftlm.umon.target.check.state</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
observer_host
service=object-storage
url</pre></div>
     </td><td>
      <p>
       This metric reports the state of each component after N-iterations of
       checks. If the initial check succeeds, the checks move onto the next
       component until all components are queried, then the checks sleep for
       ‘main_loop_interval’ seconds. If a check fails, it is retried every
       second for ‘retries’ number of times per component. If the check
       fails ‘retries’ times, it is reported as a fail instance.
      </p>
      <p>
       A successful state will be reported in JSON:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
    "dimensions": {
        "component": "rest-api",
        "hostname": "ardana-ccp-vip-admin-SWF-PRX-mgmt",
        "observer_host": "ardana-ccp-c1-m1-mgmt",
        "service": "object-storage",
        "url": "http://ardana-ccp-vip-admin-SWF-PRX-mgmt:8080"
    },
    "metric": "swiftlm.umon.target.check.state",
    "timestamp": 1453111805,
    "value": 0
},</pre></div>
      <p>
       A failed state will report a “fail” value and the value_meta will
       provide the http response error.
      </p>
<div class="verbatim-wrap"><pre class="screen">{
    "dimensions": {
        "component": "rest-api",
        "hostname": "ardana-ccp-vip-admin-SWF-PRX-mgmt",
        "observer_host": "ardana-ccp-c1-m1-mgmt",
        "service": "object-storage",
        "url": "http://ardana-ccp-vip-admin-SWF-PRX-mgmt:8080"
    },
    "metric": "swiftlm.umon.target.check.state",
    "timestamp": 1453112841,
    "value": 2,
    "value_meta": {
        "msg": "HTTPConnectionPool(host='192.168.245.9', port=8080): Max retries exceeded with url: /v1/AUTH_76538ce683654a35983b62e333001b47 (Caused by NewConnectionError('&lt;requests.packages.urllib3.connection.HTTPConnection object at 0x7fd857d7f550&gt;: Failed to establish a new connection: [Errno 110] Connection timed out',))"
    }
}</pre></div>
     </td></tr><tr><td>swiftlm.umon.target.max.latency_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
observer_host
service=object-storage
url</pre></div>
     </td><td>
      <p>
       This metric reports the maximum response time in seconds of a REST call
       from the observer to the component REST API listening on the reported
       host
      </p>
      <p>
       A response time query will be reported in JSON:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
    "dimensions": {
        "component": "rest-api",
        "hostname": "ardana-ccp-vip-admin-SWF-PRX-mgmt",
        "observer_host": "ardana-ccp-c1-m1-mgmt",
        "service": "object-storage",
        "url": "http://ardana-ccp-vip-admin-SWF-PRX-mgmt:8080"
    },
    "metric": "swiftlm.umon.target.max.latency_sec",
    "timestamp": 1453111805,
    "value": 0.2772650718688965
}</pre></div>
      <p>
       A failed query will have a much longer time value:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
    "dimensions": {
        "component": "rest-api",
        "hostname": "ardana-ccp-vip-admin-SWF-PRX-mgmt",
        "observer_host": "ardana-ccp-c1-m1-mgmt",
        "service": "object-storage",
        "url": "http://ardana-ccp-vip-admin-SWF-PRX-mgmt:8080"
    },
    "metric": "swiftlm.umon.target.max.latency_sec",
    "timestamp": 1453112841,
    "value": 127.288015127182
}</pre></div>
     </td></tr><tr><td>swiftlm.umon.target.min.latency_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
observer_host
service=object-storage
url</pre></div>
     </td><td>
      <p>
       This metric reports the minimum response time in seconds of a REST call
       from the observer to the component REST API listening on the reported
       host
      </p>
      <p>
       A response time query will be reported in JSON:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
    "dimensions": {
        "component": "rest-api",
        "hostname": "ardana-ccp-vip-admin-SWF-PRX-mgmt",
        "observer_host": "ardana-ccp-c1-m1-mgmt",
        "service": "object-storage",
        "url": "http://ardana-ccp-vip-admin-SWF-PRX-mgmt:8080"
    },
    "metric": "swiftlm.umon.target.min.latency_sec",
    "timestamp": 1453111805,
    "value": 0.10025882720947266
}</pre></div>
      <p>
       A failed query will have a much longer time value:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
    "dimensions": {
        "component": "rest-api",
        "hostname": "ardana-ccp-vip-admin-SWF-PRX-mgmt",
        "observer_host": "ardana-ccp-c1-m1-mgmt",
        "service": "object-storage",
        "url": "http://ardana-ccp-vip-admin-SWF-PRX-mgmt:8080"
    },
    "metric": "swiftlm.umon.target.min.latency_sec",
    "timestamp": 1453112841,
    "value": 127.25378203392029
}</pre></div>
     </td></tr><tr><td>swiftlm.umon.target.val.avail_day</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
observer_host
service=object-storage
url</pre></div>
     </td><td>
      <p>
       This metric reports the average of all the collected records in the
       swiftlm.umon.target.val.avail_minute metric data. This is a walking
       average data set of these approximately per-minute states of the swift
       Object Store. The most basic case is a whole day of successful
       per-minute records, which will average to 100% availability. If there is
       any downtime throughout the day resulting in gaps of data which are two
       minutes or longer, the per-minute availability data will be “back
       filled” with an assumption of a down state for all the per-minute
       records which did not exist during the non-reported time. Because this
       is a walking average of approximately 24 hours worth of data, any
       outtage will take 24 hours to be purged from the dataset.
      </p>
      <p>
       A 24-hour average availability report:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
    "dimensions": {
        "component": "rest-api",
        "hostname": "ardana-ccp-vip-admin-SWF-PRX-mgmt",
        "observer_host": "ardana-ccp-c1-m1-mgmt",
        "service": "object-storage",
        "url": "http://ardana-ccp-vip-admin-SWF-PRX-mgmt:8080"
    },
    "metric": "swiftlm.umon.target.val.avail_day",
    "timestamp": 1453645405,
    "value": 7.894736842105263
}</pre></div>
     </td></tr><tr><td>swiftlm.umon.target.val.avail_minute</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
observer_host
service=object-storage
url</pre></div>
     </td><td>
      <p>
       A value of 100 indicates that swift-uptime-monitor was able to get a
       token from keystone and was able to perform operations against the swift
       API during the reported minute. A value of zero indicates that either
       keystone or swift failed to respond successfully. A metric is produced
       every minute that swift-uptime-monitor is running.
      </p>
      <p>
       An “up” minute report value will report 100 [percent]:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
    "dimensions": {
        "component": "rest-api",
        "hostname": "ardana-ccp-vip-admin-SWF-PRX-mgmt",
        "observer_host": "ardana-ccp-c1-m1-mgmt",
        "service": "object-storage",
        "url": "http://ardana-ccp-vip-admin-SWF-PRX-mgmt:8080"
    },
    "metric": "swiftlm.umon.target.val.avail_minute",
    "timestamp": 1453645405,
    "value": 100.0
}</pre></div>
      <p>
       A “down” minute report value will report 0 [percent]:
      </p>
<div class="verbatim-wrap"><pre class="screen">{
    "dimensions": {
        "component": "rest-api",
        "hostname": "ardana-ccp-vip-admin-SWF-PRX-mgmt",
        "observer_host": "ardana-ccp-c1-m1-mgmt",
        "service": "object-storage",
        "url": "http://ardana-ccp-vip-admin-SWF-PRX-mgmt:8080"
    },
    "metric": "swiftlm.umon.target.val.avail_minute",
    "timestamp": 1453649139,
    "value": 0.0
}</pre></div>
     </td></tr><tr><td>swiftlm.hp_hardware.hpssacli.smart_array.firmware</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
service=object-storage
component
model
controller_slot</pre></div>
     </td><td>
      <p>
       This metric reports the firmware version of a component of a Smart Array
       controller.
      </p>
     </td></tr><tr><td>swiftlm.hp_hardware.hpssacli.smart_array</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
service=object-storage
component
sub_component
model
controller_slot</pre></div>
     </td><td>
      <p>
       This reports the status of various sub-components of a Smart Array
       Controller.
      </p>
      <p>
       A failure is considered to have occured if:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Controller is failed
        </p></li><li class="listitem "><p>
         Cache is not enabled or has failed
        </p></li><li class="listitem "><p>
         Battery or capacitor is not installed
        </p></li><li class="listitem "><p>
         Battery or capacitor has failed
        </p></li></ul></div>
     </td></tr><tr><td>swiftlm.hp_hardware.hpssacli.physical_drive</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
service=object-storage
component
controller_slot
box
bay</pre></div>
     </td><td>
      <p>
       This reports the status of a disk drive attached to a Smart Array
       controller.
      </p>
     </td></tr><tr><td>swiftlm.hp_hardware.hpssacli.logical_drive</td><td>
<div class="verbatim-wrap"><pre class="screen">component
hostname
observer_host
service=object-storage
controller_slot
array
logical_drive
sub_component</pre></div>
     </td><td>
      <p>
       This reports the status of a LUN presented by a Smart Array controller.
      </p>
      <p>
       A LUN is considered failed if the LUN has failed or if the LUN cache is
       not enabled and working.
      </p>
     </td></tr></tbody></table></div><div id="id-1.5.15.3.6.22.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     HPE Smart Storage Administrator (HPE SSA) CLI component will have to be
     installed on all control nodes that are swift nodes, in order to generate
     the following swift metrics:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       swiftlm.hp_hardware.hpssacli.smart_array
      </p></li><li class="listitem "><p>
       swiftlm.hp_hardware.hpssacli.logical_drive
      </p></li><li class="listitem "><p>
       swiftlm.hp_hardware.hpssacli.smart_array.firmware
      </p></li><li class="listitem "><p>
       swiftlm.hp_hardware.hpssacli.physical_drive
      </p></li></ul></div></li><li class="listitem "><p>
     HPE-specific binaries that are not based on open source are distributed
     directly from and supported by HPE. To download and install the SSACLI
     utility, please refer to: <a class="link" href="https://support.hpe.com/hpsc/swd/public/detail?swItemId=MTX_3d16386b418a443388c18da82f" target="_blank">https://support.hpe.com/hpsc/swd/public/detail?swItemId=MTX_3d16386b418a443388c18da82f</a>
    </p></li><li class="listitem "><p>
     After the HPE SSA CLI component is installed on the swift nodes, the
     metrics will be generated automatically during the next agent polling
     cycle. Manual reboot of the node is not required.
    </p></li></ul></div></div></div><div class="sect3" id="system-metrics"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.20 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">System Metrics</span> <a title="Permalink" class="permalink" href="#system-metrics">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-system_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-system_metrics.xml</li><li><span class="ds-label">ID: </span>system-metrics</li></ul></div></div></div></div><p>
  A list of metrics associated with the System.
 </p><div class="table" id="id-1.5.15.3.6.23.3"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 13.8: </span><span class="name">CPU Metrics </span><a title="Permalink" class="permalink" href="#id-1.5.15.3.6.23.3">#</a></h6></div><div class="table-contents"><table class="table" summary="CPU Metrics" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>cpu.frequency_mhz</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Maximum MHz value for the cpu frequency.
      </p>
      <div id="id-1.5.15.3.6.23.3.2.5.1.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
        This value is dynamic, and driven by CPU governor depending on current
        resource need.
       </p></div>
     </td></tr><tr><td>cpu.idle_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Percentage of time the CPU is idle when no I/O requests are in progress
      </p>
     </td></tr><tr><td>cpu.idle_time</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Time the CPU is idle when no I/O requests are in progress
      </p>
     </td></tr><tr><td>cpu.percent</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Percentage of time the CPU is used in total
      </p>
     </td></tr><tr><td>cpu.stolen_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Percentage of stolen CPU time, that is, the time spent in other OS
       contexts when running in a virtualized environment
      </p>
     </td></tr><tr><td>cpu.system_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Percentage of time the CPU is used at the system level
      </p>
     </td></tr><tr><td>cpu.system_time</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Time the CPU is used at the system level
      </p>
     </td></tr><tr><td>cpu.time_ns</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Time the CPU is used at the host level
      </p>
     </td></tr><tr><td>cpu.total_logical_cores</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Total number of logical cores available for an entire node (Includes
       hyper threading).
      </p>
      <div id="id-1.5.15.3.6.23.3.2.5.9.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: </h6><p>
        This is an optional metric that is only sent when send_rollup_stats is
        set to true.
       </p></div>
     </td></tr><tr><td>cpu.user_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Percentage of time the CPU is used at the user level
      </p>
     </td></tr><tr><td>cpu.user_time</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Time the CPU is used at the user level
      </p>
     </td></tr><tr><td>cpu.wait_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Percentage of time the CPU is idle AND there is at least one I/O request
       in progress
      </p>
     </td></tr><tr><td>cpu.wait_time</td><td>
<div class="verbatim-wrap"><pre class="screen">cluster
hostname
service=system</pre></div>
     </td><td>
      <p>
       Time the CPU is idle AND there is at least one I/O request in progress
      </p>
     </td></tr></tbody></table></div></div><div class="table" id="id-1.5.15.3.6.23.4"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 13.9: </span><span class="name">Disk Metrics </span><a title="Permalink" class="permalink" href="#id-1.5.15.3.6.23.4">#</a></h6></div><div class="table-contents"><table class="table" summary="Disk Metrics" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>disk.inode_used_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       The percentage of inodes that are used on a device
      </p>
     </td></tr><tr><td>disk.space_used_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       The percentage of disk space that is being used on a device
      </p>
     </td></tr><tr><td>disk.total_space_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       The total amount of disk space in Mbytes aggregated across all the disks
       on a particular node.
      </p>
      <div id="id-1.5.15.3.6.23.4.2.5.3.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
        This is an optional metric that is only sent when send_rollup_stats is
        set to true.
       </p></div>
     </td></tr><tr><td>disk.total_used_space_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       The total amount of used disk space in Mbytes aggregated across all the
       disks on a particular node.
      </p>
      <div id="id-1.5.15.3.6.23.4.2.5.4.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
        This is an optional metric that is only sent when send_rollup_stats is
        set to true.
       </p></div>
     </td></tr><tr><td>io.read_kbytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       Kbytes/sec read by an io device
      </p>
     </td></tr><tr><td>io.read_req_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       Number of read requests/sec to an io device
      </p>
     </td></tr><tr><td>io.read_time_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       Amount of read time in seconds to an io device
      </p>
     </td></tr><tr><td>io.write_kbytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       Kbytes/sec written by an io device
      </p>
     </td></tr><tr><td>io.write_req_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       Number of write requests/sec to an io device
      </p>
     </td></tr><tr><td>io.write_time_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">mount_point
service=system
hostname
cluster
device</pre></div>
     </td><td>
      <p>
       Amount of write time in seconds to an io device
      </p>
     </td></tr></tbody></table></div></div><div class="table" id="id-1.5.15.3.6.23.5"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 13.10: </span><span class="name">Load Metrics </span><a title="Permalink" class="permalink" href="#id-1.5.15.3.6.23.5">#</a></h6></div><div class="table-contents"><table class="table" summary="Load Metrics" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>load.avg_15_min</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       The normalized (by number of logical cores) average system load over a
       15 minute period
      </p>
     </td></tr><tr><td>load.avg_1_min</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       The normalized (by number of logical cores) average system load over a 1
       minute period
      </p>
     </td></tr><tr><td>load.avg_5_min</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       The normalized (by number of logical cores) average system load over a 5
       minute period
      </p>
     </td></tr></tbody></table></div></div><div class="table" id="id-1.5.15.3.6.23.6"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 13.11: </span><span class="name">Memory Metrics </span><a title="Permalink" class="permalink" href="#id-1.5.15.3.6.23.6">#</a></h6></div><div class="table-contents"><table class="table" summary="Memory Metrics" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>mem.free_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Mbytes of free memory
      </p>
     </td></tr><tr><td>mem.swap_free_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Percentage of free swap memory that is free
      </p>
     </td></tr><tr><td>mem.swap_free_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Mbytes of free swap memory that is free
      </p>
     </td></tr><tr><td>mem.swap_total_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Mbytes of total physical swap memory
      </p>
     </td></tr><tr><td>mem.swap_used_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Mbytes of total swap memory used
      </p>
     </td></tr><tr><td>mem.total_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Total Mbytes of memory
      </p>
     </td></tr><tr><td>mem.usable_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Total Mbytes of usable memory
      </p>
     </td></tr><tr><td>mem.usable_perc</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Percentage of total memory that is usable
      </p>
     </td></tr><tr><td>mem.used_buffers</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Number of buffers in Mbytes being used by the kernel for block io
      </p>
     </td></tr><tr><td>mem.used_cache</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Mbytes of memory used for the page cache
      </p>
     </td></tr><tr><td>mem.used_mb</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
cluster</pre></div>
     </td><td>
      <p>
       Total Mbytes of used memory
      </p>
     </td></tr></tbody></table></div></div><div class="table" id="id-1.5.15.3.6.23.7"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 13.12: </span><span class="name">Network Metrics </span><a title="Permalink" class="permalink" href="#id-1.5.15.3.6.23.7">#</a></h6></div><div class="table-contents"><table class="table" summary="Network Metrics" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>net.in_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
device</pre></div>
     </td><td>
      <p>
       Number of network bytes received per second
      </p>
     </td></tr><tr><td>net.in_errors_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
device</pre></div>
     </td><td>
      <p>
       Number of network errors on incoming network traffic per second
      </p>
     </td></tr><tr><td>net.in_packets_dropped_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
device</pre></div>
     </td><td>
      <p>
       Number of inbound network packets dropped per second
      </p>
     </td></tr><tr><td>net.in_packets_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
device</pre></div>
     </td><td>
      <p>
       Number of network packets received per second
      </p>
     </td></tr><tr><td>net.out_bytes_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
device</pre></div>
     </td><td>
      <p>
       Number of network bytes sent per second
      </p>
     </td></tr><tr><td>net.out_errors_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
device</pre></div>
     </td><td>
      <p>
       Number of network errors on outgoing network traffic per second
      </p>
     </td></tr><tr><td>net.out_packets_dropped_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
device</pre></div>
     </td><td>
      <p>
       Number of outbound network packets dropped per second
      </p>
     </td></tr><tr><td>net.out_packets_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">service=system
hostname
device</pre></div>
     </td><td>
      <p>
       Number of network packets sent per second
      </p>
     </td></tr></tbody></table></div></div></div><div class="sect3" id="idg-all-operations-monitoring-zookeeper-metrics-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.1.4.21 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Zookeeper Metrics</span> <a title="Permalink" class="permalink" href="#idg-all-operations-monitoring-zookeeper-metrics-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-monitoring-zookeeper_metrics.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-monitoring-zookeeper_metrics.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-monitoring-zookeeper-metrics-xml-1</li></ul></div></div></div></div><p>
  A list of metrics associated with the Zookeeper service.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Metric Name</th><th>Dimensions</th><th>Description</th></tr></thead><tbody><tr><td>zookeeper.avg_latency_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Average latency in second</td></tr><tr><td>zookeeper.connections_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Number of connections</td></tr><tr><td>zookeeper.in_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Received bytes</td></tr><tr><td>zookeeper.max_latency_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Maximum latency in second</td></tr><tr><td>zookeeper.min_latency_sec</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Minimum latency in second</td></tr><tr><td>zookeeper.node_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Number of nodes</td></tr><tr><td>zookeeper.out_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Sent bytes</td></tr><tr><td>zookeeper.outstanding_bytes</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Outstanding bytes</td></tr><tr><td>zookeeper.zxid_count</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Count number</td></tr><tr><td>zookeeper.zxid_epoch</td><td>
<div class="verbatim-wrap"><pre class="screen">hostname
mode
service=zookeeper</pre></div>
     </td><td>Epoch number</td></tr></tbody></table></div></div></div></div><div class="sect1" id="centralized-logging"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Centralized Logging Service</span> <a title="Permalink" class="permalink" href="#centralized-logging">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-centralized_logging.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-centralized_logging.xml</li><li><span class="ds-label">ID: </span>centralized-logging</li></ul></div></div></div></div><p>
  You can use the Centralized Logging Service to evaluate and troubleshoot your
  distributed cloud environment from a single location.
 </p><div class="sect2" id="central-log-GS"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Getting Started with Centralized Logging Service</span> <a title="Permalink" class="permalink" href="#central-log-GS">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_GS.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_GS.xml</li><li><span class="ds-label">ID: </span>central-log-GS</li></ul></div></div></div></div><p>
  A typical cloud consists of multiple servers which makes locating a specific
  log from a single server difficult. The Centralized Logging feature helps the
  administrator evaluate and troubleshoot the distributed cloud deployment from
  a single location.
 </p><p>
  The Logging API is a component in the centralized logging architecture. It
  works between log producers and log storage. In most cases it works by
  default after installation with no additional configuration. To use Logging
  API with logging-as-a-service, you must
  configure an end-point. This component adds flexibility and supportability
  for features in the future.
 </p><p>
  <span class="bold"><strong>Do I need to Configure monasca-log-api?</strong></span> If
  you are only using Cloud Lifecycle Manager , then the default
  configuration is ready to use.
 </p><div id="id-1.5.15.4.3.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   If you are using logging in any of the following deployments, then you will
   need to query keystone to get an end-point to use.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Logging as a Service
    </p></li><li class="listitem "><p>
     Platform as a Service
    </p></li></ul></div></div><p>
  The Logging API is protected by keystone’s role-based access control. To
  ensure that logging is allowed and monasca alarms can be triggered, the user
  must have the monasca-user role. <span class="bold"><strong>To get an end-point
  from keystone:</strong></span>
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Log on to Cloud Lifecycle Manager (deployer node).
   </p></li><li class="listitem "><p>
    To list the Identity service catalog, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ./service.osrc
<code class="prompt user">ardana &gt; </code>openstack catalog list</pre></div></li><li class="listitem "><p>
    In the output, find Kronos. For example:
   </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Name</th><th>Type</th><th>Endpoints</th></tr></thead><tbody><tr><td>kronos</td><td>region0</td><td>
        <p>
         public: http://myardana.test:5607/v3.0, admin:
         http://192.168.245.5:5607/v3.0, internal:
         http://192.168.245.5:5607/v3.0
        </p>
       </td></tr></tbody></table></div></li><li class="listitem "><p>
    Use the same port number as found in the output. In the example, you would
    use port 5607.
   </p></li></ol></div><p>
  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the logging-ansible restart playbook has been updated to manage
  the start,stop, and restart of the Centralized Logging Service in a specific
  way. This change was made to ensure the proper stop, start, and restart of
  Elasticsearch.
 </p><div id="id-1.5.15.4.3.9" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   It is recommended that you only use the logging playbooks to perform the
   start, stop, and restart of the Centralized Logging Service. Manually mixing
   the start, stop, and restart operations with the logging playbooks will
   result in complex failures.
  </p></div><p>
  For more information, see <a class="xref" href="#central-log-manage" title="13.2.4. Managing the Centralized Logging Feature">Section 13.2.4, “Managing the Centralized Logging Feature”</a>.
 </p><div class="sect3" id="idg-all-operations-central-log-GS-xml-4"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="#idg-all-operations-central-log-GS-xml-4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_GS.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_GS.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-central-log-GS-xml-4</li></ul></div></div></div></div><p>
   For more information about the centralized logging components, see the
   following sites:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <a class="link" href="https://www.elastic.co/guide/en/logstash/current/introduction.html" target="_blank">Logstash</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="http://www.elasticsearch.org/guide" target="_blank">Elasticsearch Guide</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="http://www.elasticsearch.org/blog/scripting-security" target="_blank">Elasticsearch
     Scripting and Security</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="https://python-beaver.readthedocs.io/en/latest/" target="_blank">Beaver</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="http://www.elasticsearch.org/guide/en/kibana/current/index.html" target="_blank">Kibana
     Dashboard</a>
    </p></li><li class="listitem "><p>
     <a class="link" href="http://kafka.apache.org/" target="_blank">Apache Kafka</a>
    </p></li></ul></div></div></div><div class="sect2" id="central-log-concepts"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Understanding the Centralized Logging Service</span> <a title="Permalink" class="permalink" href="#central-log-concepts">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>central-log-concepts</li></ul></div></div></div></div><p>
  The Centralized Logging feature collects logs on a central system, rather
  than leaving the logs scattered across the network. The administrator can use
  a single Kibana interface to view log information in charts, graphs, tables,
  histograms, and other forms.
 </p><div class="sect3" id="idg-all-operations-central-log-understanding-xml-2"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">What Components are Part of Centralized Logging?</span> <a title="Permalink" class="permalink" href="#idg-all-operations-central-log-understanding-xml-2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-central-log-understanding-xml-2</li></ul></div></div></div></div><p>
   Centralized logging consists of several components, detailed below:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="formalpara-title">Administrator's Browser: </span>
      Operations Console can be used to access logging alarms or to access Kibana's
      dashboards to review logging data.
     </p></li><li class="listitem "><p><span class="formalpara-title">Apache Website for Kibana: </span>
      A standard Apache website that proxies web/REST requests to the Kibana
      NodeJS server.
     </p></li><li class="listitem "><p><span class="formalpara-title">Beaver: </span>
      A Python daemon that collects information in log files and sends it to
      the Logging API (monasca-log API) over a secure connection.
     </p></li><li class="listitem "><p><span class="formalpara-title">Cloud Auditing Data Federation (CADF): </span>
      Defines a standard, full-event model anyone can use to fill in the
      essential data needed to certify, self-manage and self-audit
      application security in cloud environments.
     </p></li><li class="listitem "><p><span class="formalpara-title">Centralized Logging and Monitoring (CLM): </span>
      Used to evaluate and troubleshoot your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> distributed cloud
      environment from a single location.
     </p></li><li class="listitem "><p>
     <span class="bold"><strong>Curator:</strong></span> a tool provided by
     Elasticsearch to manage indices.
    </p></li><li class="listitem "><p><span class="formalpara-title">Elasticsearch: </span>
      A data store offering fast indexing and querying.
     </p></li><li class="listitem "><p><span class="formalpara-title"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>: </span>
      Provides public, private, and managed cloud solutions to get you moving
      on your cloud journey.
     </p></li><li class="listitem "><p><span class="formalpara-title">JavaScript Object Notation (JSON) log file: </span>
      A file stored in the JSON format and used to exchange data. JSON uses
      JavaScript syntax, but the JSON format is text only. Text can be read
      and used as a data format by any programming language. This format is
      used by the Beaver and Logstash components.
     </p></li><li class="listitem "><p><span class="formalpara-title">Kafka: </span>
      A messaging broker used for collection of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> centralized
      logging data across nodes. It is highly available, scalable and
      performant. Kafka stores logs in disk instead of memory and is
      therefore more tolerant to consumer down times.
     </p><div id="id-1.5.15.4.4.3.3.10.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      Make sure not to undersize your Kafka partition or the data retention
      period may be lower than expected. If the Kafka partition capacity is
      lower than 85%, the retention period will increase to 30 minutes. Over
      time Kafka will also eject old data.
     </p></div></li><li class="listitem "><p><span class="formalpara-title">Kibana: </span>
      A client/server application with rich dashboards to visualize the data
      in Elasticsearch through a web browser. Kibana enables you to create
      charts and graphs using the log data.
     </p></li><li class="listitem "><p>
     <span class="bold"><strong>Logging API (monasca-log-api):</strong></span> <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
     API provides a standard REST interface to store logs. It uses keystone
     authentication and role-based access control support.
    </p></li><li class="listitem "><p><span class="formalpara-title">Logstash: </span>
      A log processing system for receiving, processing and outputting logs.
      Logstash retrieves logs from Kafka, processes and enriches the data,
      then stores the data in Elasticsearch.
     </p></li><li class="listitem "><p><span class="formalpara-title">MML Service Node: </span>
      Metering, Monitoring, and Logging (MML) service node. All services
      associated with metering, monitoring, and logging run on a dedicated
      three-node cluster. Three nodes are required for high availability with
      quorum.
     </p></li><li class="listitem "><p><span class="formalpara-title">Monasca: </span>
      <span class="productname">OpenStack</span> monitoring at scale infrastructure for the cloud that supports
      alarms and reporting.
     </p></li><li class="listitem "><p><span class="formalpara-title"><span class="productname">OpenStack</span> Service. </span>
      An <span class="productname">OpenStack</span> service process that requires logging services.
     </p></li><li class="listitem "><p><span class="formalpara-title">Oslo.log. </span>
      An <span class="productname">OpenStack</span> library for log handling. The library functions automate
      configuration, deployment and scaling of complete, ready-for-work
      application platforms. Some PaaS solutions, such as Cloud Foundry,
      combine operating systems, containers, and orchestrators with developer
      tools, operations utilities, metrics, and security to create a
      developer-rich solution.
     </p></li><li class="listitem "><p><span class="formalpara-title">Text log: </span>
      A type of file used in the logging process that contains human-readable
      records.
     </p></li></ul></div><p>
   These components are configured to work out-of-the-box and the admin should
   be able to view log data using the default configurations.
  </p><p>
   In addition to each of the services, Centralized Logging also processes logs
   for the following features:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     HAProxy
    </p></li><li class="listitem "><p>
     Syslog
    </p></li><li class="listitem "><p>
     keepalived
    </p></li></ul></div><p>
   The purpose of the logging service is to provide a common logging
   infrastructure with centralized user access. Since there are numerous
   services and applications running in each node of a <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud, and
   there could be hundreds of nodes, all of these services and applications can
   generate enough log files to make it very difficult to search for specific
   events in log files across all of the nodes. Centralized Logging addresses
   this issue by sending log messages in real time to a central Elasticsearch,
   Logstash, and Kibana cluster. In this cluster they are indexed and organized
   for easier and visual searches. The following illustration describes the
   architecture used to collect operational logs.
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-logservice_arch.png" target="_blank"><img src="images/media-logservice_arch.png" width="" /></a></div></div><div id="id-1.5.15.4.4.3.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    The arrows come from the active (requesting) side to the passive
    (listening) side. The active side is always the one providing
    credentials, so the arrows may also be seen as coming from the credential
    holder to the application requiring authentication.
   </p></div></div><div class="sect3" id="log-arch-st1to2"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Steps 1- 2</span> <a title="Permalink" class="permalink" href="#log-arch-st1to2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>log-arch-st1to2</li></ul></div></div></div></div><p>
   Services configured to generate log files record the data. Beaver listens
   for changes to the files and sends the log files to the Logging Service. The
   first step the Logging service takes is to re-format the original log file
   to a new log file with text only and to remove all network operations. In
   Step 1a, the Logging service uses the Oslo.log library to re-format the file
   to text-only. In Step 1b, the Logging service uses the Python-Logstash
   library to format the original audit log file to a JSON file.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.15.4.4.4.3.1"><span class="term ">Step 1a</span></dt><dd><p>
      Beaver watches configured service operational log files for changes and
      reads incremental log changes from the files.
     </p></dd><dt id="id-1.5.15.4.4.4.3.2"><span class="term ">Step 1b</span></dt><dd><p>
      Beaver watches configured service operational log files for changes and
      reads incremental log changes from the files.
     </p></dd><dt id="id-1.5.15.4.4.4.3.3"><span class="term ">Step 2a</span></dt><dd><p>
      The monascalog transport of Beaver makes a token request call to keystone
      passing in credentials. The token returned is cached to avoid multiple
      network round-trips.
     </p></dd><dt id="id-1.5.15.4.4.4.3.4"><span class="term ">Step 2b</span></dt><dd><p>
      The monascalog transport of Beaver batches multiple logs (operational or
      audit) and posts them to the monasca-log-api VIP over a secure
      connection. Failure logs are written to the local Beaver log.
     </p></dd><dt id="id-1.5.15.4.4.4.3.5"><span class="term ">Step 2c</span></dt><dd><p>
      The REST API client for monasca-log-api makes a token-request call to
      keystone passing in credentials. The token returned is cached to avoid
      multiple network round-trips.
     </p></dd><dt id="id-1.5.15.4.4.4.3.6"><span class="term ">Step 2d</span></dt><dd><p>
      The REST API client for monasca-log-api batches multiple logs
      (operational or audit) and posts them to the monasca-log-api VIP over a
      secure connection.
     </p></dd></dl></div></div><div class="sect3" id="log-arch-st3ab"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Steps 3a- 3b</span> <a title="Permalink" class="permalink" href="#log-arch-st3ab">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>log-arch-st3ab</li></ul></div></div></div></div><p>
   The Logging API (monasca-log API) communicates with keystone to validate the
   incoming request, and then sends the logs to Kafka.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.15.4.4.5.3.1"><span class="term ">Step 3a</span></dt><dd><p>
      The monasca-log-api WSGI pipeline is configured to validate incoming
      request tokens with keystone. The keystone middleware used for this
      purpose is configured to use the monasca-log-api admin user, password and
      project that have the required keystone role to validate a token.
     </p></dd><dt id="id-1.5.15.4.4.5.3.2"><span class="term ">Step 3b</span></dt><dd><p>
      monasca-log-api sends log messages to Kafka using a language-agnostic TCP
      protocol.
     </p></dd></dl></div></div><div class="sect3" id="log-arch-st4to8"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Steps 4- 8</span> <a title="Permalink" class="permalink" href="#log-arch-st4to8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>log-arch-st4to8</li></ul></div></div></div></div><p>
   Logstash pulls messages from Kafka, identifies the log type, and transforms
   the messages into either the audit log format or operational format. Then
   Logstash sends the messages to Elasticsearch, using either an audit or
   operational indices.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.15.4.4.6.3.1"><span class="term ">Step 4</span></dt><dd><p>
      Logstash input workers pull log messages from the Kafka-Logstash topic
      using TCP.
     </p></dd><dt id="id-1.5.15.4.4.6.3.2"><span class="term ">Step 5</span></dt><dd><p>
      This Logstash filter processes the log message in-memory in the request
      pipeline. Logstash identifies the log type from this field.
     </p></dd><dt id="id-1.5.15.4.4.6.3.3"><span class="term ">Step 6</span></dt><dd><p>
      This Logstash filter processes the log message in-memory in the request
      pipeline. If the message is of audit-log type, Logstash transforms it
      from the monasca-log-api envelope format to the original CADF format.
     </p></dd><dt id="id-1.5.15.4.4.6.3.4"><span class="term ">Step 7</span></dt><dd><p>
      This Logstash filter determines which index should receive the log
      message. There are separate indices in Elasticsearch for operational
      versus audit logs.
     </p></dd><dt id="id-1.5.15.4.4.6.3.5"><span class="term ">Step 8</span></dt><dd><p>
      Logstash output workers write the messages read from Kafka to the daily
      index in the local Elasticsearch instance.
     </p></dd></dl></div></div><div class="sect3" id="log-arch-st9to12"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Steps 9- 12</span> <a title="Permalink" class="permalink" href="#log-arch-st9to12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>log-arch-st9to12</li></ul></div></div></div></div><p>
   When an administrator who has access to the guest network accesses the
   Kibana client and makes a request, Apache forwards the request to the Kibana
   NodeJS server. Then the server uses the Elasticsearch REST API to service
   the client requests.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.15.4.4.7.3.1"><span class="term ">Step 9</span></dt><dd><p>
      An administrator who has access to the guest network accesses the Kibana
      client to view and search log data. The request can originate from the
      external network in the cloud through a tenant that has a pre-defined
      access route to the guest network.
     </p></dd><dt id="id-1.5.15.4.4.7.3.2"><span class="term ">Step 10</span></dt><dd><p>
      An administrator who has access to the guest network uses a web browser
      and points to the Kibana URL. This allows the user to search logs and
      view Dashboard reports.
     </p></dd><dt id="id-1.5.15.4.4.7.3.3"><span class="term ">Step 11</span></dt><dd><p>
      The authenticated request is forwarded to the Kibana NodeJS server to
      render the required dashboard, visualization, or search page.
     </p></dd><dt id="id-1.5.15.4.4.7.3.4"><span class="term ">Step 12</span></dt><dd><p>
      The Kibana NodeJS web server uses the Elasticsearch REST API in localhost
      to service the UI requests.
     </p></dd></dl></div></div><div class="sect3" id="log-arch-st13to15"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Steps 13- 15</span> <a title="Permalink" class="permalink" href="#log-arch-st13to15">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>log-arch-st13to15</li></ul></div></div></div></div><p>
   Log data is backed-up and deleted in the final steps.
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.15.4.4.8.3.1"><span class="term ">Step 13</span></dt><dd><p>
      A daily cron job running in the ELK node runs curator to prune old
      Elasticsearch log indices.
     </p></dd><dt id="id-1.5.15.4.4.8.3.2"><span class="term ">Step 14</span></dt><dd><p>
      The curator configuration is done at the deployer node through the
      Ansible role logging-common. Curator is scripted to then prune or clone
      old indices based on this configuration.
     </p></dd><dt id="id-1.5.15.4.4.8.3.3"><span class="term ">Step 15</span></dt><dd><p>
      The audit logs must be backed up manually. For more information about
      Backup and Recovery, see <a class="xref" href="#bura-overview" title="Chapter 17. Backup and Restore">Chapter 17, <em>Backup and Restore</em></a>.
     </p></dd></dl></div></div><div class="sect3" id="retaining-logs"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.2.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How Long are Log Files Retained?</span> <a title="Permalink" class="permalink" href="#retaining-logs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>retaining-logs</li></ul></div></div></div></div><p>
   The logs that are centrally stored are saved to persistent storage as
   Elasticsearch indices. These indices are stored in the partition
   <code class="literal">/var/lib/elasticsearch</code> on each of the Elasticsearch
   cluster nodes. Out of the box, logs are stored in one Elasticsearch index
   per service. As more days go by, the number of indices stored in this disk
   partition grows. Eventually the partition fills up. If they are
   <span class="bold"><strong>open</strong></span>, each of these indices takes up CPU
   and memory. If these indices are left unattended they will continue to
   consume system resources and eventually deplete them.
  </p><p>
   Elasticsearch, by itself, does not prevent this from happening.
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses a tool called curator that is developed by the Elasticsearch
   community to handle these situations. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installs and uses a curator
   in conjunction with several configurable settings. This curator is called by
   cron and performs the following checks:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>First Check.</strong></span> The hourly cron job checks
     to see if the currently used Elasticsearch partition size is over the
     value set in:
    </p><div class="verbatim-wrap"><pre class="screen">curator_low_watermark_percent</pre></div><p>
     If it is higher than this value, the curator deletes old indices according
     to the value set in:
    </p><div class="verbatim-wrap"><pre class="screen">curator_num_of_indices_to_keep</pre></div></li><li class="listitem "><p>
     <span class="bold"><strong>Second Check.</strong></span> Another check is made to
     verify if the partition size is below the high watermark percent. If it is
     still too high, curator will delete all indices except the current one
     that is over the size as set in:
    </p><div class="verbatim-wrap"><pre class="screen">curator_max_index_size_in_gb</pre></div></li><li class="listitem "><p>
     <span class="bold"><strong>Third Check.</strong></span> A third check verifies if
     the partition size is still too high. If it is, curator will delete all
     indices except the current one.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Final Check.</strong></span> A final check verifies if
     the partition size is still high. If it is, an error message is written to
     the log file but the current index is NOT deleted.
    </p></li></ul></div><p>
   In the case of an extreme network issue, log files can run out of disk space
   in under an hour. To avoid this <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses a shell script called
   <code class="literal">logrotate_if_needed.sh</code>. The cron process runs this script
   every 5 minutes to see if the size of <code class="literal">/var/log</code> has
   exceeded the high_watermark_percent (95% of the disk, by default). If it is
   at or above this level, <code class="literal">logrotate_if_needed.sh</code> runs the
   <code class="literal">logrotate</code> script to rotate logs and to free up extra
   space. This script helps to minimize the chance of running out of disk space
   on <code class="literal">/var/log</code>.
  </p></div><div class="sect3" id="rotating-logs"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.2.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How Are Logs Rotated?</span> <a title="Permalink" class="permalink" href="#rotating-logs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>rotating-logs</li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses the cron process which in turn calls Logrotate to provide
   rotation, compression, and removal of log files. Each log file can be
   rotated hourly, daily, weekly, or monthly. If no rotation period is set then
   the log file will only be rotated when it grows too large.
  </p><p>
   Rotating a file means that the Logrotate process creates a copy of the log
   file with a new extension, for example, the .1 extension, then empties the
   contents of the original file. If a .1 file already exists, then that file
   is first renamed with a .2 extension. If a .2 file already exists, it is
   renamed to .3, etc., up to the maximum number of rotated files specified in
   the settings file. When Logrotate reaches the last possible file extension,
   it will delete the last file first on the next rotation. By the time the
   Logrotate process needs to delete a file, the results will have been copied
   to Elasticsearch, the central logging database.
  </p><p>
   The log rotation setting files can be found in the following directory
  </p><div class="verbatim-wrap"><pre class="screen">~/scratch/ansible/next/ardana/ansible/roles/logging-common/vars</pre></div><p>
   These files allow you to set the following options:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.15.4.4.10.7.1"><span class="term ">Service</span></dt><dd><p>
      The name of the service that creates the log entries.
     </p></dd><dt id="id-1.5.15.4.4.10.7.2"><span class="term ">Rotated Log Files</span></dt><dd><p>
      List of log files to be rotated. These files are kept locally on the
      server and will continue to be rotated. If the file is also listed as
      Centrally Logged, it will also be copied to Elasticsearch.
     </p></dd><dt id="id-1.5.15.4.4.10.7.3"><span class="term ">Frequency</span></dt><dd><p>
      The timing of when the logs are rotated. Options include:hourly, daily,
      weekly, or monthly.
     </p></dd><dt id="id-1.5.15.4.4.10.7.4"><span class="term ">Max Size</span></dt><dd><p>
      The maximum file size the log can be before it is rotated out.
     </p></dd><dt id="id-1.5.15.4.4.10.7.5"><span class="term ">Rotation</span></dt><dd><p>
      The number of log files that are rotated.
     </p></dd><dt id="id-1.5.15.4.4.10.7.6"><span class="term ">Centrally Logged Files</span></dt><dd><p>
      These files will be indexed by Elasticsearch and will be available for
      searching in the Kibana user interface.
     </p></dd></dl></div><p>
   Only files that are listed in the <span class="bold"><strong>Centrally Logged
   Files</strong></span> section are copied to Elasticsearch.
  </p><p>
   All of the variables for the Logrotate process are found in the following
   file:
  </p><div class="verbatim-wrap"><pre class="screen">~/scratch/ansible/next/ardana/ansible/roles/logging-ansible/logging-common/defaults/main.yml</pre></div><p>
   Cron runs Logrotate hourly. Every 5 minutes another process is run called
   <span class="bold"><strong>"logrotate_if_needed"</strong></span> which uses a
   watermark value to determine if the Logrotate process needs to be run. If
   the <span class="bold"><strong>"high watermark"</strong></span> has been reached, and
   the /var/log partition is more than 95% full (by default - this can be
   adjusted), then Logrotate will be run within 5 minutes.
  </p></div><div class="sect3" id="BUElasticsearch"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.2.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Are Log Files Backed-Up To Elasticsearch?</span> <a title="Permalink" class="permalink" href="#BUElasticsearch">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_understanding.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_understanding.xml</li><li><span class="ds-label">ID: </span>BUElasticsearch</li></ul></div></div></div></div><p>
   While centralized logging is enabled out of the box, the backup of these
   logs is not. The reason is because Centralized Logging relies on the
   Elasticsearch FileSystem Repository plugin, which in turn requires shared
   disk partitions to be configured and accessible from each of the
   Elasticsearch nodes. Since there are multiple ways to setup a shared disk
   partition, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> allows you to choose an approach that works best for
   your deployment before enabling the back-up of log files to Elasticsearch.
  </p><p>
   If you enable automatic back-up of centralized log files, then all the logs
   collected from the cloud nodes will be backed-up to Elasticsearch. Every
   hour, in the management controller nodes where Elasticsearch is setup, a
   cron job runs to check if Elasticsearch is running low on disk space. If the
   check succeeds, it further checks if the backup feature is enabled. If
   enabled, the cron job saves a snapshot of the Elasticsearch indices to the
   configured shared disk partition using curator. Next, the script starts
   deleting the oldest index and moves down from there checking each time if
   there is enough space for Elasticsearch. A check is also made to ensure that
   the backup runs only once a day.
  </p><p>
   For steps on how to enable automatic back-up, see
   <a class="xref" href="#central-log-configure-settings" title="13.2.5. Configuring Centralized Logging">Section 13.2.5, “Configuring Centralized Logging”</a>.
  </p></div></div><div class="sect2" id="central-log-access-data"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Accessing Log Data</span> <a title="Permalink" class="permalink" href="#central-log-access-data">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_access_data.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_access_data.xml</li><li><span class="ds-label">ID: </span>central-log-access-data</li></ul></div></div></div></div><p>
  All logging data in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is managed by the Centralized Logging Service
  and can be viewed or analyzed by Kibana. Kibana is the only graphical
  interface provided with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to search or create a report from log data.
  Operations Console provides only a link to the Kibana Logging dashboard.
 </p><p>
  The following two methods allow you to access the Kibana Logging dashboard to
  search log data:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="#CL-access-OpsConsole" title="13.2.3.1. Use the Operations Console Link">Section 13.2.3.1, “Use the Operations Console Link”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#CL-access-Kibana" title="13.2.3.2. Using Kibana to Access Log Data">Section 13.2.3.2, “Using Kibana to Access Log Data”</a>
   </p></li></ul></div><p>
  To learn more about Kibana, read the
  <a class="link" href="https://www.elastic.co/guide/en/kibana/current/getting-started.html" target="_blank">Getting
  Started with Kibana</a> guide.
 </p><div class="sect3" id="CL-access-OpsConsole"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Use the Operations Console Link</span> <a title="Permalink" class="permalink" href="#CL-access-OpsConsole">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_access_data.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_access_data.xml</li><li><span class="ds-label">ID: </span>CL-access-OpsConsole</li></ul></div></div></div></div><p>
   Operations Console allows you to access Kibana in the same tool that you use
   to manage the other <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> resources in your deployment. To use Operations Console,
   you must have the correct permissions.
  </p><p>
   To use Operations Console:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     In a browser, open the Operations Console.
    </p></li><li class="listitem "><p>
     On the login page, enter the user name, and the
     <span class="bold"><strong>Password</strong></span>, and then click
     <span class="bold"><strong>LOG IN</strong></span>.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Home/Central Dashboard</strong></span> page, click
     the menu represented by 3 horizontal lines (<span class="inlinemediaobject"><img xmlns="" src="images/media-opsconsole-OpsConsoleBurgerMenu.png" width="" alt="Three-Line Icon" /></span>).
    </p></li><li class="listitem "><p>
     From the menu that slides in on the left, select
     <span class="bold"><strong>Home</strong></span>, and then select
     <span class="bold"><strong>Logging</strong></span>.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Home/Logging</strong></span> page, click
     <span class="bold"><strong>View Logging Dashboard</strong></span>.
    </p></li></ol></div><div id="id-1.5.15.4.5.6.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, Kibana usually runs on a different network than Operations Console.
    Due to this configuration, it is possible that using Operations Console
    to access Kibana will result in an “404 not found” error. This
    error only occurs if the user has access only to the public facing network.
   </p></div></div><div class="sect3" id="CL-access-Kibana"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using Kibana to Access Log Data</span> <a title="Permalink" class="permalink" href="#CL-access-Kibana">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_access_data.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_access_data.xml</li><li><span class="ds-label">ID: </span>CL-access-Kibana</li></ul></div></div></div></div><p>
   Kibana is an open-source, data-visualization plugin for Elasticsearch.
   Kibana provides visualization capabilities using the log content indexed on
   an Elasticsearch cluster. Users can create bar and pie charts, line and
   scatter plots, and maps using the data collected by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> in the cloud
   log files.
  </p><p>
   While creating Kibana dashboards is beyond the scope of this document, it is
   important to know that the dashboards you create are JSON files that you can
   modify or create new dashboards based on existing dashboards.
  </p><div id="id-1.5.15.4.5.7.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    Kibana is client-server software. To operate properly, the browser must be
    able to access port 5601 on the control plane.
   </p></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Field</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td>user</td><td>kibana</td><td>
       <p>
        Username that will be required for logging into the Kibana UI.
       </p>
      </td></tr><tr><td>password</td><td>random password is generated</td><td>
       <p>
        Password generated during installation that is used to login to the
        Kibana UI.
       </p>
      </td></tr></tbody></table></div></div><div class="sect3" id="Login-creds-Kibana"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Logging into Kibana</span> <a title="Permalink" class="permalink" href="#Login-creds-Kibana">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_access_data.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_access_data.xml</li><li><span class="ds-label">ID: </span>Login-creds-Kibana</li></ul></div></div></div></div><p>
   To log into Kibana to view data, you must make sure you have the required
   login configuration.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Verify login credentials: <a class="xref" href="#KLogin-Creds" title="13.2.3.3.1. Verify Login Credentials">Section 13.2.3.3.1, “Verify Login Credentials”</a>
    </p></li><li class="listitem "><p>
     Find the randomized password: <a class="xref" href="#KLogin-Psswd" title="13.2.3.3.2. Find the Randomized Password">Section 13.2.3.3.2, “Find the Randomized Password”</a>
    </p></li><li class="listitem "><p>
     Access Kibana using a direct link: <a class="xref" href="#KLogin-DLink" title="13.2.3.3.3. Access Kibana Using a Direct Link:">Section 13.2.3.3.3, “Access Kibana Using a Direct Link:”</a>
    </p></li></ol></div><div class="sect4" id="KLogin-Creds"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.3.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Verify Login Credentials</span> <a title="Permalink" class="permalink" href="#KLogin-Creds">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_access_data.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_access_data.xml</li><li><span class="ds-label">ID: </span>KLogin-Creds</li></ul></div></div></div></div><p>
    During the installation of Kibana, a password is automatically set and it
    is randomized. Therefore, unless an administrator has already changed it,
    you need to retrieve the default password from a file on the control plane
    node.
   </p></div><div class="sect4" id="KLogin-Psswd"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.3.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Find the Randomized Password</span> <a title="Permalink" class="permalink" href="#KLogin-Psswd">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_access_data.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_access_data.xml</li><li><span class="ds-label">ID: </span>KLogin-Psswd</li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
      To find the Kibana password, run:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>grep kibana ~/scratch/ansible/next/my_cloud/stage/internal/CloudModel.yaml</pre></div></li></ol></div></div><div class="sect4" id="KLogin-DLink"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.3.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Access Kibana Using a Direct Link:</span> <a title="Permalink" class="permalink" href="#KLogin-DLink">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_access_data.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_access_data.xml</li><li><span class="ds-label">ID: </span>KLogin-DLink</li></ul></div></div></div></div><p>
    This section helps you verify the horizon virtual IP (VIP) address that you
    should use. To provide enhanced security, access to Kibana is not available on the
    External network.
   </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
      To determine which IP address to use to access Kibana, from your Cloud Lifecycle Manager, run:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>grep HZN-WEB /etc/hosts</pre></div><p>
      The output of the grep command should show you the virtual IP address for
      Kibana that you should use.
     </p><div id="id-1.5.15.4.5.8.6.3.1.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
       If nothing is returned by the grep command, you can open the following
       file to look for the IP address manually:
      </p><div class="verbatim-wrap"><pre class="screen">/etc/hosts</pre></div></div><p>
      Access to Kibana will be over port 5601 of that virtual IP address.
      Example:
     </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable ">VIP</em>:5601</pre></div></li></ol></div></div></div></div><div class="sect2" id="central-log-manage"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing the Centralized Logging Feature</span> <a title="Permalink" class="permalink" href="#central-log-manage">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_manage.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_manage.xml</li><li><span class="ds-label">ID: </span>central-log-manage</li></ul></div></div></div></div><p>
  No specific configuration tasks are required to use Centralized Logging, as
  it is enabled by default after installation. However, you can configure the
  individual components as needed for your environment.
 </p><div class="sect3" id="CL-stop-start"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How Do I Stop and Start the Logging Service?</span> <a title="Permalink" class="permalink" href="#CL-stop-start">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_manage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_manage.xml</li><li><span class="ds-label">ID: </span>CL-stop-start</li></ul></div></div></div></div><p>
   Although you might not need to stop and start the logging service very
   often, you may need to if, for example, one of the logging services is not
   behaving as expected or not working.
  </p><p>
   You cannot enable or disable centralized logging across all services unless
   you stop all centralized logging. Instead, it is recommended that you enable
   or disable individual log files in the &lt;service&gt;-clr.yml files and
   then reconfigure logging. You would enable centralized logging for a file
   when you want to make sure you are able to monitor those logs in Kibana.
  </p><p>
   In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the logging-ansible restart playbook has been updated to manage
   the start,stop, and restart of the Centralized Logging Service in a specific
   way. This change was made to ensure the proper stop, start, and restart of
   Elasticsearch.
  </p><div id="id-1.5.15.4.6.3.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    It is recommended that you only use the logging playbooks to perform the
    start, stop, and restart of the Centralized Logging Service. Manually
    mixing the start, stop, and restart operations with the logging playbooks
    will result in complex failures.
   </p></div><p>
   The steps in this section only impact centralized logging. Logrotate is an
   essential feature that keeps the service log files from filling the disk and
   will not be affected.
  </p><div id="id-1.5.15.4.6.3.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    These playbooks must be run from the Cloud Lifecycle Manager.
   </p></div><p>
   <span class="bold"><strong>To stop the Logging service:</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To change to the directory containing the ansible playbook, run
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible</pre></div></li><li class="listitem "><p>
     To run the ansible playbook that will stop the logging service, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts logging-stop.yml</pre></div></li></ol></div><p>
   <span class="bold"><strong>To start the Logging service:</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To change to the directory containing the ansible playbook, run
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible</pre></div></li><li class="listitem "><p>
     To run the ansible playbook that will stop the logging service, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts logging-start.yml</pre></div></li></ol></div></div><div class="sect3" id="CL-disable"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How Do I Enable or Disable Centralized Logging For a Service?</span> <a title="Permalink" class="permalink" href="#CL-disable">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_manage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_manage.xml</li><li><span class="ds-label">ID: </span>CL-disable</li></ul></div></div></div></div><p>
   To enable or disable Centralized Logging for a service you need to modify
   the configuration for the service, set the
   <span class="bold"><strong>enabled</strong></span> flag to
   <span class="bold"><strong>true</strong></span> or
   <span class="bold"><strong>false</strong></span>, and then reconfigure logging.
  </p><div id="id-1.5.15.4.6.4.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    There are consequences if you enable too many logging files for a service.
    If there is not enough storage to support the increased logging, the
    retention period of logs in Elasticsearch is decreased. Alternatively, if
    you wanted to increase the retention period of log files or if you did not
    want those logs to show up in Kibana, you would disable centralized logging
    for a file.
   </p></div><p>
   To enable Centralized Logging for a service:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Use the documentation provided with the service to ensure it is not
     configured for logging.
    </p></li><li class="listitem "><p>
     To find the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> file to edit, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>find ~/openstack/my_cloud/config/logging/vars/ -name "*<em class="replaceable ">service-name</em>*"</pre></div></li><li class="listitem "><p>
     Edit the file for the service for which you want to enable logging.
    </p></li><li class="listitem "><p>
     To enable Centralized Logging, find the following code and change the
     enabled flag to <span class="bold"><strong>true</strong></span>, to disable, change
     the enabled flag to <span class="bold"><strong>false</strong></span>:
    </p><div class="verbatim-wrap"><pre class="screen">logging_options:
 - centralized_logging:
        enabled: true
        format: json</pre></div></li><li class="listitem "><p>
     Save the changes to the file.
    </p></li><li class="listitem "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="listitem "><p>
     To reconfigure logging, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts kronos-reconfigure.yml
<code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li></ol></div></div></div><div class="sect2" id="central-log-configure-settings"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Centralized Logging</span> <a title="Permalink" class="permalink" href="#central-log-configure-settings">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_CL.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_CL.xml</li><li><span class="ds-label">ID: </span>central-log-configure-settings</li></ul></div></div></div></div><p>
  You can adjust the settings for centralized logging when you are
  troubleshooting problems with a service or to decrease log size and retention
  to save on disk space. For steps on how to configure logging settings, refer
  to the following tasks:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="#CL-config-files" title="13.2.5.1. Configuration Files">Section 13.2.5.1, “Configuration Files”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#CL-general-config" title="13.2.5.2. Planning Resource Requirements">Section 13.2.5.2, “Planning Resource Requirements”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#CL-BU-Elasticsearch" title="13.2.5.3. Backing Up Elasticsearch Log Indices">Section 13.2.5.3, “Backing Up Elasticsearch Log Indices”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#restore-elastic-logs" title="13.2.5.4. Restoring Logs From an Elasticsearch Backup">Section 13.2.5.4, “Restoring Logs From an Elasticsearch Backup”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#tuning-logging-parameters" title="13.2.5.5. Tuning Logging Parameters">Section 13.2.5.5, “Tuning Logging Parameters”</a>
   </p></li></ul></div><div class="sect3" id="CL-config-files"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuration Files</span> <a title="Permalink" class="permalink" href="#CL-config-files">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_CL.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_CL.xml</li><li><span class="ds-label">ID: </span>CL-config-files</li></ul></div></div></div></div><p>
   Centralized Logging settings are stored in the configuration files in the
   following directory on the Cloud Lifecycle Manager:
   <code class="literal">~/openstack/my_cloud/config/logging/</code>
  </p><p>
   The configuration files and their use are described below:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>File</th><th>Description</th></tr></thead><tbody><tr><td>main.yml</td><td>Main configuration file for all centralized logging components.</td></tr><tr><td>elasticsearch.yml.j2</td><td>Main configuration file for Elasticsearch.</td></tr><tr><td>elasticsearch-default.j2</td><td>Default overrides for the Elasticsearch init script.</td></tr><tr><td>kibana.yml.j2</td><td>Main configuration file for Kibana.</td></tr><tr><td>kibana-apache2.conf.j2</td><td>Apache configuration file for Kibana.</td></tr><tr><td>logstash.conf.j2</td><td>Logstash inputs/outputs configuration.</td></tr><tr><td>logstash-default.j2</td><td>Default overrides for the Logstash init script.</td></tr><tr><td>beaver.conf.j2</td><td>Main configuration file for Beaver.</td></tr><tr><td>vars</td><td>Path to logrotate configuration files.</td></tr></tbody></table></div></div><div class="sect3" id="CL-general-config"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Planning Resource Requirements</span> <a title="Permalink" class="permalink" href="#CL-general-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_CL.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_CL.xml</li><li><span class="ds-label">ID: </span>CL-general-config</li></ul></div></div></div></div><p>
   The Centralized Logging service needs to have enough resources available to
   it to perform adequately for different scale environments. The base logging
   levels are tuned during installation according to the amount of RAM
   allocated to your control plane nodes to ensure optimum performance.
  </p><p>
   These values can be viewed and changed in the
   <code class="literal">~/openstack/my_cloud/config/logging/main.yml</code> file, but you
   will need to run a reconfigure of the Centralized Logging service if changes
   are made.
  </p><div id="id-1.5.15.4.7.5.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
    The total process memory consumption for Elasticsearch will be the above
    allocated heap value (in
    <code class="literal">~/openstack/my_cloud/config/logging/main.yml</code>) plus any Java
    Virtual Machine (JVM) overhead.
   </p></div><p>
   <span class="bold"><strong>Setting Disk Size Requirements</strong></span>
  </p><p>
   In the entry-scale models, the disk partition sizes on your controller nodes
   for the logging and Elasticsearch data are set as a percentage of your total
   disk size. You can see these in the following file on the Cloud Lifecycle Manager
   (deployer):
   <code class="literal">~/openstack/my_cloud/definition/data/&lt;controller_disk_files_used&gt;</code>
  </p><p>
   Sample file settings:
  </p><div class="verbatim-wrap"><pre class="screen"># Local Log files.
- name: log
  size: 13%
  mount: /var/log
  fstype: ext4
  mkfs-opts: -O large_file

# Data storage for centralized logging. This holds log entries from all
# servers in the cloud and hence can require a lot of disk space.
- name: elasticsearch
  size: 30%
  mount: /var/lib/elasticsearch
  fstype: ext4</pre></div><div id="id-1.5.15.4.7.5.9" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    The disk size is set automatically based on the hardware configuration. If
    you need to adjust it, you can set it manually with the following steps.
   </p></div><p>
   To set disk sizes:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="listitem "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/disks.yml</pre></div></li><li class="listitem "><p>
     Make any desired changes.
    </p></li><li class="listitem "><p>
     Save the changes to the file.
    </p></li><li class="listitem "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A git
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="listitem "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     To run the logging reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts kronos-reconfigure.yml</pre></div></li></ol></div></div><div class="sect3" id="CL-BU-Elasticsearch"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Backing Up Elasticsearch Log Indices</span> <a title="Permalink" class="permalink" href="#CL-BU-Elasticsearch">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_CL.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_CL.xml</li><li><span class="ds-label">ID: </span>CL-BU-Elasticsearch</li></ul></div></div></div></div><p>
   The log files that are centrally collected in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are stored by
   Elasticsearch on disk in the <code class="literal">/var/lib/elasticsearch</code>
   partition. However, this is distributed across each of the Elasticsearch
   cluster nodes as shards. A cron job runs periodically to see if the disk
   partition runs low on space, and, if so, it runs curator to delete the old
   log indices to make room for new logs. This deletion is permanent and the
   logs are lost forever. If you want to backup old logs, for example to comply
   with certain regulations, you can configure automatic backup of
   Elasticsearch indices.
  </p><div id="id-1.5.15.4.7.6.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    If you need to restore data that was archived prior to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 and
    used the older versions of Elasticsearch, then this data will need to be
    restored to a separate deployment of Elasticsearch.
   </p><p>
    This can be accomplished using the following steps:
   </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
      Deploy a separate distinct Elasticsearch instance version matching the
      version in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
     </p></li><li class="listitem "><p>
      Configure the backed-up data using NFS or some other share mechanism to
      be available to the Elasticsearch instance matching the version in
      <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
     </p></li></ol></div></div><p>
   Before enabling automatic back-ups, make sure you understand how much disk
   space you will need, and configure the disks that will store the data. Use
   the following checklist to prepare your deployment for enabling automatic
   backups:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>☐</th><th>Item</th></tr></thead><tbody><tr><td>☐</td><td>
       <p>
        Add a shared disk partition to each of the Elasticsearch controller
        nodes.
       </p>
       <p>
        The default partition name used for backup is
       </p>
<div class="verbatim-wrap"><pre class="screen">/var/lib/esbackup</pre></div>
       <p>
        You can change this by:
       </p>
       <div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
          Open the following file:
          <code class="literal">my_cloud/config/logging/main.yml</code>
         </p></li><li class="listitem "><p>
          Edit the following variable <code class="literal">curator_es_backup_partition
          </code>
         </p></li></ol></div>
      </td></tr><tr><td>☐</td><td>
       <p>
        Ensure the shared disk has enough storage to retain backups for the
        desired retention period.
       </p>
      </td></tr></tbody></table></div><p>
   To enable automatic back-up of centralized logs to Elasticsearch:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager (deployer node).
    </p></li><li class="listitem "><p>
     Open the following file in a text editor:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/logging/main.yml</pre></div></li><li class="listitem "><p>
     Find the following variables:
    </p><div class="verbatim-wrap"><pre class="screen">curator_backup_repo_name: "es_{{host.my_dimensions.cloud_name}}"
curator_es_backup_partition: /var/lib/esbackup</pre></div></li><li class="listitem "><p>
     To enable backup, change the
     <span class="bold"><strong>curator_enable_backup</strong></span> value to
     <span class="bold"><strong>true</strong></span> in the curator section:
    </p><div class="verbatim-wrap"><pre class="screen">curator_enable_backup: true</pre></div></li><li class="listitem "><p>
     Save your changes and re-run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git add -A
# Verify the added files
<code class="prompt user">ardana &gt; </code>git status
<code class="prompt user">ardana &gt; </code>git commit -m "Enabling Elasticsearch Backup"

$ cd ~/openstack/ardana/ansible
$ ansible-playbook -i hosts/localhost config-processor-run.yml
$ ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     To re-configure logging:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts kronos-reconfigure.yml</pre></div></li><li class="listitem "><p>
     To verify that the indices are backed up, check the contents of the
     partition:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls /var/lib/esbackup</pre></div></li></ol></div></div><div class="sect3" id="restore-elastic-logs"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restoring Logs From an Elasticsearch Backup</span> <a title="Permalink" class="permalink" href="#restore-elastic-logs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_CL.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_CL.xml</li><li><span class="ds-label">ID: </span>restore-elastic-logs</li></ul></div></div></div></div><p>
   To restore logs from an Elasticsearch backup, see
   <a class="link" href="https://www.elastic.co/guide/en/elasticsearch/reference/2.4/modules-snapshots.html" target="_blank">https://www.elastic.co/guide/en/elasticsearch/reference/2.4/modules-snapshots.html</a>.
  </p><div id="id-1.5.15.4.7.7.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    We do not recommend restoring to the original <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Centralized Logging
    cluster as it may cause storage/capacity issues. We rather recommend setting
    up a separate ELK cluster of the same version and restoring the logs there.
   </p></div></div><div class="sect3" id="tuning-logging-parameters"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.5.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Tuning Logging Parameters</span> <a title="Permalink" class="permalink" href="#tuning-logging-parameters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_CL.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_CL.xml</li><li><span class="ds-label">ID: </span>tuning-logging-parameters</li></ul></div></div></div></div><p>
   When centralized logging is installed in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, parameters for
   Elasticsearch heap size and logstash heap size are automatically configured
   based on the amount of RAM on the system. These values are typically the
   required values, but they may need to be adjusted if performance issues
   arise, or disk space issues are encountered. These values may also need to
   be adjusted if hardware changes are made after an installation.
  </p><p>
   These values are defined at the top of the following file
   <code class="literal">.../logging-common/defaults/main.yml</code>. An example of the
   contents of the file is below:
  </p><div class="verbatim-wrap"><pre class="screen">1. Select heap tunings based on system RAM
#-------------------------------------------------------------------------------
threshold_small_mb: 31000
threshold_medium_mb: 63000
threshold_large_mb: 127000
tuning_selector: " {% if ansible_memtotal_mb &lt; threshold_small_mb|int %}
demo
{% elif ansible_memtotal_mb &lt; threshold_medium_mb|int %}
small
{% elif ansible_memtotal_mb &lt; threshold_large_mb|int %}
medium
{% else %}
large
{%endif %}
"

logging_possible_tunings:
2. RAM &lt; 32GB
demo:
elasticsearch_heap_size: 512m
logstash_heap_size: 512m
3. RAM &lt; 64GB
small:
elasticsearch_heap_size: 8g
logstash_heap_size: 2g
4. RAM &lt; 128GB
medium:
elasticsearch_heap_size: 16g
logstash_heap_size: 4g
5. RAM &gt;= 128GB
large:
elasticsearch_heap_size: 31g
logstash_heap_size: 8g
logging_tunings: "{{ logging_possible_tunings[tuning_selector] }}"</pre></div><p>
   This specifies thresholds for what a <span class="bold"><strong>small</strong></span>,
   <span class="bold"><strong>medium</strong></span>, or
   <span class="bold"><strong>large</strong></span> system would look like, in terms of
   memory. To see what values will be used, see what RAM your system uses, and
   see where it fits in with the thresholds to see what values you will be
   installed with. To modify the values, you can either adjust the threshold
   values so that your system will change from a
   <span class="bold"><strong>small</strong></span> configuration to a
   <span class="bold"><strong>medium</strong></span> configuration, for example, or keep
   the threshold values the same, and modify the heap_size variables directly
   for the selector that your system is set for. For example, if your
   configuration is a <span class="bold"><strong>medium</strong></span> configuration,
   which sets heap_sizes to 16 GB for Elasticsearch and 4 GB for logstash, and
   you want twice as much set aside for logstash, then you could increase the
   4 GB for logstash to 8 GB.
  </p></div></div><div class="sect2" id="central-log-configure-services"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Settings for Other Services</span> <a title="Permalink" class="permalink" href="#central-log-configure-services">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>central-log-configure-services</li></ul></div></div></div></div><p>
  When you configure settings for the Centralized Logging Service, those
  changes impact all services that are enabled for centralized logging.
  However, if you only need to change the logging configuration for one
  specific service, you will want to modify the service's files instead of
  changing the settings for the entire Centralized Logging service. This topic
  helps you complete the following tasks:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="#CL-log-level-srv" title="13.2.6.1. Setting Logging Levels for Services">Section 13.2.6.1, “Setting Logging Levels for Services”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#CL-select-central-logging" title="13.2.6.19. Selecting Files for Centralized Logging">Section 13.2.6.19, “Selecting Files for Centralized Logging”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#CL-space-allocation" title="13.2.6.20. Controlling Disk Space Allocation and Retention of Log Files">Section 13.2.6.20, “Controlling Disk Space Allocation and Retention of Log Files”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#CL-elasticsearch-config" title="13.2.6.21. Configuring Elasticsearch for Centralized Logging">Section 13.2.6.21, “Configuring Elasticsearch for Centralized Logging”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#CL-safeguards" title="13.2.6.22. Safeguards for the Log Partitions Disk Capacity">Section 13.2.6.22, “Safeguards for the Log Partitions Disk Capacity”</a>
   </p></li></ul></div><div class="sect3" id="CL-log-level-srv"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting Logging Levels for Services</span> <a title="Permalink" class="permalink" href="#CL-log-level-srv">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>CL-log-level-srv</li></ul></div></div></div></div><p>
   When it is necessary to increase the logging level for a specific service to
   troubleshoot an issue, or to decrease logging levels to save disk space, you
   can edit the service's config file and then reconfigure logging. All changes
   will be made to the service's files and not to the Centralized Logging
   service files.
  </p><p>
   Messages only appear in the log files if they are the same as or more severe
   than the log level you set. The DEBUG level logs everything. Most services
   default to the INFO logging level, which lists informational events, plus
   warnings, errors, and critical errors. Some services provide other logging
   options which will narrow the focus to help you debug an issue, receive a
   warning if an operation fails, or if there is a serious issue with the
   cloud.
  </p><p>
   For more information on logging levels, see the
   <a class="link" href="http://specs.openstack.org/openstack/openstack-specs/specs/log-guidelines.html" target="_blank">OpenStack
   Logging Guidelines</a> documentation.
  </p></div><div class="sect3" id="Loglvl-intro"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Logging Level for a Service</span> <a title="Permalink" class="permalink" href="#Loglvl-intro">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-intro</li></ul></div></div></div></div><p>
   If you want to increase or decrease the amount of details that are logged by
   a service, you can change the current logging level in the configuration
   files. Most services support, at a minimum, the DEBUG and INFO logging
   levels. For more information about what levels are supported by a service,
   check the documentation or Website for the specific service.
  </p></div><div class="sect3" id="Loglvl-barb"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Barbican</span> <a title="Permalink" class="permalink" href="#Loglvl-barb">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-barb</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>barbican</td><td>
       <p>
        barbican-api
       </p>
       <p>
        barbican-worker
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the barbican logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/
<code class="prompt user">ardana &gt; </code>vi my_cloud/config/barbican/barbican_deploy_config.yml</pre></div></li><li class="step "><p>
     To change the logging level, replace the values in these lines with
     the desired threshold (in ALL CAPS) for the standard log file on disk
     and the JSON log entries forwarded to centralized log services.
    </p><div class="verbatim-wrap"><pre class="screen">barbican_loglevel: "{{ ardana_loglevel | default('INFO') }}"
barbican_logstash_loglevel: "{{ ardana_loglevel | default('INFO') }}"</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts barbican-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-cinder"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Block Storage (cinder)</span> <a title="Permalink" class="permalink" href="#Loglvl-cinder">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-cinder</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>cinder</td><td>
       <p>
        cinder-api
       </p>
       <p>
        cinder-scheduler
       </p>
       <p>
        cinder-backup
       </p>
       <p>
        cinder-volume
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To manage cinder logging:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>vi roles/_CND-CMN/defaults/main.yml</pre></div></li><li class="step "><p>
     To change the logging level, replace the values in these lines with
     the desired threshold (in ALL CAPS) for the standard log file on disk
     and the JSON log entries forwarded to centralized log services.
    </p><div class="verbatim-wrap"><pre class="screen">cinder_loglevel: {{ ardana_loglevel | default('INFO') }}
cinder_logstash_loglevel: {{ ardana_loglevel | default('INFO') }}</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-ceilo"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ceilometer</span> <a title="Permalink" class="permalink" href="#Loglvl-ceilo">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-ceilo</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>ceilometer</td><td>
       <p>
        ceilometer-collector
       </p>
       <p>
        ceilometer-agent-notification
       </p>
       <p>
        ceilometer-polling
       </p>
       <p>
        ceilometer-expirer
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the ceilometer logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>vi roles/_CEI-CMN/defaults/main.yml</pre></div></li><li class="step "><p>
     To change the logging level, replace the values in these lines with
     the desired threshold (in ALL CAPS) for the standard log file on disk
     and the JSON log entries forwarded to centralized log services.
    </p><div class="verbatim-wrap"><pre class="screen">ceilometer_loglevel:  INFO
ceilometer_logstash_loglevel:  INFO</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ceilometer-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-nova"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute (nova)</span> <a title="Permalink" class="permalink" href="#Loglvl-nova">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-nova</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>nova</td><td> </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the nova logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     The nova service component logging can be changed by modifying the
     following files:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/nova/novncproxy-logging.conf.j2
~/openstack/my_cloud/config/nova/api-logging.conf.j2
~/openstack/my_cloud/config/nova/compute-logging.conf.j2
~/openstack/my_cloud/config/nova/conductor-logging.conf.j2
~/openstack/my_cloud/config/nova/scheduler-logging.conf.j2</pre></div></li><li class="step "><p>
     The threshold for default text log files may be set by
     editing the [handler_watchedfile] section, or the JSON content
     forwarded to centralized logging may be set by editing the
     [handler_logstash] section. In either section, replace the value
     of the following line with the desired log level:
    </p><div class="verbatim-wrap"><pre class="screen">level: INFO</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-designate"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Designate (DNS)</span> <a title="Permalink" class="permalink" href="#Loglvl-designate">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-designate</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>designate</td><td>
       <p>
        designate-api
       </p>
       <p>
        designate-central
       </p>
       <p>
        designate-mdns
       </p>
       <p>
        designate-producer
       </p>
       <p>
        designate-worker
       </p>
       <p>
        designate-pool-manager
       </p>
       <p>
        designate-zone-manager
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the designate logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/
<code class="prompt user">ardana &gt; </code>vi my_cloud/config/designate/designate.conf.j2</pre></div></li><li class="step "><p>
     To change the logging level, set the value of the following line:
    </p><div class="verbatim-wrap"><pre class="screen">debug = False</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts designate-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-keystone"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Identity (keystone)</span> <a title="Permalink" class="permalink" href="#Loglvl-keystone">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-keystone</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>keystone</td><td>keystone</td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
       <p>
        WARN
       </p>
       <p>
        ERROR
       </p>
      </td></tr></tbody></table></div><p>
   To change the keystone logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/keystone/keystone_deploy_config.yml</pre></div></li><li class="step "><p>
     To change the logging level, replace the values in these lines with
     the desired threshold (in ALL CAPS) for the standard log file on disk
     and the JSON log entries forwarded to centralized log services.
    </p><div class="verbatim-wrap"><pre class="screen">keystone_loglevel: INFO
keystone_logstash_loglevel: INFO</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-glance"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Image (glance)</span> <a title="Permalink" class="permalink" href="#Loglvl-glance">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-glance</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>glance</td><td>
       <p>
        glance-api
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the glance logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/glance/glance-api-logging.conf.j2</pre></div></li><li class="step "><p>
     The threshold for default text log files may be set by
     editing the [handler_watchedfile] section, or the JSON content
     forwarded to centralized logging may be set by editing the
     [handler_logstash] section. In either section, replace the value
     of the following line with the desired log level:
    </p><div class="verbatim-wrap"><pre class="screen">level: INFO</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-ironic"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Bare Metal (ironic)</span> <a title="Permalink" class="permalink" href="#Loglvl-ironic">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-ironic</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>ironic</td><td>
       <p>
        ironic-api-logging.conf.j2
       </p>
       <p>
        ironic-conductor-logging.conf.j2
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the ironic logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>vi roles/ironic-common/defaults/main.yml</pre></div></li><li class="step "><p>
     To change the logging level, replace the values in these lines with
     the desired threshold (in ALL CAPS) for the standard log file on disk
     and the JSON log entries forwarded to centralized log services.
    </p><div class="verbatim-wrap"><pre class="screen">ironic_api_loglevel: "{{ ardana_loglevel | default('INFO') }}"
ironic_api_logstash_loglevel: "{{ ardana_loglevel | default('INFO') }}"
ironic_conductor_loglevel: "{{ ardana_loglevel | default('INFO') }}"
ironic_conductor_logstash_loglevel: "{{ ardana_loglevel | default('INFO') }}"</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-monasca"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring (monasca)</span> <a title="Permalink" class="permalink" href="#Loglvl-monasca">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-monasca</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>monasca</td><td>
       <p>
        monasca-persister
       </p>
       <p>
        zookeeper
       </p>
       <p>
        storm
       </p>
       <p>
        monasca-notification
       </p>
       <p>
        monasca-api
       </p>
       <p>
        kafka
       </p>
       <p>
        monasca-agent
       </p>
      </td><td>
       <p>
        WARN (default)
       </p>
       <p>
        INFO
       </p>
      </td></tr></tbody></table></div><p>
   To change the monasca logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Monitoring service component logging can be changed by modifying the
     following files:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/monasca-persister/defaults/main.yml
~/openstack/ardana/ansible/roles/storm/defaults/main.yml
~/openstack/ardana/ansible/roles/monasca-notification/defaults/main.yml
~/openstack/ardana/ansible/roles/monasca-api/defaults/main.yml
~/openstack/ardana/ansible/roles/kafka/defaults/main.yml
~/openstack/ardana/ansible/roles/monasca-agent/defaults/main.yml (For this file, you will need to add the variable)</pre></div></li><li class="step "><p>
     To change the logging level, use ALL CAPS to set the desired level in the
     following line:
    </p><div class="verbatim-wrap"><pre class="screen">monasca_log_level: WARN</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-neutron"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networking (neutron)</span> <a title="Permalink" class="permalink" href="#Loglvl-neutron">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-neutron</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>neutron</td><td>
       <p>
        neutron-server
       </p>
       <p>
        dhcp-agent
       </p>
       <p>
        l3-agent
       </p>
       <p>
        metadata-agent
       </p>
       <p>
        openvswitch-agent
       </p>
       <p>
        ovsvapp-agent
       </p>
       <p>
        sriov-agent
       </p>
       <p>
        infoblox-ipam-agent
       </p>
       <p>
        l2gateway-agent
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the neutron logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     The neutron service component logging can be changed by modifying the
     following files:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/neutron-common/templates/dhcp-agent-logging.conf.j2
~/openstack/ardana/ansible/roles/neutron-common/templates/infoblox-ipam-agent-logging.conf.j2
~/openstack/ardana/ansible/roles/neutron-common/templates/l2gateway-agent-logging.conf.j2
~/openstack/ardana/ansible/roles/neutron-common/templates/l3-agent-logging.conf.j2
~/openstack/ardana/ansible/roles/neutron-common/templates/logging.conf.j2
~/openstack/ardana/ansible/roles/neutron-common/templates/metadata-agent-logging.conf.j2
~/openstack/ardana/ansible/roles/neutron-common/templates/openvswitch-agent-logging.conf.j2
~/openstack/ardana/ansible/roles/neutron-common/templates/ovsvapp-agent-logging.conf.j2
~/openstack/ardana/ansible/roles/neutron-common/templates/sriov-agent-logging.conf.j2</pre></div></li><li class="step "><p>
     The threshold for default text log files may be set by
     editing the [handler_watchedfile] section, or the JSON content
     forwarded to centralized logging may be set by editing the
     [handler_logstash] section. In either section, replace the value
     of the following line with the desired log level:
    </p><div class="verbatim-wrap"><pre class="screen">level: INFO</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-swift"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Object Storage (swift)</span> <a title="Permalink" class="permalink" href="#Loglvl-swift">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-swift</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>swift</td><td> </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><div id="id-1.5.15.4.8.16.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    Currently it is not recommended to log at any level other than INFO.
   </p></div></div><div class="sect3" id="Loglvl-octavia"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Octavia</span> <a title="Permalink" class="permalink" href="#Loglvl-octavia">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-octavia</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>octavia</td><td>
       <p>
        octavia-api
       </p>
       <p>
        octavia-worker
       </p>
       <p>
        octavia-hk
       </p>
       <p>
        octavia-hm
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the Octavia logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     The Octavia service component logging can be changed by modifying the
     following files:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/octavia/octavia-api-logging.conf.j2
~/openstack/my_cloud/config/octavia/octavia-worker-logging.conf.j2
~/openstack/my_cloud/config/octavia/octavia-hk-logging.conf.j2
~/openstack/my_cloud/config/octavia/octavia-hm-logging.conf.j2</pre></div></li><li class="step "><p>
     The threshold for default text log files may be set by
     editing the [handler_watchedfile] section, or the JSON content
     forwarded to centralized logging may be set by editing the
     [handler_logstash] section. In either section, replace the value
     of the following line with the desired log level:
    </p><div class="verbatim-wrap"><pre class="screen">level: INFO</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts octavia-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-opsconsole"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.15 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Operations Console</span> <a title="Permalink" class="permalink" href="#Loglvl-opsconsole">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-opsconsole</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>opsconsole</td><td>
       <p>
        ops-web
       </p>
       <p>
        ops-mon
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the Operations Console logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/OPS-WEB/defaults/main.yml</pre></div></li><li class="step "><p>
     To change the logging level, use ALL CAPS to set the desired level in the
     following line:
    </p><div class="verbatim-wrap"><pre class="screen">ops_console_loglevel: "{{ ardana_loglevel | default('INFO') }}"</pre></div></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ops-console-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-heat"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.16 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Orchestration (heat)</span> <a title="Permalink" class="permalink" href="#Loglvl-heat">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-heat</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>heat</td><td>
       <p>
        api-cfn
       </p>
       <p>
        api
       </p>
       <p>
        engine
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the heat logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/heat/*-logging.conf.j2</pre></div></li><li class="step "><p>
     The threshold for default text log files may be set by
     editing the [handler_watchedfile] section, or the JSON content
     forwarded to centralized logging may be set by editing the
     [handler_logstash] section. In either section, replace the value
     of the following line with the desired log level:
    </p><div class="verbatim-wrap"><pre class="screen">level: INFO</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts heat-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-magnum"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.17 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Magnum</span> <a title="Permalink" class="permalink" href="#Loglvl-magnum">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-magnum</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>magnum</td><td>
       <p>
        api
       </p>
       <p>
        conductor
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the Magnum logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/magnum/api-logging.conf.j2
~/openstack/my_cloud/config/magnum/conductor-logging.conf.j2</pre></div></li><li class="step "><p>
     The threshold for default text log files may be set by
     editing the [handler_watchedfile] section, or the JSON content
     forwarded to centralized logging may be set by editing the
     [handler_logstash] section. In either section, replace the value
     of the following line with the desired log level:
    </p><div class="verbatim-wrap"><pre class="screen">level: INFO</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts magnum-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="Loglvl-manila"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.18 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">File Storage (manila)</span> <a title="Permalink" class="permalink" href="#Loglvl-manila">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>Loglvl-manila</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Service</th><th>Sub-component</th><th>Supported Logging Levels</th></tr></thead><tbody><tr><td>manila</td><td>
       <p>
        api
       </p>
      </td><td>
       <p>
        INFO (default)
       </p>
       <p>
        DEBUG
       </p>
      </td></tr></tbody></table></div><p>
   To change the manila logging level:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager (deployer).
    </p></li><li class="step "><p>
     Open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/manila/manila-logging.conf.j2</pre></div></li><li class="step "><p>
     To change the logging level, replace the values in these lines with
     the desired threshold (in ALL CAPS) for the standard log file on disk
     and the JSON log entries forwarded to centralized log services.
    </p><div class="verbatim-wrap"><pre class="screen">manila_loglevel: INFO
manila_logstash_loglevel: INFO</pre></div></li><li class="step "><p>
     Save the changes to the file.
    </p></li><li class="step "><p>
     To commit the changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     To run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     To create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     To run the reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts manila-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="CL-select-central-logging"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.19 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Selecting Files for Centralized Logging</span> <a title="Permalink" class="permalink" href="#CL-select-central-logging">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>CL-select-central-logging</li></ul></div></div></div></div><p>
   As you use <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, you might find a need to redefine which log files are
   rotated on disk or transferred to centralized logging. These changes are all
   made in the centralized logging definition files.
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses the logrotate service to provide rotation, compression, and
   removal of log files. All of the tunable variables for the logrotate process
   itself can be controlled in the following file:
   <code class="literal">~/openstack/ardana/ansible/roles/logging-common/defaults/main.yml</code>
  </p><p>
   You can find the centralized logging definition files for each service in
   the following directory:
   <code class="literal">~/openstack/ardana/ansible/roles/logging-common/vars</code>
  </p><p>
   You can change log settings for a service by following these steps.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p><p>
     Open the *.yml file for the service or sub-component that you want to
     modify.
    </p><p>
     Using keystone, the Identity service as an example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>vi ~/openstack/ardana/ansible/roles/logging-common/vars/keystone-clr.yml</pre></div><p>
     Consider the opening clause of the file:
    </p><div class="verbatim-wrap"><pre class="screen">sub_service:
  hosts: KEY-API
  name: keystone
  service: keystone</pre></div><p>
     The <span class="bold"><strong>hosts</strong></span> setting defines the role which
     will trigger this logrotate definition being applied to a particular host.
     It can use regular expressions for pattern matching, that is,
     <span class="bold"><strong>NEU-.*</strong></span>.
    </p><p>
     The <span class="bold"><strong>service</strong></span> setting identifies the
     high-level service name associated with this content, which will be used
     for determining log files' collective quotas for storage on disk.
    </p></li><li class="step "><p>
     Verify logging is enabled by locating the following lines:
    </p><div class="verbatim-wrap"><pre class="screen">centralized_logging:
  enabled: true
  format: rawjson</pre></div><div id="id-1.5.15.4.8.22.6.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      When possible, centralized logging is most effective on log files
      generated using logstash-formatted JSON. These files should specify
      <span class="emphasis"><em>format: rawjson</em></span>. When only plaintext log files are
      available, <span class="emphasis"><em>format: json</em></span> is appropriate. (This will
      cause their plaintext log lines to be wrapped in a json envelope before
      being sent to centralized logging storage.)
     </p></div></li><li class="step "><p>
     Observe log files selected for rotation:
    </p><div class="verbatim-wrap"><pre class="screen">- files:
  - /var/log/keystone/keystone.log
  log_rotate:
  - daily
  - maxsize 300M
  - rotate 7
  - compress
  - missingok
  - notifempty
  - copytruncate
  - create 640 keystone adm</pre></div><div id="id-1.5.15.4.8.22.6.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      With the introduction of dynamic log rotation, the frequency (that is,
      <span class="emphasis"><em>daily</em></span>) and file size threshold (that is,
      <span class="emphasis"><em>maxsize</em></span>) settings no longer have any effect. The
      <span class="emphasis"><em>rotate</em></span> setting may be easily overridden on a
      service-by-service basis.
     </p></div></li><li class="step "><p>
     Commit any changes to your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the logging reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts kronos-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="CL-space-allocation"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.20 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Controlling Disk Space Allocation and Retention of Log Files</span> <a title="Permalink" class="permalink" href="#CL-space-allocation">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>CL-space-allocation</li></ul></div></div></div></div><p>
   Each service is assigned a weighted allocation of the
   <code class="literal">/var/log</code> filesystem's capacity. When all its log files'
   cumulative sizes exceed this allocation, a rotation is triggered for that
   service's log files according to the behavior specified in the
   <code class="literal">/etc/logrotate.d/*</code> specification.
  </p><p>
   These specification files are auto-generated based on YML sources delivered
   with the Cloud Lifecycle Manager codebase. The source files can be edited and
   reapplied to control the allocation of disk space across services or the
   behavior during a rotation.
  </p><p>
   Disk capacity is allocated as a percentage of the total weighted value of
   all services running on a particular node. For example, if 20 services run
   on the same node, all with a default weight of
   <span class="bold"><strong>100</strong></span>, they will each be granted 1/20th of
   the log filesystem's capacity. If the configuration is updated to change one
   service's weight to <span class="bold"><strong>150</strong></span>, all the services'
   allocations will be adjusted to make it possible for that one service to
   consume 150% of the space available to other individual services.
  </p><p>
   These policies are enforced by the script
   <code class="literal">/opt/kronos/rotate_if_exceeded_quota.py</code>, which will be
   executed every 5 minutes via a cron job and will rotate the log files of any
   services which have exceeded their respective quotas. When log rotation
   takes place for a service, logs are generated to describe the activity in
   <code class="literal">/var/log/kronos/check_if_exceeded_quota.log</code>.
  </p><p>
   When logrotate is performed on a service, its existing log files are
   compressed and archived to make space available for fresh log entries. Once
   the number of archived log files exceeds that service's retention
   thresholds, the oldest files are deleted. Thus, longer retention thresholds
   (that is, 10 to 15) will result in more space in the service's allocated log
   capacity being used for historic logs, while shorter retention thresholds
   (that is, 1 to 5) will keep more space available for its active plaintext log
   files.
  </p><p>
   Use the following process to make adjustments to services' log capacity
   allocations or retention thresholds:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Navigate to the following directory on your Cloud Lifecycle Manager:
    </p><div class="verbatim-wrap"><pre class="screen">~/stack/scratch/ansible/next/ardana/ansible</pre></div></li><li class="step "><p>
     Open and edit the service weights file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>vi roles/kronos-logrotation/vars/rotation_config.yml</pre></div></li><li class="step "><p>
     Edit the service parameters to set the desired parameters. Example:
    </p><div class="verbatim-wrap"><pre class="screen">cinder:
  weight: 300
  retention: 2</pre></div><div id="id-1.5.15.4.8.23.8.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The retention setting of <span class="emphasis"><em>default</em></span> will use recommend
      defaults for each services' log files.
     </p></div></li><li class="step "><p>
     Run the kronos-logrotation-deploy playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts kronos-logrotation-deploy.yml</pre></div></li><li class="step "><p>
     Verify the changes to the quotas have been changed:
    </p><p>
     Login to a node and check the contents of the file
     /opt/kronos/service_info.yml to see the active quotas for that node, and
     the specifications in /etc/logrotate.d/* for rotation thresholds.
    </p></li></ol></div></div></div><div class="sect3" id="CL-elasticsearch-config"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.21 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Elasticsearch for Centralized Logging</span> <a title="Permalink" class="permalink" href="#CL-elasticsearch-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>CL-elasticsearch-config</li></ul></div></div></div></div><p>
   Elasticsearch includes some tunable options exposed in its configuration.
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses these options in Elasticsearch to prioritize indexing speed
   over search speed. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> also configures Elasticsearch for optimal
   performance in low RAM environments. The options that <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> modifies are
   listed below along with an explanation about why they were modified.
  </p><p>
   These configurations are defined in the
   <code class="literal">~/openstack/my_cloud/config/logging/main.yml</code> file and are
   implemented in the Elasticsearch configuration file
   <code class="literal">~/openstack/my_cloud/config/logging/elasticsearch.yml.j2</code>.
  </p></div><div class="sect3" id="CL-safeguards"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.6.22 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Safeguards for the Log Partitions Disk Capacity</span> <a title="Permalink" class="permalink" href="#CL-safeguards">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_configure_services.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_configure_services.xml</li><li><span class="ds-label">ID: </span>CL-safeguards</li></ul></div></div></div></div><p>
   Because the logging partitions are at a high risk of filling up over time, a
   condition which can cause many negative side effects on services running, it
   is important to safeguard against log files consuming 100 % of available
   capacity.
  </p><p>
   This protection is implemented by pairs of low/high
   <span class="bold"><strong>watermark</strong></span> thresholds, with values
   established in
   <code class="literal">~/stack/scratch/ansible/next/ardana/ansible/roles/logging-common/defaults/main.yml</code>
   and applied by the <code class="literal">kronos-logrotation-deploy</code> playbook.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>var_log_low_watermark_percent</strong></span> (default:
     80) sets a capacity level for the contents of the
     <code class="literal">/var/log</code> partition beyond which alarms will be
     triggered (visible to administrators in monasca).
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>var_log_high_watermark_percent</strong></span> (default:
     95) defines how much capacity of the <code class="literal">/var/log</code> partition
     to make available for log rotation (in calculating weighted service
     allocations).
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>var_audit_low_watermark_percent</strong></span> (default:
     80) sets a capacity level for the contents of the
     <code class="literal">/var/audit</code> partition beyond which alarm notifications
     will be triggered.
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>var_audit_high_watermark_percent</strong></span>
     (default: 95) sets a capacity level for the contents of the
     <code class="literal">/var/audit</code> partition which will cause log rotation to
     be forced according to the specification in
     <code class="literal">/etc/auditlogrotate.conf</code>.
    </p></li></ul></div></div></div><div class="sect2" id="topic-overview-audit-logs"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Audit Logging Overview</span> <a title="Permalink" class="permalink" href="#topic-overview-audit-logs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-audit_logs_overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_overview.xml</li><li><span class="ds-label">ID: </span>topic-overview-audit-logs</li></ul></div></div></div></div><p>
  Existing OpenStack service logging varies widely across services. Generally,
  log messages do not have enough detail about who is requesting the
  application program interface (API), or enough context-specific details about
  an action performed. Often details are not even consistently logged across
  various services, leading to inconsistent data formats being used across
  services. These issues make it difficult to integrate logging with existing
  audit tools and processes.
 </p><p>
  To help you monitor your workload and data in compliance with your corporate,
  industry or regional policies, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> provides auditing support as a basic
  security feature. The audit logging can be integrated with customer Security
  Information and Event Management (SIEM) tools and support your efforts to
  correlate threat forensics.
 </p><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> audit logging feature uses Audit Middleware for Python services.
  This middleware service is based on OpenStack services which use the Paste
  Deploy system. Most OpenStack services use the paste deploy mechanism to find
  and configure WSGI servers and applications. Utilizing the paste deploy
  system provides auditing support in services with minimal changes.
 </p><p>
  By default, audit logging as a post-installation feature is disabled in the
  cloudConfig file on the Cloud Lifecycle Manager and it can only be enabled after
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installation or upgrade.
 </p><p>
  The tasks in this section explain how to enable services for audit logging in
  your environment. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> provides audit logging for the following services:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    nova
   </p></li><li class="listitem "><p>
    barbican
   </p></li><li class="listitem "><p>
    keystone
   </p></li><li class="listitem "><p>
    cinder
   </p></li><li class="listitem "><p>
    ceilometer
   </p></li><li class="listitem "><p>
    neutron
   </p></li><li class="listitem "><p>
    glance
   </p></li><li class="listitem "><p>
    heat
   </p></li></ul></div><p>
  For audit log backup information see <a class="xref" href="#manual-audit-log-bur" title="17.3.4. Audit Log Backup and Restore">Section 17.3.4, “Audit Log Backup and Restore”</a>
 </p><div class="sect3" id="idg-all-operations-audit-logs-checklist-xml-1"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Audit Logging Checklist</span> <a title="Permalink" class="permalink" href="#idg-all-operations-audit-logs-checklist-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-audit_logs_checklist.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_checklist.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-audit-logs-checklist-xml-1</li></ul></div></div></div></div><p>
  Before enabling audit logging, make sure you understand how much disk space
  you will need, and configure the disks that will store the logging data. Use
  the following table to complete these tasks:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>☐</th><th>Item</th></tr></thead><tbody><tr><td> </td><td>
      <p>
       <a class="xref" href="#audit-FAQ" title="13.2.7.1.1. Frequently Asked Questions">Section 13.2.7.1.1, “Frequently Asked Questions”</a>
      </p>
     </td></tr><tr><td> </td><td>
      <p>
       <a class="xref" href="#audit-log-est" title="13.2.7.1.2. Estimate Disk Size">Section 13.2.7.1.2, “Estimate Disk Size”</a>
      </p>
     </td></tr><tr><td> </td><td>
      <p>
       <a class="xref" href="#audit-add-disks" title="13.2.7.1.3. Add disks to the controller nodes">Section 13.2.7.1.3, “Add disks to the controller nodes”</a>
      </p>
     </td></tr><tr><td> </td><td>
      <p>
       <a class="xref" href="#audit-update-disks" title="13.2.7.1.4. Update the disk template for the controller nodes">Section 13.2.7.1.4, “Update the disk template for the controller nodes”</a>
      </p>
     </td></tr><tr><td> </td><td>
      <p>
       <a class="xref" href="#audit-save-update-disks" title="13.2.7.1.5. Save your changes">Section 13.2.7.1.5, “Save your changes”</a>
      </p>
     </td></tr></tbody></table></div><div class="sect4" id="audit-FAQ"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.7.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Frequently Asked Questions</span> <a title="Permalink" class="permalink" href="#audit-FAQ">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-audit_logs_checklist.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_checklist.xml</li><li><span class="ds-label">ID: </span>audit-FAQ</li></ul></div></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.15.4.9.9.4.2.1"><span class="term ">How are audit logs generated?</span></dt><dd><p>
      The audit logs are created by services running in the cloud management
      controller nodes. The events that create auditing entries are formatted
      using a structure that is compliant with Cloud Auditing Data Federation
      (CADF) policies. The formatted audit entries are then saved to disk
      files. For more information, see the
      <a class="link" href="http://www.dmtf.org/standards/cadf" target="_blank">Cloud Auditing Data
      Federation Website.</a>
     </p></dd><dt id="id-1.5.15.4.9.9.4.2.2"><span class="term ">Where are audit logs stored?</span></dt><dd><p>
      We strongly recommend adding a dedicated disk volume for
      <code class="literal">/var/audit</code>.
     </p><p>
      If the disk templates for the controllers are not updated to create a
      separate volume for <code class="filename">/var/audit</code>,
      the audit logs will still be created in
      the root partition under the folder <code class="filename">/var/audit</code>. This
      could be problematic if the root partition does not have adequate space to
      hold the audit logs.
     </p><div id="id-1.5.15.4.9.9.4.2.2.2.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
       We recommend that you do <span class="bold"><strong>not</strong></span> store
       audit logs in the <code class="filename">/var/log</code> volume. The
       <code class="filename">/var/log</code> volume is used for storing operational logs
       and logrotation/alarms have been preconfigured for various services
       based on the size of this volume. Adding audit logs here may impact
       these causing undesired alarms. This would also impact the retention
       times for the operational logs.
      </p></div></dd><dt id="id-1.5.15.4.9.9.4.2.3"><span class="term ">Are audit logs centrally stored?</span></dt><dd><p>
      Yes. The existing operational log profiles have been configured to
      centrally log audit logs as well, once their generation has been enabled.
      The audit logs will be stored in separate Elasticsearch indices separate
      from the operational logs.
     </p></dd><dt id="id-1.5.15.4.9.9.4.2.4"><span class="term ">How long are audit log files retained?</span></dt><dd><p>
      By default, audit logs are configured to be retained for 7 days on disk.
      The audit logs are rotated each day and the rotated files are stored in a
      compressed format and retained up to 7 days (configurable). The backup
      service has been configured to back up the audit logs to a location
      outside of the controller nodes for much longer retention periods.
     </p></dd><dt id="id-1.5.15.4.9.9.4.2.5"><span class="term ">Do I lose audit data if a management controller node goes down?</span></dt><dd><p>
      Yes. For this reason, it is strongly recommended that you back up the
      audit partition in each of the management controller nodes for protection
      against any data loss.
     </p></dd></dl></div></div><div class="sect4" id="audit-log-est"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.7.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Estimate Disk Size</span> <a title="Permalink" class="permalink" href="#audit-log-est">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-audit_logs_checklist.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_checklist.xml</li><li><span class="ds-label">ID: </span>audit-log-est</li></ul></div></div></div></div><p>
   The table below provides estimates from each service of audit log size
   generated per day. The estimates are provided for environments with 100
   nodes, 300 nodes, and 500 nodes.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /></colgroup><thead><tr><th>Service</th><th>
       <p>
        Log File Size: 100 nodes
       </p>
      </th><th>
       <p>
        Log File Size: 300 nodes
       </p>
      </th><th>
       <p>
        Log File Size: 500 nodes
       </p>
      </th></tr></thead><tbody><tr><td>barbican</td><td>2.6 MB</td><td>4.2 MB</td><td>5.6 MB</td></tr><tr><td>keystone</td><td>96 - 131 MB</td><td>288 - 394 MB</td><td>480 - 657 MB</td></tr><tr><td>nova</td><td>186 (with a margin of 46) MB</td><td>557 (with a margin of 139) MB</td><td>928 (with a margin of 232) MB</td></tr><tr><td>ceilometer</td><td>12 MB</td><td>12 MB</td><td>12 MB</td></tr><tr><td>cinder</td><td>2 - 250 MB</td><td>2 - 250 MB</td><td>2 - 250 MB</td></tr><tr><td>neutron</td><td>145 MB</td><td>433 MB</td><td>722 MB</td></tr><tr><td>glance</td><td>20 (with a margin of 8) MB</td><td>60 (with a margin of 22) MB</td><td>100 (with a margin of 36) MB</td></tr><tr><td>heat</td><td>432 MB (1 transaction per second)</td><td>432 MB (1 transaction per second)</td><td>432 MB (1 transaction per second)</td></tr><tr><td>swift</td><td>33 GB (700 transactions per second)</td><td>102 GB (2100 transactions per second)</td><td>172 GB (3500 transactions per second)</td></tr></tbody></table></div></div><div class="sect4" id="audit-add-disks"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.7.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Add disks to the controller nodes</span> <a title="Permalink" class="permalink" href="#audit-add-disks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-audit_logs_checklist.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_checklist.xml</li><li><span class="ds-label">ID: </span>audit-add-disks</li></ul></div></div></div></div><p>
   You need to add disks for the audit log partition to store the data in a
   secure manner. The steps to complete this task will vary depending on the
   type of server you are running. Please refer to the manufacturer’s
   instructions on how to add disks for the type of server node used by the
   management controller cluster. If you already have extra disks in the
   controller node, you can identify any unused one and use it for the audit
   log partition.
  </p></div><div class="sect4" id="audit-update-disks"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.7.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Update the disk template for the controller nodes</span> <a title="Permalink" class="permalink" href="#audit-update-disks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-audit_logs_checklist.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_checklist.xml</li><li><span class="ds-label">ID: </span>audit-update-disks</li></ul></div></div></div></div><p>
   Since audit logging is disabled by default, the audit volume groups in the
   disk templates are commented out. If you want to turn on audit logging, the
   template needs to be updated first. If it is not updated, there will be no
   back-up volume group. To update the disk template, you will need to copy
   templates from the examples folder to the definition folder and then edit
   the disk controller settings. Changes to the disk template used for
   provisioning cloud nodes must be made prior to deploying the nodes.
  </p><p>
   To update the disk controller template:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     To copy the example templates folder, run the following command:
    </p><div id="id-1.5.15.4.9.9.7.4.2.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      If you already have the required templates in the definition folder, you
      can skip this step.
     </p></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cp -r ~/openstack/examples/entry-scale-esx/* ~/openstack/my_cloud/definition/</pre></div></li><li class="listitem "><p>
     To change to the data folder, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition/</pre></div></li><li class="listitem "><p>
     To edit the disks controller settings, open the file that matches your
     server model and disk model in a text editor:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Model</th><th>File</th></tr></thead><tbody><tr><td>entry-scale-kvm</td><td>
<div class="verbatim-wrap"><pre class="screen">disks_controller_1TB.yml</pre></div>
<div class="verbatim-wrap"><pre class="screen">disks_controller_600GB.yml</pre></div>
        </td></tr><tr><td>mid-scale</td><td>
<div class="verbatim-wrap"><pre class="screen">disks_compute.yml</pre></div>
<div class="verbatim-wrap"><pre class="screen">disks_control_common_600GB.yml</pre></div>
<div class="verbatim-wrap"><pre class="screen">disks_dbmq_600GB.yml</pre></div>
<div class="verbatim-wrap"><pre class="screen">disks_mtrmon_2TB.yml</pre></div>
<div class="verbatim-wrap"><pre class="screen">disks_mtrmon_4.5TB.yml</pre></div>
<div class="verbatim-wrap"><pre class="screen">disks_mtrmon_600GB.yml</pre></div>
<div class="verbatim-wrap"><pre class="screen">disks_swobj.yml</pre></div>
<div class="verbatim-wrap"><pre class="screen">disks_swpac.yml</pre></div>
        </td></tr></tbody></table></div></li><li class="listitem "><p>
     To update the settings and enable an audit log volume group, edit the
     appropriate file(s) listed above and remove the '#' comments from these
     lines, confirming that they are appropriate for your environment.
    </p><div class="verbatim-wrap"><pre class="screen">- name: audit-vg
  physical-volumes:
    - /dev/sdz
  logical-volumes:
    - name: audit
      size: 95%
      mount: /var/audit
      fstype: ext4
      mkfs-opts: -O large_file</pre></div></li></ol></div></div><div class="sect4" id="audit-save-update-disks"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.7.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Save your changes</span> <a title="Permalink" class="permalink" href="#audit-save-update-disks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-audit_logs_checklist.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_checklist.xml</li><li><span class="ds-label">ID: </span>audit-save-update-disks</li></ul></div></div></div></div><p>
   To save your changes you will use the GIT repository to add the setup disk
   files.
  </p><p>
   To save your changes:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To change to the openstack directory, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack</pre></div></li><li class="listitem "><p>
     To add the new and updated files, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A</pre></div></li><li class="listitem "><p>
     To verify the files are added, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git status</pre></div></li><li class="listitem "><p>
     To commit your changes, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git commit -m "Setup disks for audit logging"</pre></div></li></ol></div></div></div><div class="sect3" id="topic-enable-audit-logs"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.2.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enable Audit Logging</span> <a title="Permalink" class="permalink" href="#topic-enable-audit-logs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-audit_logs_enable.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_enable.xml</li><li><span class="ds-label">ID: </span>topic-enable-audit-logs</li></ul></div></div></div></div><p>
  To enable audit logging you must edit your cloud configuration settings, save
  your changes and re-run the configuration processor. Then you can run the
  playbooks to create the volume groups and configure them.
 </p><p>
  In the <code class="literal">~/openstack/my_cloud/definition/cloudConfig.yml</code> file,
  service names defined under enabled-services or disabled-services override
  the default setting.
 </p><p>
  The following is an example of your audit-settings section:
 </p><div class="verbatim-wrap"><pre class="screen"># Disc space needs to be allocated to the audit directory before enabling
# auditing.
# Default can be either "disabled" or "enabled". Services listed in
# "enabled-services" and "disabled-services" override the default setting.
audit-settings:
   default: disabled
   #enabled-services:
   #  - keystone
   #  - barbican
   disabled-services:
     - nova
     - barbican
     - keystone
     - cinder
     - ceilometer
     - neutron</pre></div><p>
  In this example, although the default setting for all services is set to
  <span class="bold"><strong>disabled</strong></span>, keystone and barbican may be
  explicitly enabled by removing the comments from these lines and this setting
  overrides the default.
 </p><div class="sect4" id="audit-edit-config"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.7.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To edit the configuration file:</span> <a title="Permalink" class="permalink" href="#audit-edit-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-audit_logs_enable.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_enable.xml</li><li><span class="ds-label">ID: </span>audit-edit-config</li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     To change to the cloud definition folder, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition</pre></div></li><li class="listitem "><p>
     To edit the auditing settings, in a text editor, open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">cloudConfig.yml</pre></div></li><li class="listitem "><p>
     To enable audit logging, begin by uncommenting the "enabled-services:"
     block.
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       enabled-service:
      </p></li><li class="listitem "><p>
       any service you want to enable for audit logging.
      </p></li></ul></div><p>
     For example, keystone has been enabled in the following text:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Default cloudConfig.yml file</th><th>Enabling keystone audit logging</th></tr></thead><tbody><tr><td>
<div class="verbatim-wrap"><pre class="screen">audit-settings:
default: disabled
enabled-services:
#  - keystone</pre></div>
        </td><td>
<div class="verbatim-wrap"><pre class="screen">audit-settings:
default: disabled
enabled-services:
  - keystone</pre></div>
        </td></tr></tbody></table></div></li><li class="listitem "><p>
     To move the services you want to enable, comment out the service in the
     disabled section and add it to the enabled section. For example, barbican
     has been enabled in the following text:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>cloudConfig.yml file</th><th>Enabling barbican audit logging</th></tr></thead><tbody><tr><td>
<div class="verbatim-wrap"><pre class="screen">audit-settings:
default: disabled
enabled-services:
  - keystone
disabled-services:
   - nova
   # - keystone
   - barbican
   - cinder</pre></div>
        </td><td>
<div class="verbatim-wrap"><pre class="screen">audit-settings:
default: disabled
enabled-services:
 - keystone
 - barbican
disabled-services:
 - nova
 # - barbican
 # - keystone
 - cinder</pre></div>
        </td></tr></tbody></table></div></li></ol></div></div><div class="sect4" id="audit-save-config2"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.7.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To save your changes and run the configuration processor:</span> <a title="Permalink" class="permalink" href="#audit-save-config2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-audit_logs_enable.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_enable.xml</li><li><span class="ds-label">ID: </span>audit-save-config2</li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To change to the openstack directory, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack</pre></div></li><li class="listitem "><p>
     To add the new and updated files, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A</pre></div></li><li class="listitem "><p>
     To verify the files are added, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git status</pre></div></li><li class="listitem "><p>
     To commit your changes, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git commit -m "Enable audit logging"</pre></div></li><li class="listitem "><p>
     To change to the directory with the ansible playbooks, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible</pre></div></li><li class="listitem "><p>
     To rerun the configuration processor, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li></ol></div></div><div class="sect4" id="audit-create-vgroup"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.7.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To create the volume group:</span> <a title="Permalink" class="permalink" href="#audit-create-vgroup">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-audit_logs_enable.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_enable.xml</li><li><span class="ds-label">ID: </span>audit-create-vgroup</li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To change to the directory containing the osconfig playbook, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible</pre></div></li><li class="listitem "><p>
     To remove the stub file that osconfig uses to decide if the disks are
     already configured, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible -i hosts/verb_hosts KEY-API -a 'sudo rm -f /etc/openstack/osconfig-ran'</pre></div><div id="id-1.5.15.4.9.10.9.2.2.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      The osconfig playbook uses the stub file to mark already configured disks
      as "idempotent." To stop osconfig from identifying your new disk as
      already configured, you must remove the stub file /etc/hos/osconfig-ran
      before re-running the osconfig playbook.
     </p></div></li><li class="listitem "><p>
     To run the playbook that enables auditing for a service, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit KEY-API</pre></div><div id="id-1.5.15.4.9.10.9.2.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      The variable KEY-API is used as an example to cover the management
      controller cluster. To enable auditing for a service that is not run on
      the same cluster, add the service to the –limit flag in the above
      command. For example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit KEY-API:NEU-SVR</pre></div></div></li></ol></div></div><div class="sect4" id="audit-reconfig-services"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.2.7.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To Reconfigure services for audit logging:</span> <a title="Permalink" class="permalink" href="#audit-reconfig-services">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-audit_logs_enable.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-audit_logs_enable.xml</li><li><span class="ds-label">ID: </span>audit-reconfig-services</li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To change to the directory containing the service playbooks, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible</pre></div></li><li class="listitem "><p>
     To run the playbook that reconfigures a service for audit logging, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts <em class="replaceable ">SERVICE_NAME</em>-reconfigure.yml</pre></div><p>
     For example, to reconfigure keystone for audit logging, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</pre></div></li><li class="listitem "><p>
     Repeat steps 1 and 2 for each service you need to reconfigure.
    </p><div id="id-1.5.15.4.9.10.10.2.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      You must reconfigure each service that you changed to be enabled or
      disabled in the cloudConfig.yml file.
     </p></div></li></ol></div></div></div></div><div class="sect2" id="id-1.5.15.4.10"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting</span> <a title="Permalink" class="permalink" href="#id-1.5.15.4.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-centralized_logging.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-centralized_logging.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   For information on troubleshooting Central Logging, see
   <a class="xref" href="#sec-central-log-troubleshoot" title="18.7.1. Troubleshooting Centralized Logging">Section 18.7.1, “Troubleshooting Centralized Logging”</a>.
  </p></div></div><div class="sect1" id="ceilo-metering-overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">13.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Metering Service (ceilometer) Overview</span> <a title="Permalink" class="permalink" href="#ceilo-metering-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_overview.xml</li><li><span class="ds-label">ID: </span>ceilo-metering-overview</li></ul></div></div></div></div><p>
   The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> metering service collects and provides access to OpenStack
   usage data that can be used for billing reporting such as showback and
   chargeback. The metering service can also provide general usage reporting.
   ceilometer acts as the central collection and data access service to the
   meters provided by all the OpenStack services. The data collected is
   available through the monasca API. ceilometer V2 API was deprecated in the
   Pike release upstream.
   </p><div class="sect2" id="Metering-NewFunctions"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Metering Service New Functionality</span> <a title="Permalink" class="permalink" href="#Metering-NewFunctions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_newfunctions.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_newfunctions.xml</li><li><span class="ds-label">ID: </span>Metering-NewFunctions</li></ul></div></div></div></div><div class="sect3" id="newfunct"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">New Metering Functionality in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9</span> <a title="Permalink" class="permalink" href="#newfunct">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_newfunctions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_newfunctions.xml</li><li><span class="ds-label">ID: </span>newfunct</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     ceilometer is now integrated with monasca, using it as the datastore.
    </p></li><li class="listitem "><p>
     The default meters and other items configured for ceilometer can now be
     modified and additional meters can be added. We recommend that users test
     overall <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> performance prior to deploying any ceilometer
     modifications to ensure the addition of new notifications or polling
     events does not negatively affect overall system performance.
    </p></li><li class="listitem "><p>
     ceilometer Central Agent (pollster) is now called Polling Agent and is
     configured to support HA (Active-Active).
    </p></li><li class="listitem "><p>
     Notification Agent has built-in HA (Active-Active) with support for
     pipeline transformers, but workload partitioning has been disabled in
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
    </p></li><li class="listitem "><p>
     SWIFT Poll-based account level meters will be enabled by default with an
     hourly collection cycle.
    </p></li><li class="listitem "><p>
     Integration with centralized monitoring (monasca) and centralized logging
    </p></li><li class="listitem "><p>
     Support for upgrade and reconfigure operations
    </p></li></ul></div></div><div class="sect3" id="idg-all-metering-metering-newfunctions-xml-7"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations</span> <a title="Permalink" class="permalink" href="#idg-all-metering-metering-newfunctions-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_newfunctions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_newfunctions.xml</li><li><span class="ds-label">ID: </span>idg-all-metering-metering-newfunctions-xml-7</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The Number of metadata attributes that can be extracted from
     resource_metadata has a maximum of 16. This is the number of fields in the
     metadata section of the
     <span class="bold"><strong>monasca_field_definitions.yaml</strong></span> file for
     any service. It is also the number that is equal to fields in
     metadata.common and fields in metadata.&lt;service.meters&gt; sections.
     The total number of these fields cannot be more than 16.
    </p></li><li class="listitem "><p>
     Several network-related attributes are accessible using a colon ":" but
     are returned as a period ".". For example, you can access a sample list
     using the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>ceilometer --debug sample-list network -q "resource_id=421d50a5-156e-4cb9-b404-
d2ce5f32f18b;resource_metadata.provider.network_type=flat"</pre></div><p>
     However, in response you will see the following:
    </p><div class="verbatim-wrap"><pre class="screen">provider.network_type</pre></div><p>
     instead of
    </p><div class="verbatim-wrap"><pre class="screen">provider:network_type</pre></div><p>
     This limitation is known for the following attributes:
    </p><div class="verbatim-wrap"><pre class="screen">provider:network_type
provider:physical_network
provider:segmentation_id</pre></div></li><li class="listitem "><p>
     ceilometer Expirer is not supported. Data retention expiration is handled
     by monasca with a default retention period of 45 days.
    </p></li><li class="listitem "><p>
     ceilometer Collector is not supported.
    </p></li></ul></div></div></div><div class="sect2" id="ceilo-metering-concepts-overview"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Understanding the Metering Service Concepts</span> <a title="Permalink" class="permalink" href="#ceilo-metering-concepts-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_concepts.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_concepts.xml</li><li><span class="ds-label">ID: </span>ceilo-metering-concepts-overview</li></ul></div></div></div></div><div class="sect3" id="ceilo-concept-intro"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ceilometer Introduction</span> <a title="Permalink" class="permalink" href="#ceilo-concept-intro">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_concepts.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_concepts.xml</li><li><span class="ds-label">ID: </span>ceilo-concept-intro</li></ul></div></div></div></div><p>
   Before configuring the ceilometer Metering Service, it is important to
   understand how it works.
  </p><div class="sect4" id="ceilo-architecture"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.3.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Metering Architecture</span> <a title="Permalink" class="permalink" href="#ceilo-architecture">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_concepts.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_concepts.xml</li><li><span class="ds-label">ID: </span>ceilo-architecture</li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> automatically configures ceilometer to use Logging and
   Monitoring Service (monasca) as its backend. ceilometer is deployed on the
   same control plane nodes as monasca.
  </p><p>
   The installation of Celiometer creates several management nodes running
   different metering components.
  </p><p>
   <span class="bold"><strong>ceilometer Components on Controller nodes</strong></span>
  </p><p>
   This controller node is the first of the High Available (HA) cluster.
  </p><p>
   <span class="bold"><strong>ceilometer Sample Polling</strong></span>
  </p><p>
   Sample Polling is part of the Polling Agent. Messages are posted by the
   Notification Agent directly to monasca API.
  </p><p>
   <span class="bold"><strong>ceilometer Polling Agent</strong></span>
  </p><p>
   The Polling Agent is responsible for coordinating the polling activity. It
   parses the <code class="filename">pipeline.yml</code> configuration file and
   identifies all the sources that need to be polled. The sources are then
   evaluated using a discovery mechanism and all the sources are translated to
   resources where a dedicated pollster can retrieve and publish data. At each
   identified interval the discovery mechanism is triggered, the resource list
   is composed, and the data is polled and sent to the queue.
  </p><p>
   <span class="bold"><strong>ceilometer Collector No Longer Required</strong></span>
  </p><p>
   In previous versions, the collector was responsible for getting the
   samples/events from the RabbitMQ service and storing it in the main
   database. The ceilometer Collector is no longer enabled. Now that
   Notification Agent posts the data directly to monasca API, the collector is
   no longer required
  </p></div><div class="sect4" id="ceilo-about-meters"><div class="titlepage"><div><div><h5 class="title"><span class="number">13.3.2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Meter Reference</span> <a title="Permalink" class="permalink" href="#ceilo-about-meters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_concepts.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_concepts.xml</li><li><span class="ds-label">ID: </span>ceilo-about-meters</li></ul></div></div></div></div><p>
   ceilometer collects basic information grouped into categories known as
   <code class="literal">meters</code>. A meter is the unique resource-usage measurement
   of a particular OpenStack service. Each OpenStack service defines what type
   of data is exposed for metering.
  </p><p>
   Each meter has the following characteristics:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Attribute</th><th>Description</th></tr></thead><tbody><tr><td>Name</td><td>Description of the meter</td></tr><tr><td>Unit of Measurement</td><td>The method by which the data is measured. For example: storage meters are
                defined in Gigabytes (GB) and network bandwidth is measured in Gigabits
                (Gb).</td></tr><tr><td>Type</td><td><p>The origin of the meter's data. OpenStack defines the following origins: </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          Cumulative - Increasing over time (instance hours)
         </p></li><li class="listitem "><p>
          Gauge - a discrete value. For example: the number of floating IP
          addresses or image uploads.
         </p></li><li class="listitem "><p>
          Delta - Changing over time (bandwidth)
         </p></li></ul></div>
      </td></tr></tbody></table></div><p>
   A meter is defined for every measurable resource. A meter can exist beyond
   the actual existence of a particular resource, such as an active instance,
   to provision long-cycle use cases such as billing.
  </p><div id="id-1.5.15.5.4.2.4.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    For a list of meter types and default meters installed with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, see
    <a class="xref" href="#topic3051" title="13.3.3. Ceilometer Metering Available Meter Types">Section 13.3.3, “Ceilometer Metering Available Meter Types”</a>
   </p></div><p>
   The most common meter submission method is notifications. With this method,
   each service sends the data from their respective meters on a periodic basis
   to a common notifications bus.
  </p><p>
   ceilometer, in turn, pulls all of the events from the bus and saves the
   notifications in a ceilometer-specific database. The period of time that the
   data is collected and saved is known as the ceilometer expiry and is
   configured during ceilometer installation. Each meter is collected from one
   or more samples, gathered from the messaging queue or polled by agents. The
   samples are represented by counter objects. Each counter has the following
   fields:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Attribute</th><th>Description</th></tr></thead><tbody><tr><td>counter_name</td><td>Description of the counter</td></tr><tr><td>counter_unit</td><td>The method by which the data is measured. For example: data can be
                defined in Gigabytes (GB) or for network bandwidth, measured in Gigabits
                (Gb).</td></tr><tr><td>counter_typee</td><td>
       <p>The origin of the counter's data. OpenStack defines the following origins:</p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          Cumulative - Increasing over time (instance hours)
         </p></li><li class="listitem "><p>
          Gauge - a discrete value. For example: the number of floating IP
          addresses or image uploads.
         </p></li><li class="listitem "><p>
          Delta - Changing over time (bandwidth)
         </p></li></ul></div>
      </td></tr><tr><td>counter_volume</td><td>The volume of data measured (CPU ticks, bytes transmitted, etc.). Not used for gauge
                counters. Set to a default value such as 1.</td></tr><tr><td>resource_id</td><td>The identifier of the resource measured (UUID)</td></tr><tr><td>project_id</td><td>The project (tenant) ID to which the resource belongs.</td></tr><tr><td>user_id</td><td>The ID of the user who owns the resource.</td></tr><tr><td>resource_metadata</td><td>Other data transmitted in the metering notification payload.</td></tr></tbody></table></div></div></div></div><div class="sect2" id="topic3051"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ceilometer Metering Available Meter Types</span> <a title="Permalink" class="permalink" href="#topic3051">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_metertypes.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_metertypes.xml</li><li><span class="ds-label">ID: </span>topic3051</li></ul></div></div></div></div><p>
  The Metering service contains three types of meters:
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.15.5.5.3.1"><span class="term ">Cumulative</span></dt><dd><p>
     A cumulative meter measures data over time (for example, instance hours).
    </p></dd><dt id="id-1.5.15.5.5.3.2"><span class="term ">Gauge</span></dt><dd><p>
     A gauge measures discrete items (for example, floating IPs or image
     uploads) or fluctuating values (such as disk input or output).
    </p></dd><dt id="id-1.5.15.5.5.3.3"><span class="term ">Delta</span></dt><dd><p>
     A delta measures change over time, for example, monitoring bandwidth.
    </p></dd></dl></div><p>
  Each meter is populated from one or more <span class="emphasis"><em>samples</em></span>, which
  are gathered from the messaging queue (listening agent), polling agents, or
  push agents. Samples are populated by <span class="emphasis"><em>counter</em></span> objects.
 </p><p>
  Each counter contains the following <span class="emphasis"><em>fields</em></span>:
 </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.15.5.5.6.1"><span class="term ">name</span></dt><dd><p>
     the name of the meter
    </p></dd><dt id="id-1.5.15.5.5.6.2"><span class="term ">type</span></dt><dd><p>
     the type of meter (cumulative, gauge, or delta)
    </p></dd><dt id="id-1.5.15.5.5.6.3"><span class="term ">amount</span></dt><dd><p>
     the amount of data measured
    </p></dd><dt id="id-1.5.15.5.5.6.4"><span class="term ">unit</span></dt><dd><p>
     the unit of measure
    </p></dd><dt id="id-1.5.15.5.5.6.5"><span class="term ">resource</span></dt><dd><p>
     the resource being measured
    </p></dd><dt id="id-1.5.15.5.5.6.6"><span class="term ">project ID</span></dt><dd><p>
     the project the resource is assigned to
    </p></dd><dt id="id-1.5.15.5.5.6.7"><span class="term ">user</span></dt><dd><p>
     the user the resource is assigned to.
    </p></dd></dl></div><p>
  <span class="bold"><strong>Note</strong></span>: The metering service shares the same
  High-availability proxy, messaging, and database clusters with the other
  Information services. To avoid unnecessarily high loads,
  <a class="xref" href="#Ceilo-optimize" title="13.3.8. Optimizing the Ceilometer Metering Service">Section 13.3.8, “Optimizing the Ceilometer Metering Service”</a>.
 </p><div class="sect3" id="openstack-default-meters"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Default Meters</span> <a title="Permalink" class="permalink" href="#openstack-default-meters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_metertypes.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_metertypes.xml</li><li><span class="ds-label">ID: </span>openstack-default-meters</li></ul></div></div></div></div><p>
   These meters are installed and enabled by default during an <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   installation. More information about ceilometer can be found at <a class="link" href="https://docs.openstack.org/ceilometer/latest/" target="_blank">OpenStack
   ceilometer</a>.
  </p></div><div class="sect3" id="nova-meters"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute (nova) Meters</span> <a title="Permalink" class="permalink" href="#nova-meters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-nova_meters.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-nova_meters.xml</li><li><span class="ds-label">ID: </span>nova-meters</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /><col class="col6" /></colgroup><thead><tr><th>Meter</th><th>Type</th><th>Unit</th><th>Resource</th><th>Origin</th><th>Note</th></tr></thead><tbody><tr><td>vcpus</td><td>Gauge</td><td>vcpu</td><td>Instance ID</td><td>Notification</td><td>Number of virtual CPUs allocated to the instance</td></tr><tr><td>memory</td><td>Gauge</td><td>MB</td><td>Instance ID</td><td>Notification</td><td>Volume of RAM allocated to the instance</td></tr><tr><td>memory.resident</td><td>Gauge</td><td>MB</td><td>Instance ID</td><td>Pollster</td><td>Volume of RAM used by the instance on the physical machine</td></tr><tr><td>memory.usage</td><td>Gauge</td><td>MB</td><td>Instance ID</td><td>Pollster</td><td>Volume of RAM used by the instance from the amount of its allocated
                memory</td></tr><tr><td>cpu</td><td>Cumulative</td><td>ns</td><td>Instance ID</td><td>Pollster</td><td>CPU time used</td></tr><tr><td>cpu_util</td><td>Gauge</td><td>%</td><td>Instance ID</td><td>Pollster</td><td>Average CPU utilization</td></tr><tr><td>disk.read.requests</td><td>Cumulative</td><td>request</td><td>Instance ID</td><td>Pollster</td><td>Number of read requests</td></tr><tr><td>disk.read.requests.rate</td><td>Gauge</td><td>request/s</td><td>Instance ID</td><td>Pollster</td><td>Average rate of read requests</td></tr><tr><td>disk.write.requests</td><td>Cumulative</td><td>request</td><td>Instance ID</td><td>Pollster</td><td>Number of write requests</td></tr><tr><td>disk.write.requests.rate</td><td>Gauge</td><td>request/s</td><td>Instance ID</td><td>Pollster</td><td>Average rate of write requests</td></tr><tr><td>disk.read.bytes</td><td>Cumulative</td><td>B</td><td>Instance ID</td><td>Pollster</td><td>Volume of reads</td></tr><tr><td>disk.read.bytes.rate</td><td>Gauge</td><td>B/s</td><td>Instance ID</td><td>Pollster</td><td>Average rate of reads</td></tr><tr><td>disk.write.bytes</td><td>Cumulative</td><td>B</td><td>Instance ID</td><td>Pollster</td><td>Volume of writes</td></tr><tr><td>disk.write.bytes.rate</td><td>Gauge</td><td>B/s</td><td>Instance ID</td><td>Pollster</td><td>Average rate of writes</td></tr><tr><td>disk.root.size</td><td>Gauge</td><td>GB</td><td>Instance ID</td><td>Notification</td><td>Size of root disk</td></tr><tr><td>disk.ephemeral.size</td><td>Gauge</td><td>GB</td><td>Instance ID</td><td>Notification</td><td>Size of ephemeral disk</td></tr><tr><td>disk.device.read.requests</td><td>Cumulative</td><td>request</td><td>Disk ID</td><td>Pollster</td><td>Number of read requests</td></tr><tr><td>disk.device.read.requests.rate</td><td>Gauge</td><td>request/s</td><td>Disk ID</td><td>Pollster</td><td>Average rate of read requests</td></tr><tr><td>disk.device.write.requests</td><td>Cumulative</td><td>request</td><td>Disk ID</td><td>Pollster</td><td>Number of write requests</td></tr><tr><td>disk.device.write.requests.rate</td><td>Gauge</td><td>request/s</td><td>Disk ID</td><td>Pollster</td><td>Average rate of write requests</td></tr><tr><td>disk.device.read.bytes</td><td>Cumulative</td><td>B</td><td>Disk ID</td><td>Pollster</td><td>Volume of reads</td></tr><tr><td>disk.device.read.bytes .rate</td><td>Gauge</td><td>B/s</td><td>Disk ID</td><td>Pollster</td><td>Average rate of reads</td></tr><tr><td>disk.device.write.bytes</td><td>Cumulative</td><td>B</td><td>Disk ID</td><td>Pollster</td><td>Volume of writes</td></tr><tr><td>disk.device.write.bytes .rate</td><td>Gauge</td><td>B/s</td><td>Disk ID</td><td>Pollster</td><td>Average rate of writes</td></tr><tr><td>disk.capacity</td><td>Gauge</td><td>B</td><td>Instance ID</td><td>Pollster</td><td>The amount of disk that the instance can see</td></tr><tr><td>disk.allocation</td><td>Gauge</td><td>B</td><td>Instance ID</td><td>Pollster</td><td>The amount of disk occupied by the instance on the host machine</td></tr><tr><td>disk.usage</td><td>Gauge</td><td>B</td><td>Instance ID</td><td>Pollster</td><td>The physical size in bytes of the image container on the host</td></tr><tr><td>disk.device.capacity</td><td>Gauge</td><td>B</td><td>Disk ID</td><td>Pollster</td><td>The amount of disk per device that the instance can see</td></tr><tr><td>disk.device.allocation</td><td>Gauge</td><td>B</td><td>Disk ID</td><td>Pollster</td><td>The amount of disk per device occupied by the instance on the host
                machine</td></tr><tr><td>disk.device.usage</td><td>Gauge</td><td>B</td><td>Disk ID</td><td>Pollster</td><td>The physical size in bytes of the image container on the host per
                device</td></tr><tr><td>network.incoming.bytes</td><td>Cumulative</td><td>B</td><td>Interface ID</td><td>Pollster</td><td>Number of incoming bytes</td></tr><tr><td>network.outgoing.bytes</td><td>Cumulative</td><td>B</td><td>Interface ID</td><td>Pollster</td><td>Number of outgoing bytes</td></tr><tr><td>network.incoming.packets</td><td>Cumulative</td><td>packet</td><td>Interface ID</td><td>Pollster</td><td>Number of incoming packets</td></tr><tr><td>network.outgoing.packets</td><td>Cumulative</td><td>packet</td><td>Interface ID</td><td>Pollster</td><td>Number of outgoing packets</td></tr></tbody></table></div></div><div class="sect3" id="computehost-meters"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute Host Meters</span> <a title="Permalink" class="permalink" href="#computehost-meters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-computehost_meters.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-computehost_meters.xml</li><li><span class="ds-label">ID: </span>computehost-meters</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /><col class="col6" /></colgroup><thead><tr><th>Meter</th><th>Type</th><th>Unit</th><th>Resource</th><th>Origin</th><th>Note</th></tr></thead><tbody><tr><td>compute.node.cpu.frequency</td><td>Gauge</td><td>MHz</td><td>Host ID</td><td>Notification</td><td>CPU frequency</td></tr><tr><td>compute.node.cpu.kernel.time</td><td>Cumulative</td><td>ns</td><td>Host ID</td><td>Notification</td><td>CPU kernel time</td></tr><tr><td>compute.node.cpu.idle.time</td><td>Cumulative</td><td>ns</td><td>Host ID</td><td>Notification</td><td>CPU idle time</td></tr><tr><td>compute.node.cpu.user.time</td><td>Cumulative</td><td>ns</td><td>Host ID</td><td>Notification</td><td>CPU user mode time</td></tr><tr><td>compute.node.cpu.iowait.time</td><td>Cumulative</td><td>ns</td><td>Host ID</td><td>Notification</td><td>CPU I/O wait time</td></tr><tr><td>compute.node.cpu.kernel.percent</td><td>Gauge</td><td>%</td><td>Host ID</td><td>Notification</td><td>CPU kernel percentage</td></tr><tr><td>compute.node.cpu.idle.percent</td><td>Gauge</td><td>%</td><td>Host ID</td><td>Notification</td><td>CPU idle percentage</td></tr><tr><td>compute.node.cpu.user.percent</td><td>Gauge</td><td>%</td><td>Host ID</td><td>Notification</td><td>CPU user mode percentage</td></tr><tr><td>compute.node.cpu.iowait.percent</td><td>Gauge</td><td>%</td><td>Host ID</td><td>Notification</td><td>CPU I/O wait percentage</td></tr><tr><td>compute.node.cpu.percent</td><td>Gauge</td><td>%</td><td>Host ID</td><td>Notification</td><td>CPU utilization</td></tr></tbody></table></div></div><div class="sect3" id="glance-meters"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Image (glance) Meters</span> <a title="Permalink" class="permalink" href="#glance-meters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-glance_meters.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-glance_meters.xml</li><li><span class="ds-label">ID: </span>glance-meters</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /><col class="col6" /></colgroup><thead><tr><th>Meter</th><th>Type</th><th>Unit</th><th>Resource</th><th>Origin</th><th>Note</th></tr></thead><tbody><tr><td>image.size</td><td>Gauge</td><td>B</td><td>Image ID</td><td>Notification</td><td>Uploaded image size</td></tr><tr><td>image.update</td><td>Delta</td><td>Image</td><td>Image ID</td><td>Notification</td><td>Number of uploads of the image</td></tr><tr><td>image.upload</td><td>Delta</td><td>Image</td><td>image ID</td><td>notification</td><td>Number of uploads of the image</td></tr><tr><td>image.delete</td><td>Delta</td><td>Image</td><td>Image ID</td><td>Notification</td><td>Number of deletes on the image</td></tr></tbody></table></div></div><div class="sect3" id="cinder-meters"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Volume (cinder) Meters</span> <a title="Permalink" class="permalink" href="#cinder-meters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-cinder_meters.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-cinder_meters.xml</li><li><span class="ds-label">ID: </span>cinder-meters</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /><col class="col6" /></colgroup><thead><tr><th>Meter</th><th>Type</th><th>Unit</th><th>Resource</th><th>Origin</th><th>Note</th></tr></thead><tbody><tr><td>volume.size</td><td>Gauge</td><td>GB</td><td>Vol ID</td><td>Notification</td><td>Size of volume</td></tr><tr><td>snapshot.size</td><td>Gauge</td><td>GB</td><td>Snap ID</td><td>Notification</td><td>Size of snapshot's volume</td></tr></tbody></table></div></div><div class="sect3" id="swift-meters"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.3.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Storage (swift) Meters</span> <a title="Permalink" class="permalink" href="#swift-meters">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-swift_meters.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-swift_meters.xml</li><li><span class="ds-label">ID: </span>swift-meters</li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /><col class="col6" /></colgroup><thead><tr><th>Meter</th><th>Type</th><th>Unit</th><th>Resource</th><th>Origin</th><th>Note</th></tr></thead><tbody><tr><td>storage.objects</td><td>Gauge</td><td>Object</td><td>Storage ID</td><td>Pollster</td><td>Number of objects</td></tr><tr><td>storage.objects.size</td><td>Gauge</td><td>B</td><td>Storage ID</td><td>Pollster</td><td>Total size of stored objects</td></tr><tr><td>storage.objects.containers</td><td>Gauge</td><td>Container</td><td>Storage ID</td><td>Pollster</td><td>Number of containers</td></tr></tbody></table></div><p>
  The <code class="literal">resource_id</code> for any ceilometer query is the
  <code class="literal">tenant_id</code> for the swift object because swift usage is
  rolled up at the tenant level.
 </p></div></div><div class="sect2" id="reconfig-metering"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure the Ceilometer Metering Service</span> <a title="Permalink" class="permalink" href="#reconfig-metering">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_reconfig.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_reconfig.xml</li><li><span class="ds-label">ID: </span>reconfig-metering</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 automatically deploys ceilometer to use the monasca database.
  ceilometer is deployed on the same control plane nodes along with other
  OpenStack services such as keystone, nova, neutron, glance, and swift.
 </p><p>
  The Metering Service can be configured using one of the procedures described
  below.
 </p><div class="sect3" id="idg-all-metering-metering-reconfig-xml-7"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Run the Upgrade Playbook</span> <a title="Permalink" class="permalink" href="#idg-all-metering-metering-reconfig-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_reconfig.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_reconfig.xml</li><li><span class="ds-label">ID: </span>idg-all-metering-metering-reconfig-xml-7</li></ul></div></div></div></div><p>
   Follow Standard Service upgrade mechanism available in the Cloud Lifecycle Manager
   distribution. For ceilometer, the playbook included with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is
   <span class="bold"><strong>ceilometer-upgrade.yml</strong></span>
  </p></div><div class="sect3" id="metering-services"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enable Services for Messaging Notifications</span> <a title="Permalink" class="permalink" href="#metering-services">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_reconfig.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_reconfig.xml</li><li><span class="ds-label">ID: </span>metering-services</li></ul></div></div></div></div><p>
   After installation of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the following services are enabled
   by default to send notifications:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     nova
    </p></li><li class="listitem "><p>
     cinder
    </p></li><li class="listitem "><p>
     glance
    </p></li><li class="listitem "><p>
     neutron
    </p></li><li class="listitem "><p>
     swift
    </p></li></ul></div><p>
   The list of meters for these services are specified in the Notification
   Agent or Polling Agent's pipeline configuration file.
  </p><p>
   For steps on how to edit the pipeline configuration files, see:
   <a class="xref" href="#notifications" title="13.3.5. Ceilometer Metering Service Notifications">Section 13.3.5, “Ceilometer Metering Service Notifications”</a>
  </p></div><div class="sect3" id="Ceilo-StopStart"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restart the Polling Agent</span> <a title="Permalink" class="permalink" href="#Ceilo-StopStart">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_reconfig.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_reconfig.xml</li><li><span class="ds-label">ID: </span>Ceilo-StopStart</li></ul></div></div></div></div><p>
   The Polling Agent is responsible for coordinating the polling activity. It
   parses the <span class="bold"><strong>pipeline.yml</strong></span> configuration file
   and identifies all the sources where data is collected. The sources are then
   evaluated and are translated to resources that a dedicated pollster can
   retrieve. The Polling Agent follows this process:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     At each identified interval, the
     <span class="bold"><strong>pipeline.yml</strong></span> configuration file is
     parsed.
    </p></li><li class="listitem "><p>
     The resource list is composed.
    </p></li><li class="listitem "><p>
     The pollster collects the data.
    </p></li><li class="listitem "><p>
     The pollster sends data to the queue.
    </p></li></ol></div><p>
   Metering processes should normally be operating at all times. This need is
   addressed by the Upstart event engine which is designed to run on any Linux
   system. Upstart creates events, handles the consequences of those events,
   and starts and stops processes as required. Upstart will continually attempt
   to restart stopped processes even if the process was stopped manually. To
   stop or start the Polling Agent and avoid the conflict with Upstart, using
   the following steps.
  </p><p>
   <span class="bold"><strong>To restart the Polling Agent:</strong></span>
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     To determine whether the process is running, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl status ceilometer-agent-notification
#SAMPLE OUTPUT:
ceilometer-agent-notification.service - ceilometer-agent-notification Service
   Loaded: loaded (/etc/systemd/system/ceilometer-agent-notification.service; enabled; vendor preset: disabled)
   Active: active (running) since Tue 2018-06-12 05:07:14 UTC; 2 days ago
 Main PID: 31529 (ceilometer-agen)
    Tasks: 69
   CGroup: /system.slice/ceilometer-agent-notification.service
           ├─31529 ceilometer-agent-notification: master process [/opt/stack/service/ceilometer-agent-notification/venv/bin/ceilometer-agent-notification --config-file /opt/stack/service/ceilometer-agent-noti...
           └─31621 ceilometer-agent-notification: NotificationService worker(0)

Jun 12 05:07:14 ardana-qe201-cp1-c1-m2-mgmt systemd[1]: Started ceilometer-agent-notification Service.</pre></div></li><li class="step "><p>
     To stop the process, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl stop ceilometer-agent-notification</pre></div></li><li class="step "><p>
     To start the process, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl start ceilometer-agent-notification</pre></div></li></ol></div></div></div><div class="sect3" id="ceilo-replace-cntrler"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replace a Logging, Monitoring, and Metering Controller</span> <a title="Permalink" class="permalink" href="#ceilo-replace-cntrler">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_reconfig.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_reconfig.xml</li><li><span class="ds-label">ID: </span>ceilo-replace-cntrler</li></ul></div></div></div></div><p>
   In a medium-scale environment, if a metering controller has to be replaced
   or rebuilt, use the following steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     <a class="xref" href="#replacing-controller" title="15.1.2.1. Replacing a Controller Node">Section 15.1.2.1, “Replacing a Controller Node”</a>.
    </p></li><li class="step "><p>
     If the ceilometer nodes are not on the shared control plane, to implement
     the changes and replace the controller, you must reconfigure ceilometer.
     To do this, run the ceilometer-reconfigure.yml ansible playbook
     <span class="bold"><strong>without</strong></span> the limit option
    </p></li></ol></div></div></div><div class="sect3" id="ceilo-monitoring"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure Monitoring</span> <a title="Permalink" class="permalink" href="#ceilo-monitoring">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_reconfig.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_reconfig.xml</li><li><span class="ds-label">ID: </span>ceilo-monitoring</li></ul></div></div></div></div><p>
   The monasca HTTP Process monitors ceilometer's notification and polling
   agents are monitored. If these agents are down, monasca monitoring alarms
   are triggered. You can use the notification alarms to debug the issue and
   restart the notifications agent. However, for
   <code class="literal">Central-Agent</code> (polling) and <code class="literal">Collector</code>
   the alarms need to be deleted. These two processes are not started after an
   upgrade so when the monitoring process checks the alarms for these
   components, they will be in <code class="literal">UNDETERMINED</code>
   state. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> does not monitor these processes anymore. To resolve
   this issue, manually delete alarms that are no longer used but are
   installed.
  </p><p>
   To resolve notification alarms, first check the
   <span class="bold"><strong>ceilometer-agent-notification</strong></span> logs for
   errors in the <span class="bold"><strong>/var/log/ceilometer</strong></span>
   directory. You can also use the Operations Console to access Kibana and
   check the logs. This will help you understand and debug the error.
  </p><p>
   To restart the service, run the
   <span class="bold"><strong>ceilometer-start.yml</strong></span>. This playbook starts
   the ceilometer processes that has stopped and only restarts during install,
   upgrade or reconfigure which is what is needed in this case. Restarting the
   process that has stopped will resolve this alarm because this monasca alarm
   means that ceilometer-agent-notification is no longer running on certain
   nodes.
  </p><p>
   You can access ceilometer data through monasca. ceilometer publishes samples
   to monasca with credentials of the following accounts:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>ceilometer</strong></span> user
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>services</strong></span>
    </p></li></ul></div><p>
   Data collected by ceilometer can also be retrieved by the monasca REST API.
   Make sure you use the following guidelines when requesting data from the
   monasca REST API:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Verify you have the monasca-admin role. This role is configured in the
     monasca-api configuration file.
    </p></li><li class="listitem "><p>
     Specify the <code class="literal">tenant id</code> of the
     <span class="bold"><strong>services</strong></span> project.
    </p></li></ul></div><p>
   For more details, read the
   <a class="link" href="https://github.com/openstack/monasca-api/blob/master/docs/monasca-api-spec.md" target="_blank">monasca
   API Specification</a>.
  </p><p>
   To run monasca commands at the command line, you must be have the
   <span class="bold"><strong>admin</strong></span> role. This allows you to use the
   ceilometer account credentials to replace the default admin account
   credentials defined in the <span class="bold"><strong>service.osrc</strong></span>
   file. When you use the ceilometer account credentials, monasca commands will
   only return data collected by ceilometer. At this time, monasca command line
   interface (CLI) does not support the data retrieval of other tenants or
   projects.
  </p></div></div><div class="sect2" id="notifications"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ceilometer Metering Service Notifications</span> <a title="Permalink" class="permalink" href="#notifications">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_notifications.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_notifications.xml</li><li><span class="ds-label">ID: </span>notifications</li></ul></div></div></div></div><p>
  ceilometer uses the notification agent to listen to the message queue,
  convert notifications to Events and Samples, and apply pipeline actions.
 </p><div class="sect3" id="whitelist"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Manage Whitelisting and Polling</span> <a title="Permalink" class="permalink" href="#whitelist">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_notifications.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_notifications.xml</li><li><span class="ds-label">ID: </span>whitelist</li></ul></div></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is designed to reduce the amount of data that is stored.
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>'s use of a SQL-based cluster, which is not recommended for big data,
   means you must control the data that ceilometer collects. You can do this by
   filtering (whitelisting) the data or by using the configuration files for
   the ceilometer Polling Agent and the ceilometer Notificfoation Agent.
  </p><p>
   Whitelisting is used in a rule specification as a positive filtering
   parameter. Whitelist is only included in rules that can be used in direct
   mappings, for identity service issues such as service discovery,
   provisioning users, groups, roles, projects, domains as well as user
   authentication and authorization.
  </p><p>
   You can run tests against specific scenarios to see if filtering reduces the
   amount of data stored. You can create a test by editing or creating a run
   filter file (whitelist). For steps on how to do this, see:
   <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 38 “Post Installation Tasks”, Section 38.1 “API Verification”</span>.
  </p><p>
   ceilometer Polling Agent (polling agent) and ceilometer Notification Agent
   (notification agent) use different pipeline.yaml files to configure meters
   that are collected. This prevents accidentally polling for meters which can
   be retrieved by the polling agent as well as the notification agent. For
   example, glance image and image.size are meters which can be retrieved both
   by polling and notifications.
  </p><p>
   In both of the separate configuration files, there is a setting for
   <code class="literal">interval</code>. The interval attribute determines the
   frequency, in seconds, of how often data is collected. You can use this
   setting to control the amount of resources that are used for notifications
   and for polling. For example, you want to use more resources for
   notifications and less for polling. To accomplish this you would set the
   <code class="literal">interval</code> in the polling configuration file to a large
   amount of time, such as 604800 seconds, which polls only once a week. Then
   in the notifications configuration file, you can set the
   <code class="literal">interval</code> to a higher amount, such as collecting data
   every 30 seconds.
  </p><div id="id-1.5.15.5.7.3.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    swift account data will be collected using the polling mechanism in an
    hourly interval.
   </p></div><p>
   Setting this interval to manage both notifications and polling is the
   recommended procedure when using a SQL cluster back-end.
  </p><p>
   <span class="bold"><strong>Sample ceilometer Polling Agent file:</strong></span>
  </p><div class="verbatim-wrap"><pre class="screen">#File: ~/opt/stack/service/ceilometer-polling/etc/pipeline-polling.yaml
---
sources:
    - name: swift_source
      interval: 3600
      meters:
          - "storage.objects"
          - "storage.objects.size"
          - "storage.objects.containers"
      resources:
      discovery:
      sinks:
          - meter_sink
sinks:
    - name: meter_sink
      transformers:
      publishers:
         - notifier://</pre></div><p>
   <span class="bold"><strong>Sample ceilometer Notification Agent(notification
   agent) file:</strong></span>
  </p><div class="verbatim-wrap"><pre class="screen">#File:    ~/opt/stack/service/ceilometer-agent-notification/etc/pipeline-agent-notification.yaml
---
sources:
    - name: meter_source
      interval: 30
      meters:
          - "instance"
          - "image"
          - "image.size"
          - "image.upload"
          - "image.delete"
          - "volume"
          - "volume.size"
          - "snapshot"
          - "snapshot.size"
          - "ip.floating"
          - "network"
          - "network.create"
          - "network.update"
resources:
discovery:
sinks:
          - meter_sink
sinks:
    - name: meter_sink
      transformers:
      publishers:
         - notifier://</pre></div><p>
   Both of the pipeline files have two major sections:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.15.5.7.3.14.1"><span class="term ">Sources</span></dt><dd><p>
      represents the data that is collected either from notifications posted by
      services or through polling. In the Sources section there is a list of
      meters. These meters define what kind of data is collected. For a full
      list refer to the ceilometer documentation available at:
      <a class="link" href="http://docs.openstack.org/admin-guide/telemetry-measurements.html" target="_blank">Telemetry
      Measurements</a>
     </p></dd><dt id="id-1.5.15.5.7.3.14.2"><span class="term ">Sinks</span></dt><dd><p>
      represents how the data is modified before it is published to the
      internal queue for collection and storage.
     </p></dd></dl></div><p>
   You will only need to change a setting in the Sources section to control the
   data collection interval.
  </p><p>
   For more information, see
   <a class="link" href="http://docs.openstack.org/admin-guide-cloud/telemetry-measurements.html" target="_blank">Telemetry
   Measurements</a>
  </p><p>
   <span class="bold"><strong>To change the ceilometer Polling Agent interval
   setting:</strong></span>
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     To find the polling agent configuration file, run:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/opt/stack/service/ceilometer-polling/etc</pre></div></li><li class="step "><p>
     In a text editor, open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">pipeline-polling.yaml</pre></div></li><li class="step "><p>
     In the following section, change the value of <code class="literal">interval</code>
     to the desired amount of time:
    </p><div class="verbatim-wrap"><pre class="screen">---
sources:
    - name: swift_source
      interval: 3600
      meters:
          - "storage.objects"
          - "storage.objects.size"
          - "storage.objects.containers"
      resources:
      discovery:
      sinks:
          - meter_sink
sinks:
    - name: meter_sink
      transformers:
      publishers:
         - notifier://</pre></div><p>
     In the sample code above, the polling agent will collect data every 600
     seconds, or 10 minutes.
    </p></li></ol></div></div><p>
   <span class="bold"><strong>To change the ceilometer Notification Agent
   (notification agent) interval setting:</strong></span>
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     To find the notification agent configuration file, run:
    </p><div class="verbatim-wrap"><pre class="screen">cd /opt/stack/service/ceilometer-agent-notification</pre></div></li><li class="step "><p>
     In a text editor, open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">pipeline-agent-notification.yaml</pre></div></li><li class="step "><p>
     In the following section, change the value of <code class="literal">interval</code>
     to the desired amount of time:
    </p><div class="verbatim-wrap"><pre class="screen">sources:
    - name: meter_source
      interval: 30
      meters:
          - "instance"
          - "image"
          - "image.size"
          - "image.upload"
          - "image.delete"
          - "volume"
          - "volume.size"
          - "snapshot"
          - "snapshot.size"
          - "ip.floating"
          - "network"
          - "network.create"
          - "network.update"</pre></div><p>
     In the sample code above, the notification agent will collect data every
     30 seconds.
    </p></li></ol></div></div><div id="id-1.5.15.5.7.3.21" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    The <code class="literal">pipeline-agent-notification.yaml</code> file needs to be changed on all
    controller nodes to change the white-listing and polling strategy.
   </p></div></div><div class="sect3" id="idg-all-metering-metering-notifications-xml-7"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Edit the List of Meters</span> <a title="Permalink" class="permalink" href="#idg-all-metering-metering-notifications-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_notifications.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_notifications.xml</li><li><span class="ds-label">ID: </span>idg-all-metering-metering-notifications-xml-7</li></ul></div></div></div></div><p>
   The number of enabled meters can be reduced or increased by editing the
   pipeline configuration of the notification and polling agents. To deploy
   these changes you must then restart the agent. If pollsters and
   notifications are both modified, then you will have to restart both the
   Polling Agent and the Notification Agent. ceilometer Collector will also
   need to be restarted. The following code is an example of a compute-only
   ceilometer Notification Agent (notification agent)
   <span class="bold"><strong>pipeline-agent-notification.yaml </strong></span>file:
  </p><div class="verbatim-wrap"><pre class="screen">---
sources:
    - name: meter_source
      interval: 86400
      meters:
          - "instance"
          - "memory"
          - "vcpus"
          - "compute.instance.create.end"
          - "compute.instance.delete.end"
          - "compute.instance.update"
          - "compute.instance.exists"
      sinks:
          - meter_sink
sinks:
    - name: meter_sink
      transformers:
      publishers:
          - notifier://</pre></div><div id="id-1.5.15.5.7.4.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    If you enable meters at the container level in this file, every time the
    polling interval triggers a collection, at least 5 messages per existing
    container in swift are collected.
   </p></div><p>
   The following table illustrates the amount of data produced hourly in
   different scenarios:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /><col /><col /></colgroup><tbody><tr><td>swift Containers</td><td>swift Objects per container</td><td>Samples per Hour</td><td>Samples stored per 24 hours</td></tr><tr><td>10</td><td>10</td><td>500</td><td>12000</td></tr><tr><td>10</td><td>100</td><td>5000</td><td>120000</td></tr><tr><td>100</td><td>100</td><td>50000</td><td>1200000</td></tr><tr><td>100</td><td>1000</td><td>500000</td><td>12000000</td></tr></tbody></table></div><p>
   The data in the table shows that even a very small swift storage with 10
   containers and 100 files will store 120,000 samples in 24 hours, generating
   a total of 3.6 million samples.
  </p><div id="id-1.5.15.5.7.4.8" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    The size of each file does not have any impact on the number of samples
    collected. As shown in the table above, the smallest number of samples
    results from polling when there are a small number of files and a small
    number of containers. When there are a lot of small files and containers,
    the number of samples is the highest.
   </p></div></div><div class="sect3" id="meters-add"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Add Resource Fields to Meters</span> <a title="Permalink" class="permalink" href="#meters-add">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_notifications.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_notifications.xml</li><li><span class="ds-label">ID: </span>meters-add</li></ul></div></div></div></div><p>
   By default, not all the resource metadata fields for an event are recorded
   and stored in ceilometer. If you want to collect metadata fields for a
   consumer application, for example, it is easier to add a field to an
   existing meter rather than creating a new meter. If you create a new meter,
   you must also reconfigure ceilometer.
  </p><div id="id-1.5.15.5.7.5.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    Consider the following information before you add or edit a meter:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      You can add a maximum of 12 new fields.
     </p></li><li class="listitem "><p>
      Adding or editing a meter causes all non-default meters to STOP receiving
      notifications. You will need to restart ceilometer.
     </p></li><li class="listitem "><p>
      New meters added to the <code class="literal">pipeline-polling.yaml.j2</code> file
      must also be added to the
      <code class="literal">pipeline-agent-notification.yaml.j2</code> file. This is due
      to the fact that polling meters are drained by the notification agent and
      not by the collector.
     </p></li><li class="listitem "><p>
      After <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is installed, services like compute, cinder, glance, and
      neutron are configured to publish ceilometer meters by default. Other
      meters can also be enabled after the services are configured to start
      publishing the meter. The only requirement for publishing a meter is that
      the <code class="literal">origin</code> must have a value of
      <code class="literal">notification</code>. For a complete list of meters, see the
      OpenStack documentation on
      <a class="link" href="http://docs.openstack.org/admin-guide/telemetry-measurements.html" target="_blank">Measurements</a>.
     </p></li><li class="listitem "><p>
      Not all meters are supported. Meters collected by ceilometer Compute
      Agent or any agent other than ceilometer Polling are not supported or
      tested with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
     </p></li><li class="listitem "><p>
      Identity meters are disabled by keystone.
     </p></li><li class="listitem "><p>
      To enable ceilometer to start collecting meters, some services require
      you enable the meters you need in the service first before enabling them
      in ceilometer. Refer to the documentation for the specific service before
      you add new meters or resource fields.
     </p></li></ul></div></div><p>
   <span class="bold"><strong>To add Resource Metadata fields:</strong></span>
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log on to the Cloud Lifecycle Manager (deployer node).
    </p></li><li class="step "><p>
     To change to the ceilometer directory, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/config/ceilometer</pre></div></li><li class="step "><p>
     In a text editor, open the target configuration file (for example,
     monasca-field-definitions.yaml.j2).
    </p></li><li class="step "><p>
     In the metadata section, either add a new meter or edit an existing one
     provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
    </p></li><li class="step "><p>
     Include the metadata fields you need. You can use the <code class="literal">instance
     meter</code> in the file as an example.
    </p></li><li class="step "><p>
     Save and close the configuration file.
    </p></li><li class="step "><p>
     To save your changes in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config"</pre></div></li><li class="step "><p>
     If you added a new meter, reconfigure ceilometer:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/
# To run the config-processor playbook:
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
#To run the ready-deployment playbook:
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ceilometer-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="update-pollSwift"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Update the Polling Strategy and Swift Considerations</span> <a title="Permalink" class="permalink" href="#update-pollSwift">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_notifications.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_notifications.xml</li><li><span class="ds-label">ID: </span>update-pollSwift</li></ul></div></div></div></div><p>
   Polling can be very taxing on the system due to the sheer volume of data
   that the system may have to process. It also has a severe impact on
   queries since the database will now have a very large amount of data to scan
   to respond to the query. This consumes a great amount of cpu and memory.
   This can result in long wait times for query responses, and in extreme cases
   can result in timeouts.
  </p><p>
   There are 3 polling meters in swift:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     storage.objects
    </p></li><li class="listitem "><p>
     storage.objects.size
    </p></li><li class="listitem "><p>
     storage.objects.containers
    </p></li></ul></div><p>
   Here is an example of <code class="filename">pipeline.yml</code> in which
   swift polling is set to occur hourly.
  </p><div class="verbatim-wrap"><pre class="screen">---
      sources:
      - name: swift_source
      interval: 3600
      meters:
      - "storage.objects"
      - "storage.objects.size"
      - "storage.objects.containers"
      resources:
      discovery:
      sinks:
      - meter_sink
      sinks:
      - name: meter_sink
      transformers:
      publishers:
      - notifier://</pre></div><p>
   With this configuration above, we did not enable polling of container based
   meters and we only collect 3 messages for any given tenant, one for each
   meter listed in the configuration files. Since we have 3 messages only per
   tenant, it does not create a heavy load on the MySQL database as it would
   have if container-based meters were enabled. Hence, other APIs are not
   hit because of this data collection configuration.
  </p></div></div><div class="sect2" id="topic15050"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.3.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ceilometer Metering Setting Role-based Access Control</span> <a title="Permalink" class="permalink" href="#topic15050">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_rbac.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_rbac.xml</li><li><span class="ds-label">ID: </span>topic15050</li></ul></div></div></div></div><p>
  Role Base Access Control (RBAC) is a technique that limits access to
  resources based on a specific set of roles associated with each user's
  credentials.
 </p><p>
  keystone has a set of users that are associated with each project. Each user
  has at least one role. After a user has authenticated with keystone using a
  valid set of credentials, keystone will augment that request with the Roles
  that are associated with that user. These roles are added to the Request
  Header under the X-Roles attribute and are presented as a comma-separated
  list.
 </p><div class="sect3" id="display-users"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Displaying All Users</span> <a title="Permalink" class="permalink" href="#display-users">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_rbac.xml</li><li><span class="ds-label">ID: </span>display-users</li></ul></div></div></div></div><p>
   To discover the list of users available in the system, an administrator can
   run the following command using the keystone command-line interface:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>openstack user list</pre></div><p>
   The output should resemble this response, which is a list of all the users
   currently available in this system.
  </p><div class="verbatim-wrap"><pre class="screen">+----------------------------------+-----------------------------------------+----+
|                id                |    name      | enabled |       email        |
+----------------------------------+-----------------------------------------+----+
| 1c20d327c92a4ea8bb513894ce26f1f1 |   admin      |   True  | admin.example.com  |
| 0f48f3cc093c44b4ad969898713a0d65 | ceilometer   |   True  | nobody@example.com |
| 85ba98d27b1c4c8f97993e34fcd14f48 |   cinder     |   True  | nobody@example.com |
| d2ff982a0b6547d0921b94957db714d6 |    demo      |   True  |  demo@example.com  |
| b2d597e83664489ebd1d3c4742a04b7c |    ec2       |   True  | nobody@example.com |
| 2bd85070ceec4b608d9f1b06c6be22cb |   glance     |   True  | nobody@example.com |
| 0e9e2daebbd3464097557b87af4afa4c |    heat      |   True  | nobody@example.com |
| 0b466ddc2c0f478aa139d2a0be314467 |  neutron     |   True  | nobody@example.com |
| 5cda1a541dee4555aab88f36e5759268 |    nova      |   True  | nobody@example.com ||
| 5cda1a541dee4555aab88f36e5759268 |    nova      |   True  | nobody@example.com |
| 1cefd1361be8437d9684eb2add8bdbfa |   swift      |   True  | nobody@example.com |
| f05bac3532c44414a26c0086797dab23 | user20141203213957|True| nobody@example.com |
| 3db0588e140d4f88b0d4cc8b5ca86a0b | user20141205232231|True| nobody@example.com |
+----------------------------------+-----------------------------------------+----+</pre></div></div><div class="sect3" id="display-roles"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Displaying All Roles</span> <a title="Permalink" class="permalink" href="#display-roles">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_rbac.xml</li><li><span class="ds-label">ID: </span>display-roles</li></ul></div></div></div></div><p>
   To see all the roles that are currently available in the deployment, an
   administrator (someone with the admin role) can run the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>openstack role list</pre></div><p>
   The output should resemble the following response:
  </p><div class="verbatim-wrap"><pre class="screen">+----------------------------------+-------------------------------------+
|                id                |                 name                |
+----------------------------------+-------------------------------------+
| 507bface531e4ac2b7019a1684df3370 |            ResellerAdmin            |
| 9fe2ff9ee4384b1894a90878d3e92bab |               member                |
| e00e9406b536470dbde2689ce1edb683 |                admin                |
| aa60501f1e664ddab72b0a9f27f96d2c |           heat_stack_user           |
| a082d27b033b4fdea37ebb2a5dc1a07b |               service               |
| 8f11f6761534407585feecb5e896922f |            swiftoperator            |
+----------------------------------+-------------------------------------+</pre></div></div><div class="sect3" id="assign-role"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Assigning a Role to a User</span> <a title="Permalink" class="permalink" href="#assign-role">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_rbac.xml</li><li><span class="ds-label">ID: </span>assign-role</li></ul></div></div></div></div><p>
   In this example, we want to add the role
   <span class="bold"><strong>ResellerAdmin</strong></span> to the demo user who has the
   ID <span class="bold"><strong>d2ff982a0b6547d0921b94957db714d6</strong></span>.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Determine which Project/Tenant the user belongs to.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>openstack user show d2ff982a0b6547d0921b94957db714d6</pre></div><p>
     The response should resemble the following output:
    </p><div class="verbatim-wrap"><pre class="screen">+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | default                          |
| enabled             | True                             |
|    id               | d2ff982a0b6547d0921b94957db714d6 |
| name                | admin                            |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+</pre></div></li><li class="step "><p>
     We need to link the ResellerAdmin Role to a Project/Tenant. To start,
     determine which tenants are available on this deployment.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>openstack project list</pre></div><p>
     The response should resemble the following output:
    </p><div class="verbatim-wrap"><pre class="screen">+----------------------------------+-------------------------------+--+
|                id                |        name       | enabled |
+----------------------------------+-------------------------------+--+
| 4a8f4207a13444089a18dc524f41b2cf |       admin       |   True  |
| 00cbaf647bf24627b01b1a314e796138 |        demo       |   True  |
| 8374761f28df43b09b20fcd3148c4a08 |        gf1        |   True  |
| 0f8a9eef727f4011a7c709e3fbe435fa |        gf2        |   True  |
| 6eff7b888f8e470a89a113acfcca87db |        gf3        |   True  |
| f0b5d86c7769478da82cdeb180aba1b0 |        jaq1       |   True  |
| a46f1127e78744e88d6bba20d2fc6e23 |        jaq2       |   True  |
| 977b9b7f9a6b4f59aaa70e5a1f4ebf0b |        jaq3       |   True  |
| 4055962ba9e44561ab495e8d4fafa41d |        jaq4       |   True  |
| 33ec7f15476545d1980cf90b05e1b5a8 |        jaq5       |   True  |
| 9550570f8bf147b3b9451a635a1024a1 |      service      |   True  |
+----------------------------------+-------------------------------+--+</pre></div></li><li class="step "><p>
     Now that we have all the pieces, we can assign the ResellerAdmin role to
     this User on the Demo project.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack role add --user d2ff982a0b6547d0921b94957db714d6 --project 00cbaf647bf24627b01b1a314e796138 507bface531e4ac2b7019a1684df3370</pre></div><p>
     This will produce no response if everything is correct.
    </p></li><li class="step "><p>
     Validate that the role has been assigned correctly. Pass in the user and
     tenant ID and request a list of roles assigned.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack role list --user d2ff982a0b6547d0921b94957db714d6 --project 00cbaf647bf24627b01b1a314e796138</pre></div><p>
     Note that all members have the <span class="emphasis"><em>member</em></span> role as a
     default role in addition to any other roles that have been assigned.
    </p><div class="verbatim-wrap"><pre class="screen">+----------------------------------+---------------+----------------------------------+----------------------------------+
|                id                |      name     |             user_id              | tenant_id             |
+----------------------------------+---------------+----------------------------------+----------------------------------+
| 507bface531e4ac2b7019a1684df3370 | ResellerAdmin | d2ff982a0b6547d0921b94957db714d6 | 00cbaf647bf24627b01b1a314e796138 |
| 9fe2ff9ee4384b1894a90878d3e92bab |    member     | d2ff982a0b6547d0921b94957db714d6 | 00cbaf647bf24627b01b1a314e796138 |
+----------------------------------+---------------+----------------------------------+----------------------------------+</pre></div></li></ol></div></div></div><div class="sect3" id="create-role"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a New Role</span> <a title="Permalink" class="permalink" href="#create-role">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_rbac.xml</li><li><span class="ds-label">ID: </span>create-role</li></ul></div></div></div></div><p>
   In this example, we will create a Level 3 Support role called
   <span class="bold"><strong>L3Support</strong></span>.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Add the new role to the list of roles.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack role create L3Support</pre></div><p>
     The response should resemble the following output:
    </p><div class="verbatim-wrap"><pre class="screen">+----------+----------------------------------+
| Property |              Value               |
+----------+----------------------------------+
|    id    | 7e77946db05645c4ba56c6c82bf3f8d2 |
|   name   |            L3Support             |
+----------+----------------------------------+</pre></div></li><li class="step "><p>
     Now that we have the new role's ID, we can add that role to the Demo user
     from the previous example.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack role add --user d2ff982a0b6547d0921b94957db714d6  --project 00cbaf647bf24627b01b1a314e796138 7e77946db05645c4ba56c6c82bf3f8d2</pre></div><p>
     This will produce no response if everything is correct.
    </p></li><li class="step "><p>
     Verify that the user Demo has both the ResellerAdmin and L3Support roles.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack role list --user d2ff982a0b6547d0921b94957db714d6 --project 00cbaf647bf24627b01b1a314e796138</pre></div></li><li class="step "><p>
     The response should resemble the following output. Note that this user has
     the L3Support role, the ResellerAdmin role, and the default member role.
    </p><div class="verbatim-wrap"><pre class="screen">+----------------------------------+---------------+----------------------------------+----------------------------------+
|                id                |      name     |             user_id              |            tenant_id             |
+----------------------------------+---------------+----------------------------------+----------------------------------+
| 7e77946db05645c4ba56c6c82bf3f8d2 |   L3Support   | d2ff982a0b6547d0921b94957db714d6 | 00cbaf647bf24627b01b1a314e796138 |
| 507bface531e4ac2b7019a1684df3370 | ResellerAdmin | d2ff982a0b6547d0921b94957db714d6 | 00cbaf647bf24627b01b1a314e796138 |
| 9fe2ff9ee4384b1894a90878d3e92bab |    member     | d2ff982a0b6547d0921b94957db714d6 | 00cbaf647bf24627b01b1a314e796138 |
+----------------------------------+---------------+----------------------------------+----------------------------------+</pre></div></li></ol></div></div></div><div class="sect3" id="access-policies"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.6.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Access Policies</span> <a title="Permalink" class="permalink" href="#access-policies">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_rbac.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_rbac.xml</li><li><span class="ds-label">ID: </span>access-policies</li></ul></div></div></div></div><p>
   Before introducing RBAC, ceilometer had very simple access control. There
   were two types of user: admins and users. Admins will be able to access any
   API and perform any operation. Users will only be able to access non-admin
   APIs and perform operations only on the Project/Tenant where they belonged.
  </p></div></div><div class="sect2" id="topic-zx2-mmd-5t"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.3.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ceilometer Metering Failover HA Support</span> <a title="Permalink" class="permalink" href="#topic-zx2-mmd-5t">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_failover_ha.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_failover_ha.xml</li><li><span class="ds-label">ID: </span>topic-zx2-mmd-5t</li></ul></div></div></div></div><p>
  In the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> environment, the ceilometer metering service supports native
  Active-Active high-availability (HA) for the notification and polling agents.
  Implementing HA support includes workload-balancing, workload-distribution
  and failover.
 </p><p>
  Tooz is the coordination engine that is used to coordinate workload among
  multiple active agent instances. It also maintains the knowledge of
  active-instance-to-handle failover and group membership using hearbeats
  (pings).
 </p><p>
  Zookeeper is used as the coordination backend. Zookeeper uses Tooz to expose
  the APIs that manage group membership and retrieve workload specific to each
  agent.
 </p><p>
  The following section in the configuration file is used to implement
  high-availability (HA):
 </p><div class="verbatim-wrap"><pre class="screen">[coordination]
backend_url = &lt;IP address of Zookeeper host: port&gt; (port is usually 2181 as a zookeeper default)
heartbeat = 1.0
check_watchers = 10.0</pre></div><p>
  For the notification agent to be configured in HA mode, additional
  configuration is needed in the configuration file:
 </p><div class="verbatim-wrap"><pre class="screen">[notification]
workload_partitioning = true</pre></div><p>
  The HA notification agent distributes workload among multiple queues that are
  created based on the number of unique source:sink combinations. The
  combinations are configured in the notification agent pipeline configuration
  file. If there are additional services to be metered using notifications,
  then the recommendation is to use a separate source for those events. This is
  recommended especially if the expected load of data from that source is
  considered high. Implementing HA support should lead to better workload
  balancing among multiple active notification agents.
 </p><p>
  ceilometer-expirer is also an Active-Active HA. Tooz is used to pick an
  expirer process that acquires a lock when there are multiple contenders and
  the winning process runs. There is no failover support, as expirer is not a
  daemon and is scheduled to run at pre-determined intervals.
 </p><div id="id-1.5.15.5.9.11" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   You must ensure that a single expirer process runs when multiple processes
   are scheduled to run at the same time. This must be done using cron-based
   scheduling. on multiple controller nodes
  </p></div><p>
  The following configuration is needed to enable expirer HA:
 </p><div class="verbatim-wrap"><pre class="screen">[coordination]
backend_url = &lt;IP address of Zookeeper host: port&gt; (port is usually 2181 as a zookeeper default)
heartbeat = 1.0
check_watchers = 10.0</pre></div><p>
  The notification agent HA support is mainly designed to coordinate among
  notification agents so that correlated samples can be handled by the same
  agent. This happens when samples get transformed from other samples. The
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> ceilometer pipeline has no transformers, so this task of coordination
  and workload partitioning does not need to be enabled. The notification agent
  is deployed on multiple controller nodes and they distribute workload among
  themselves by randomly fetching the data from the queue.
 </p><p>
  To disable coordination and workload partitioning by OpenStack, set the
  following value in the configuration file:
 </p><div class="verbatim-wrap"><pre class="screen">        [notification]
        workload_partitioning = False</pre></div><div id="id-1.5.15.5.9.17" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   When a configuration change is made to an API running under the HA Proxy,
   that change needs to be replicated in <span class="bold"><strong>all</strong></span>
   controllers.
  </p></div></div><div class="sect2" id="Ceilo-optimize"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.3.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Optimizing the Ceilometer Metering Service</span> <a title="Permalink" class="permalink" href="#Ceilo-optimize">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_bestpractice.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_bestpractice.xml</li><li><span class="ds-label">ID: </span>Ceilo-optimize</li></ul></div></div></div></div><p>
  You can improve ceilometer responsiveness by configuring metering to store
  only the data you are require. This topic provides strategies for getting the
  most out of metering while not overloading your resources.
 </p><div class="sect3" id="changing-meter-list"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Change the List of Meters</span> <a title="Permalink" class="permalink" href="#changing-meter-list">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_bestpractice.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_bestpractice.xml</li><li><span class="ds-label">ID: </span>changing-meter-list</li></ul></div></div></div></div><p>
   The list of meters can be easily reduced or increased by editing the
   pipeline.yaml file and restarting the polling agent.
  </p><p>
   Sample compute-only pipeline.yaml file with the daily poll interval:
  </p><div class="verbatim-wrap"><pre class="screen">---
sources:
    - name: meter_source
      interval: 86400
      meters:
          - "instance"
          - "memory"
          - "vcpus"
          - "compute.instance.create.end"
          - "compute.instance.delete.end"
          - "compute.instance.update"
          - "compute.instance.exists"
      sinks:
          - meter_sink
sinks:
    - name: meter_sink
      transformers:
      publishers:
          - notifier://</pre></div><div id="id-1.5.15.5.10.3.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    This change will cause all non-default meters to stop receiving
    notifications.
   </p></div></div><div class="sect3" id="ceilometer-nova"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enable Nova Notifications</span> <a title="Permalink" class="permalink" href="#ceilometer-nova">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_bestpractice.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_bestpractice.xml</li><li><span class="ds-label">ID: </span>ceilometer-nova</li></ul></div></div></div></div><p>
   You can configure nova to send notifications by enabling the setting in the
   configuration file. When enabled, nova will send information to ceilometer
   related to its usage and VM status. You must restart nova for these changes
   to take effect.
  </p><p>
   The Openstack notification daemon, also known as a polling agent, monitors
   the message bus for data being provided by other OpenStack components such
   as nova. The notification daemon loads one or more listener plugins, using
   the <code class="literal">ceilometer.notification</code> namespace. Each plugin can
   listen to any topic, but by default it will listen to the
   <code class="literal">notifications.info</code> topic. The listeners grab messages off
   the defined topics and redistribute them to the appropriate plugins
   (endpoints) to be processed into Events and Samples. After the nova service
   is restarted, you should verify that the notification daemons are receiving
   traffic.
  </p><p>
   For a more in-depth look at how information is sent over
   <span class="emphasis"><em>openstack.common.rpc</em></span>, refer to the
   <a class="link" href="http://docs.openstack.org/developer/ceilometer/measurements.html" target="_blank">OpenStack
   ceilometer documentation</a>.
  </p><p>
   nova can be configured to send following data to ceilometer:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col1" /><col class="col2" /><col class="col3" /><col class="col4" /><col class="col5" /></colgroup><tbody><tr><td><span class="bold"><strong>Name</strong></span>
      </td><td><span class="bold"><strong>Unit</strong></span>
      </td><td><span class="bold"><strong>Type</strong></span>
      </td><td><span class="bold"><strong>Resource</strong></span>
      </td><td><span class="bold"><strong>Note</strong></span>
      </td></tr><tr><td>instance</td><td>g</td><td>instance</td><td> inst ID</td><td>Existence of instance</td></tr><tr><td>instance: <code class="varname">type</code>
      </td><td>g</td><td>instance</td><td> inst ID</td><td>Existence of instance of <code class="varname">type</code> (Where
                                    <code class="varname">type</code> is a valid OpenStack type.) </td></tr><tr><td>memory</td><td>g</td><td>MB</td><td> inst ID</td><td>Amount of allocated RAM. Measured in MB.</td></tr><tr><td>vcpus</td><td>g</td><td>vcpu</td><td> inst ID</td><td>Number of VCPUs</td></tr><tr><td>disk.root.size</td><td>g</td><td>GB</td><td> inst ID</td><td>Size of root disk. Measured in GB.</td></tr><tr><td>disk.ephemeral.size</td><td>g</td><td>GB</td><td> inst ID</td><td>Size of ephemeral disk. Measured in GB.</td></tr></tbody></table></div><p>
   To enable nova to publish notifications:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     In a text editor, open the following file:
    </p><div class="verbatim-wrap"><pre class="screen">nova.conf</pre></div></li><li class="listitem "><p>
     Compare the example of a working configuration file with the necessary
     changes to your configuration file. If there is anything missing in your
     file, add it, and then save the file.
    </p><div class="verbatim-wrap"><pre class="screen">notification_driver=messaging
notification_topics=notifications
notify_on_state_change=vm_and_task_state
instance_usage_audit=True
instance_usage_audit_period=hour</pre></div><div id="id-1.5.15.5.10.4.8.2.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      The <code class="literal">instance_usage_audit_period</code> interval can be set to
      check the instance's status every hour, once a day, once a week or once a
      month. Every time the audit period elapses, nova sends a notification to
      ceilometer to record whether or not the instance is alive and running.
      Metering this statistic is critical if billing depends on usage.
     </p></div></li><li class="listitem "><p>
     To restart nova service, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl restart nova-api.service
<code class="prompt user">tux &gt; </code>sudo systemctl restart nova-conductor.service
<code class="prompt user">tux &gt; </code>sudo systemctl restart nova-scheduler.service
<code class="prompt user">tux &gt; </code>sudo systemctl restart nova-novncproxy.service</pre></div><div id="id-1.5.15.5.10.4.8.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      Different platforms may use their own unique command to restart
      nova-compute services. If the above command does not work, please refer
      to the documentation for your specific platform.
     </p></div></li><li class="listitem "><p>
     To verify successful launch of each process, list the service components:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>openstack compute service list
+----+------------------+------------+----------+---------+-------+----------------------------+-----------------+
| Id | Binary           | Host       | Zone     | Status  | State | Updated_at                 | Disabled Reason |
+----+------------------+------------+----------+---------+-------+----------------------------+-----------------+
| 1  | nova-conductor   | controller | internal | enabled | up    | 2014-09-16T23:54:02.000000 | -               |
| 3  | nova-scheduler   | controller | internal | enabled | up    | 2014-09-16T23:54:07.000000 | -               |
| 4  | nova-cert        | controller | internal | enabled | up    | 2014-09-16T23:54:00.000000 | -               |
| 5  | nova-compute     | compute1   | nova     | enabled | up    | 2014-09-16T23:54:06.000000 | -               |
+----+------------------+------------+----------+---------+-------+----------------------------+-----------------+</pre></div></li></ol></div></div><div class="sect3" id="webserverapi"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.8.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Improve Reporting API Responsiveness</span> <a title="Permalink" class="permalink" href="#webserverapi">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_bestpractice.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_bestpractice.xml</li><li><span class="ds-label">ID: </span>webserverapi</li></ul></div></div></div></div><p>
   Reporting APIs are the main access to the metering data stored in
   ceilometer. These APIs are accessed by horizon to provide basic usage data
   and information.
  </p><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> uses Apache2 Web Server to provide the API access. This topic
   provides some strategies to help you optimize the front-end and back-end
   databases.
  </p><p>
   To improve the responsiveness you can increase the number of threads and
   processes in the ceilometer configuration file. Each process can have a
   certain amount of threads managing the filters and applications, which can
   comprise the processing pipeline.
  </p><p>
   <span class="bold"><strong>To configure Apache2 to use increase the number of
   threads</strong></span>, use the steps in <a class="xref" href="#reconfig-metering" title="13.3.4. Configure the Ceilometer Metering Service">Section 13.3.4, “Configure the Ceilometer Metering Service”</a>
  </p><div id="id-1.5.15.5.10.5.6" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
    The resource usage panel could take some time to load depending on the
    number of metrics selected.
   </p></div></div><div class="sect3" id="update-polling-strategy"><div class="titlepage"><div><div><h4 class="title"><span class="number">13.3.8.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Update the Polling Strategy and Swift Considerations</span> <a title="Permalink" class="permalink" href="#update-polling-strategy">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_bestpractice.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_bestpractice.xml</li><li><span class="ds-label">ID: </span>update-polling-strategy</li></ul></div></div></div></div><p>
   Polling can put an excessive amount of strain on the system due to the
   amount of data the system may have to process. Polling also has a severe
   impact on queries since the database can have very large amount of data to
   scan before responding to the query. This process usually consumes a large
   amount of CPU and memory to complete the requests. Clients can also
   experience long waits for queries to come back and, in extreme cases, even
   timeout.
  </p><p>
   There are 3 polling meters in swift:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     storage.objects
    </p></li><li class="listitem "><p>
     storage.objects.size
    </p></li><li class="listitem "><p>
     storage.objects.containers
    </p></li></ul></div><p>
   Sample section of the pipeline.yaml configuration file with swift polling on
   an hourly interval:
  </p><div class="verbatim-wrap"><pre class="screen">---
sources:
    - name: swift_source
      interval: 3600
      sources:
            meters:
          - "storage.objects"
          - "storage.objects.size"
          - "storage.objects.containers"
sinks:
    - name: meter_sink
      transformers:
      publishers:
          - notifier://</pre></div><p>
   Every time the polling interval occurs, at least 3 messages per existing
   object/container in swift are collected. The following table illustrates the
   amount of data produced hourly in different scenarios:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /><col /><col /></colgroup><tbody><tr><td>swift Containers</td><td>swift Objects per container</td><td>Samples per Hour</td><td>Samples stored per 24 hours</td></tr><tr><td>10</td><td>10</td><td>500</td><td>12000</td></tr><tr><td>10</td><td>100</td><td>5000</td><td>120000</td></tr><tr><td>100</td><td>100</td><td>50000</td><td>1200000</td></tr><tr><td>100</td><td>1000</td><td>500000</td><td>12000000</td></tr></tbody></table></div><p>
   Looking at the data we can see that even a very small swift storage with 10
   containers and 100 files will store 120K samples in 24 hours, bringing it to
   a total of 3.6 million samples.
  </p><div id="id-1.5.15.5.10.6.10" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    The file size of each file does not have any impact on the number of samples
    collected. In fact the smaller the number of containers or files, the
    smaller the sample size. In the scenario where there a large number of small
    files and containers, the sample size is also large and the performance is
    at its worst.
   </p></div></div></div><div class="sect2" id="ceilo-samples"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.3.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Metering Service Samples</span> <a title="Permalink" class="permalink" href="#ceilo-samples">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/metering-metering_samples.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>metering-metering_samples.xml</li><li><span class="ds-label">ID: </span>ceilo-samples</li></ul></div></div></div></div><p>
  Samples are discrete collections of a particular meter or the actual usage
  data defined by a meter description. Each sample is time-stamped and includes
  a variety of data that varies per meter but usually includes the project ID
  and UserID of the entity that consumed the resource represented by the meter
  and sample.
 </p><p>
  In a typical deployment, the number of samples can be in the tens of
  thousands if not higher for a specific collection period depending on overall
  activity.
 </p><p>
  Sample collection and data storage expiry settings are configured in
  ceilometer. Use cases that include collecting data for monthly billing cycles
  are usually stored over a period of 45 days and require a large, scalable,
  back-end database to support the large volume of samples generated by
  production OpenStack deployments.
 </p><p>
  <span class="bold"><strong>Example configuration:</strong></span>
 </p><div class="verbatim-wrap"><pre class="screen">[database]
metering_time_to_live=-1</pre></div><p>
  In our example use case, to construct a complete billing record, an external
  billing application must collect all pertinent samples. Then the results must
  be sorted, summarized, and combine with the results of other types of metered
  samples that are required. This function is known as aggregation and is
  external to the ceilometer service.
 </p><p>
  Meter data, or samples, can also be collected directly from the service APIs
  by individual ceilometer polling agents. These polling agents directly access
  service usage by calling the API of each service.
 </p><p>
  OpenStack services such as swift currently only provide metered data through
  this function and some of the other OpenStack services provide specific
  metrics only through a polling action.
 </p></div></div></div><div class="chapter " id="using-container-as-a-service-overview"><div class="titlepage"><div><div><h1 class="title"><span class="number">14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Container as a Service (Magnum)</span> <a title="Permalink" class="permalink" href="#using-container-as-a-service-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-managing_magnum.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing_magnum.xml</li><li><span class="ds-label">ID: </span>using-container-as-a-service-overview</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#deploying-kubernetes-fedora-atomic"><span class="number">14.1 </span><span class="name">Deploying a Kubernetes Cluster on Fedora Atomic</span></a></span></dt><dt><span class="section"><a href="#deploying-kubernetes-coreos"><span class="number">14.2 </span><span class="name">Deploying a Kubernetes Cluster on CoreOS</span></a></span></dt><dt><span class="section"><a href="#deploying-docker-fedora-atomic"><span class="number">14.3 </span><span class="name">Deploying a Docker Swarm Cluster on Fedora Atomic</span></a></span></dt><dt><span class="section"><a href="#deploying-apache-mesos-ubuntu"><span class="number">14.4 </span><span class="name">Deploying an Apache Mesos Cluster on Ubuntu</span></a></span></dt><dt><span class="section"><a href="#create-magnum-cluster"><span class="number">14.5 </span><span class="name">Creating a Magnum Cluster with the Dashboard</span></a></span></dt></dl></div></div><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Magnum Service provides container orchestration engines such as
  Docker Swarm, Kubernetes, and Apache Mesos available as first class
  resources. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Magnum uses heat to orchestrate an OS image which
  contains Docker and Kubernetes and runs that image in either virtual machines
  or bare metal in a cluster configuration.
 </p><div class="sect1" id="deploying-kubernetes-fedora-atomic"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying a Kubernetes Cluster on Fedora Atomic</span> <a title="Permalink" class="permalink" href="#deploying-kubernetes-fedora-atomic">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-magnum-deploying_kubernetes_fedora_atomic.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-magnum-deploying_kubernetes_fedora_atomic.xml</li><li><span class="ds-label">ID: </span>deploying-kubernetes-fedora-atomic</li></ul></div></div></div></div><div class="sect2" id="idg-all-userguide-container-service-deploying-kubernetes-fedora-atomic-xml-6"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#idg-all-userguide-container-service-deploying-kubernetes-fedora-atomic-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-magnum-deploying_kubernetes_fedora_atomic.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-magnum-deploying_kubernetes_fedora_atomic.xml</li><li><span class="ds-label">ID: </span>idg-all-userguide-container-service-deploying-kubernetes-fedora-atomic-xml-6</li></ul></div></div></div></div><p>
   These steps assume the following have been completed:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The Magnum service has been installed. For more information, see
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 26 “Magnum Overview”, Section 26.2 “Install the Magnum Service”</span>.
    </p></li><li class="listitem "><p>
     Deploying a Kubernetes Cluster on Fedora Atomic requires the Fedora Atomic
     image <span class="bold"><strong>fedora-atomic-26-20170723.0.x86_64.qcow2</strong></span> prepared
     specifically for the OpenStack release. You can download the
     <span class="bold"><strong>fedora-atomic-26-20170723.0.x86_64.qcow2</strong></span>
     image from
     <a class="link" href="https://fedorapeople.org/groups/magnum/" target="_blank">https://fedorapeople.org/groups/magnum/</a>
    </p></li></ul></div></div><div class="sect2" id="idg-all-userguide-container-service-deploying-kubernetes-fedora-atomic-xml-7"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating the Cluster</span> <a title="Permalink" class="permalink" href="#idg-all-userguide-container-service-deploying-kubernetes-fedora-atomic-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-magnum-deploying_kubernetes_fedora_atomic.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-magnum-deploying_kubernetes_fedora_atomic.xml</li><li><span class="ds-label">ID: </span>idg-all-userguide-container-service-deploying-kubernetes-fedora-atomic-xml-7</li></ul></div></div></div></div><p>
   The following example is created using Kubernetes Container Orchestration
   Engine (COE) running on Fedora Atomic guest OS on <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> VMs.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     As <span class="bold"><strong>stack</strong></span> user, login to the lifecycle
     manager.
    </p></li><li class="listitem "><p>
     Source openstack admin credentials.
    </p><div class="verbatim-wrap"><pre class="screen">$ source service.osrc</pre></div></li><li class="listitem "><p>
     If you haven't already, download Fedora Atomic image, prepared for the
     Openstack Pike release.
    </p><div class="verbatim-wrap"><pre class="screen">$ wget https://download.fedoraproject.org/pub/alt/atomic/stable/Fedora-Atomic-26-20170723.0/CloudImages/x86_64/images/Fedora-Atomic-26-20170723.0.x86_64.qcow2</pre></div></li><li class="listitem "><p>
     Create a glance image.
    </p><div class="verbatim-wrap"><pre class="screen">$ openstack image create --name fedora-atomic-26-20170723.0.x86_64 --visibility public \
  --disk-format qcow2 --os-distro fedora-atomic --container-format bare \
  --file Fedora-Atomic-26-20170723.0.x86_64.qcow2 --progress
[=============================&gt;] 100%
+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | 9d233b8e7fbb7ea93f20cc839beb09ab     |
| container_format | bare                                 |
| created_at       | 2017-04-10T21:13:48Z                 |
| disk_format      | qcow2                                |
| id               | 4277115a-f254-46c0-9fb0-fffc45d2fd38 |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | fedora-atomic-26-20170723.0.x86_64   |
| os_distro        | fedora-atomic                        |
| owner            | 2f5b83ab49d54aaea4b39f5082301d09     |
| protected        | False                                |
| size             | 515112960                            |
| status           | active                               |
| tags             | []                                   |
| updated_at       | 2017-04-10T21:13:56Z                 |
| virtual_size     | None                                 |
| visibility       | public                               |
+------------------+--------------------------------------+</pre></div></li><li class="listitem "><p>
     Create a nova keypair.
    </p><div class="verbatim-wrap"><pre class="screen">$ test -f ~/.ssh/id_rsa.pub || ssh-keygen -t rsa -N "" -f ~/.ssh/id_rsa
$ openstack keypair create --pub-key ~/.ssh/id_rsa.pub testkey</pre></div></li><li class="listitem "><p>
     Create a Magnum cluster template.
    </p><div class="verbatim-wrap"><pre class="screen">$ magnum cluster-template-create --name my-template \
  --image-id 4277115a-f254-46c0-9fb0-fffc45d2fd38 \
  --keypair-id testkey \
  --external-network-id ext-net \
  --dns-nameserver 8.8.8.8 \
  --flavor-id m1.small \
  --docker-volume-size 5 \
  --network-driver flannel \
  --coe kubernetes \
  --http-proxy http://proxy.yourcompany.net:8080/ \
  --https-proxy http://proxy.yourcompany.net:8080/</pre></div><div id="id-1.5.16.3.3.3.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
        Use the <span class="emphasis"><em>image_id</em></span> from <code class="literal">openstack image
        create</code> command output in the previous step.
       </p></li><li class="listitem "><p>
        Use your organization's DNS server. If the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> public endpoint is
        configured with the hostname, this server should provide resolution for
        this hostname.
       </p></li><li class="listitem "><p>
        The proxy is only needed if public internet (for example,
        <code class="literal">https://discovery.etcd.io/</code> or
        <code class="literal">https://gcr.io/</code>) is not accessible without proxy.
       </p></li></ol></div></div></li><li class="listitem "><p>
     Create cluster. The command below will create a minimalistic cluster
     consisting of a single Kubernetes Master (kubemaster) and single
     Kubernetes Node (worker, kubeminion).
    </p><div class="verbatim-wrap"><pre class="screen">$ magnum cluster-create --name my-cluster --cluster-template my-template --node-count 1 --master-count 1</pre></div></li><li class="listitem "><p>
     Immediately after issuing <code class="literal">cluster-create</code> command,
     cluster status should turn to
     <span class="bold"><strong>CREATE_IN_PROGRESS</strong></span> and stack_id assigned.
    </p><div class="verbatim-wrap"><pre class="screen">$ magnum cluster-show my-cluster
+---------------------+------------------------------------------------------------+
| Property            | Value                                                      |
+---------------------+------------------------------------------------------------+
| status              | CREATE_IN_PROGRESS                                         |
| cluster_template_id | 245c6bf8-c609-4ea5-855a-4e672996cbbc                       |
| uuid                | 0b78a205-8543-4589-8344-48b8cfc24709                       |
| stack_id            | 22385a42-9e15-49d9-a382-f28acef36810                       |
| status_reason       | -                                                          |
| created_at          | 2017-04-10T21:25:11+00:00                                  |
| name                | my-cluster                                                 |
| updated_at          | -                                                          |
| discovery_url       | https://discovery.etcd.io/193d122f869c497c2638021eae1ab0f7 |
| api_address         | -                                                          |
| coe_version         | -                                                          |
| master_addresses    | []                                                         |
| create_timeout      | 60                                                         |
| node_addresses      | []                                                         |
| master_count        | 1                                                          |
| container_version   | -                                                          |
| node_count          | 1                                                          |
+---------------------+------------------------------------------------------------+</pre></div></li><li class="listitem "><p>
     You can monitor cluster creation progress by listing the resources of the
     heat stack. Use the <code class="literal">stack_id</code> value from the
     <code class="literal">magnum cluster-status</code> output above in the following
     command:
    </p><div class="verbatim-wrap"><pre class="screen">$ heat resource-list -n2 22385a42-9e15-49d9-a382-f28acef36810
WARNING (shell) "heat resource-list" is deprecated, please use "openstack stack resource list" instead
+-------------------------------+--------------------------------------+-----------------------------------+--------------------+----------------------+-------------------------+
| resource_name                 | physical_resource_id                 | resource_type                     | resource_status    | updated_time         | stack_name              |
+-------------------------------+--------------------------------------+-----------------------------------+--------------------+----------------------+-------------------------+
| api_address_floating_switch   | 06b2cc0d-77f9-4633-8d96-f51e2db1faf3 | Magnum::FloatingIPAddressSwitcher | CREATE_COMPLETE    | 2017-04-10T21:25:10Z | my-cluster-z4aquda2mgpv |
| api_address_lb_switch         | 965124ca-5f62-4545-bbae-8d9cda7aff2e | Magnum::ApiGatewaySwitcher        | CREATE_COMPLETE    | 2017-04-10T21:25:10Z | my-cluster-z4aquda2mgpv |
. . .</pre></div></li><li class="listitem "><p>
     The cluster is complete when all resources show
     <span class="bold"><strong>CREATE_COMPLETE</strong></span>.
    </p></li><li class="listitem "><p>
     Install kubectl onto your Cloud Lifecycle Manager.
    </p><div class="verbatim-wrap"><pre class="screen">$ export https_proxy=http://proxy.yourcompany.net:8080
$ wget https://storage.googleapis.com/kubernetes-release/release/v1.2.0/bin/linux/amd64/kubectl
$ chmod +x ./kubectl
$ sudo mv ./kubectl /usr/local/bin/kubectl</pre></div></li><li class="listitem "><p>
     Generate the cluster configuration using
     <code class="command">magnum cluster-config</code>. If the CLI option
     <code class="option">--tls-disabled</code> was not
     specified during cluster template creation, authentication in the cluster
     will be turned on. In this case, <code class="command">magnum cluster-config</code>
     command will generate client authentication certificate
     (<code class="filename">cert.pem</code>) and key (<code class="filename">key.pem</code>).
     Copy and paste <code class="command">magnum cluster-config</code> output
     to your command line input to finalize configuration (that is, export
     KUBECONFIG environment variable).
    </p><div class="verbatim-wrap"><pre class="screen">$ mkdir my_cluster
$ cd my_cluster
/my_cluster $ ls
/my_cluster $ magnum cluster-config my-cluster
export KUBECONFIG=./config
/my_cluster $ ls
ca.pem cert.pem config key.pem
/my_cluster $ export KUBECONFIG=./config
/my_cluster $ kubectl version
Client Version: version.Info{Major:"1", Minor:"2", GitVersion:"v1.2.0", GitCommit:"5cb86ee022267586db386f62781338b0483733b3", GitTreeState:"clean"}
Server Version: version.Info{Major:"1", Minor:"2", GitVersion:"v1.2.0", GitCommit:"cffae0523cfa80ddf917aba69f08508b91f603d5", GitTreeState:"clean"}</pre></div></li><li class="listitem "><p>
     Create a simple Nginx replication controller, exposed as a service of type
     NodePort.
    </p><div class="verbatim-wrap"><pre class="screen">$ cat &gt;nginx.yml &lt;&lt;-EOF
apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx-controller
spec:
  replicas: 1
  selector:
    app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx
          ports:
            - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  type: NodePort
  ports:
  - port: 80
    nodePort: 30080
  selector:
    app: nginx
EOF

$ kubectl create -f nginx.yml</pre></div></li><li class="listitem "><p>
     Check pod status until it turns from
     <span class="bold"><strong>Pending</strong></span> to
     <span class="bold"><strong>Running</strong></span>.
    </p><div class="verbatim-wrap"><pre class="screen">$ kubectl get pods
NAME                      READY    STATUS     RESTARTS    AGE
nginx-controller-5cmev    1/1      Running    0           2m</pre></div></li><li class="listitem "><p>
     Ensure that the Nginx welcome page is displayed at port 30080 using the
     kubemaster floating IP.
    </p><div class="verbatim-wrap"><pre class="screen">$ http_proxy= curl http://172.31.0.6:30080
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;</pre></div></li></ol></div></div></div><div class="sect1" id="deploying-kubernetes-coreos"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying a Kubernetes Cluster on CoreOS</span> <a title="Permalink" class="permalink" href="#deploying-kubernetes-coreos">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-magnum-deploying_kubernetes_coreos.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-magnum-deploying_kubernetes_coreos.xml</li><li><span class="ds-label">ID: </span>deploying-kubernetes-coreos</li></ul></div></div></div></div><div class="sect2" id="idg-all-userguide-container-service-deploying-kubernetes-coreos-xml-6"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#idg-all-userguide-container-service-deploying-kubernetes-coreos-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-magnum-deploying_kubernetes_coreos.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-magnum-deploying_kubernetes_coreos.xml</li><li><span class="ds-label">ID: </span>idg-all-userguide-container-service-deploying-kubernetes-coreos-xml-6</li></ul></div></div></div></div><p>
   These steps assume the following have been completed:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The Magnum service has been installed. For more information, see
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 26 “Magnum Overview”, Section 26.2 “Install the Magnum Service”</span>.
    </p></li><li class="listitem "><p>
     Creating the Magnum cluster requires the CoreOS image for OpenStack. You
     can download compressed image file
     <span class="bold"><strong>coreos_production_openstack_image.img.bz2</strong></span>
     from
     <a class="link" href="http://stable.release.core-os.net/amd64-usr/current/" target="_blank">http://stable.release.core-os.net/amd64-usr/current/</a>.
    </p></li></ul></div></div><div class="sect2" id="idg-all-userguide-container-service-deploying-kubernetes-coreos-xml-7"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating the Cluster</span> <a title="Permalink" class="permalink" href="#idg-all-userguide-container-service-deploying-kubernetes-coreos-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-magnum-deploying_kubernetes_coreos.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-magnum-deploying_kubernetes_coreos.xml</li><li><span class="ds-label">ID: </span>idg-all-userguide-container-service-deploying-kubernetes-coreos-xml-7</li></ul></div></div></div></div><p>
   The following example is created using Kubernetes Container Orchestration
   Engine (COE) running on CoreOS guest OS on <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> VMs.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Login to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Source openstack admin credentials.
    </p><div class="verbatim-wrap"><pre class="screen">$ source service.osrc</pre></div></li><li class="listitem "><p>
     If you haven't already, download CoreOS image that is compatible for the OpenStack
     release.
    </p><div id="id-1.5.16.4.3.3.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The https_proxy is only needed if your environment requires a proxy.
     </p></div><div class="verbatim-wrap"><pre class="screen">$ export https_proxy=http://proxy.yourcompany.net:8080
$ wget https://stable.release.core-os.net/amd64-usr/current/coreos_production_openstack_image.img.bz2
$ bunzip2 coreos_production_openstack_image.img.bz2</pre></div></li><li class="listitem "><p>
     Create a glance image.
    </p><div class="verbatim-wrap"><pre class="screen">$ openstack image create --name coreos-magnum --visibility public \
  --disk-format raw --os-distro coreos --container-format bare \
  --file coreos_production_openstack_image.img --progress
[=============================&gt;] 100%
+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | 4110469bb15af72ec0cf78c2da4268fa     |
| container_format | bare                                 |
| created_at       | 2017-04-25T18:10:52Z                 |
| disk_format      | raw                                  |
| id               | c25fc719-2171-437f-9542-fcb8a534fbd1 |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | coreos-magnum                        |
| os_distro        | coreos                               |
| owner            | 2f5b83ab49d54aaea4b39f5082301d09     |
| protected        | False                                |
| size             | 806551552                            |
| status           | active                               |
| tags             | []                                   |
| updated_at       | 2017-04-25T18:11:07Z                 |
| virtual_size     | None                                 |
| visibility       | public                               |
+------------------+--------------------------------------+</pre></div></li><li class="listitem "><p>
     Create a nova keypair.
    </p><div class="verbatim-wrap"><pre class="screen">$ test -f ~/.ssh/id_rsa.pub || ssh-keygen -t rsa -N "" -f ~/.ssh/id_rsa
$ openstack keypair create --pub-key ~/.ssh/id_rsa.pub testkey</pre></div></li><li class="listitem "><p>
     Create a Magnum cluster template.
    </p><div class="verbatim-wrap"><pre class="screen">$ magnum cluster-template-create --name my-coreos-template \
  --image-id c25fc719-2171-437f-9542-fcb8a534fbd1 \
  --keypair-id testkey \
  --external-network-id ext-net \
  --dns-nameserver 8.8.8.8 \
  --flavor-id m1.small \
  --docker-volume-size 5 \
  --network-driver flannel \
  --coe kubernetes \
  --http-proxy http://proxy.yourcompany.net:8080/ \
  --https-proxy http://proxy.yourcompany.net:8080/</pre></div><div id="id-1.5.16.4.3.3.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
         Use the <span class="emphasis"><em>image_id</em></span> from
         <code class="literal">openstack image create</code> command output in the
         previous step.
        </p></li><li class="listitem "><p>
         Use your organization's DNS server. If the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> public endpoint
         is configured with the hostname, this server should provide
         resolution for this hostname.
        </p></li><li class="listitem "><p>
         The proxy is only needed if public internet (for example,
         <code class="literal">https://discovery.etcd.io/</code> or
         <code class="literal">https://gcr.io/</code>) is not accessible without proxy.
        </p></li></ol></div></div></li><li class="listitem "><p>
     Create cluster. The command below will create a minimalistic cluster
     consisting of a single Kubernetes Master (kubemaster) and single
     Kubernetes Node (worker, kubeminion).
    </p><div class="verbatim-wrap"><pre class="screen">$ magnum cluster-create --name my-coreos-cluster --cluster-template my-coreos-template --node-count 1 --master-count 1</pre></div></li><li class="listitem "><p>
     Almost immediately after issuing <code class="literal">cluster-create</code>
     command, cluster status should turn to
     <span class="bold"><strong>CREATE_IN_PROGRESS</strong></span> and stack_id assigned.
    </p><div class="verbatim-wrap"><pre class="screen">$ magnum cluster-show my-coreos-cluster
+---------------------+------------------------------------------------------------+
| Property            | Value                                                      |
+---------------------+------------------------------------------------------------+
| status              | CREATE_IN_PROGRESS                                         |
| cluster_template_id | c48fa7c0-8dd9-4da4-b599-9e62dc942ca5                       |
| uuid                | 6b85e013-f7c3-4fd3-81ea-4ea34201fd45                       |
| stack_id            | c93f873a-d563-4721-9bd9-3bae2340750a                       |
| status_reason       | -                                                          |
| created_at          | 2017-04-25T22:38:43+00:00                                  |
| name                | my-coreos-cluster                                          |
| updated_at          | -                                                          |
| discovery_url       | https://discovery.etcd.io/6e4c0e5ff5e5b9872173d06880886a0c |
| api_address         | -                                                          |
| coe_version         | -                                                          |
| master_addresses    | []                                                         |
| create_timeout      | 60                                                         |
| node_addresses      | []                                                         |
| master_count        | 1                                                          |
| container_version   | -                                                          |
| node_count          | 1                                                          |
+---------------------+------------------------------------------------------------+</pre></div></li><li class="listitem "><p>
     You can monitor cluster creation progress by listing the resources of the
     heat stack. Use the <code class="literal">stack_id</code> value from the
     <code class="literal">magnum cluster-status</code> output above in the following
     command:
    </p><div class="verbatim-wrap"><pre class="screen">$ heat resource-list -n2 c93f873a-d563-4721-9bd9-3bae2340750a
WARNING (shell) "heat resource-list" is deprecated, please use "openstack stack resource list" instead
+--------------------------------+-------------------------------------------------------------------------------------+-------------------------------------------------------------------
----------------------------------------------------------------+--------------------+----------------------+-------------------------------------------------------------------------+
| resource_name                  | physical_resource_id                                                                | resource_type
                                                                | resource_status    | updated_time         | stack_name                                                              |
+--------------------------------+-------------------------------------------------------------------------------------+-------------------------------------------------------------------
----------------------------------------------------------------+--------------------+----------------------+-------------------------------------------------------------------------+
| api_address_switch             |                                                                                     | Magnum::ApiGatewaySwitcher
                                                                | INIT_COMPLETE      | 2017-04-25T22:38:42Z | my-coreos-cluster-mscybll54eoj                                          |
. . .</pre></div></li><li class="listitem "><p>
     The cluster is complete when all resources show
     <span class="bold"><strong>CREATE_COMPLETE</strong></span>.
    </p></li><li class="listitem "><p>
     Install kubectl onto your Cloud Lifecycle Manager.
    </p><div class="verbatim-wrap"><pre class="screen">$ export https_proxy=http://proxy.yourcompany.net:8080
$ wget https://storage.googleapis.com/kubernetes-release/release/v1.2.0/bin/linux/amd64/kubectl
$ chmod +x ./kubectl
$ sudo mv ./kubectl /usr/local/bin/kubectl</pre></div></li><li class="listitem "><p>
     Generate the cluster configuration using
     <code class="command">magnum cluster-config</code>. If the CLI option
     <code class="option">--tls-disabled</code> was not
     specified during cluster template creation, authentication in the cluster
     will be turned on. In this case, <code class="command">magnum cluster-config</code>
     command will generate client authentication certificate
     (<code class="filename">cert.pem</code>) and key (<code class="filename">key.pem</code>).
     Copy and paste <code class="command">magnum cluster-config</code> output
     to your command line input to finalize configuration (that is, export
     KUBECONFIG environment variable).
    </p><div class="verbatim-wrap"><pre class="screen">$ mkdir my_cluster
$ cd my_cluster
/my_cluster $ ls
/my_cluster $ magnum cluster-config my-cluster
export KUBECONFIG=./config
/my_cluster $ ls
ca.pem cert.pem config key.pem
/my_cluster $ export KUBECONFIG=./config
/my_cluster $ kubectl version
Client Version: version.Info{Major:"1", Minor:"2", GitVersion:"v1.2.0", GitCommit:"5cb86ee022267586db386f62781338b0483733b3", GitTreeState:"clean"}
Server Version: version.Info{Major:"1", Minor:"2", GitVersion:"v1.2.0", GitCommit:"cffae0523cfa80ddf917aba69f08508b91f603d5", GitTreeState:"clean"}</pre></div></li><li class="listitem "><p>
     Create a simple Nginx replication controller, exposed as a service of type
     NodePort.
    </p><div class="verbatim-wrap"><pre class="screen">$ cat &gt;nginx.yml &lt;&lt;-EOF
apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx-controller
spec:
  replicas: 1
  selector:
    app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx
          ports:
            - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  type: NodePort
  ports:
  - port: 80
    nodePort: 30080
  selector:
    app: nginx
EOF

$ kubectl create -f nginx.yml</pre></div></li><li class="listitem "><p>
     Check pod status until it turns from
     <span class="bold"><strong>Pending</strong></span> to
     <span class="bold"><strong>Running</strong></span>.
    </p><div class="verbatim-wrap"><pre class="screen">$ kubectl get pods
NAME                      READY    STATUS     RESTARTS    AGE
nginx-controller-5cmev    1/1      Running    0           2m</pre></div></li><li class="listitem "><p>
     Ensure that the Nginx welcome page is displayed at port 30080 using the
     kubemaster floating IP.
    </p><div class="verbatim-wrap"><pre class="screen">$ http_proxy= curl http://172.31.0.6:30080
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;</pre></div></li></ol></div></div></div><div class="sect1" id="deploying-docker-fedora-atomic"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying a Docker Swarm Cluster on Fedora Atomic</span> <a title="Permalink" class="permalink" href="#deploying-docker-fedora-atomic">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-magnum-deploying_docker_fedora_atomic.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-magnum-deploying_docker_fedora_atomic.xml</li><li><span class="ds-label">ID: </span>deploying-docker-fedora-atomic</li></ul></div></div></div></div><div class="sect2" id="idg-all-userguide-container-service-deploying-docker-fedora-atomic-xml-6"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#idg-all-userguide-container-service-deploying-docker-fedora-atomic-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-magnum-deploying_docker_fedora_atomic.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-magnum-deploying_docker_fedora_atomic.xml</li><li><span class="ds-label">ID: </span>idg-all-userguide-container-service-deploying-docker-fedora-atomic-xml-6</li></ul></div></div></div></div><p>
   These steps assume the following have been completed:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The Magnum service has been installed. For more information, see
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 26 “Magnum Overview”, Section 26.2 “Install the Magnum Service”</span>.
    </p></li><li class="listitem "><p>
     Deploying a Docker Swarm Cluster on Fedora Atomic requires the Fedora
     Atomic image
     <span class="bold"><strong>fedora-atomic-26-20170723.0.x86_64.qcow2</strong></span>
     prepared specifically for the OpenStack Pike release. You can download
     the <span class="bold"><strong>fedora-atomic-26-20170723.0.x86_64.qcow2</strong></span>
     image from
     <a class="link" href="https://download.fedoraproject.org/pub/alt/atomic/stable/Fedora-Atomic-26-20170723.0/CloudImages/x86_64/" target="_blank">https://download.fedoraproject.org/pub/alt/atomic/stable/Fedora-Atomic-26-20170723.0/CloudImages/x86_64/</a>
    </p></li></ul></div></div><div class="sect2" id="idg-all-userguide-container-service-deploying-docker-fedora-atomic-xml-7"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating the Cluster</span> <a title="Permalink" class="permalink" href="#idg-all-userguide-container-service-deploying-docker-fedora-atomic-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-magnum-deploying_docker_fedora_atomic.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-magnum-deploying_docker_fedora_atomic.xml</li><li><span class="ds-label">ID: </span>idg-all-userguide-container-service-deploying-docker-fedora-atomic-xml-7</li></ul></div></div></div></div><p>
   The following example is created using Kubernetes Container Orchestration
   Engine (COE) running on Fedora Atomic guest OS on <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> VMs.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     As <span class="bold"><strong>stack</strong></span> user, login to the lifecycle
     manager.
    </p></li><li class="listitem "><p>
     Source openstack admin credentials.
    </p><div class="verbatim-wrap"><pre class="screen">$ source service.osrc</pre></div></li><li class="listitem "><p>
     If you haven't already, download Fedora Atomic image, prepared for
     Openstack Pike release.
    </p><div class="verbatim-wrap"><pre class="screen">$ wget https://download.fedoraproject.org/pub/alt/atomic/stable/Fedora-Atomic-26-20170723.0/CloudImages/x86_64/images/Fedora-Atomic-26-20170723.0.x86_64.qcow2</pre></div></li><li class="listitem "><p>
     Create a glance image.
    </p><div class="verbatim-wrap"><pre class="screen">$ openstack image create --name fedora-atomic-26-20170723.0.x86_64 --visibility public \
  --disk-format qcow2 --os-distro fedora-atomic --container-format bare \
  --file Fedora-Atomic-26-20170723.0.x86_64.qcow2 --progress
[=============================&gt;] 100%
+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | 9d233b8e7fbb7ea93f20cc839beb09ab     |
| container_format | bare                                 |
| created_at       | 2017-04-10T21:13:48Z                 |
| disk_format      | qcow2                                |
| id               | 4277115a-f254-46c0-9fb0-fffc45d2fd38 |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | fedora-atomic-26-20170723.0.x86_64   |
| os_distro        | fedora-atomic                        |
| owner            | 2f5b83ab49d54aaea4b39f5082301d09     |
| protected        | False                                |
| size             | 515112960                            |
| status           | active                               |
| tags             | []                                   |
| updated_at       | 2017-04-10T21:13:56Z                 |
| virtual_size     | None                                 |
| visibility       | public                               |
+------------------+--------------------------------------+</pre></div></li><li class="listitem "><p>
     Create a nova keypair.
    </p><div class="verbatim-wrap"><pre class="screen">$ test -f ~/.ssh/id_rsa.pub || ssh-keygen -t rsa -N "" -f ~/.ssh/id_rsa
$ openstack keypair create --pub-key ~/.ssh/id_rsa.pub testkey</pre></div></li><li class="listitem "><p>
     Create a Magnum cluster template.
    </p><div id="id-1.5.16.5.3.3.6.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The <code class="literal">--tls-disabled</code> flag is not specified in the
      included template. Authentication via client certificate will be turned
      on in clusters created from this template.
     </p></div><div class="verbatim-wrap"><pre class="screen">$  magnum cluster-template-create --name my-swarm-template \
  --image-id 4277115a-f254-46c0-9fb0-fffc45d2fd38 \
  --keypair-id testkey \
  --external-network-id ext-net \
  --dns-nameserver 8.8.8.8 \
  --flavor-id m1.small \
  --docker-volume-size 5 \
  --network-driver docker \
  --coe swarm \
  --http-proxy http://proxy.yourcompany.net:8080/ \
  --https-proxy http://proxy.yourcompany.net:8080/</pre></div><div id="id-1.5.16.5.3.3.6.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
        Use the <code class="literal">image_id</code> from
        <code class="literal">openstack image create</code> command output in the previous
        step.
       </p></li><li class="listitem "><p>
        Use your organization's DNS server. If the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> public endpoint
        is configured with the hostname, this server should provide
        resolution for this hostname.
       </p></li><li class="listitem "><p>
        The proxy is only needed if public internet (for example,
        <code class="literal">https://discovery.etcd.io/</code> or
        <code class="literal">https://gcr.io/</code>) is not accessible without proxy.
       </p></li></ol></div></div></li><li class="listitem "><p>
     Create cluster. The command below will create a minimalistic cluster
     consisting of a single Kubernetes Master (kubemaster) and single
     Kubernetes Node (worker, kubeminion).
    </p><div class="verbatim-wrap"><pre class="screen">$ magnum cluster-create --name my-swarm-cluster --cluster-template my-swarm-template \
  --node-count 1 --master-count 1</pre></div></li><li class="listitem "><p>
     Immediately after issuing <code class="literal">cluster-create</code> command,
     cluster status should turn to
     <span class="bold"><strong>CREATE_IN_PROGRESS</strong></span>
     and <code class="literal">stack_id</code> assigned.
    </p><div class="verbatim-wrap"><pre class="screen">$ magnum cluster-show my-swarm-cluster
+---------------------+------------------------------------------------------------+
| Property            | Value                                                      |
+---------------------+------------------------------------------------------------+
| status              | CREATE_IN_PROGRESS                                         |
| cluster_template_id | 17df266e-f8e1-4056-bdee-71cf3b1483e3                       |
| uuid                | c3e13e5b-85c7-44f4-839f-43878fe5f1f8                       |
| stack_id            | 3265d843-3677-4fed-bbb7-e0f56c27905a                       |
| status_reason       | -                                                          |
| created_at          | 2017-04-21T17:13:08+00:00                                  |
| name                | my-swarm-cluster                                           |
| updated_at          | -                                                          |
| discovery_url       | https://discovery.etcd.io/54e83ea168313b0c2109d0f66cd0aa6f |
| api_address         | -                                                          |
| coe_version         | -                                                          |
| master_addresses    | []                                                         |
| create_timeout      | 60                                                         |
| node_addresses      | []                                                         |
| master_count        | 1                                                          |
| container_version   | -                                                          |
| node_count          | 1                                                          |
+---------------------+------------------------------------------------------------+</pre></div></li><li class="listitem "><p>
     You can monitor cluster creation progress by listing the resources of the
     heat stack. Use the <code class="literal">stack_id</code> value from the
     <code class="literal">magnum cluster-status</code> output above in the following
     command:
    </p><div class="verbatim-wrap"><pre class="screen">$ heat resource-list -n2 3265d843-3677-4fed-bbb7-e0f56c27905a
WARNING (shell) "heat resource-list" is deprecated, please use "openstack stack resource list" instead
+--------------------+--------------------------------------+--------------------------------------------+-----------------+----------------------+-------------------------------+
| resource_name      | physical_resource_id                 | resource_type                              | resource_status | updated_time         | stack_name                    |
|--------------------+--------------------------------------+--------------------------------------------+-----------------+----------------------+-------------------------------+
| api_address_switch | 430f82f2-03e3-4085-8c07-b4a6b6d7e261 | Magnum::ApiGatewaySwitcher                 | CREATE_COMPLETE | 2017-04-21T17:13:07Z | my-swarm-cluster-j7gbjcxaremy |
. . .</pre></div></li><li class="listitem "><p>
     The cluster is complete when all resources show
     <span class="bold"><strong>CREATE_COMPLETE</strong></span>. You can also obtain the
     floating IP address once the cluster has been created.
    </p><div class="verbatim-wrap"><pre class="screen">$ magnum cluster-show my-swarm-cluster
+---------------------+------------------------------------------------------------+
| Property            | Value                                                      |
+---------------------+------------------------------------------------------------+
| status              | CREATE_COMPLETE                                            |
| cluster_template_id | 17df266e-f8e1-4056-bdee-71cf3b1483e3                       |
| uuid                | c3e13e5b-85c7-44f4-839f-43878fe5f1f8                       |
| stack_id            | 3265d843-3677-4fed-bbb7-e0f56c27905a                       |
| status_reason       | Stack CREATE completed successfully                        |
| created_at          | 2017-04-21T17:13:08+00:00                                  |
| name                | my-swarm-cluster                                           |
| updated_at          | 2017-04-21T17:18:26+00:00                                  |
| discovery_url       | https://discovery.etcd.io/54e83ea168313b0c2109d0f66cd0aa6f |
| api_address         | tcp://172.31.0.7:2376                                      |
| coe_version         | 1.0.0                                                      |
| master_addresses    | ['172.31.0.7']                                             |
| create_timeout      | 60                                                         |
| node_addresses      | ['172.31.0.5']                                             |
| master_count        | 1                                                          |
| container_version   | 1.9.1                                                      |
| node_count          | 1                                                          |
+---------------------+------------------------------------------------------------+</pre></div></li><li class="listitem "><p>
     Generate and sign client certificate using <code class="literal">magnum
     cluster-config</code> command.
    </p><div class="verbatim-wrap"><pre class="screen">$ mkdir my_swarm_cluster
$ cd my_swarm_cluster/
~/my_swarm_cluster $ magnum cluster-config my-swarm-cluster
{'tls': True, 'cfg_dir': '.', 'docker_host': u'tcp://172.31.0.7:2376'}
~/my_swarm_cluster $ ls
ca.pem  cert.pem  key.pem</pre></div></li><li class="listitem "><p>
     Copy generated certificates and key to ~/.docker folder on first cluster
     master node.
    </p><div class="verbatim-wrap"><pre class="screen">$ scp -r ~/my_swarm_cluster fedora@172.31.0.7:.docker
ca.pem                                             100% 1066     1.0KB/s   00:00
key.pem                                            100% 1679     1.6KB/s   00:00
cert.pem                                           100% 1005     1.0KB/s   00:00</pre></div></li><li class="listitem "><p>
     Login to first master node and set up cluster access environment
     variables.
    </p><div class="verbatim-wrap"><pre class="screen">$ ssh fedora@172.31.0.7
[fedora@my-6zxz5ukdu-0-bvqbsn2z2uwo-swarm-master-n6wfplu7jcwo ~]$ export DOCKER_TLS_VERIFY=1
[fedora@my-6zxz5ukdu-0-bvqbsn2z2uwo-swarm-master-n6wfplu7jcwo ~]$ export DOCKER_HOST=tcp://172.31.0.7:2376</pre></div></li><li class="listitem "><p>
     Verfy that the swarm container is up and running.
    </p><div class="verbatim-wrap"><pre class="screen">[fedora@my-6zxz5ukdu-0-bvqbsn2z2uwo-swarm-master-n6wfplu7jcwo ~]$ docker ps -a
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES
fcbfab53148c        swarm:1.0.0         "/swarm join --addr 1"   24 minutes ago      Up 24 minutes       2375/tcp            my-xggjts5zbgr-0-d4qhxhdujh4q-swarm-node-vieanhwdonon.novalocal/swarm-agent</pre></div></li><li class="listitem "><p>
     Deploy a sample docker application (nginx) and verify that Nginx is
     serving requests at port 8080 on worker node(s), on both floating and
     private IPs:
    </p><div class="verbatim-wrap"><pre class="screen">[fedora@my-6zxz5ukdu-0-bvqbsn2z2uwo-swarm-master-n6wfplu7jcwo ~]$ docker run -itd -p 8080:80 nginx
192030325fef0450b7b917af38da986edd48ac5a6d9ecb1e077b017883d18802

[fedora@my-6zxz5ukdu-0-bvqbsn2z2uwo-swarm-master-n6wfplu7jcwo ~]$ docker port 192030325fef
80/tcp -&gt; 10.0.0.11:8080

[fedora@my-6zxz5ukdu-0-bvqbsn2z2uwo-swarm-master-n6wfplu7jcwo ~]$ curl http://10.0.0.11:8080
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
...
[fedora@my-6zxz5ukdu-0-bvqbsn2z2uwo-swarm-master-n6wfplu7jcwo ~]$ curl http://172.31.0.5:8080
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
...</pre></div></li></ol></div></div></div><div class="sect1" id="deploying-apache-mesos-ubuntu"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying an Apache Mesos Cluster on Ubuntu</span> <a title="Permalink" class="permalink" href="#deploying-apache-mesos-ubuntu">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-magnum-deploying_apache_mesos_ubuntu.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-magnum-deploying_apache_mesos_ubuntu.xml</li><li><span class="ds-label">ID: </span>deploying-apache-mesos-ubuntu</li></ul></div></div></div></div><div class="sect2" id="idg-all-userguide-container-service-deploying-apache-mesos-ubuntu-xml-6"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#idg-all-userguide-container-service-deploying-apache-mesos-ubuntu-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-magnum-deploying_apache_mesos_ubuntu.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-magnum-deploying_apache_mesos_ubuntu.xml</li><li><span class="ds-label">ID: </span>idg-all-userguide-container-service-deploying-apache-mesos-ubuntu-xml-6</li></ul></div></div></div></div><p>
   These steps assume the following have been completed:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The Magnum service has been installed. For more information, see
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 26 “Magnum Overview”, Section 26.2 “Install the Magnum Service”</span>.
    </p></li><li class="listitem "><p>
     Deploying an Apache Mesos Cluster requires the Fedora Atomic image
     that is compatible for the OpenStack release. You can download
     the <span class="bold"><strong>ubuntu-mesos-latest.qcow2</strong></span>
     image from
     <a class="link" href="https://fedorapeople.org/groups/magnum/" target="_blank">https://fedorapeople.org/groups/magnum/</a>
    </p></li></ul></div></div><div class="sect2" id="idg-all-userguide-container-service-deploying-apache-mesos-ubuntu-xml-7"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating the Cluster</span> <a title="Permalink" class="permalink" href="#idg-all-userguide-container-service-deploying-apache-mesos-ubuntu-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-magnum-deploying_apache_mesos_ubuntu.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-magnum-deploying_apache_mesos_ubuntu.xml</li><li><span class="ds-label">ID: </span>idg-all-userguide-container-service-deploying-apache-mesos-ubuntu-xml-7</li></ul></div></div></div></div><p>
   The following example is created using Kubernetes Container Orchestration
   Engine (COE) running on Fedora Atomic guest OS on <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> VMs.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     As <span class="bold"><strong>stack</strong></span> user, login to the lifecycle
     manager.
    </p></li><li class="step "><p>
     Source openstack admin credentials.
    </p><div class="verbatim-wrap"><pre class="screen">$ source service.osrc</pre></div></li><li class="step "><p>
     If you haven't already, download Fedora Atomic image that is compatible for the
     OpenStack release.
    </p><div id="id-1.5.16.6.3.3.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The <code class="literal">https_proxy</code> is only needed if your environment
      requires a proxy.
     </p></div><div class="verbatim-wrap"><pre class="screen">$ https_proxy=http://proxy.yourcompany.net:8080 wget https://fedorapeople.org/groups/magnum/ubuntu-mesos-latest.qcow2</pre></div></li><li class="step "><p>
     Create a glance image.
    </p><div class="verbatim-wrap"><pre class="screen">$ openstack image create --name ubuntu-mesos-latest --visibility public --disk-format qcow2 --os-distro ubuntu --container-format bare --file ubuntu-mesos-latest.qcow2 --progress
[=============================&gt;] 100%
+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | 97cc1fdb9ca80bf80dbd6842aab7dab5     |
| container_format | bare                                 |
| created_at       | 2017-04-21T19:40:20Z                 |
| disk_format      | qcow2                                |
| id               | d6a4e6f9-9e34-4816-99fe-227e0131244f |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | ubuntu-mesos-latest                  |
| os_distro        | ubuntu                               |
| owner            | 2f5b83ab49d54aaea4b39f5082301d09     |
| protected        | False                                |
| size             | 753616384                            |
| status           | active                               |
| tags             | []                                   |
| updated_at       | 2017-04-21T19:40:32Z                 |
| virtual_size     | None                                 |
| visibility       | public                               |
+------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     Create a nova keypair.
    </p><div class="verbatim-wrap"><pre class="screen">$ test -f ~/.ssh/id_rsa.pub || ssh-keygen -t rsa -N "" -f ~/.ssh/id_rsa
$ openstack keypair create --pub-key ~/.ssh/id_rsa.pub testkey</pre></div></li><li class="step "><p>
     Create a Magnum cluster template.
    </p><div class="verbatim-wrap"><pre class="screen">$ magnum cluster-template-create --name my-mesos-template \
  --image-id d6a4e6f9-9e34-4816-99fe-227e0131244f \
  --keypair-id testkey \
  --external-network-id ext-net \
  --dns-nameserver 8.8.8.8 \
  --flavor-id m1.small \
  --docker-volume-size 5 \
  --network-driver docker \
  --coe mesos \
  --http-proxy http://proxy.yourcompany.net:8080/ \
  --https-proxy http://proxy.yourcompany.net:8080/</pre></div><div id="id-1.5.16.6.3.3.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
        Use the <span class="emphasis"><em>image_id</em></span> from <code class="literal">openstack image
        create</code> command output in the previous step.
       </p></li><li class="listitem "><p>
        Use your organization's DNS server. If the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> public endpoint is
        configured with the hostname, this server should provide resolution for
        this hostname.
       </p></li><li class="listitem "><p>
        The proxy is only needed if public internet (for example,
        <code class="literal">https://discovery.etcd.io/</code> or
        <code class="literal">https://gcr.io/</code>) is not accessible
        without proxy.
       </p></li></ol></div></div></li><li class="step "><p>
     Create cluster. The command below will create a minimalistic cluster
     consisting of a single Kubernetes Master (kubemaster) and single
     Kubernetes Node (worker, kubeminion).
    </p><div class="verbatim-wrap"><pre class="screen">$ magnum cluster-create --name my-mesos-cluster --cluster-template my-mesos-template --node-count 1 --master-count 1</pre></div></li><li class="step "><p>
     Immediately after issuing <code class="literal">cluster-create</code> command,
     cluster status should turn to
     <span class="bold"><strong>CREATE_IN_PROGRESS</strong></span> and stack_id assigned.
    </p><div class="verbatim-wrap"><pre class="screen">$ magnum cluster-show my-mesos-cluster
+---------------------+--------------------------------------+
| Property            | Value                                |
+---------------------+--------------------------------------+
| status              | CREATE_IN_PROGRESS                   |
| cluster_template_id | be354919-fa6c-4db8-9fd1-69792040f095 |
| uuid                | b1493402-8571-4683-b81e-ddc129ff8937 |
| stack_id            | 50aa20a6-bf29-4663-9181-cf7ba3070a25 |
| status_reason       | -                                    |
| created_at          | 2017-04-21T19:50:34+00:00            |
| name                | my-mesos-cluster                     |
| updated_at          | -                                    |
| discovery_url       | -                                    |
| api_address         | -                                    |
| coe_version         | -                                    |
| master_addresses    | []                                   |
| create_timeout      | 60                                   |
| node_addresses      | []                                   |
| master_count        | 1                                    |
| container_version   | -                                    |
| node_count          | 1                                    |
+---------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     You can monitor cluster creation progress by listing the resources of the
     heat stack. Use the <code class="literal">stack_id</code> value from the
     <code class="literal">magnum cluster-status</code> output above in the following
     command:
    </p><div class="verbatim-wrap"><pre class="screen">$ heat resource-list -n2 50aa20a6-bf29-4663-9181-cf7ba3070a25
WARNING (shell) "heat resource-list" is deprecated, please use "openstack stack resource list" instead
+------------------------------+--------------------------------------+-----------------------------------+-----------------+----------------------+-------------------------------+
| resource_name                | physical_resource_id                 | resource_type                     | resource_status | updated_time         | stack_name                    |
+------------------------------+--------------------------------------+-----------------------------------+-----------------+----------------------+-------------------------------+
| add_proxy_master             | 10394a74-1503-44b4-969a-44258c9a7be1 | OS::heat::SoftwareConfig          | CREATE_COMPLETE | 2017-04-21T19:50:33Z | my-mesos-cluster-w2trq7m46qus |
| add_proxy_master_deployment  |                                      | OS::heat::SoftwareDeploymentGroup | INIT_COMPLETE   | 2017-04-21T19:50:33Z | my-mesos-cluster-w2trq7m46qus |
...</pre></div></li><li class="step "><p>
     The cluster is complete when all resources show
     <span class="bold"><strong>CREATE_COMPLETE</strong></span>.
    </p><div class="verbatim-wrap"><pre class="screen">$ magnum cluster-show my-mesos-cluster
+---------------------+--------------------------------------+
| Property            | Value                                |
+---------------------+--------------------------------------+
| status              | CREATE_COMPLETE                      |
| cluster_template_id | 9e942bfa-2c78-4837-82f5-6bea88ba1bf9 |
| uuid                | 9d7bb502-8865-4cbd-96fa-3cd75f0f6945 |
| stack_id            | 339a72b4-a131-47c6-8d10-365e6f6a18cf |
| status_reason       | Stack CREATE completed successfully  |
| created_at          | 2017-04-24T20:54:31+00:00            |
| name                | my-mesos-cluster                     |
| updated_at          | 2017-04-24T20:59:18+00:00            |
| discovery_url       | -                                    |
| api_address         | 172.31.0.10                          |
| coe_version         | -                                    |
| master_addresses    | ['172.31.0.10']                      |
| create_timeout      | 60                                   |
| node_addresses      | ['172.31.0.5']                       |
| master_count        | 1                                    |
| container_version   | 1.9.1                                |
| node_count          | 1                                    |
+---------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     Verify that
     <a class="link" href="https://mesosphere.github.io/marathon/" target="_blank">Marathon</a>
     web console is available at http://${MASTER_IP}:8080/, and
     <a class="link" href="http://mesos.apache.org/documentation/latest/" target="_blank">Mesos</a>
     UI is available at http://${MASTER_IP}:5050/
    </p><div class="verbatim-wrap"><pre class="screen">$ https_proxy=http://proxy.yourcompany.net:8080 curl -LO \
  https://storage.googleapis.com/kubernetes-release/release/v1.2.0/bin/linux/amd64/kubectl
$ chmod +x ./kubectl
$ sudo mv ./kubectl /usr/local/bin/kubectl</pre></div></li><li class="step "><p>
     Create an example Mesos application.
    </p><div class="verbatim-wrap"><pre class="screen">$ mkdir my_mesos_cluster
$ cd my_mesos_cluster/
$ cat &gt; sample.json &lt;&lt;-EOFc
{
  "id": "sample",
  "cmd": "python3 -m http.server 8080",
  "cpus": 0.5,
  "mem": 32.0,
  "container": {
    "type": "DOCKER",
    "docker": {
      "image": "python:3",
      "network": "BRIDGE",
      "portMappings": [
        { "containerPort": 8080, "hostPort": 0 }
      ]
    }
  }
}
EOF</pre></div><div class="verbatim-wrap"><pre class="screen">$ curl -s -X POST -H "Content-Type: application/json" \
  http://172.31.0.10:8080/v2/apps -d@sample.json | json_pp
{
   "dependencies" : [],
   "healthChecks" : [],
   "user" : null,
   "mem" : 32,
   "requirePorts" : false,
   "tasks" : [],
   "cpus" : 0.5,
   "upgradeStrategy" : {
      "minimumHealthCapacity" : 1,
      "maximumOverCapacity" : 1
   },
   "maxLaunchDelaySeconds" : 3600,
   "disk" : 0,
   "constraints" : [],
   "executor" : "",
   "cmd" : "python3 -m http.server 8080",
   "id" : "/sample",
   "labels" : {},
   "ports" : [
      0
   ],
   "storeUrls" : [],
   "instances" : 1,
   "tasksRunning" : 0,
   "tasksHealthy" : 0,
   "acceptedResourceRoles" : null,
   "env" : {},
   "tasksStaged" : 0,
   "tasksUnhealthy" : 0,
   "backoffFactor" : 1.15,
   "version" : "2017-04-25T16:37:40.657Z",
   "uris" : [],
   "args" : null,
   "container" : {
      "volumes" : [],
      "docker" : {
         "portMappings" : [
            {
               "containerPort" : 8080,
               "hostPort" : 0,
               "servicePort" : 0,
               "protocol" : "tcp"
            }
         ],
         "parameters" : [],
         "image" : "python:3",
         "forcePullImage" : false,
         "network" : "BRIDGE",
         "privileged" : false
      },
      "type" : "DOCKER"
   },
   "deployments" : [
      {
         "id" : "6fbe48f0-6a3c-44b7-922e-b172bcae1be8"
      }
   ],
   "backoffSeconds" : 1
}</pre></div></li><li class="step "><p>
     Wait for sample application to start. Use REST API or Marathon web console
     to monitor status:
    </p><div class="verbatim-wrap"><pre class="screen">$ curl -s http://172.31.0.10:8080/v2/apps/sample | json_pp
{
   "app" : {
      "deployments" : [],
      "instances" : 1,
      "tasks" : [
         {
            "id" : "sample.7fdd1ee4-29d5-11e7-9ee0-02427da4ced1",
            "stagedAt" : "2017-04-25T16:37:40.807Z",
            "version" : "2017-04-25T16:37:40.657Z",
            "ports" : [
               31827
            ],
            "appId" : "/sample",
            "slaveId" : "21444bc5-3eb8-49cd-b020-77041e0c88d0-S0",
            "host" : "10.0.0.9",
            "startedAt" : "2017-04-25T16:37:42.003Z"
         }
      ],
      "upgradeStrategy" : {
         "maximumOverCapacity" : 1,
         "minimumHealthCapacity" : 1
      },
      "storeUrls" : [],
      "requirePorts" : false,
      "user" : null,
      "id" : "/sample",
      "acceptedResourceRoles" : null,
      "tasksRunning" : 1,
      "cpus" : 0.5,
      "executor" : "",
      "dependencies" : [],
      "args" : null,
      "backoffFactor" : 1.15,
      "ports" : [
         10000
      ],
      "version" : "2017-04-25T16:37:40.657Z",
      "container" : {
         "volumes" : [],
         "docker" : {
            "portMappings" : [
               {
                  "servicePort" : 10000,
                  "protocol" : "tcp",
                  "hostPort" : 0,
                  "containerPort" : 8080
               }
            ],
            "forcePullImage" : false,
            "parameters" : [],
            "image" : "python:3",
            "privileged" : false,
            "network" : "BRIDGE"
         },
         "type" : "DOCKER"
      },
      "constraints" : [],
      "tasksStaged" : 0,
      "env" : {},
      "mem" : 32,
      "disk" : 0,
      "labels" : {},
      "tasksHealthy" : 0,
      "healthChecks" : [],
      "cmd" : "python3 -m http.server 8080",
      "backoffSeconds" : 1,
      "maxLaunchDelaySeconds" : 3600,
      "versionInfo" : {
         "lastConfigChangeAt" : "2017-04-25T16:37:40.657Z",
         "lastScalingAt" : "2017-04-25T16:37:40.657Z"
      },
      "uris" : [],
      "tasksUnhealthy" : 0
   }
}</pre></div></li><li class="step "><p>
     Verify that deployed application is responding on automatically assigned
     port on floating IP address of worker node.
    </p><div class="verbatim-wrap"><pre class="screen">$ curl http://172.31.0.5:31827
&lt;!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd"&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta http-equiv="Content-Type" content="text/html; charset=utf-8"&gt;
&lt;title&gt;Directory listing for /&lt;/title&gt;
...</pre></div></li></ol></div></div></div></div><div class="sect1" id="create-magnum-cluster"><div class="titlepage"><div><div><h2 class="title"><span class="number">14.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a Magnum Cluster with the Dashboard</span> <a title="Permalink" class="permalink" href="#create-magnum-cluster">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-magnum-create_magnum_cluster_dashboard.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-magnum-create_magnum_cluster_dashboard.xml</li><li><span class="ds-label">ID: </span>create-magnum-cluster</li></ul></div></div></div></div><p>
  You can alternatively create a cluster template and cluster with the Magnum
  UI in horizon. The example instructions below demonstrate how to deploy a
  Kubernetes Cluster using the Fedora Atomic image. Other deployments such as
  Kubernetes on CoreOS, Docker Swarm on Fedora, and Mesos on Ubuntu all follow
  the same set of instructions mentioned below with slight variations to their
  parameters. You can determine those parameters by looking at the previous
  set of CLI instructions in the
  <code class="command">magnum cluster-template-create</code> and
  <code class="command">magnum cluster-create</code> commands.
 </p><div class="sect2" id="idg-all-userguide-container-service-create-magnum-cluster-dashboard-xml-7"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#idg-all-userguide-container-service-create-magnum-cluster-dashboard-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-magnum-create_magnum_cluster_dashboard.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-magnum-create_magnum_cluster_dashboard.xml</li><li><span class="ds-label">ID: </span>idg-all-userguide-container-service-create-magnum-cluster-dashboard-xml-7</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Magnum must be installed before proceeding. For more information, see
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 26 “Magnum Overview”, Section 26.2 “Install the Magnum Service”</span>.
    </p><div id="id-1.5.16.7.3.2.1.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      Pay particular attention to <code class="literal">external-name:</code> in
      <code class="literal">data/network_groups.yml</code>. This cannot be set to the
      default <code class="literal">myardana.test</code> and must be a valid
      DNS-resolvable FQDN. If you do not have a DNS-resolvable FQDN, remove or
      comment out the <code class="literal">external-name</code> entry and the public
      endpoint will use an IP address instead of a name.
     </p></div></li><li class="listitem "><p>
     The image for which you want to base your cluster on must already have
     been uploaded into glance. See the previous CLI instructions regarding
     deploying a cluster on how this is done.
    </p></li></ul></div></div><div class="sect2" id="cluster-template"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating the Cluster Template</span> <a title="Permalink" class="permalink" href="#cluster-template">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-magnum-create_magnum_cluster_dashboard.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-magnum-create_magnum_cluster_dashboard.xml</li><li><span class="ds-label">ID: </span>cluster-template</li></ul></div></div></div></div><p>
   You will need access to the Dashboard to create the cluster
   template.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Open a web browser that has both JavaScript and cookies enabled. In the
     address bar, enter the host name or IP address for the dashboard.
    </p></li><li class="step "><p>
     On the <span class="guimenu ">Log In</span> page, enter your user name
     and password and then click <span class="guimenu ">Connect</span>.
    </p></li><li class="step "><p>
     Make sure you are in the appropriate domain and project in the left pane.
     Below is an example image of the drop-down box:
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-horizon-domain_selector.png" target="_blank"><img src="images/media-horizon-domain_selector.png" width="" /></a></div></div></li><li class="step "><p>
     A key pair is required for cluster template creation. It is applied to VMs
     created during the cluster creation process. This allows SSH access to your
     cluster's VMs. If you would like to create a new key pair, do so by going
     to <span class="guimenu ">Project</span> › <span class="guimenu ">Compute</span> › <span class="guimenu ">Access &amp; Security</span> › <span class="guimenu ">Key Pairs</span>.
    </p></li><li class="step "><p>
     Go to <span class="guimenu ">Project</span> › <span class="guimenu ">Container Infra</span> › <span class="guimenu ">Cluster Templates</span>.
     Insert
     <em class="replaceable ">CLUSTER_NAME</em> and click on
     <span class="guimenu ">+ Create Cluster Template</span> with the following options:
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-horizon-create_cluster_template.png" target="_blank"><img src="images/media-horizon-create_cluster_template.png" width="" /></a></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="guimenu ">Public</span> - makes the template available
       for others to use.
      </p></li><li class="listitem "><p>
       <span class="guimenu ">Enable Registry</span> - creates and uses a
       private docker registry backed by OpenStack swift in addition to using
       the public docker registry.
      </p></li><li class="listitem "><p>
       <span class="guimenu ">Disable TLS</span> - turns off TLS encryption.
       For Kubernetes clusters which use client certificate authentication,
       disabling TLS also involves disabling authentication.
      </p></li></ul></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-horizon-create_cluster_template2.png" target="_blank"><img src="images/media-horizon-create_cluster_template2.png" width="" /></a></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-horizon-create_cluster_template3.png" target="_blank"><img src="images/media-horizon-create_cluster_template3.png" width="" /></a></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Proxies are only needed if the created VMs require a proxy to connect
       externally.
      </p></li><li class="listitem "><p>
       <span class="guimenu ">Master LB</span> – This should be turned off; LbaaS v2
       (Octavia) is not available in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
      </p></li><li class="listitem "><p>
       <span class="guimenu ">Floating IP</span> – This assigns floating
       IPs to the cluster nodes when the cluster is being created. This should
       be selected if you wish to ssh into the cluster nodes, perform
       diagnostics and additional tuning to Kubernetes.
      </p></li></ul></div></li><li class="step "><p>
     Click the <span class="guimenu ">Submit</span> button to create the cluster template
     and you should see <span class="emphasis"><em>my-template</em></span> in the list of
     templates.
    </p></li></ol></div></div></div><div class="sect2" id="creating-the-cluster"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating the Cluster</span> <a title="Permalink" class="permalink" href="#creating-the-cluster">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-magnum-create_magnum_cluster_dashboard.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-magnum-create_magnum_cluster_dashboard.xml</li><li><span class="ds-label">ID: </span>creating-the-cluster</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Click <span class="guimenu ">Create Cluster</span> for
     <span class="emphasis"><em>my-template</em></span> or go to
     <span class="guimenu ">Project</span> › <span class="guimenu ">Container Infra</span> › <span class="guimenu ">Clusters</span> and click <span class="guimenu ">+ Create
     Cluster</span> with the following options.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-horizon-create_cluster_img1.png" target="_blank"><img src="images/media-horizon-create_cluster_img1.png" width="" /></a></div></div></li><li class="step "><p>
     Click <span class="guimenu ">Create</span> to start the cluster
     creation process.
    </p></li><li class="step "><p>
     Click <span class="guimenu ">Clusters</span> in the left pane to see
     the list of clusters. You will see
     <span class="emphasis"><em>my-cluster</em></span> in this list. If you select
     <span class="emphasis"><em>my-cluster</em></span>, you will see additional
     information regarding your cluster.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-horizon-create_cluster_img3.png" target="_blank"><img src="images/media-horizon-create_cluster_img3.png" width="" /></a></div></div></li></ol></div></div></div></div></div><div class="chapter " id="system-maintenance"><div class="titlepage"><div><div><h1 class="title"><span class="number">15 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">System Maintenance</span> <a title="Permalink" class="permalink" href="#system-maintenance">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-system_maintenance.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-system_maintenance.xml</li><li><span class="ds-label">ID: </span>system-maintenance</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#planned-maintenance"><span class="number">15.1 </span><span class="name">Planned System Maintenance</span></a></span></dt><dt><span class="section"><a href="#unplanned-maintenance"><span class="number">15.2 </span><span class="name">Unplanned System Maintenance</span></a></span></dt><dt><span class="section"><a href="#maintenance-update"><span class="number">15.3 </span><span class="name">Cloud Lifecycle Manager Maintenance Update Procedure</span></a></span></dt><dt><span class="section"><a href="#upgrade-soc"><span class="number">15.4 </span><span class="name">Upgrading Cloud Lifecycle Manager 8 to Cloud Lifecycle Manager 9</span></a></span></dt><dt><span class="section"><a href="#deploy-ptf"><span class="number">15.5 </span><span class="name">Cloud Lifecycle Manager Program Temporary Fix (PTF) Deployment</span></a></span></dt><dt><span class="section"><a href="#database-maintenance"><span class="number">15.6 </span><span class="name">Periodic OpenStack Maintenance Tasks</span></a></span></dt></dl></div></div><p>
  This section contains the following subsections to help you manage, configure,
  and maintain your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud as well as procedures
  for performing node maintenance.
 </p><div class="sect1" id="planned-maintenance"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Planned System Maintenance</span> <a title="Permalink" class="permalink" href="#planned-maintenance">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-planned_maintenance.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-planned_maintenance.xml</li><li><span class="ds-label">ID: </span>planned-maintenance</li></ul></div></div></div></div><p>
  Planned maintenance tasks for your cloud. See sections below for:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="#cont-planned" title="15.1.2. Planned Control Plane Maintenance">Section 15.1.2, “Planned Control Plane Maintenance”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#comp-planned" title="15.1.3. Planned Compute Maintenance">Section 15.1.3, “Planned Compute Maintenance”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#planned-maintenance-task-for-networking-nodes" title="15.1.4. Planned Network Maintenance">Section 15.1.4, “Planned Network Maintenance”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#storage-maintenance" title="15.1.5. Planned Storage Maintenance">Section 15.1.5, “Planned Storage Maintenance”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#mariadb-manual-update" title="15.1.6. Updating MariaDB with Galera">Section 15.1.6, “Updating MariaDB with Galera”</a>
   </p></li></ul></div><div class="sect2" id="sysmn-gen"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Whole Cloud Maintenance</span> <a title="Permalink" class="permalink" href="#sysmn-gen">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-general_procedures.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-general_procedures.xml</li><li><span class="ds-label">ID: </span>sysmn-gen</li></ul></div></div></div></div><p>
  Planned maintenance procedures for your whole cloud.
 </p><div class="sect3" id="stop-restart"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Bringing Down Your Cloud: Services Down Method</span> <a title="Permalink" class="permalink" href="#stop-restart">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-reboot_cloud_down.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-reboot_cloud_down.xml</li><li><span class="ds-label">ID: </span>stop-restart</li></ul></div></div></div></div><div id="id-1.5.17.3.4.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   If you have a planned maintenance and need to bring down your entire cloud,
   update and reboot all nodes in the cloud one by one. Start with the deployer
   node, then follow the order recommended in <a class="xref" href="#rebootNodes" title="15.1.1.2. Rolling Reboot of the Cloud">Section 15.1.1.2, “Rolling Reboot of the Cloud”</a>. This method will bring down all of your services.
  </p></div><p>
  If you wish to use a method utilizing rolling reboots where your cloud
  services will continue running then see <a class="xref" href="#rebootNodes" title="15.1.1.2. Rolling Reboot of the Cloud">Section 15.1.1.2, “Rolling Reboot of the Cloud”</a>.
 </p><p>
  To perform backups prior to these steps, visit the backup and
  restore pages first at <a class="xref" href="#bura-overview" title="Chapter 17. Backup and Restore">Chapter 17, <em>Backup and Restore</em></a>.
 </p><div class="sect4" id="idg-all-operations-maintenance-reboot-cloud-down-xml-7"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Gracefully Bringing Down and Restarting Your Cloud Environment</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-reboot-cloud-down-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-reboot_cloud_down.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-reboot_cloud_down.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-reboot-cloud-down-xml-7</li></ul></div></div></div></div><p>
   You will do the following steps from your Cloud Lifecycle Manager.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Gracefully shut down your cloud by running the
     <code class="literal">ardana-stop.yml</code> playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-stop.yml</pre></div></li><li class="step "><p>
     Shut down and restart your nodes. There are multiple ways you can do this:
    </p><ol type="a" class="substeps "><li class="step "><p>
       You can SSH to each node and use <code class="literal">sudo reboot -f</code> to
       reboot the node. Reboot the control plane nodes first so that they
       become functional as early as possible.
      </p></li><li class="step "><p>
       You can shut down the nodes and then physically restart them either via a
       power button or the IPMI. If your cloud data model
       <code class="filename">servers.yml</code> specifies iLO connectivity for all
       nodes, then you can use the <code class="literal">bm-power-down.yml</code> and
       <code class="literal">bm-power-up.yml</code> playbooks on the Cloud Lifecycle Manager.
      </p><p>
       Power down the control plane nodes last so that they remain online as
       long as possible, and power them back up before other nodes to restore
       their services quickly.
      </p></li></ol></li><li class="step "><p>
     Perform the necessary maintenance.
    </p></li><li class="step "><p>
     After the maintenance is complete, power your Cloud Lifecycle Manager back up
     and then SSH to it.
    </p></li><li class="step "><p>
     Determine the current power status of the nodes in your environment:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts bm-power-status.yml</pre></div></li><li class="step "><p>
     If necessary, power up any nodes that are not already powered up, ensuring
     that you power up your controller nodes first. You can target specific
     nodes with the <code class="literal">-e nodelist=&lt;node_name&gt;</code> switch.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts bm-power-up.yml [-e nodelist=&lt;node_name&gt;]</pre></div><div id="id-1.5.17.3.4.3.5.3.7.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     Obtain the <code class="literal">&lt;node_name&gt;</code> by using the
     <code class="command">sudo cobbler system list</code> command from the Cloud Lifecycle Manager.
    </p></div></li><li class="step "><p>
     Bring the databases back up:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts galera-bootstrap.yml</pre></div></li><li class="step "><p>
     Gracefully bring up your cloud services by running the
     <code class="literal">ardana-start.yml</code> playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-start.yml</pre></div></li><li class="step "><p>
     Pause for a few minutes and give the cloud environment time to come up
     completely and then verify the status of the individual services using
     this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-status.yml</pre></div></li><li class="step "><p>
     If any services did not start properly, you can run playbooks for the
     specific services having issues.
    </p><p>
     For example:
    </p><p>
     If RabbitMQ fails, run:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts rabbitmq-start.yml</pre></div><p>
     You can check the status of RabbitMQ afterwards with this:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts rabbitmq-status.yml</pre></div><p>
     If the recovery had failed, you can run:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts rabbitmq-disaster-recovery.yml</pre></div><p>
     Each of the other services have playbooks in the
     <code class="literal">~/scratch/ansible/next/ardana/ansible</code> directory in the
     format of <code class="literal">&lt;service&gt;-start.yml</code> that you can run.
     One example, for the compute service, is
     <code class="literal">nova-start.yml</code>.
    </p></li><li class="step "><p>
     Continue checking the status of your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 cloud services until
     there are no more failed or unreachable nodes:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-status.yml</pre></div></li></ol></div></div></div></div><div class="sect3" id="rebootNodes"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Rolling Reboot of the Cloud</span> <a title="Permalink" class="permalink" href="#rebootNodes">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-reboot_cloud_rolling.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-reboot_cloud_rolling.xml</li><li><span class="ds-label">ID: </span>rebootNodes</li></ul></div></div></div></div><p>
  If you have a planned maintenance and need to bring down your entire cloud
  and restart services while minimizing downtime, follow the steps here to
  safely restart your cloud. If you do not mind your services being down, then
  another option for planned maintenance can be found at
  <a class="xref" href="#stop-restart" title="15.1.1.1. Bringing Down Your Cloud: Services Down Method">Section 15.1.1.1, “Bringing Down Your Cloud: Services Down Method”</a>.
 </p><div class="sect4" id="idg-all-operations-maintenance-reboot-cloud-rolling-xml-5"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recommended node reboot order</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-reboot-cloud-rolling-xml-5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-reboot_cloud_rolling.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-reboot_cloud_rolling.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-reboot-cloud-rolling-xml-5</li></ul></div></div></div></div><p>
   To ensure that rebooted nodes reintegrate into the cluster, the key is
   having enough time between controller reboots.
  </p><p>
   The recommended way to achieve this is as follows:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Reboot controller nodes one-by-one with a suitable interval in between. If
     you alternate between controllers and compute nodes you will gain more
     time between the controller reboots.
    </p></li><li class="step "><p>
     Reboot of compute nodes (if present in your cloud).
    </p></li><li class="step "><p>
     Reboot of swift nodes (if present in your cloud).
    </p></li><li class="step "><p>
     Reboot of ESX nodes (if present in your cloud).
    </p></li></ol></div></div></div><div class="sect4" id="idg-all-operations-maintenance-reboot-cloud-rolling-xml-7"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Rebooting controller nodes</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-reboot-cloud-rolling-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-reboot_cloud_rolling.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-reboot_cloud_rolling.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-reboot-cloud-rolling-xml-7</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Turn off the keystone Fernet Token-Signing Key
   Rotation</strong></span>
  </p><p>
   Before rebooting any controller node, you need to ensure that the keystone
   Fernet token-signing key rotation is turned off. Run the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-stop-fernet-auto-rotation.yml</pre></div><p>
   <span class="bold"><strong>Migrate singleton services first</strong></span>
  </p><div id="id-1.5.17.3.4.4.4.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    If you have previously rebooted your Cloud Lifecycle Manager for any reason, ensure that
    the <code class="systemitem">apache2</code> service is running before
    continuing. To start the <code class="systemitem">apache2</code> service, use
    this command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo systemctl start apache2</pre></div></div><p>
   The first consideration before rebooting any controller nodes is that there
   are a few services that run as singletons (non-HA), thus they will be
   unavailable while the controller they run on is down. Typically this is a
   very small window, but if you want to retain the service during the reboot
   of that server you should take special action to maintain service, such as
   migrating the service.
  </p><p>
   For these steps, if your singleton services are running on controller1 and
   you move them to controller2, then ensure you move them back to controller1
   before proceeding to reboot controller2.
  </p><p>
   <span class="bold"><strong>For the <code class="literal">cinder-volume</code> singleton
   service:</strong></span>
  </p><p>
   Execute the following command on each controller node to determine which
   node is hosting the cinder-volume singleton. It should be running on only
   one node:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ps auxww | grep cinder-volume | grep -v grep</pre></div><p>
   Run the <code class="literal">cinder-migrate-volume.yml</code> playbook - details
   about the cinder volume and backup migration instructions can be found in
   <a class="xref" href="#sec-operation-manage-block-storage" title="8.1.3. Managing cinder Volume and Backup Services">Section 8.1.3, “Managing cinder Volume and Backup Services”</a>.
  </p><p>
   <span class="bold"><strong>For the SNAT namespace singleton service:</strong></span>
  </p><p>
   If you reboot the controller node hosting the SNAT namespace service on it,
   Compute instances without floating IPs will lose network connectivity when
   that controller is rebooted. To prevent this from happening, you can use
   these steps to determine which controller node is hosting the SNAT namespace
   service and migrate it to one of the other controller nodes while that node
   is rebooted.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Locate the SNAT node where the router is providing the active
     <code class="literal">snat_service</code>:
    </p><ol type="a" class="substeps "><li class="step "><p>
       From the Cloud Lifecycle Manager, list out your ports to determine which port
       is serving as the router gateway:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>openstack port list --device_owner network:router_gateway</pre></div><p>
       Example:
      </p><div class="verbatim-wrap"><pre class="screen">$ openstack port list --device_owner network:router_gateway
+--------------------------------------+------+-------------------+-------------------------------------------------------------------------------------+
| id                                   | name | mac_address       | fixed_ips                                                                           |
+--------------------------------------+------+-------------------+-------------------------------------------------------------------------------------+
| 287746e6-7d82-4b2c-914c-191954eba342 |      | fa:16:3e:2e:26:ac | {"subnet_id": "f4152001-2500-4ebe-ba9d-a8d6149a50df", "ip_address": "10.247.96.29"} |
+--------------------------------------+------+-------------------+-------------------------------------------------------------------------------------+</pre></div></li><li class="step "><p>
       Look at the details of this port to determine what the
       <code class="literal">binding:host_id</code> value is, which will point to the
       host in which the port is bound to:
      </p><div class="verbatim-wrap"><pre class="screen">openstack port show &lt;port_id&gt;</pre></div><p>
       Example, with the value you need in bold:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port show 287746e6-7d82-4b2c-914c-191954eba342
+-----------------------+--------------------------------------------------------------------------------------------------------------+
| Field                 | Value                                                                                                        |
+-----------------------+--------------------------------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                                         |
| allowed_address_pairs |                                                                                                              |
<span class="bold"><strong>| binding:host_id       | ardana-cp1-c1-m2-mgmt</strong></span>                                                                                        |
| binding:profile       | {}                                                                                                           |
| binding:vif_details   | {"port_filter": true, "ovs_hybrid_plug": true}                                                               |
| binding:vif_type      | ovs                                                                                                          |
| binding:vnic_type     | normal                                                                                                       |
| device_id             | e122ea3f-90c5-4662-bf4a-3889f677aacf                                                                         |
| device_owner          | network:router_gateway                                                                                       |
| dns_assignment        | {"hostname": "host-10-247-96-29", "ip_address": "10.247.96.29", "fqdn": "host-10-247-96-29.openstacklocal."} |
| dns_name              |                                                                                                              |
| extra_dhcp_opts       |                                                                                                              |
| fixed_ips             | {"subnet_id": "f4152001-2500-4ebe-ba9d-a8d6149a50df", "ip_address": "10.247.96.29"}                          |
| id                    | 287746e6-7d82-4b2c-914c-191954eba342                                                                         |
| mac_address           | fa:16:3e:2e:26:ac                                                                                            |
| name                  |                                                                                                              |
| network_id            | d3cb12a6-a000-4e3e-82c4-ee04aa169291                                                                         |
| security_groups       |                                                                                                              |
| status                | DOWN                                                                                                         |
| tenant_id             |                                                                                                              |
+-----------------------+--------------------------------------------------------------------------------------------------------------+</pre></div><p>
       In this example, the <code class="literal">ardana-cp1-c1-m2-mgmt</code> is the
       node hosting the SNAT namespace service.
      </p></li></ol></li><li class="step "><p>
     SSH to the node hosting the SNAT namespace service and check the SNAT
     namespace, specifying the router_id that has the interface with the subnet
     that you are interested in:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ssh &lt;IP_of_SNAT_namespace_host&gt;
<code class="prompt user">ardana &gt; </code>sudo ip netns exec snat-&lt;router_ID&gt; bash</pre></div><p>
     Example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo ip netns exec snat-e122ea3f-90c5-4662-bf4a-3889f677aacf bash</pre></div></li><li class="step "><p>
     Obtain the ID for the L3 Agent for the node hosting your SNAT namespace:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc
<code class="prompt user">ardana &gt; </code>openstack network agent list</pre></div><p>
     Example, with the entry you need given the examples above:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network agent list
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| id                                   | agent_type           | host                     | alive | admin_state_up | binary                    |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| 0126bbbf-5758-4fd0-84a8-7af4d93614b8 | DHCP agent           | ardana-cp1-c1-m3-mgmt    | :-)   | True           | neutron-dhcp-agent        |
| 33dec174-3602-41d5-b7f8-a25fd8ff6341 | Metadata agent       | ardana-cp1-c1-m2-mgmt    | :-)   | True           | neutron-metadata-agent    |
| 3bc28451-c895-437b-999d-fdcff259b016 | L3 agent             | ardana-cp1-c1-m1-mgmt    | :-)   | True           | neutron-vpn-agent         |
| 4af1a941-61c1-4e74-9ec1-961cebd6097b | L3 agent             | ardana-cp1-comp0001-mgmt | :-)   | True           | neutron-l3-agent          |
| 65bcb3a0-4039-4d9d-911c-5bb790953297 | Open vSwitch agent   | ardana-cp1-c1-m1-mgmt    | :-)   | True           | neutron-openvswitch-agent |
| 6981c0e5-5314-4ccd-bbad-98ace7db7784 | L3 agent             | ardana-cp1-c1-m3-mgmt    | :-)   | True           | neutron-vpn-agent         |
| 7df9fa0b-5f41-411f-a532-591e6db04ff1 | Metadata agent       | ardana-cp1-comp0001-mgmt | :-)   | True           | neutron-metadata-agent    |
| 92880ab4-b47c-436c-976a-a605daa8779a | Metadata agent       | ardana-cp1-c1-m1-mgmt    | :-)   | True           | neutron-metadata-agent    |
<span class="bold"><strong>| a209c67d-c00f-4a00-b31c-0db30e9ec661 | L3 agent             | ardana-cp1-c1-m2-mgmt</strong></span>    | :-)   | True           | neutron-vpn-agent         |
| a9467f7e-ec62-4134-826f-366292c1f2d0 | DHCP agent           | ardana-cp1-c1-m1-mgmt    | :-)   | True           | neutron-dhcp-agent        |
| b13350df-f61d-40ec-b0a3-c7c647e60f75 | Open vSwitch agent   | ardana-cp1-c1-m2-mgmt    | :-)   | True           | neutron-openvswitch-agent |
| d4c07683-e8b0-4a2b-9d31-b5b0107b0b31 | Open vSwitch agent   | ardana-cp1-comp0001-mgmt | :-)   | True           | neutron-openvswitch-agent |
| e91d7f3f-147f-4ad2-8751-837b936801e3 | Open vSwitch agent   | ardana-cp1-c1-m3-mgmt    | :-)   | True           | neutron-openvswitch-agent |
| f33015c8-f4e4-4505-b19b-5a1915b6e22a | DHCP agent           | ardana-cp1-c1-m2-mgmt    | :-)   | True           | neutron-dhcp-agent        |
| fe43c0e9-f1db-4b67-a474-77936f7acebf | Metadata agent       | ardana-cp1-c1-m3-mgmt    | :-)   | True           | neutron-metadata-agent    |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+</pre></div></li><li class="step "><p>
     Also obtain the ID for the L3 Agent of the node you are going to move the
     SNAT namespace service to using the same commands as the previous step.
    </p></li><li class="step "><p>
     Use these commands to move the SNAT namespace service, with the
     <code class="literal">router_id</code> being the same value as the ID for router:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Remove the L3 Agent for the old host:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network agent remove router –agent-type l3
&lt;agent_id_of_snat_namespace_host&gt; \
&lt;qrouter_uuid&gt;</pre></div><p>
       Example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network agent remove router –agent-type l3
a209c67d-c00f-4a00-b31c-0db30e9ec661 \
e122ea3f-90c5-4662-bf4a-3889f677aacf
Removed router e122ea3f-90c5-4662-bf4a-3889f677aacf from L3 agent</pre></div></li><li class="step "><p>
       Remove the SNAT namespace:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo ip netns delete snat-&lt;router_id&gt;</pre></div><p>
       Example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo ip netns delete snat-e122ea3f-90c5-4662-bf4a-3889f677aacf</pre></div></li><li class="step "><p>
       Create a new L3 Agent for the new host:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network agent add router –agent-type l3
&lt;agent_id_of_new_snat_namespace_host&gt; \
&lt;qrouter_uuid&gt;</pre></div><p>
       Example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network agent add router –agent-type l3
3bc28451-c895-437b-999d-fdcff259b016 \
e122ea3f-90c5-4662-bf4a-3889f677aacf
Added router e122ea3f-90c5-4662-bf4a-3889f677aacf to L3 agent</pre></div></li></ol><p>
     Confirm that it has been moved by listing the details of your port from step
     1b above, noting the value of <code class="literal">binding:host_id</code> which
     should be updated to the host you moved your SNAT namespace to:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port show &lt;port_ID&gt;</pre></div><p>
     Example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack port show 287746e6-7d82-4b2c-914c-191954eba342
+-----------------------+--------------------------------------------------------------------------------------------------------------+
| Field                 | Value                                                                                                        |
+-----------------------+--------------------------------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                                         |
| allowed_address_pairs |                                                                                                              |
<span class="bold"><strong>| binding:host_id       | ardana-cp1-c1-m1-mgmt</strong></span>                                                                                        |
| binding:profile       | {}                                                                                                           |
| binding:vif_details   | {"port_filter": true, "ovs_hybrid_plug": true}                                                               |
| binding:vif_type      | ovs                                                                                                          |
| binding:vnic_type     | normal                                                                                                       |
| device_id             | e122ea3f-90c5-4662-bf4a-3889f677aacf                                                                         |
| device_owner          | network:router_gateway                                                                                       |
| dns_assignment        | {"hostname": "host-10-247-96-29", "ip_address": "10.247.96.29", "fqdn": "host-10-247-96-29.openstacklocal."} |
| dns_name              |                                                                                                              |
| extra_dhcp_opts       |                                                                                                              |
| fixed_ips             | {"subnet_id": "f4152001-2500-4ebe-ba9d-a8d6149a50df", "ip_address": "10.247.96.29"}                          |
| id                    | 287746e6-7d82-4b2c-914c-191954eba342                                                                         |
| mac_address           | fa:16:3e:2e:26:ac                                                                                            |
| name                  |                                                                                                              |
| network_id            | d3cb12a6-a000-4e3e-82c4-ee04aa169291                                                                         |
| security_groups       |                                                                                                              |
| status                | DOWN                                                                                                         |
| tenant_id             |                                                                                                              |
+-----------------------+--------------------------------------------------------------------------------------------------------------+</pre></div></li></ol></div></div><p>
   <span class="bold"><strong>Reboot the controllers</strong></span>
  </p><p>
   In order to reboot the controller nodes, you must first retrieve a list of
   nodes in your cloud running control plane services.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>for i in $(grep -w cluster-prefix
~/openstack/my_cloud/definition/data/control_plane.yml \
| awk '{print $2}'); do grep $i
~/scratch/ansible/next/ardana/ansible/hosts/verb_hosts \
| grep ansible_ssh_host | awk '{print $1}'; done</pre></div><p>
   Then perform the following steps from your Cloud Lifecycle Manager for each of
   your controller nodes:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     If any singleton services are active on this node, they will be
     unavailable while the node is down. If you want to retain the service
     during the reboot, you should take special action to maintain the service,
     such as migrating the service as appropriate as noted above.
    </p></li><li class="step "><p>
     Stop all services on the controller node that you are rebooting first:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-stop.yml --limit \
&lt;controller node&gt;</pre></div></li><li class="step "><p>
     Reboot the controller node, e.g. run the following command on the
     controller itself:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo reboot</pre></div><p>
     Note that the current node being rebooted could be hosting the lifecycle
     manager.
    </p></li><li class="step "><p>
     Wait for the controller node to become ssh-able and allow an additional
     minimum of five minutes for the controller node to settle. Start all
     services on the controller node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml \
--limit &lt;controller node&gt;</pre></div></li><li class="step "><p>
     Verify that the status of all services on that is OK on the controller
     node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-status.yml \
--limit &lt;controller node&gt;</pre></div></li><li class="step "><p>
     When above start operation has completed successfully, you may proceed to
     the next controller node. Ensure that you migrate your singleton services
     off the node first.
    </p></li></ol></div></div><div id="id-1.5.17.3.4.4.4.21" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    It is important that you not begin the reboot procedure for a new
    controller node until the reboot of the previous controller node has been
    completed successfully (that is, the ardana-status playbook has completed
    without error).
   </p></div><p>
   <span class="bold"><strong>
    Reenable the keystone Fernet Token-Signing Key Rotation
   </strong></span>
  </p><p>
   After all the controller nodes are successfully updated and back online, you
   need to re-enable the keystone Fernet token-signing key rotation job by
   running the <code class="filename">keystone-reconfigure.yml</code> playbook. On the
   deployer, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</pre></div></div><div class="sect4" id="idg-all-operations-maintenance-reboot-cloud-rolling-xml-9"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Rebooting compute nodes</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-reboot-cloud-rolling-xml-9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-reboot_cloud_rolling.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-reboot_cloud_rolling.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-reboot-cloud-rolling-xml-9</li></ul></div></div></div></div><p>
   To reboot a compute node the following operations will need to be performed:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Disable provisioning of the node to take the node offline to prevent
     further instances being scheduled to the node during the reboot.
    </p></li><li class="listitem "><p>
     Identify instances that exist on the compute node, and then either:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Live migrate the instances off the node before actioning the reboot. OR
      </p></li><li class="listitem "><p>
       Stop the instances
      </p></li></ul></div></li><li class="listitem "><p>
     Reboot the node
    </p></li><li class="listitem "><p>
     Restart the nova services
    </p></li></ul></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Disable provisioning:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service set --disable --disable-reason "<em class="replaceable ">DESCRIBE REASON</em>" compute nova-compute</pre></div><p>
     If the node has existing instances running on it these instances will need
     to be migrated or stopped prior to re-booting the node.
    </p></li><li class="step "><p>
     Live migrate existing instances. Identify the instances on the compute
     node. Note: The following command must be run with nova admin credentials.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list --host &lt;hostname&gt; --all-tenants</pre></div></li><li class="step "><p>
     Migrate or Stop the instances on the compute node.
    </p><p>
     Migrate the instances off the node by running one of the following
     commands for each of the instances:
    </p><p>
     If your instance is booted from a volume and has any number of cinder
     volume attached, use the nova live-migration command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova live-migration &lt;instance uuid&gt; [&lt;target compute host&gt;]</pre></div><p>
     If your instance has local (ephemeral) disk(s) only, you can use the
     --block-migrate option:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova live-migration --block-migrate &lt;instance uuid&gt; [&lt;target compute host&gt;]</pre></div><p>
     Note: The [&lt;target compute host&gt;] option is optional. If you do not
     specify a target host then the nova scheduler will choose a node for you.
    </p><p>
     OR
    </p><p>
     Stop the instances on the node by running the following command for each
     of the instances:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server stop &lt;instance-uuid&gt;</pre></div></li><li class="step "><p>
     Stop all services on the Compute node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-stop.yml --limit &lt;compute node&gt;</pre></div></li><li class="step "><p>
     SSH to your Compute nodes and reboot them:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo reboot</pre></div><p>
     The operating system cleanly shuts down services and then automatically
     reboots. If you want to be very thorough, run your backup jobs just before
     you reboot.
    </p></li><li class="step "><p>
     Run the ardana-start.yml playbook from the Cloud Lifecycle Manager. If needed, use
     the bm-power-up.yml playbook to restart the node. Specify just the node(s)
     you want to start in the 'nodelist' parameter arguments, that is,
     nodelist=&lt;node1&gt;[,&lt;node2&gt;][,&lt;node3&gt;].
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-power-up.yml -e nodelist=&lt;compute node&gt;</pre></div></li><li class="step "><p>
     Execute the <span class="bold"><strong>ardana-start.yml </strong></span>playbook.
     Specifying the node(s) you want to start in the 'limit' parameter
     arguments. This parameter accepts wildcard arguments and also
     '@&lt;filename&gt;' to process all hosts listed in the file.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit &lt;compute node&gt;</pre></div></li><li class="step "><p>
     Re-enable provisioning on the node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service set --enable compute nova-compute</pre></div></li><li class="step "><p>
     Restart any instances you stopped.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server start &lt;instance-uuid&gt;</pre></div></li></ol></div></div></div><div class="sect4" id="idg-all-operations-maintenance-reboot-cloud-rolling-xml-10"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Rebooting swift nodes</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-reboot-cloud-rolling-xml-10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-reboot_cloud_rolling.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-reboot_cloud_rolling.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-reboot-cloud-rolling-xml-10</li></ul></div></div></div></div><p>
   If your swift services are on controller node, please follow the controller
   node reboot instructions above.
  </p><p>
   For a dedicated swift PAC cluster or swift Object resource node:
  </p><p>
   For each swift host
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Stop all services on the swift node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-stop.yml --limit &lt;swift node&gt;</pre></div></li><li class="step "><p>
     Reboot the swift node by running the following command on the swift node
     itself:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo reboot</pre></div></li><li class="step "><p>
     Wait for the node to become ssh-able and then start all services on the
     swift node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit &lt;swift node&gt;</pre></div></li></ol></div></div></div><div class="sect4" id="idg-all-operations-maintenance-reboot-cloud-rolling-xml-14"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.1.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Get list of status playbooks</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-reboot-cloud-rolling-xml-14">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-reboot_cloud_rolling.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-reboot_cloud_rolling.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-reboot-cloud-rolling-xml-14</li></ul></div></div></div></div><p>
   The following command will display a list of status playbooks:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ls *status*</pre></div></div></div></div><div class="sect2" id="cont-planned"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Planned Control Plane Maintenance</span> <a title="Permalink" class="permalink" href="#cont-planned">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-cont_planned.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-cont_planned.xml</li><li><span class="ds-label">ID: </span>cont-planned</li></ul></div></div></div></div><p>
  Planned maintenance tasks for controller nodes such as full cloud reboots and
  replacing controller nodes.
 </p><div class="sect3" id="replacing-controller"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replacing a Controller Node</span> <a title="Permalink" class="permalink" href="#replacing-controller">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-replace_controller.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-replace_controller.xml</li><li><span class="ds-label">ID: </span>replacing-controller</li></ul></div></div></div></div><p>
  This section outlines steps for replacing a controller node in your
  environment.
 </p><p>
  For <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, you must have three controller nodes.
  Therefore, adding or removing nodes is not an option. However, if you need to
  repair or replace a controller node, you may do so by following the steps
  outlined here. Note that to run any playbooks whatsoever for cloud
  maintenance, you will always run the steps from the Cloud Lifecycle Manager.
 </p><p>
  These steps will depend on whether you need to replace a shared lifecycle
  manager/controller node or whether this is a standalone controller node.
 </p><p>Keep in mind while performing the following tasks:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Do not add entries for a new server. Instead, update the entries for
    the broken one.
   </p></li><li class="listitem "><p>
    Be aware that all management commands are run on the node where the
    Cloud Lifecycle Manager is running.
   </p></li></ul></div><div class="sect4" id="replace-shared-lm"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replacing a Shared Cloud Lifecycle Manager/Controller Node</span> <a title="Permalink" class="permalink" href="#replace-shared-lm">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-replace_shared_lm.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-replace_shared_lm.xml</li><li><span class="ds-label">ID: </span>replace-shared-lm</li></ul></div></div></div></div><p>
  If the controller node you need to replace was also being used as your
  Cloud Lifecycle Manager then use these steps below. If this is not a shared
  controller, skip to the next section.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    To ensure that you use the same version of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> that you
    previously had loaded on your Cloud Lifecycle Manager, you will need to download and install
    the lifecycle management software using the instructions from the
    installation guide:
   </p><p>
    <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 15 “Installing the Cloud Lifecycle Manager server”, Section 15.5.2 “Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Extension”</span>
   </p></li><li class="step "><p>
    Initialize the Cloud Lifecycle Manager platform by running <code class="command">ardana-init</code>.
   </p></li><li class="step "><p>
    To restore your data, see <a class="xref" href="#pit-lifecyclemanager-recovery" title="15.2.3.2.3. Point-in-time Cloud Lifecycle Manager Recovery">Section 15.2.3.2.3, “Point-in-time Cloud Lifecycle Manager Recovery”</a>.
    At this time, restore only the backup of <code class="systemitem">ardana</code>
    files on the system into <code class="filename">/var/lib/ardana</code> (the user's
    home directory.)
   </p></li><li class="step "><p>
    On the new node, update your cloud model with the new
    <code class="literal">mac-addr</code>, <code class="literal">ilo-ip</code>,
    <code class="literal">ilo-password</code>, and <code class="literal">ilo-user</code> fields to
    reflect the attributes of the node. Do not change the
    <code class="literal">id</code>, <code class="literal">ip-addr</code>, <code class="literal">role</code>,
    or <code class="literal">server-group</code> settings.
   </p><div id="id-1.5.17.3.5.3.7.3.4.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      When imaging servers with your own tooling, it is still necessary to have
      ILO/IPMI settings for all nodes. Even if you are not using Cobbler, the
      username and password fields in <code class="filename">servers.yml</code> need to
      be filled in with dummy settings. For example, add the following to
      <code class="filename">servers.yml</code>:
     </p><div class="verbatim-wrap"><pre class="screen">ilo-user: manual
ilo-password: deployment</pre></div></div></li><li class="step "><p>
    Open the <code class="filename">servers.yml</code> file describing your cloud nodes:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git -C ~/openstack checkout site
<code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition/data
<code class="prompt user">ardana &gt; </code>vi servers.yml</pre></div><p>
    Change, as necessary, the <code class="literal">mac-addr</code>,
    <code class="literal">ilo-ip</code>, <code class="literal">ilo-password</code>, and
    <code class="literal">ilo-user</code> fields of the existing controller node. Save
    and commit the change:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git commit -a -m "repaired node X"</pre></div></li><li class="step "><p>
    Run the configuration processor and ready-deployment playbooks as follows:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
    Run the <code class="filename">wipe_disks.yml</code> playbook to ensure
    all non-OS partitions on the new node are completely wiped prior to
    continuing with the installation. (The value to be used for
    <code class="literal">hostname</code> is the host's identifier from
    <code class="filename">~/scratch/ansible/next/ardana/ansible/hosts/verb_hosts</code>.)
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit <em class="replaceable ">deployer_node_name</em></pre></div><p>
    The value for <em class="replaceable ">deployer_node_name</em> should be the
    name identifying the deployer/controller being initialized as it is
    represented in the <code class="filename">hosts/verb_hosts</code> file.
   </p></li><li class="step "><p>
    Deploy Cobbler:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step "><p>
    Refer again to <a class="xref" href="#pit-lifecyclemanager-recovery" title="15.2.3.2.3. Point-in-time Cloud Lifecycle Manager Recovery">Section 15.2.3.2.3, “Point-in-time Cloud Lifecycle Manager Recovery”</a> and proceed
    to restore all remaining backups, with the exclusion of
    <code class="systemitem">/var/lib/ardana</code> (which was done earlier) and the
    cobbler content in <code class="systemitem">/var/lib/cobbler</code> and
    <code class="systemitem">/srv/www/cobbler</code>.
   </p></li><li class="step "><p>
    Install the software on your new Cloud Lifecycle Manager/controller node with these playbooks:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-rebuild-pretasks.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-run.yml \
-e rebuild=True --limit <em class="replaceable ">deployer_node_name</em>
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-deploy.yml \
-e rebuild=True --limit <em class="replaceable ">deployer_node_name</em>,localhost
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts tempest-deploy.yml</pre></div><div id="id-1.5.17.3.5.3.7.3.10.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
     If you receive the message <code class="literal">stderr: Error:
     mnesia_not_running</code> when running the
     <code class="filename">ardana-deploy.yml</code> playbook, it is likely due to one
     of the following conditions:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       RabbitMQ was not running on the clustered node
      </p></li><li class="listitem "><p>
       The old node was not removed from the cluster
      </p></li></ul></div><p>
     Correct this problem with the following steps:
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Of the remaining clustered nodes (<em class="replaceable ">M2</em> and
       <em class="replaceable ">M3</em>), <em class="replaceable ">M2</em> is the new
       master. Make sure the application has started and M1 is no longer a
       member. On the <em class="replaceable ">M2</em> node, run:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo rabbitmqctl start_app; \
<code class="prompt user">ardana &gt; </code>sudo rabbitmqctl forget_cluster_node rabbit@<em class="replaceable ">M1</em></pre></div><p>
       Check that <em class="replaceable ">M1</em> is no longer a member of the cluster.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo rabbitmqctl cluster_status</pre></div></li><li class="step "><p>
       On the newly installed node, <em class="replaceable ">M1</em>, make sure
       RabbitMQ has stopped. On <em class="replaceable ">M1</em>, run:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo rabbitmqctl stop_app</pre></div></li><li class="step "><p>
       Re-run the <code class="filename">ardana-deploy.yml</code> playbook as before.
      </p></li></ol></div></div></div></li><li class="step "><p>
    During the replacement of the node, alarms will show up during
    the process. If those do not clear after the node is back up and healthy,
    restart the threshold engine by running the following playbooks:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-stop.yml --tags thresh
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</pre></div></li></ol></div></div></div><div class="sect4" id="replace-dedicated-lm"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replacing a Standalone Controller Node</span> <a title="Permalink" class="permalink" href="#replace-dedicated-lm">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-replace_dedicated_lm.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-replace_dedicated_lm.xml</li><li><span class="ds-label">ID: </span>replace-dedicated-lm</li></ul></div></div></div></div><p>
  If the controller node you need to replace is not also being used as the
  Cloud Lifecycle Manager, follow the steps below.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="step "><p>
    Update your cloud model, specifically the <code class="literal">servers.yml</code>
    file, with the new <code class="literal">mac-addr</code>, <code class="literal">ilo-ip</code>,
    <code class="literal">ilo-password</code>, and <code class="literal">ilo-user</code> fields
    where these have changed. Do not change the <code class="literal">id</code>,
    <code class="literal">ip-addr</code>, <code class="literal">role</code>, or
    <code class="literal">server-group</code> settings.
   </p></li><li class="step "><p>
    Commit your configuration to the <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>, as follows:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
    Run the configuration processor:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
    Update your deployment directory:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
    Remove the old controller node(s) from Cobbler. You can list out the
    systems in Cobbler currently with this command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cobbler system list</pre></div><p>
    and then remove the old controller nodes with this command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cobbler system remove --name &lt;node&gt;</pre></div></li><li class="step "><p>
    Remove the SSH key of the old controller node from the known hosts file.
    You will specify the <code class="literal">ip-addr</code> value:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ssh-keygen -f "~/.ssh/known_hosts" -R &lt;ip_addr&gt;</pre></div><p>
    You should see a response similar to this one:
   </p><div class="verbatim-wrap"><pre class="screen">ardana@ardana-cp1-c1-m1-mgmt:~/openstack/ardana/ansible$ ssh-keygen -f "~/.ssh/known_hosts" -R 10.13.111.135
# Host 10.13.111.135 found: line 6 type ECDSA
~/.ssh/known_hosts updated.
Original contents retained as ~/.ssh/known_hosts.old</pre></div></li><li class="step "><p>
    Run the cobbler-deploy playbook to add the new controller node:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step "><p>
    Image the new node(s) by using the bm-reimage playbook. You will specify
    the name for the node in Cobbler in the command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node-name&gt;</pre></div><div id="id-1.5.17.3.5.3.8.3.9.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
     You must ensure that the old controller node is powered off before
     completing this step. This is because the new controller node will re-use
     the original IP address.
    </p></div></li><li class="step "><p>
    Run the <code class="filename">wipe_disks.yml</code> playbook to ensure
    all non-OS partitions on the new node are completely wiped prior to
    continuing with the installation. (The value to be used for
    <code class="literal">hostname</code> is the host's identifier from
    <code class="filename">~/scratch/ansible/next/ardana/ansible/hosts/verb_hosts</code>.)
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit &lt;hostname&gt;</pre></div></li><li class="step "><p>
    Run osconfig on the replacement controller node. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-run.yml -e rebuild=True --limit=&lt;controller-hostname&gt;</pre></div></li><li class="step "><p>
    If the controller being replaced is the swift ring builder (see
    <a class="xref" href="#topic-rtc-s3t-mt" title="18.6.2.4. Identifying the Swift Ring Building Server">Section 18.6.2.4, “Identifying the Swift Ring Building Server”</a>) you need to restore the swift ring
    builder files to the <code class="literal">/etc/swiftlm/<em class="replaceable ">CLOUD-NAME</em>/<em class="replaceable ">CONTROL-PLANE-NAME</em>/builder_dir</code> directory.
    See <a class="xref" href="#topic-gbz-13t-mt" title="18.6.2.7. Recovering swift Builder Files">Section 18.6.2.7, “Recovering swift Builder Files”</a> for details.
   </p></li><li class="step "><p>
    Run the ardana-deploy playbook on the replacement controller.
   </p><p>
    If the node being replaced is the swift ring builder server then you only
    need to use the <code class="literal">--limit</code> switch for that node, otherwise
    you need to specify the hostname of your swift ringer builder server and
    the hostname of the node being replaced.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-deploy.yml -e rebuild=True
--limit=&lt;controller-hostname&gt;,&lt;swift-ring-builder-hostname&gt;</pre></div><div id="id-1.5.17.3.5.3.8.3.13.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
     If you receive a keystone failure when running this playbook, it is
     likely due to Fernet keys being out of sync. This problem can be corrected
     by running the <code class="filename">keystone-reconfigure.yml</code> playbook to
     re-sync the Fernet keys.
    </p><p>
     In this situation, do not use the <code class="literal">--limit</code> option when
     running <code class="filename">keystone-reconfigure.yml</code>. In order to re-sync
     Fernet keys, all the controller nodes must be in the play.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-reconfigure.yml</pre></div></div><div id="id-1.5.17.3.5.3.8.3.13.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
     If you receive a RabbitMQ failure when running this playbook, review
     <a class="xref" href="#recoverrabbit" title="18.2.1. Understanding and Recovering RabbitMQ after Failure">Section 18.2.1, “Understanding and Recovering RabbitMQ after Failure”</a> for how to resolve the issue and then
     re-run the ardana-deploy playbook.
    </p></div></li><li class="step "><p>
    During the replacement of the node there will be alarms that show up during
    the process. If those do not clear after the node is back up and healthy,
    restart the threshold engine by running the following playbooks:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-stop.yml --tags thresh
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml --tags thresh</pre></div></li></ol></div></div></div></div></div><div class="sect2" id="comp-planned"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Planned Compute Maintenance</span> <a title="Permalink" class="permalink" href="#comp-planned">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-comp_planned.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-comp_planned.xml</li><li><span class="ds-label">ID: </span>comp-planned</li></ul></div></div></div></div><p>
  Planned maintenance tasks for compute nodes.
 </p><div class="sect3" id="planned-computenode"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Planned Maintenance for a Compute Node</span> <a title="Permalink" class="permalink" href="#planned-computenode">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-planned_computenode.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-planned_computenode.xml</li><li><span class="ds-label">ID: </span>planned-computenode</li></ul></div></div></div></div><p>
  If one or more of your compute nodes needs hardware maintenance and you can
  schedule a planned maintenance then this procedure should be followed.
 </p><div class="sect4" id="id-1.5.17.3.6.3.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.3.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Performing planned maintenance on a compute node</span> <a title="Permalink" class="permalink" href="#id-1.5.17.3.6.3.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-planned_computenode.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-planned_computenode.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   If you have planned maintenance to perform on a compute node, you have to
   take it offline, repair it, and restart it. To do so, follow these steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Source the administrator credentials:
    </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc</pre></div></li><li class="step "><p>
     Obtain the hostname for your compute node, which you will use in
     subsequent commands when <code class="literal">&lt;hostname&gt;</code> is requested:
    </p><div class="verbatim-wrap"><pre class="screen">openstack host list | grep compute</pre></div><p>
     The following example shows two compute nodes:
    </p><div class="verbatim-wrap"><pre class="screen">$ openstack host list | grep compute
| ardana-cp1-comp0001-mgmt | compute     | AZ1      |
| ardana-cp1-comp0002-mgmt | compute     | AZ2      |</pre></div></li><li class="step "><p>
     Disable provisioning on the compute node, which will prevent additional
     instances from being spawned on it:
    </p><div class="verbatim-wrap"><pre class="screen">openstack compute service set –disable --reason "Maintenance mode" &lt;hostname&gt;</pre></div><div id="id-1.5.17.3.6.3.3.3.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      Make sure you re-enable provisioning after the maintenance is complete
      if you want to continue to be able to spawn instances on the node. You
      can do this with the command:
     </p><div class="verbatim-wrap"><pre class="screen">openstack compute service set –enable &lt;hostname&gt;</pre></div></div></li><li class="step "><p>
     At this point you have two choices:
    </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
       <span class="bold"><strong>Live migration</strong></span>: This option enables you
       to migrate the instances off the compute node with minimal downtime so
       you can perform the maintenance without risk of losing data.
      </p></li><li class="listitem "><p>
       <span class="bold"><strong>Stop/start the instances</strong></span>: Issuing
       <code class="literal">openstack server stop</code> commands to each of the
       instances will halt them. This option lets you do maintenance and then
       start the instances back up, as long as no disk failures occur on the
       compute node data disks. This method involves downtime for the length of
       the maintenance.
      </p></li></ol></div><p>
     If you choose the live migration route, See
     <a class="xref" href="#liveInstMigration" title="15.1.3.3. Live Migration of Instances">Section 15.1.3.3, “Live Migration of Instances”</a> for more details. Skip to step #6
     after you finish live migration.
    </p><p>
     If you choose the stop start method, continue on.
    </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
       List all of the instances on the node so you can issue stop commands to
       them:
      </p><div class="verbatim-wrap"><pre class="screen">openstack server list --host &lt;hostname&gt; --all-tenants</pre></div></li><li class="listitem "><p>
       Issue the <code class="literal">openstack server stop</code> command against each of the
       instances:
      </p><div class="verbatim-wrap"><pre class="screen">openstack server stop &lt;instance uuid&gt;</pre></div></li><li class="listitem "><p>
       Confirm that the instances are stopped. If stoppage was successful you
       should see the instances in a <code class="literal">SHUTOFF</code> state, as shown
       here:
      </p><div class="verbatim-wrap"><pre class="screen">$ openstack server list --host ardana-cp1-comp0002-mgmt --all-tenants
+--------------------------------------+-----------+----------------------------------+---------+------------+-------------+-----------------------+
| ID                                   | Name      | Tenant ID                        | Status  | Task State | Power State | Networks              |
+--------------------------------------+-----------+----------------------------------+---------+------------+-------------+-----------------------+
| ef31c453-f046-4355-9bd3-11e774b1772f | instance1 | 4365472e025c407c8d751fc578b7e368 | <span class="bold"><strong>SHUTOFF</strong></span> | -          | Shutdown    | demo_network=10.0.0.5 |
+--------------------------------------+-----------+----------------------------------+---------+------------+-------------+-----------------------+</pre></div></li><li class="listitem "><p>
       Do your required maintenance. If this maintenance does not take down the
       disks completely then you should be able to list the instances again
       after the repair and confirm that they are still in their
       <code class="literal">SHUTOFF</code> state:
      </p><div class="verbatim-wrap"><pre class="screen">openstack server list --host &lt;hostname&gt; --all-tenants</pre></div></li><li class="listitem "><p>
       Start the instances back up using this command:
      </p><div class="verbatim-wrap"><pre class="screen">openstack server start &lt;instance uuid&gt;</pre></div><p>
       Example:
      </p><div class="verbatim-wrap"><pre class="screen">$ openstack server start ef31c453-f046-4355-9bd3-11e774b1772f
Request to start server ef31c453-f046-4355-9bd3-11e774b1772f has been accepted.</pre></div></li><li class="listitem "><p>
       Confirm that the instances started back up. If restarting is successful
       you should see the instances in an <code class="literal">ACTIVE</code> state, as
       shown here:
      </p><div class="verbatim-wrap"><pre class="screen">$ openstack server list --host ardana-cp1-comp0002-mgmt --all-tenants
+--------------------------------------+-----------+----------------------------------+--------+------------+-------------+-----------------------+
| ID                                   | Name      | Tenant ID                        | Status | Task State | Power State | Networks              |
+--------------------------------------+-----------+----------------------------------+--------+------------+-------------+-----------------------+
| ef31c453-f046-4355-9bd3-11e774b1772f | instance1 | 4365472e025c407c8d751fc578b7e368 | <span class="bold"><strong>ACTIVE</strong></span> | -          | Running     | demo_network=10.0.0.5 |
+--------------------------------------+-----------+----------------------------------+--------+------------+-------------+-----------------------+</pre></div></li><li class="listitem "><p>
       If the <code class="literal">openstack server start</code> fails, you can try doing a hard
       reboot:
      </p><div class="verbatim-wrap"><pre class="screen">openstack server reboot --hard &lt;instance uuid&gt;</pre></div><p>
       If this does not resolve the issue you may want to contact support.
      </p></li></ol></div></li><li class="step "><p>
     Re-enable provisioning when the node is fixed:
    </p><div class="verbatim-wrap"><pre class="screen">openstack compute service set –enable &lt;hostname&gt;</pre></div></li></ol></div></div></div></div><div class="sect3" id="reboot-computenode"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Rebooting a Compute Node</span> <a title="Permalink" class="permalink" href="#reboot-computenode">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-reboot_computenode.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-reboot_computenode.xml</li><li><span class="ds-label">ID: </span>reboot-computenode</li></ul></div></div></div></div><p>
  If all you need to do is reboot a Compute node, the following steps can be
  used.
 </p><p>
  You can choose to live migrate all Compute instances off the node prior to
  the reboot. Any instances that remain will be restarted when the node is
  rebooted. This playbook will ensure that all services on the Compute node are
  restarted properly.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="step "><p>
    Reboot the Compute node(s) with the following playbook.
   </p><p>
    You can specify either single or multiple Compute nodes using the
    <code class="literal">--limit</code> switch.
   </p><p>
    An optional reboot wait time can also be specified. If no reboot wait time
    is specified it will default to 300 seconds.
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts nova-compute-reboot.yml --limit [compute_node_or_list] [-e nova_reboot_wait_timeout=(seconds)]</pre></div><div id="id-1.5.17.3.6.4.4.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     If the Compute node fails to reboot, you should troubleshoot this issue
     separately as this playbook will not attempt to recover after a failed
     reboot.
    </p></div></li></ol></div></div></div><div class="sect3" id="liveInstMigration"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Live Migration of Instances</span> <a title="Permalink" class="permalink" href="#liveInstMigration">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-live_migration.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-live_migration.xml</li><li><span class="ds-label">ID: </span>liveInstMigration</li></ul></div></div></div></div><p>
  Live migration allows you to move active compute instances between compute
  nodes, allowing for less downtime during maintenance.
 </p><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> nova offers a set of commands that allow you to move compute
  instances between compute hosts. Which command you use will depend on the
  state of the host, what operating system is on the host, what type of storage
  the instances are using, and whether you want to migrate a single instance or
  all of the instances off of the host. We will describe these options on this
  page as well as give you step-by-step instructions for performing them.
 </p><div class="sect4" id="options"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.3.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Migration Options</span> <a title="Permalink" class="permalink" href="#options">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-live_migration.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-live_migration.xml</li><li><span class="ds-label">ID: </span>options</li></ul></div></div></div></div><p>
   <span class="bold"><strong>If your compute node has failed</strong></span>
  </p><p>
   A compute host failure could be caused by hardware failure, such as the data
   disk needing to be replaced, power has been lost, or any other type of
   failure which requires that you replace the baremetal host. In this
   scenario, the instances on the compute node are unrecoverable and any data
   on the local ephemeral storage is lost. If you are utilizing block storage
   volumes, either as a boot device or as additional storage, they should be
   unaffected.
  </p><p>
   In these cases you will want to use one of the nova evacuate commands, which
   will cause nova to rebuild the instances on other hosts.
  </p><p>
   This table describes each of the evacuate options for failed compute nodes:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Command</th><th>Description</th></tr></thead><tbody><tr><td>
       <p>
        <code class="literal">nova evacuate &lt;instance&gt; &lt;hostname&gt;</code>
       </p>
      </td><td>
       <p>
        This command is used to evacuate a single instance from a failed host.
        You specify the compute instance UUID and the target host you want to
        evacuate it to. If no host is specified then the nova scheduler will
        choose one for you.
       </p>
       <p>
        See <code class="literal">nova help evacuate</code> for more information and
        syntax. Further details can also be seen in the OpenStack documentation
        at
        <a class="link" href="http://docs.openstack.org/admin-guide/cli_nova_evacuate.html" target="_blank">http://docs.openstack.org/admin-guide/cli_nova_evacuate.html</a>.
       </p>
      </td></tr><tr><td>
       <p>
        <code class="literal">nova host-evacuate &lt;hostname&gt; --target_host
        &lt;target_hostname&gt;</code>
       </p>
      </td><td>
       <p>
        This command is used to evacuate all instances from a failed host. You
        specify the hostname of the compute host you want to evacuate.
        Optionally you can specify a target host. If no target host is
        specified then the nova scheduler will choose a target host for each
        instance.
       </p>
       <p>
        See <code class="literal">nova help host-evacuate</code> for more information and
        syntax.
       </p>
      </td></tr></tbody></table></div><p>
   If your compute host is active, powered on and the data disks are in working order
   you can utilize the migration commands to migrate your compute instances.
   There are two migration features, "cold" migration (also referred to simply
   as "migration") and live migration. Migration and live migration are two
   different functions.
  </p><p>
   <span class="bold"><strong>Cold migration</strong></span> is used to copy an instances
   data in a <code class="literal">SHUTOFF</code> status from one compute host to
   another. It does this using passwordless SSH access which has security
   concerns associated with it. For this reason, the <code class="literal">openstack server
   migrate</code> function has been disabled by default but you have the
   ability to enable this feature if you would like. Details on how to do this
   can be found in <a class="xref" href="#enabling-the-nova-resize" title="6.4. Enabling the Nova Resize and Migrate Features">Section 6.4, “Enabling the Nova Resize and Migrate Features”</a>.
  </p><p>
   <span class="bold"><strong>Live migration</strong></span> can be performed on
   instances in either an <code class="literal">ACTIVE</code> or
   <code class="literal">PAUSED</code> state and uses the QEMU hypervisor to manage the
   copy of the running processes and associated resources to the destination
   compute host using the hypervisors own protocol and thus is a more secure
   method and allows for less downtime. There may be a short network outage,
   usually a few milliseconds but could be up to a few seconds if your compute
   instances are busy, during a live migration. Also there may be some
   performance degredation during the process.
  </p><p>
   The compute host must remain powered on during the migration process.
  </p><p>
   Both the cold migration and live migration options will honor nova group
   policies, which includes affinity settings. There is a limitation to keep in
   mind if you use group policies and that is discussed in the
   <a class="xref" href="#liveInstMigration" title="15.1.3.3. Live Migration of Instances">Section 15.1.3.3, “Live Migration of Instances”</a> section.
  </p><p>
   This table describes each of the migration options for active compute nodes:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Command</th><th>Description</th><th>SLES</th></tr></thead><tbody><tr><td>
       <p>
        <code class="literal">openstack server migrate &lt;instance_uuid&gt;</code>
       </p>
      </td><td>
       <p>
        Used to cold migrate a single instance from a compute host. The
        <code class="literal">nova-scheduler</code> will choose the new host.
       </p>
       <p>
        This command will work against instances in an
        <code class="literal">ACTIVE</code> or <code class="literal">SHUTOFF</code> state. The
        instances, if active, will be shutdown and restarted. Instances in a
        <code class="literal">PAUSED</code> state cannot be cold migrated.
       </p>
       <p>
        See the difference between cold migration and live migration at the
        start of this section.
       </p>
      </td><td> </td></tr><tr><td>
       <p>
        <code class="literal">nova host-servers-migrate &lt;hostname&gt;</code>
       </p>
      </td><td>
       <p>
        Used to cold migrate all instances off a specified host to other
        available hosts, chosen by the <code class="literal">nova-scheduler</code>.
       </p>
       <p>
        This command will work against instances in an
        <code class="literal">ACTIVE</code> or <code class="literal">SHUTOFF</code> state. The
        instances, if active, will be shutdown and restarted. Instances in a
        <code class="literal">PAUSED</code> state cannot be cold migrated.
       </p>
       <p>
        See the difference between cold migration and live migration at the
        start of this section.
       </p>
      </td><td> </td></tr><tr><td>
       <p>
        <code class="literal">nova live-migration &lt;instance_uuid&gt; [&lt;target
        host&gt;]</code>
       </p>
      </td><td>
       <p>
        Used to migrate a single instance between two compute hosts. You can
        optionally specify a target host or you can allow the nova scheduler to
        choose a host for you. If you choose to specify a target host, ensure
        that the target host has enough resources to host the instance prior to
        live migration.
       </p>
       <p>
        Works for instances that are booted from a block storage volume or that
        have any number of block storage volumes attached.
       </p>
       <p>
        This command works against instances in <code class="literal">ACTIVE</code> or
        <code class="literal">PAUSED</code> states only.
       </p>
      </td><td>X</td></tr><tr><td>
       <p>
        <code class="literal">nova live-migration --block-migrate &lt;instance_uuid&gt;
        [&lt;target host&gt;]</code>
       </p>
      </td><td>
       <p>
        Used to migrate a single instance between two compute hosts. You can
        optionally specify a target host or you can allow the nova scheduler to
        choose a host for you. If you choose to specify a target host, ensure
        that the target host has enough resources to host the instance prior to
        live migration.
       </p>
       <p>
        Works for instances that are booted from local (ephemeral) disk(s) only
        or if your instance has a mix of ephemeral disk(s) and block storage
        volume(s) but are not booted from a block storage volume.
       </p>
       <p>
        This command works against instances in <code class="literal">ACTIVE</code> or
        <code class="literal">PAUSED</code> states only.
       </p>
      </td><td>X</td></tr><tr><td>
       <p>
        <code class="literal">nova host-evacuate-live &lt;hostname&gt; [--target-host
        &lt;target_hostname&gt;]</code>
       </p>
      </td><td>
       <p>
        Used to live migrate all instances off of a compute host. You can
        optionally specify a target host or you can allow the nova scheduler to
        choose a host for you. If you choose to specify a target host, ensure
        that the target host has enough resources to host the instance prior to
        live migration.
       </p>
       <p>
        Works for instances that are booted from a block storage volume or that
        have any number of block storage volumes attached.
       </p>
       <p>
        This command works against instances in <code class="literal">ACTIVE</code> or
        <code class="literal">PAUSED</code> states only.
       </p>
      </td><td>X</td></tr><tr><td>
       <p>
        <code class="literal">nova host-evacuate-live --block-migrate &lt;hostname&gt;
        [--target-host &lt;target_hostname&gt;]</code>
       </p>
      </td><td>
       <p>
        Used to live migrate all instances off of a compute host. You can
        optionally specify a target host or you can allow the nova scheduler to
        choose a host for you. If you choose to specify a target host, ensure
        that the target host has enough resources to host the instance prior to
        live migration.
       </p>
       <p>
        Works for instances that are booted from local (ephemeral) disk(s) only
        or if your instance has a mix of ephemeral disk(s) and block storage
        volume(s) but are not booted from a block storage volume.
       </p>
       <p>
        This command works against instances in <code class="literal">ACTIVE</code> or
        <code class="literal">PAUSED</code> states only.
       </p>
      </td><td>X</td></tr></tbody></table></div></div><div class="sect4" id="idg-all-operations-maintenance-live-migration-xml-10"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.3.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Limitations of these Features</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-live-migration-xml-10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-live_migration.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-live_migration.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-live-migration-xml-10</li></ul></div></div></div></div><p>
   There are limitations that may impact your use of this feature:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     To use live migration, your compute instances must be in either an
     <code class="literal">ACTIVE</code> or <code class="literal">PAUSED</code> state on the
     compute host. If you have instances in a <code class="literal">SHUTOFF</code> state
     then cold migration should be used.
    </p></li><li class="listitem "><p>
     Instances in a <code class="literal">Paused</code> state cannot be live migrated
     using the horizon dashboard. You will need to utilize the <code class="literal">python-novaclient</code>
     CLI to perform these.
    </p></li><li class="listitem "><p>
     Both cold migration and live migration honor an instance's group policies.
     If you are utilizing an affinity policy and are migrating multiple
     instances you may run into an error stating no hosts are available to
     migrate to. To work around this issue you should specify a target host
     when migrating these instances, which will bypass the
     <code class="literal">nova-scheduler</code>. You should ensure that the target host
     you choose has the resources available to host the instances.
    </p></li><li class="listitem "><p>
     The <code class="literal">nova host-evacuate-live</code> command will produce an
     error if you have a compute host that has a mix of instances that use
     local ephemeral storage and instances that are booted from a block storage
     volume or have any number of block storage volumes attached. If you have a
     mix of these instance types, you may need to run the command twice,
     utilizing the <code class="literal">--block-migrate</code> option. This is described
     in further detail in <a class="xref" href="#liveInstMigration" title="15.1.3.3. Live Migration of Instances">Section 15.1.3.3, “Live Migration of Instances”</a>.
    </p></li><li class="listitem "><p>
     Instances on KVM hosts can only be live migrated to other KVM hosts.
    </p></li><li class="listitem "><p>
     The migration options described in this document are not available on ESX
     compute hosts.
    </p></li><li class="listitem "><p>
     Ensure that you read and take into account any other limitations that
     exist in the release notes. See the release notes for
     more details.
    </p></li></ul></div></div><div class="sect4" id="liveMigrate"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.3.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Performing a Live Migration</span> <a title="Permalink" class="permalink" href="#liveMigrate">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-live_migration.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-live_migration.xml</li><li><span class="ds-label">ID: </span>liveMigrate</li></ul></div></div></div></div><p>
   Cloud administrators can perform a migration on an instance using either the
   horizon dashboard, API, or CLI. Instances in a <code class="literal">Paused</code>
   state cannot be live migrated using the horizon GUI. You will need to
   utilize the CLI to perform these.
  </p><p>
   We have documented different scenarios:
  </p></div><div class="sect4" id="failed-host"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.3.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Migrating instances off of a failed compute host</span> <a title="Permalink" class="permalink" href="#failed-host">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-live_migration.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-live_migration.xml</li><li><span class="ds-label">ID: </span>failed-host</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     If the compute node is not already powered off, do so with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-power-down.yml -e nodelist=&lt;node_name&gt;</pre></div><div id="id-1.5.17.3.6.5.7.2.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The value for <code class="literal">&lt;node_name&gt;</code> will be the name that
      Cobbler has when you run <code class="command">sudo cobbler system list</code> from
      the Cloud Lifecycle Manager.
     </p></div></li><li class="step "><p>
     Source the admin credentials necessary to run administrative commands
     against the nova API:
    </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc</pre></div></li><li class="step "><p>
     Force the <code class="literal">nova-compute</code> service to go down on the
     compute node:
    </p><div class="verbatim-wrap"><pre class="screen">openstack compute service set --down <em class="replaceable ">HOSTNAME</em> nova-compute</pre></div><div id="id-1.5.17.3.6.5.7.2.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The value for <em class="replaceable ">HOSTNAME</em> can be obtained by
      using <code class="command">openstack host list</code> from the Cloud Lifecycle Manager.
     </p></div></li><li class="step "><p>
     Evacuate the instances off of the failed compute node. This will cause the
     nova-scheduler to rebuild the instances on other valid hosts. Any local
     ephemeral data on the instances is lost.
    </p><p>
     For single instances on a failed host:
    </p><div class="verbatim-wrap"><pre class="screen">nova evacuate &lt;instance_uuid&gt; &lt;target_hostname&gt;</pre></div><p>
     For all instances on a failed host:
    </p><div class="verbatim-wrap"><pre class="screen">nova host-evacuate &lt;hostname&gt; [--target_host &lt;target_hostname&gt;]</pre></div></li><li class="step "><p>
     When you have repaired the failed node and start it back up again, when
     the <code class="command">nova-compute</code> process starts again, it will clean
     up the evacuated instances.
    </p></li></ol></div></div></div><div class="sect4" id="active-host"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.3.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Migrating instances off of an active compute host</span> <a title="Permalink" class="permalink" href="#active-host">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-live_migration.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-live_migration.xml</li><li><span class="ds-label">ID: </span>active-host</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Migrating instances using the horizon
   dashboard</strong></span>
  </p><p>
   The horizon dashboard offers a GUI method for performing live migrations.
   Instances in a <code class="literal">Paused</code> state will not provide you the live
   migration option in horizon so you will need to use the CLI instructions in
   the next section to perform these.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log into the horizon dashboard with admin credentials.
    </p></li><li class="step "><p>
     Navigate to the menu <span class="guimenu ">Admin</span> › <span class="guimenu ">Compute</span> › <span class="guimenu ">Instances</span>.
    </p></li><li class="step "><p>
     Next to the instance you want to migrate, select the drop down menu and
     choose the <span class="guimenu ">Live Migrate Instance</span> option.
    </p></li><li class="step "><p>
     In the Live Migrate wizard you will see the compute host the instance
     currently resides on and then a drop down menu that allows you to choose
     the compute host you want to migrate the instance to. Select a destination
     host from that menu. You also have two checkboxes for additional options,
     which are described below:
    </p><p>
     <span class="guimenu ">Disk Over Commit</span> - If this is not checked
     then the value will be <code class="literal">False</code>. If you check this box
     then it will allow you to override the check that occurs to ensure the
     destination host has the available disk space to host the instance.
    </p><p>
     <span class="guimenu ">Block Migration</span> - If this is not checked
     then the value will be <code class="literal">False</code>. If you check this box
     then it will migrate the local disks by using block migration. Use this
     option if you are only using ephemeral storage on your instances. If you
     are using block storage for your instance then ensure this box is not
     checked.
    </p></li><li class="step "><p>
     To begin the live migration, click <span class="guimenu ">Submit</span>.
    </p></li></ol></div></div><p>
   <span class="bold"><strong>Migrating instances using the python-novaclient
   CLI</strong></span>
  </p><p>
   To perform migrations from the command-line, use the <code class="literal">python-novaclient</code>.
   The Cloud Lifecycle Manager node in your cloud environment should have
   the <code class="literal">python-novaclient</code> already installed. If you will be accessing your environment
   through a different method, ensure that the <code class="literal">python-novaclient</code> is
   installed. You can do so using Python's <code class="command">pip</code> package
   manager.
  </p><p>
   To run the commands in the steps below, you need administrator
   credentials. From the Cloud Lifecycle Manager, you can source the
   <code class="filename">service.osrc</code> file which is provided that has the
   necessary credentials:
  </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc</pre></div><p>
   Here are the steps to perform:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Identify the instances on the compute node you wish to migrate:
    </p><div class="verbatim-wrap"><pre class="screen">openstack server list --all-tenants --host &lt;hostname&gt;</pre></div><p>
     Example showing a host with a single compute instance on it:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code> openstack server list --host ardana-cp1-comp0001-mgmt --all-tenants
+--------------------------------------+------+----------------------------------+--------+------------+-------------+-----------------------+
| ID                                   | Name | Tenant ID                        | Status | Task State | Power State | Networks              |
+--------------------------------------+------+----------------------------------+--------+------------+-------------+-----------------------+
| 553ba508-2d75-4513-b69a-f6a2a08d04e3 | test | 193548a949c146dfa1f051088e141f0b | ACTIVE | -          | Running     | adminnetwork=10.0.0.5 |
+--------------------------------------+------+----------------------------------+--------+------------+-------------+-----------------------+</pre></div></li><li class="step "><p>
     When using live migration you can either specify a target host that the
     instance will be migrated to or you can omit the target to allow the
     nova-scheduler to choose a node for you. If you want to get a list of
     available hosts you can use this command:
    </p><div class="verbatim-wrap"><pre class="screen">openstack host list</pre></div></li><li class="step "><p>
     Migrate the instance(s) on the compute node using the notes below.
    </p><p>
     If your instance is booted from a block storage volume or has any number
     of block storage volumes attached, use the <code class="literal">nova
     live-migration</code> command with this syntax:
    </p><div class="verbatim-wrap"><pre class="screen">nova live-migration &lt;instance uuid&gt; [&lt;target compute host&gt;]</pre></div><p>
     If your instance has local (ephemeral) disk(s) only or if your instance
     has a mix of ephemeral disk(s) and block storage volume(s), you should use
     the <code class="literal">--block-migrate</code> option:
    </p><div class="verbatim-wrap"><pre class="screen">nova live-migration --block-migrate &lt;instance uuid&gt; [&lt;target compute host&gt;]</pre></div><div id="id-1.5.17.3.6.5.8.10.4.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The <code class="literal">[&lt;target compute host&gt;]</code> option is
      optional. If you do not specify a target host then the nova scheduler
      will choose a node for you.
     </p></div><p>
     <span class="bold"><strong>Multiple instances</strong></span>
    </p><p>
     If you want to live migrate all of the instances off a single compute host
     you can utilize the <code class="literal">nova host-evacuate-live</code> command.
    </p><p>
     Issue the host-evacuate-live command, which will begin the live migration
     process.
    </p><p>
     If all of the instances on the host are using at least one local
     (ephemeral) disk, you should use this syntax:
    </p><div class="verbatim-wrap"><pre class="screen">nova host-evacuate-live --block-migrate &lt;hostname&gt;</pre></div><p>
     Alternatively, if all of the instances are only using block storage
     volumes then omit the <code class="literal">--block-migrate</code> option:
    </p><div class="verbatim-wrap"><pre class="screen">nova host-evacuate-live &lt;hostname&gt;</pre></div><div id="id-1.5.17.3.6.5.8.10.4.14" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      You can either let the nova-scheduler choose a suitable target host or
      you can specify one using the
      <code class="literal">--target-host &lt;hostname&gt;</code> switch. See
      <code class="command">nova help host-evacuate-live</code> for details.
     </p></div></li></ol></div></div></div><div class="sect4" id="idg-all-operations-maintenance-live-migration-xml-14"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.3.3.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting migration or host evacuate issues</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-live-migration-xml-14">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-live_migration.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-live_migration.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-live-migration-xml-14</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Issue:</strong></span> When attempting to use <code class="literal">nova
   host-evacuate-live</code> against a node, you receive the error below:
  </p><div class="verbatim-wrap"><pre class="screen">$ nova host-evacuate-live ardana-cp1-comp0001-mgmt --target-host ardana-cp1-comp0003-mgmt
+--------------------------------------+-------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Server UUID                          | Live Migration Accepted | Error Message                                                                                                                                                                                                                                                                        |
+--------------------------------------+-------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 95a7ded8-ebfc-4848-9090-2df378c88a4c | False                   | Error while live migrating instance: ardana-cp1-comp0001-mgmt is not on shared storage: Live migration can not be used without shared storage except a booted from volume VM which does not have a local disk. (HTTP 400) (Request-ID: req-9fd79670-a780-40ed-a515-c14e28e0a0a7)     |
| 13ab4ef7-0623-4d00-bb5a-5bb2f1214be4 | False                   | Error while live migrating instance: ardana-cp1-comp0001-mgmt is not on shared storage: Live migration cannot be used without shared storage except a booted from volume VM which does not have a local disk. (HTTP 400) (Request-ID: req-26834267-c3ec-4f8b-83cc-5193d6a394d6)     |
+--------------------------------------+-------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</pre></div><p>
   <span class="bold"><strong>Fix:</strong></span> This occurs when you are attempting to
   live evacuate a host that contains instances booted from local storage and
   you are not specifying <code class="literal">--block-migrate</code> in your command.
   Re-attempt the live evacuation with this syntax:
  </p><div class="verbatim-wrap"><pre class="screen">nova host-evacuate-live <span class="bold"><strong>--block-migrate</strong></span> &lt;hostname&gt; [--target-host &lt;target_hostname&gt;]</pre></div><p>
   <span class="bold"><strong>Issue:</strong></span> When attempting to use <code class="literal">nova
   host-evacuate-live</code> against a node, you receive the error below:
  </p><div class="verbatim-wrap"><pre class="screen">$ nova host-evacuate-live --block-migrate ardana-cp1-comp0001-mgmt --target-host ardana-cp1-comp0003-mgmt
+--------------------------------------+-------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Server UUID                          | Live Migration Accepted | Error Message                                                                                                                                                                                                     |
+--------------------------------------+-------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| e9874122-c5dc-406f-9039-217d9258c020 | False                   | Error while live migrating instance: ardana-cp1-comp0001-mgmt is not on local storage: Block migration can not be used with shared storage. (HTTP 400) (Request-ID: req-60b1196e-84a0-4b71-9e49-96d6f1358e1a)     |
| 84a02b42-9527-47ac-bed9-8fde1f98e3fe | False                   | Error while live migrating instance: ardana-cp1-comp0001-mgmt is not on local storage: Block migration can not be used with shared storage. (HTTP 400) (Request-ID: req-0cdf1198-5dbd-40f4-9e0c-e94aa1065112)     |
+--------------------------------------+-------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</pre></div><p>
   <span class="bold"><strong>Fix:</strong></span> This occurs when you are attempting to
   live evacuate a host that contains instances booted from a block storage
   volume and you are specifying <code class="literal">--block-migrate</code> in your
   command. Re-attempt the live evacuation with this syntax:
  </p><div class="verbatim-wrap"><pre class="screen">nova host-evacuate-live &lt;hostname&gt; [--target-host &lt;target_hostname&gt;]</pre></div><p>
   <span class="bold"><strong>Issue:</strong></span> When attempting to use <code class="literal">nova
   live-migration</code> against an instance, you receive the error below:
  </p><div class="verbatim-wrap"><pre class="screen">$ nova live-migration 2a13ffe6-e269-4d75-8e46-624fec7a5da0 ardana-cp1-comp0002-mgmt
ERROR (BadRequest): ardana-cp1-comp0001-mgmt is not on shared storage: Live migration can not be used without shared storage except a booted from volume VM which does not have a local disk. (HTTP 400) (Request-ID: req-158dd415-0bb7-4613-8529-6689265387e7)</pre></div><p>
   <span class="bold"><strong>Fix:</strong></span> This occurs when you are attempting to
   live migrate an instance that was booted from local storage and you are not
   specifying <code class="literal">--block-migrate</code> in your command. Re-attempt
   the live migration with this syntax:
  </p><div class="verbatim-wrap"><pre class="screen">nova live-migration <span class="bold"><strong>--block-migrate</strong></span> &lt;instance_uuid&gt; &lt;target_hostname&gt;</pre></div><p>
   <span class="bold"><strong>Issue:</strong></span> When attempting to use <code class="literal">nova
   live-migration</code> against an instance, you receive the error below:
  </p><div class="verbatim-wrap"><pre class="screen">$ nova live-migration --block-migrate 84a02b42-9527-47ac-bed9-8fde1f98e3fe ardana-cp1-comp0001-mgmt
ERROR (BadRequest): ardana-cp1-comp0002-mgmt is not on local storage: Block migration can not be used with shared storage. (HTTP 400) (Request-ID: req-51fee8d6-6561-4afc-b0c9-7afa7dc43a5b)</pre></div><p>
   <span class="bold"><strong>Fix:</strong></span> This occurs when you are attempting to
   live migrate an instance that was booted from a block storage volume and you
   are specifying <code class="literal">--block-migrate</code> in your command.
   Re-attempt the live migration with this syntax:
  </p><div class="verbatim-wrap"><pre class="screen">nova live-migration &lt;instance_uuid&gt; &lt;target_hostname&gt;</pre></div></div></div><div class="sect3" id="adding-compute-nodes"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding Compute Node</span> <a title="Permalink" class="permalink" href="#adding-compute-nodes">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-adding_compute_nodes.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-adding_compute_nodes.xml</li><li><span class="ds-label">ID: </span>adding-compute-nodes</li></ul></div></div></div></div><p>
  Adding a Compute Node allows you to add capacity.
 </p><div class="sect4" id="add-sles-compute"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.3.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a SLES Compute Node</span> <a title="Permalink" class="permalink" href="#add-sles-compute">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-add_sles_compute.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-add_sles_compute.xml</li><li><span class="ds-label">ID: </span>add-sles-compute</li></ul></div></div></div></div><p>
  Adding a SLES compute node allows you to add additional capacity for more
  virtual machines.
 </p><p>
  You may have a need to add additional SLES compute hosts for more virtual
  machine capacity or another purpose and these steps will help you achieve
  this.
 </p><p>
  There are two methods you can use to add SLES compute hosts to your
  environment:
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Adding SLES pre-installed compute hosts. This method does not require the
    SLES ISO be on the Cloud Lifecycle Manager to complete.
   </p></li><li class="listitem "><p>
    Using the provided Ansible playbooks and Cobbler, SLES will be installed on
    your new compute hosts. This method requires that you provided a SUSE Linux Enterprise Server 12 SP4
    ISO during the initial installation of your cloud, following the
    instructions at <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 31 “Installing SLES Compute”, Section 31.1 “SLES Compute Node Installation Overview”</span>.
   </p><p>
    If you want to use the provided Ansible playbooks and Cobbler to setup and
    configure your SLES hosts and you did not have the SUSE Linux Enterprise Server 12 SP4 ISO on your
    Cloud Lifecycle Manager during your initial installation then ensure you look at
    the note at the top of that section before proceeding.
   </p></li></ol></div><div class="sect5" id="idg-all-operations-maintenance-compute-add-sles-compute-xml-5"><div class="titlepage"><div><div><h6 class="title"><span class="number">15.1.3.4.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-compute-add-sles-compute-xml-5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-add_sles_compute.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-add_sles_compute.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-compute-add-sles-compute-xml-5</li></ul></div></div></div></div><p>
   You need to ensure your input model files are properly setup for SLES
   compute host clusters. This must be done during the installation process of
   your cloud and is discussed further at <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 31 “Installing SLES Compute”, Section 31.3 “Using the Cloud Lifecycle Manager to Deploy SLES Compute Nodes”</span> and
   <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 10 “Modifying Example Configurations for Compute Nodes”, Section 10.1 “SLES Compute Nodes”</span>.
  </p></div><div class="sect5" id="idg-all-operations-maintenance-compute-add-sles-compute-xml-6"><div class="titlepage"><div><div><h6 class="title"><span class="number">15.1.3.4.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a SLES compute node</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-compute-add-sles-compute-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-add_sles_compute.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-add_sles_compute.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-compute-add-sles-compute-xml-6</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Adding pre-installed SLES compute hosts</strong></span>
  </p><p>
   This method requires that you have SUSE Linux Enterprise Server 12 SP4 pre-installed on the
   baremetal host prior to beginning these steps.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Ensure you have SUSE Linux Enterprise Server 12 SP4 pre-installed on your baremetal host.
    </p></li><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit your <code class="literal">~/openstack/my_cloud/definition/data/servers.yml</code>
     file to include the details about your new compute host(s).
    </p><p>
     For example, if you already had a cluster of three SLES compute hosts
     using the <code class="literal">SLES-COMPUTE-ROLE</code> role and needed to add a
     fourth one you would add your details to the bottom of the file in the
     format. Note that we left out the IPMI details because they will not be
     needed since you pre-installed the SLES OS on your host(s).
    </p><div class="verbatim-wrap"><pre class="screen">- id: compute4
  ip-addr: 192.168.102.70
  role: SLES-COMPUTE-ROLE
  server-group: RACK1</pre></div><p>
     You can find detailed descriptions of these fields in
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.5 “Servers”</span>. Ensure that you use the same role for
     any new SLES hosts you are adding as you specified on your existing SLES
     hosts.
    </p><div id="id-1.5.17.3.6.6.3.7.4.3.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      You will need to verify that the <code class="literal">ip-addr</code> value you
      choose for this host does not conflict with any other IP address in your
      cloud environment. You can confirm this by checking the
      <code class="literal">~/openstack/my_cloud/info/address_info.yml</code> file on your
      Cloud Lifecycle Manager.
     </p></div></li><li class="listitem "><p>
     In your
     <code class="literal">~/openstack/my_cloud/definition/data/control_plane.yml</code>
     file you will need to check the values for
     <code class="literal">member-count</code>, <code class="literal">min-count</code>, and
     <code class="literal">max-count</code>. If you specified them, ensure that they
     match up with your new total node count. For example, if you had
     previously specified <code class="literal">member-count: 3</code> and are adding a
     fourth compute node, you will need to change that value to
     <code class="literal">member-count: 4</code>.
    </p><p>
     See for <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.2 “Control Plane”</span> more details.
    </p></li><li class="listitem "><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "Add node &lt;name&gt;"</pre></div></li><li class="listitem "><p>
     Run the configuration processor and resolve any errors that are indicated:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div><p>
     Before proceeding, you may want to take a look at
     <span class="bold"><strong>info/server_info.yml</strong></span> to see if the
     assignment of the node you have added is what you expect. It may not be,
     as nodes will not be numbered consecutively if any have previously been
     removed. This is to prevent loss of data; the config processor retains
     data about removed nodes and keeps their ID numbers from being
     reallocated. See <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 7 “Other Topics”, Section 7.3 “Persisted Data”, Section 7.3.1 “Persisted Server Allocations”</span> for
     information on how this works.
    </p></li><li class="listitem "><p>
     [OPTIONAL] - Run the <code class="literal">wipe_disks.yml</code> playbook to ensure
     all of your non-OS partitions on your nodes are completely wiped prior to
     continuing with the installation.
    </p><div id="id-1.5.17.3.6.6.3.7.4.8.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The <code class="filename">wipe_disks.yml</code> playbook is only meant to be run
      on systems immediately after running
      <code class="filename">bm-reimage.yml</code>. If used for any other case, it may
      not wipe all of the expected partitions.
     </p></div><p>
     The value to be used for <code class="literal">hostname</code> is host's identifier from
     <code class="filename">~/scratch/ansible/next/ardana/ansible/hosts/verb_hosts</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit &lt;hostname&gt;</pre></div></li><li class="listitem "><p>
     Complete the compute host deployment with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --tag "generate_hosts_file"
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --limit &lt;hostname&gt;</pre></div></li></ol></div><p>
   <span class="bold"><strong>Adding SLES compute hosts with Ansible playbooks and
   Cobbler</strong></span>
  </p><p>
   These steps will show you how to add the new SLES compute host to your
   <code class="literal">servers.yml</code> file and then run the playbooks that update
   your cloud configuration. You will run these playbooks from the lifecycle
   manager.
  </p><p>
   If you did not have the SUSE Linux Enterprise Server 12 SP4 ISO available on your Cloud Lifecycle Manager
   during your initial installation, it must be installed before proceeding
   further. Instructions can be found in <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 31 “Installing SLES Compute”</span>.
  </p><p>
   When you are prepared to continue, use these steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Checkout the <code class="literal">site</code> branch of your local git so you can
     begin to make the necessary edits:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition/data
<code class="prompt user">ardana &gt; </code>git checkout site</pre></div></li><li class="step "><p>
     Edit your <code class="literal">~/openstack/my_cloud/definition/data/servers.yml</code>
     file to include the details about your new compute host(s).
    </p><p>
     For example, if you already had a cluster of three SLES compute hosts
     using the <code class="literal">SLES-COMPUTE-ROLE</code> role and needed to add a
     fourth one you would add your details to the bottom of the file in this
     format:
    </p><div class="verbatim-wrap"><pre class="screen">- id: compute4
  ip-addr: 192.168.102.70
  role: SLES-COMPUTE-ROLE
  server-group: RACK1
  mac-addr: e8:39:35:21:32:4e
  ilo-ip: 10.1.192.36
  ilo-password: password
  ilo-user: admin
  distro-id: sles12sp4-x86_64</pre></div><p>
     You can find detailed descriptions of these fields in
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.5 “Servers”</span>. Ensure that you use the same role for
     any new SLES hosts you are adding as you specified on your existing SLES
     hosts.
    </p><div id="id-1.5.17.3.6.6.3.7.9.3.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      You will need to verify that the <code class="literal">ip-addr</code> value you
      choose for this host does not conflict with any other IP address in your
      cloud environment. You can confirm this by checking the
      <code class="literal">~/openstack/my_cloud/info/address_info.yml</code> file on your
      Cloud Lifecycle Manager.
     </p></div></li><li class="step "><p>
     In your
     <code class="literal">~/openstack/my_cloud/definition/data/control_plane.yml</code>
     file you will need to check the values for
     <code class="literal">member-count</code>, <code class="literal">min-count</code>, and
     <code class="literal">max-count</code>. If you specified them, ensure that they
     match up with your new total node count. For example, if you had
     previously specified <code class="literal">member-count: 3</code> and are adding a
     fourth compute node, you will need to change that value to
     <code class="literal">member-count: 4</code>.
    </p><p>
     See <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.2 “Control Plane”</span> for more details.
    </p></li><li class="step "><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "Add node &lt;name&gt;"</pre></div></li><li class="step "><p>
     Run the configuration processor and resolve any errors that are indicated:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     The following playbook confirms that your servers are accessible over their IPMI ports.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-power-status.yml -e nodelist=compute4</pre></div></li><li class="step "><p>
     Add the new node into Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step "><p>
     Run the following playbook, ensuring that you specify only your UEFI
     SLES nodes using the nodelist. This playbook will reconfigure Cobbler
     for the nodes listed.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook prepare-sles-grub2.yml -e nodelist=node1[,node2,node3]</pre></div></li><li class="step "><p>
     Then you can image the node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node name&gt;</pre></div><div id="id-1.5.17.3.6.6.3.7.9.10.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
       If you do not know the <code class="literal">&lt;node name&gt;</code>, you can
       get it by using <code class="command">sudo cobbler system list</code>.
      </p></div><p>
     Before proceeding, you may want to take a look at
     <span class="bold"><strong>info/server_info.yml</strong></span> to see if the
     assignment of the node you have added is what you expect. It may not be,
     as nodes will not be numbered consecutively if any have previously been
     removed. This is to prevent loss of data; the config processor retains
     data about removed nodes and keeps their ID numbers from being
     reallocated. See <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 7 “Other Topics”, Section 7.3 “Persisted Data”, Section 7.3.1 “Persisted Server Allocations”</span> for
     information on how this works.
    </p></li><li class="step "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     [OPTIONAL] - Run the <code class="literal">wipe_disks.yml</code> playbook to ensure
     all of your non-OS partitions on your hosts are completely wiped prior to
     continuing with the installation. The <code class="filename">wipe_disks.yml</code>
     playbook is only meant to be run on systems immediately after running
     <code class="filename">bm-reimage.yml</code>. If used for any other case, it may
     not wipe all of the expected partitions.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit &lt;hostname&gt;</pre></div><div id="id-1.5.17.3.6.6.3.7.9.12.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      You can obtain the <code class="literal">&lt;hostname&gt;</code> from the file
      <code class="filename">~/scratch/ansible/next/ardana/ansible/hosts/verb_hosts</code>.
     </p></div></li><li class="step "><p>
     You should verify that the netmask, bootproto, and other necessary
     settings are correct and if they are not then re-do them. See
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 31 “Installing SLES Compute”</span> for details.
    </p></li><li class="step "><p>
     Complete the compute host deployment with these playbooks. For the last
     one, ensure you specify the compute hosts you are added with the
     <code class="literal">--limit</code> switch:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --tag "generate_hosts_file"
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --limit &lt;hostname&gt;</pre></div></li></ol></div></div></div><div class="sect5" id="idg-all-operations-maintenance-compute-add-sles-compute-xml-8"><div class="titlepage"><div><div><h6 class="title"><span class="number">15.1.3.4.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a new SLES compute node to monitoring</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-compute-add-sles-compute-xml-8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-add_sles_compute.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-add_sles_compute.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-compute-add-sles-compute-xml-8</li></ul></div></div></div></div><p>
   If you want to add a new Compute node to the monitoring service checks,
   there is an additional playbook that must be run to ensure this happens:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-deploy.yml --tags "active_ping_checks"</pre></div></div></div></div><div class="sect3" id="remove-compute-node"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Removing a Compute Node</span> <a title="Permalink" class="permalink" href="#remove-compute-node">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>remove-compute-node</li></ul></div></div></div></div><p>
  Removing a Compute node allows you to remove capacity.
 </p><p>
  You may have a need to remove a Compute node and these steps will help you
  achieve this.
 </p><div class="sect4" id="disable-provisioning"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.3.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Disable Provisioning on the Compute Host</span> <a title="Permalink" class="permalink" href="#disable-provisioning">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>disable-provisioning</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Get a list of the nova services running which will provide us with the
     details we need to disable the provisioning on the Compute host you are
     wanting to remove:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service list</pre></div><p>
     Here is an example below. I've highlighted the Compute node we are going
     to remove in the examples:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service list
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+
| Id | Binary           | Host                     | Zone     | Status  | State | Updated_at                 | Disabled Reason |
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+
| 1  | nova-conductor   | ardana-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -               |
| 10 | nova-scheduler   | ardana-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:34.000000 | -               |
| 13 | nova-conductor   | ardana-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -               |
| 16 | nova-conductor   | ardana-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -               |
| 28 | nova-scheduler   | ardana-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:38.000000 | -               |
| 31 | nova-scheduler   | ardana-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:42.000000 | -               |
| 34 | nova-compute     | ardana-cp1-comp0001-mgmt | AZ1      | enabled | up    | 2015-11-22T22:50:35.000000 | -               |
<span class="bold"><strong>| 37 | nova-compute     | ardana-cp1-comp0002-mgmt | AZ2      | enabled | up    | 2015-11-22T22:50:44.000000 | -               |</strong></span>
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+</pre></div></li><li class="step "><p>
     Disable the nova service on the Compute node you are wanting to remove
     which will ensure it is taken out of the scheduling rotation:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>compute service set --disable --reason "<em class="replaceable ">enter reason here</em>" <em class="replaceable ">node hostname</em></pre></div><p>
     Here is an example if I wanted to remove the
     <code class="literal">ardana-cp1-comp0002-mgmt</code> in the output above:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>compute service set –disable --reason "hardware reallocation" ardana-cp1-comp0002-mgmt
+--------------------------+--------------+----------+-----------------------+
| Host                     | Binary       | Status   | Disabled Reason       |
+--------------------------+--------------+----------+-----------------------+
| ardana-cp1-comp0002-mgmt | nova-compute | disabled | hardware reallocation |
+--------------------------+--------------+----------+-----------------------+</pre></div></li></ol></div></div></div><div class="sect4" id="remove-az"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.3.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Remove the Compute Host from its Availability Zone</span> <a title="Permalink" class="permalink" href="#remove-az">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>remove-az</li></ul></div></div></div></div><p>
   If you configured the Compute host to be part of an availability zone, these
   steps will show you how to remove it.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Get a list of the nova services running which will provide us with the
     details we need to remove a Compute node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service list</pre></div><p>
     Here is an example below. I've highlighted the Compute node we are going
     to remove in the examples:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service list
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------------+
| Id | Binary           | Host                     | Zone     | Status  | State | Updated_at                 | Disabled Reason       |
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------------+
| 1  | nova-conductor   | ardana-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -                     |
| 10 | nova-scheduler   | ardana-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-11-22T22:50:34.000000 | -                     |
| 13 | nova-conductor   | ardana-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -                     |
| 16 | nova-conductor   | ardana-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:43.000000 | -                     |
| 28 | nova-scheduler   | ardana-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-11-22T22:50:38.000000 | -                     |
| 31 | nova-scheduler   | ardana-cp1-c1-m3-mgmt    | internal | enabled | up    | 2015-11-22T22:50:42.000000 | -                     |
| 34 | nova-compute     | ardana-cp1-comp0001-mgmt | AZ1      | enabled | up    | 2015-11-22T22:50:35.000000 | -                     |
<span class="bold"><strong>| 37 | nova-compute     | ardana-cp1-comp0002-mgmt | AZ2      | enabled | up    | 2015-11-22T22:50:44.000000 | hardware reallocation |</strong></span>
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------------+</pre></div></li><li class="step "><p>
     If the <code class="literal">Zone</code> reported for this host is simply "nova",
     then it is not a member of a particular availability zone, and this step
     will not be necessary. Otherwise, you must remove the Compute host from
     its availability zone:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack aggregate remove host <em class="replaceable ">availability zone</em> <em class="replaceable ">nova hostname</em></pre></div><p>
     So for the same example in the previous step, the
     <code class="literal">ardana-cp1-comp0002-mgmt</code> host was in the
     <code class="literal">AZ2</code> availability zone so you would use this command to
     remove it:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack aggregate remove host AZ2 ardana-cp1-comp0002-mgmt
Host ardana-cp1-comp0002-mgmt has been successfully removed from aggregate 4
+----+------+-------------------+-------+-------------------------+
| Id | Name | Availability Zone | Hosts | Metadata                |
+----+------+-------------------+-------+-------------------------+
| 4  | AZ2  | AZ2               |       | 'availability_zone=AZ2' |
+----+------+-------------------+-------+-------------------------+</pre></div></li><li class="step "><p>
     You can confirm the last two steps completed successfully by running
     another <code class="literal">openstack compute service list</code>.
    </p><p>
     Here is an example which confirms that the node has been disabled and that
     it has been removed from the availability zone. I have highlighted these:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service list
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| Id | Binary           | Host                     | Zone     | Status   | State | Updated_at                 | Disabled Reason       |
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| 1  | nova-conductor   | ardana-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 10 | nova-scheduler   | ardana-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:34.000000 | -                     |
| 13 | nova-conductor   | ardana-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 16 | nova-conductor   | ardana-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 28 | nova-scheduler   | ardana-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:28.000000 | -                     |
| 31 | nova-scheduler   | ardana-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:32.000000 | -                     |
| 34 | nova-compute     | ardana-cp1-comp0001-mgmt | AZ1      | enabled  | up    | 2015-11-22T23:04:25.000000 | -                     |
<span class="bold"><strong>| 37 | nova-compute     | ardana-cp1-comp0002-mgmt | nova     | disabled | up    | 2015-11-22T23:04:34.000000 | hardware reallocation |</strong></span>
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+</pre></div></li></ol></div></div></div><div class="sect4" id="live-migration"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.3.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Use Live Migration to Move Any Instances on this Host to Other Hosts</span> <a title="Permalink" class="permalink" href="#live-migration">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>live-migration</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     You will need to verify if the Compute node is currently hosting any
     instances on it. You can do this with the command below:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list --host <em class="replaceable ">nova hostname</em> --all_tenants=1</pre></div><p>
     Here is an example below which shows that we have a single running
     instance on this node currently:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list --host ardana-cp1-comp0002-mgmt --all-projects
+--------------------------------------+--------+----------------------------------+--------+------------+-------------+-----------------+
| ID                                   | Name   | Tenant ID                        | Status | Task State | Power State | Networks        |
+--------------------------------------+--------+----------------------------------+--------+------------+-------------+-----------------+
| 78fdb938-a89c-4a0c-a0d4-b88f1555c3b9 | paul4d | 5e9998f1b1824ea9a3b06ad142f09ca5 | ACTIVE | -          | Running     | paul=10.10.10.7 |
+--------------------------------------+--------+----------------------------------+--------+------------+-------------+-----------------+</pre></div></li><li class="step "><p>
     You will likely want to migrate this instance off of this node before
     removing it. You can do this with the live migration functionality within
     nova. The command will look like this:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova live-migration --block-migrate <em class="replaceable ">nova instance ID</em></pre></div><p>
     Here is an example using the instance in the previous step:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova live-migration --block-migrate 78fdb938-a89c-4a0c-a0d4-b88f1555c3b9</pre></div><p>
     You can check the status of the migration using the same command from the
     previous step:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list --host ardana-cp1-comp0002-mgmt --all-projects
+--------------------------------------+--------+----------------------------------+-----------+------------+-------------+-----------------+
| ID                                   | Name   | Tenant ID                        | Status    | Task State | Power State | Networks        |
+--------------------------------------+--------+----------------------------------+-----------+------------+-------------+-----------------+
| 78fdb938-a89c-4a0c-a0d4-b88f1555c3b9 | paul4d | 5e9998f1b1824ea9a3b06ad142f09ca5 | MIGRATING | migrating  | Running     | paul=10.10.10.7 |
+--------------------------------------+--------+----------------------------------+-----------+------------+-------------+-----------------+</pre></div></li><li class="step "><p>
     List the compute instances again to see that the running instance has been
     migrated:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list --host ardana-cp1-comp0002-mgmt --all-projects
+----+------+-----------+--------+------------+-------------+----------+
| ID | Name | Tenant ID | Status | Task State | Power State | Networks |
+----+------+-----------+--------+------------+-------------+----------+
+----+------+-----------+--------+------------+-------------+----------+</pre></div></li></ol></div></div></div><div class="sect4" id="id-1.5.17.3.6.7.7"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.3.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Disable Neutron Agents on Node to be Removed</span> <a title="Permalink" class="permalink" href="#id-1.5.17.3.6.7.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   You should also locate and disable or remove neutron agents. To see the
   neutron agents running:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network agent list | grep NODE_NAME
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| id                                   | agent_type           | host                     | alive | admin_state_up | binary                    |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| 08f16dbc-4ba2-4c1d-a4a3-a2ff2526ebe4 | L3 agent             | ardana-cp1-comp0002-mgmt | :-)   | True           | neutron-l3-agent          |
| dbe4fe11-8f08-4306-8244-cc68e98bb770 | Metadata agent       | ardana-cp1-comp0002-mgmt | :-)   | True           | neutron-metadata-agent    |
| f0d262d1-7139-40c7-bdc2-f227c6dee5c8 | Open vSwitch agent   | ardana-cp1-comp0002-mgmt | :-)   | True           | neutron-openvswitch-agent |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+

<code class="prompt user">ardana &gt; </code>openstack network agent set --disable 08f16dbc-4ba2-4c1d-a4a3-a2ff2526ebe4
<code class="prompt user">ardana &gt; </code>openstack network agent set --disable dbe4fe11-8f08-4306-8244-cc68e98bb770
<code class="prompt user">ardana &gt; </code>openstack network agent set --disable f0d262d1-7139-40c7-bdc2-f227c6dee5c8</pre></div></div><div class="sect4" id="shutdown-node"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.3.5.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Shut down or Stop the Nova and Neutron Services on the Compute Host</span> <a title="Permalink" class="permalink" href="#shutdown-node">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>shutdown-node</li></ul></div></div></div></div><p>
   To perform this step you have a few options. You can SSH into the Compute
   host and run the following commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl stop nova-compute</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl stop neutron-*</pre></div><p>
   Because the neutron agent self-registers against neutron server, you may
   want to prevent the following services from coming back online. Here is how
   you can get the list:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl list-units neutron-* --all</pre></div><p>
   Here are the results:
  </p><div class="verbatim-wrap"><pre class="screen">UNIT                                  LOAD        ACTIVE     SUB      DESCRIPTION
neutron-common-rundir.service         loaded      inactive   dead     Create /var/run/neutron
•neutron-dhcp-agent.service         not-found     inactive   dead     neutron-dhcp-agent.service
neutron-l3-agent.service              loaded      inactive   dead     neutron-l3-agent Service
neutron-metadata-agent.service        loaded      inactive   dead     neutron-metadata-agent Service
•neutron-openvswitch-agent.service    loaded      failed     failed   neutron-openvswitch-agent Service
neutron-ovs-cleanup.service           loaded      inactive   dead     neutron OVS Cleanup Service

        LOAD   = Reflects whether the unit definition was properly loaded.
        ACTIVE = The high-level unit activation state, i.e. generalization of SUB.
        SUB    = The low-level unit activation state, values depend on unit type.

        7 loaded units listed.
        To show all installed unit files use 'systemctl list-unit-files'.</pre></div><p>
   For each loaded service issue the command
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl disable <em class="replaceable ">service-name</em></pre></div><p>
   In the above example that would be each service, <span class="emphasis"><em>except neutron-dhcp-agent.service
   </em></span>
  </p><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo systemctl disable neutron-common-rundir neutron-l3-agent neutron-metadata-agent neutron-openvswitch-agent</pre></div><p>
   Now you can shut down the node:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo shutdown now</pre></div><p>
   OR
  </p><p>
   From the Cloud Lifecycle Manager you can use the
   <code class="literal">bm-power-down.yml</code> playbook to shut down the node:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-power-down.yml -e nodelist=<em class="replaceable ">node name</em></pre></div><div id="id-1.5.17.3.6.7.8.19" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   The <code class="literal"><em class="replaceable ">node name</em></code> value will be the value
   corresponding to this node in Cobbler. You can run
   <code class="command">sudo cobbler system list</code> to retrieve these names.
  </p></div></div><div class="sect4" id="delete-node"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.3.5.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Delete the Compute Host from Nova</span> <a title="Permalink" class="permalink" href="#delete-node">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>delete-node</li></ul></div></div></div></div><p>
   Retrieve the list of nova services:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service list</pre></div><p>
   Here is an example highlighting the Compute host we're going to remove:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service list
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| Id | Binary           | Host                     | Zone     | Status   | State | Updated_at                 | Disabled Reason       |
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+
| 1  | nova-conductor   | ardana-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 10 | nova-scheduler   | ardana-cp1-c1-m1-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:34.000000 | -                     |
| 13 | nova-conductor   | ardana-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 16 | nova-conductor   | ardana-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:33.000000 | -                     |
| 28 | nova-scheduler   | ardana-cp1-c1-m2-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:28.000000 | -                     |
| 31 | nova-scheduler   | ardana-cp1-c1-m3-mgmt    | internal | enabled  | up    | 2015-11-22T23:04:32.000000 | -                     |
| 34 | nova-compute     | ardana-cp1-comp0001-mgmt | AZ1      | enabled  | up    | 2015-11-22T23:04:25.000000 | -                     |
<span class="bold"><strong>| 37 | nova-compute     | ardana-cp1-comp0002-mgmt | nova     | disabled | up    | 2015-11-22T23:04:34.000000 | hardware reallocation |</strong></span>
+----+------------------+--------------------------+----------+----------+-------+----------------------------+-----------------------+</pre></div><p>
   Delete the host from nova using the command below:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service delete <em class="replaceable ">service ID</em></pre></div><p>
   Following our example above, you would use:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service delete 37</pre></div><p>
   Use the command below to confirm that the Compute host has been completely
   removed from nova:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack hypervisor list</pre></div></div><div class="sect4" id="deletefromneutron"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.3.5.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Delete the Compute Host from Neutron</span> <a title="Permalink" class="permalink" href="#deletefromneutron">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>deletefromneutron</li></ul></div></div></div></div><p>
   Multiple neutron agents are running on the compute node. You have to remove
   all of the agents running on the node using the <code class="command">openstack network
   agent delete</code> command. In the example below, the l3-agent,
   openvswitch-agent and metadata-agent are running:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network agent list | grep NODE_NAME
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| id                                   | agent_type           | host                     | alive | admin_state_up | binary                    |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+
| 08f16dbc-4ba2-4c1d-a4a3-a2ff2526ebe4 | L3 agent             | ardana-cp1-comp0002-mgmt | :-)   | False          | neutron-l3-agent          |
| dbe4fe11-8f08-4306-8244-cc68e98bb770 | Metadata agent       | ardana-cp1-comp0002-mgmt | :-)   | False          | neutron-metadata-agent    |
| f0d262d1-7139-40c7-bdc2-f227c6dee5c8 | Open vSwitch agent   | ardana-cp1-comp0002-mgmt | :-)   | False          | neutron-openvswitch-agent |
+--------------------------------------+----------------------+--------------------------+-------+----------------+---------------------------+

$ openstack network agent delete AGENT_ID

$ openstack network agent delete 08f16dbc-4ba2-4c1d-a4a3-a2ff2526ebe4
$ openstack network agent delete dbe4fe11-8f08-4306-8244-cc68e98bb770
$ openstack network agent delete f0d262d1-7139-40c7-bdc2-f227c6dee5c8</pre></div></div><div class="sect4" id="remove-node"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.3.5.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Remove the Compute Host from the servers.yml File and Run the Configuration Processor</span> <a title="Permalink" class="permalink" href="#remove-node">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>remove-node</li></ul></div></div></div></div><p>
   Complete these steps from the Cloud Lifecycle Manager to remove the Compute node:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager
    </p></li><li class="step "><p>
     Edit your <code class="literal">servers.yml</code> file in the location below to
     remove references to the Compute node(s) you want to remove:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition/data
<code class="prompt user">ardana &gt; </code>vi servers.yml</pre></div></li><li class="step "><p>
     You may also need to edit your <code class="literal">control_plane.yml</code> file
     to update the values for <code class="literal">member-count</code>,
     <code class="literal">min-count</code>, and <code class="literal">max-count</code> if you used
     those to ensure they reflect the exact number of nodes you are using.
    </p><p>
     See <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.2 “Control Plane”</span> for more details.
    </p></li><li class="step "><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git commit -a -m "Remove node <em class="replaceable ">NODE_NAME</em>"</pre></div></li><li class="step "><p>
     To release the network capacity allocated to the deleted server(s), use
     the switches <code class="literal">remove_deleted_servers</code> and
     <code class="literal">free_unused_addresses</code> when running the configuration
     processors. (For more information, see
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 7 “Other Topics”, Section 7.3 “Persisted Data”</span>.)
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml \
  -e remove_deleted_servers="y" -e free_unused_addresses="y"</pre></div></li><li class="step "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Refresh the <code class="literal">/etc/hosts</code> file through the cloud to remove
     references to the old node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --tag "generate_hosts_file"</pre></div></li></ol></div></div></div><div class="sect4" id="remove-cobbler"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.3.5.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Remove the Compute Host from Cobbler</span> <a title="Permalink" class="permalink" href="#remove-cobbler">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>remove-cobbler</li></ul></div></div></div></div><p>
   Complete these steps to remove the node from Cobbler:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Confirm the system name in Cobbler with this command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo  cobbler system list</pre></div></li><li class="step "><p>
     Remove the system from Cobbler using this command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo  cobbler system remove --name=<em class="replaceable ">node</em></pre></div></li><li class="step "><p>
     Run the <code class="literal">cobbler-deploy.yml</code> playbook to complete the
     process:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li></ol></div></div></div><div class="sect4" id="idg-all-operations-maintenance-compute-remove-compute-node-xml-14"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.3.5.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Remove the Compute Host from Monitoring</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-compute-remove-compute-node-xml-14">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-remove_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-remove_compute_node.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-compute-remove-compute-node-xml-14</li></ul></div></div></div></div><p>
   Once you have removed the Compute nodes, the alarms against them will
   trigger so there are additional steps to take to resolve this issue.
  </p><p>
    To find all monasca API servers
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo cat /etc/haproxy/haproxy.cfg | grep MON
listen ardana-cp1-vip-public-MON-API-extapi-8070
    bind ardana-cp1-vip-public-MON-API-extapi:8070  ssl crt /etc/ssl/private//my-public-cert-entry-scale
    server ardana-cp1-c1-m1-mgmt-MON_API-8070 ardana-cp1-c1-m1-mgmt:8070 check inter 5000 rise 2 fall 5
    server ardana-cp1-c1-m2-mgmt-MON_API-8070 ardana-cp1-c1-m2-mgmt:8070 check inter 5000 rise 2 fall 5
    server ardana-cp1-c1-m3-mgmt-MON_API-8070 ardana-cp1-c1-m3-mgmt:8070 check inter 5000 rise 2 fall 5
listen ardana-cp1-vip-MON-API-mgmt-8070
    bind ardana-cp1-vip-MON-API-mgmt:8070  ssl crt /etc/ssl/private//ardana-internal-cert
    server ardana-cp1-c1-m1-mgmt-MON_API-8070 ardana-cp1-c1-m1-mgmt:8070 check inter 5000 rise 2 fall 5
    server ardana-cp1-c1-m2-mgmt-MON_API-8070 ardana-cp1-c1-m2-mgmt:8070 check inter 5000 rise 2 fall 5
    server ardana-cp1-c1-m3-mgmt-MON_API-8070 ardana-cp1-c1-m3-mgmt:8070 check inter 5000 rise 2 fall 5</pre></div><p>In above example <code class="literal">ardana-cp1-c1-m1-mgmt</code>,<code class="literal">ardana-cp1-c1-m2-mgmt</code>,
  <code class="literal">ardana-cp1-c1-m3-mgmt</code> are Monasa API servers</p><p>
   You will want to SSH to each of the monasca API servers and edit the
   <code class="literal">/etc/monasca/agent/conf.d/host_alive.yaml</code> file to remove
   references to the Compute node you removed. This will require
   <code class="literal">sudo</code> access. The entries will look similar to the one
   below:
  </p><div class="verbatim-wrap"><pre class="screen">- alive_test: ping
  built_by: HostAlive
  host_name: ardana-cp1-comp0001-mgmt
  name: ardana-cp1-comp0001-mgmt ping</pre></div><p>
   Once you have removed the references on each of your monasca API servers you
   then need to restart the monasca-agent on each of those servers with this
   command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo service openstack-monasca-agent restart</pre></div><p>
   With the Compute node references removed and the monasca-agent restarted,
   you can then delete the corresponding alarm to finish this process. To do so
   we recommend using the monasca CLI which should be installed on each of your
   monasca API servers by default:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>monasca alarm-list --metric-dimensions hostname=<em class="replaceable ">compute node deleted</em></pre></div><p>
   For example, if your Compute node looked like the example above then you
   would use this command to get the alarm ID:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>monasca alarm-list --metric-dimensions hostname=ardana-cp1-comp0001-mgmt</pre></div><p>
   You can then delete the alarm with this command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>monasca alarm-delete <em class="replaceable ">alarm ID</em></pre></div></div></div></div><div class="sect2" id="planned-maintenance-task-for-networking-nodes"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Planned Network Maintenance</span> <a title="Permalink" class="permalink" href="#planned-maintenance-task-for-networking-nodes">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-networking_nodes.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-networking_nodes.xml</li><li><span class="ds-label">ID: </span>planned-maintenance-task-for-networking-nodes</li></ul></div></div></div></div><p>
  Planned maintenance task for networking nodes.
 </p><div class="sect3" id="add-network-node"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a Network Node</span> <a title="Permalink" class="permalink" href="#add-network-node">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-networking-add_network_node.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-networking-add_network_node.xml</li><li><span class="ds-label">ID: </span>add-network-node</li></ul></div></div></div></div><p>
  Adding an additional neutron networking node allows you to increase the
  performance of your cloud.
 </p><p>
  You may have a need to add an additional neutron network node for increased
  performance or another purpose and these steps will help you achieve this.
 </p><div class="sect4" id="idg-all-operations-maintenance-networking-add-network-node-xml-6"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.4.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-networking-add-network-node-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-networking-add_network_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-networking-add_network_node.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-networking-add-network-node-xml-6</li></ul></div></div></div></div><p>
   If you are using the mid-scale model then your networking nodes are already
   separate and the roles are defined. If you are not already using this model
   and wish to add separate networking nodes then you need to ensure that those
   roles are defined. You can look in the <code class="literal">~/openstack/examples</code>
   folder on your Cloud Lifecycle Manager for the mid-scale example model files which
   show how to do this. We have also added the basic edits that need to be made
   below:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     In your <code class="literal">server_roles.yml</code> file, ensure you have the
     <code class="literal">NEUTRON-ROLE</code> defined.
    </p><p>
     Path to file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/server_roles.yml</pre></div><p>
     Example snippet:
    </p><div class="verbatim-wrap"><pre class="screen">- name: NEUTRON-ROLE
  interface-model: NEUTRON-INTERFACES
  disk-model: NEUTRON-DISKS</pre></div></li><li class="listitem "><p>
     In your <code class="literal">net_interfaces.yml</code> file, ensure you have the
     <code class="literal">NEUTRON-INTERFACES</code> defined.
    </p><p>
     Path to file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/net_interfaces.yml</pre></div><p>
     Example snippet:
    </p><div class="verbatim-wrap"><pre class="screen">- name: NEUTRON-INTERFACES
  network-interfaces:
  - device:
      name: hed3
    name: hed3
    network-groups:
    - EXTERNAL-VM
    - GUEST
    - MANAGEMENT</pre></div></li><li class="listitem "><p>
     Create a <code class="literal">disks_neutron.yml</code> file, ensure you have the
     <code class="literal">NEUTRON-DISKS</code> defined in it.
    </p><p>
     Path to file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/disks_neutron.yml</pre></div><p>
     Example snippet:
    </p><div class="verbatim-wrap"><pre class="screen">  product:
    version: 2

  disk-models:
  - name: NEUTRON-DISKS
    volume-groups:
      - name: ardana-vg
        physical-volumes:
         - /dev/sda_root

        logical-volumes:
        # The policy is not to consume 100% of the space of each volume group.
        # 5% should be left free for snapshots and to allow for some flexibility.
          - name: root
            size: 35%
            fstype: ext4
            mount: /
          - name: log
            size: 50%
            mount: /var/log
            fstype: ext4
            mkfs-opts: -O large_file
          - name: crash
            size: 10%
            mount: /var/crash
            fstype: ext4
            mkfs-opts: -O large_file</pre></div></li><li class="listitem "><p>
     Modify your <code class="literal">control_plane.yml</code> file, ensure you have the
     <code class="literal">NEUTRON-ROLE</code> defined as well as the neutron services
     added.
    </p><p>
     Path to file:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/control_plane.yml</pre></div><p>
     Example snippet:
    </p><div class="verbatim-wrap"><pre class="screen">  - allocation-policy: strict
    cluster-prefix: neut
    member-count: 1
    name: neut
    server-role: NEUTRON-ROLE
    service-components:
    - ntp-client
    - neutron-vpn-agent
    - neutron-dhcp-agent
    - neutron-metadata-agent
    - neutron-openvswitch-agent</pre></div></li></ol></div><p>
   You should also have one or more baremetal servers that meet the minimum
   hardware requirements for a network node which are documented in the
   <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 2 “Hardware and Software Support Matrix”</span>.
  </p></div><div class="sect4" id="idg-all-operations-maintenance-networking-add-network-node-xml-7"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.4.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a network node</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-networking-add-network-node-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-networking-add_network_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-networking-add_network_node.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-networking-add-network-node-xml-7</li></ul></div></div></div></div><p>
   These steps will show you how to add the new network node to your
   <code class="literal">servers.yml</code> file and then run the playbooks that update
   your cloud configuration. You will run these playbooks from the lifecycle
   manager.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Checkout the <code class="literal">site</code> branch of your local git so you can
     begin to make the necessary edits:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition/data
<code class="prompt user">ardana &gt; </code>git checkout site</pre></div></li><li class="listitem "><p>
     In the same directory, edit your <code class="literal">servers.yml</code> file to
     include the details about your new network node(s).
    </p><p>
     For example, if you already had a cluster of three network nodes and
     needed to add a fourth one you would add your details to the bottom of the
     file in this format:
    </p><div class="verbatim-wrap"><pre class="screen"># network nodes
- id: neut3
  ip-addr: 10.13.111.137
  role: NEUTRON-ROLE
  server-group: RACK2
  mac-addr: "5c:b9:01:89:b6:18"
  nic-mapping: HP-DL360-6PORT
  ip-addr: 10.243.140.22
  ilo-ip: 10.1.12.91
  ilo-password: password
  ilo-user: admin</pre></div><div id="id-1.5.17.3.7.3.5.3.3.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      You will need to verify that the <code class="literal">ip-addr</code> value you
      choose for this node does not conflict with any other IP address in your
      cloud environment. You can confirm this by checking the
      <code class="literal">~/openstack/my_cloud/info/address_info.yml</code> file on your
      Cloud Lifecycle Manager.
     </p></div></li><li class="listitem "><p>
     In your <code class="literal">control_plane.yml</code> file you will need to check
     the values for <code class="literal">member-count</code>,
     <code class="literal">min-count</code>, and <code class="literal">max-count</code>, if you
     specified them, to ensure that they match up with your new total node
     count. So for example, if you had previously specified
     <code class="literal">member-count: 3</code> and are adding a fourth network node,
     you will need to change that value to <code class="literal">member-count: 4</code>.
    </p></li><li class="listitem "><p>
     Commit the changes to git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git commit -a -m "Add new networking node &lt;name&gt;"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Add the new node into Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="listitem "><p>
     Then you can image the node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;hostname&gt;</pre></div><div id="id-1.5.17.3.7.3.5.3.9.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      If you do not know the <code class="literal">&lt;hostname&gt;</code>, you can
      get it by using <code class="command">sudo cobbler system list</code>.
     </p></div></li><li class="listitem "><p>
     [OPTIONAL] - Run the <code class="literal">wipe_disks.yml</code> playbook to ensure
     all of your non-OS partitions on your nodes are completely wiped prior to
     continuing with the installation. The <code class="filename">wipe_disks.yml</code>
     playbook is only meant to be run on systems immediately after running
     <code class="filename">bm-reimage.yml</code>. If used for any other case, it may
     not wipe all of the expected partitions.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit &lt;hostname&gt;</pre></div></li><li class="listitem "><p>
     Configure the operating system on the new networking node with this
     playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostname&gt;</pre></div></li><li class="listitem "><p>
     Complete the networking node deployment with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-deploy.yml --limit &lt;hostname&gt;</pre></div></li><li class="listitem "><p>
     Run the <code class="literal">site.yml</code> playbook with the required tag so that
     all other services become aware of the new node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --tags "generate_hosts_file"</pre></div></li></ol></div></div><div class="sect4" id="idg-all-operations-maintenance-networking-add-network-node-xml-8"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.4.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a New Network Node to Monitoring</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-networking-add-network-node-xml-8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-networking-add_network_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-networking-add_network_node.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-networking-add-network-node-xml-8</li></ul></div></div></div></div><p>
   If you want to add a new networking node to the monitoring service checks,
   there is an additional playbook that must be run to ensure this happens:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-deploy.yml --tags "active_ping_checks"</pre></div></div></div></div><div class="sect2" id="storage-maintenance"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Planned Storage Maintenance</span> <a title="Permalink" class="permalink" href="#storage-maintenance">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-storage_maintenance.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-storage_maintenance.xml</li><li><span class="ds-label">ID: </span>storage-maintenance</li></ul></div></div></div></div><p>
  Planned maintenance procedures for swift storage nodes.
 </p><div class="sect3" id="planned-maintenance-for-swift-nodes"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.1.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Planned Maintenance Tasks for swift Nodes</span> <a title="Permalink" class="permalink" href="#planned-maintenance-for-swift-nodes">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift_nodes.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift_nodes.xml</li><li><span class="ds-label">ID: </span>planned-maintenance-for-swift-nodes</li></ul></div></div></div></div><p>
  Planned maintenance tasks including recovering, adding, and removing swift
  nodes.
 </p><div class="sect4" id="sec-swift-add-object-node"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.5.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a Swift Object Node</span> <a title="Permalink" class="permalink" href="#sec-swift-add-object-node">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-add_swift_object_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-add_swift_object_node.xml</li><li><span class="ds-label">ID: </span>sec-swift-add-object-node</li></ul></div></div></div></div><p>
  Adding additional object nodes allows you to increase capacity.
 </p><p>
  This topic describes how to add additional swift object server nodes to an
  existing system.
 </p><div class="sect5" id="id-1.5.17.3.8.3.3.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">15.1.5.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To add a new node</span> <a title="Permalink" class="permalink" href="#id-1.5.17.3.8.3.3.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-add_swift_object_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-add_swift_object_node.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To add a new node to your cloud, you will need to add it to
   <code class="literal">servers.yml</code>, and then run the scripts that update your
   cloud configuration. To begin, access the <code class="literal">servers.yml
   file</code> by checking out the Git branch where you are required to make
   the changes:


  </p><p>
   Then, perform the following steps to add a new node:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager node.
    </p></li><li class="listitem "><p>
     Get the <code class="literal">servers.yml</code> file stored in Git:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/my_cloud/definition/data
git checkout site</pre></div></li><li class="listitem "><p>
     If not already done, set the <code class="literal">weight-step</code> attribute. For
     instructions, see <a class="xref" href="#swift-weight-att" title="9.5.2. Using the Weight-Step Attributes to Prepare for Ring Changes">Section 9.5.2, “Using the Weight-Step Attributes to Prepare for Ring Changes”</a>.
    </p></li><li class="listitem "><p>
     Add the details of new nodes to the <code class="literal">servers.yml</code> file.
     In the following example only one new server
     <span class="bold"><strong>swobj4</strong></span> is added. However, you can add
     multiple servers by providing the server details in the
     <code class="literal">servers.yml</code> file:
    </p><div class="verbatim-wrap"><pre class="screen">servers:
...
- id: swobj4
  role: SWOBJ_ROLE
  server-group: &lt;server-group-name&gt;
  mac-addr: &lt;mac-address&gt;
  nic-mapping: &lt;nic-mapping-name&gt;
  ip-addr: &lt;ip-address&gt;
  ilo-ip: &lt;ilo-ip-address&gt;
  ilo-user: &lt;ilo-username&gt;
  ilo-password: &lt;ilo-password&gt;</pre></div></li><li class="listitem "><p>
     Commit your changes:
    </p><div class="verbatim-wrap"><pre class="screen">git add -A
git commit -m "Add Node &lt;name&gt;"</pre></div><div id="id-1.5.17.3.8.3.3.4.4.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      Before you run any playbooks, remember that you need to export the
      encryption key in the following environment variable:
     </p><div class="verbatim-wrap"><pre class="screen">export HOS_USER_PASSWORD_ENCRYPT_KEY=<em class="replaceable ">ENCRYPTION_KEY</em></pre></div><p>
      For instructions, see <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 30 “Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Swift Only”</span>.
     </p></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Configure Cobbler to include the new node, and then reimage the node (if
     you are adding several nodes, use a comma-separated list with the
     <code class="literal">nodelist</code> argument):
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;server-id&gt;</pre></div><p>
     In the following example, the server id is
     <span class="bold"><strong>swobj4</strong></span> (mentioned in step 3):
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=swobj4</pre></div><div id="id-1.5.17.3.8.3.3.4.4.8.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      You must use the server id as it appears in the file
      <code class="filename">servers.yml</code> in the field
      <code class="literal">id</code>.
     </p></div></li><li class="listitem "><p>
     Configure the operating system:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostname&gt;</pre></div><p>
     The hostname of the newly added server can be found in the list generated
     from the output of the following command:
    </p><div class="verbatim-wrap"><pre class="screen">grep hostname ~/openstack/my_cloud/info/server_info.yml</pre></div><p>
     For example, for <span class="bold"><strong>swobj4</strong></span>, the hostname is
     <span class="bold"><strong>ardana-cp1-swobj0004-mgmt</strong></span>.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit ardana-cp1-swobj0004-mgmt</pre></div></li><li class="listitem "><p>
     Validate that the disk drives of the new node are compatible with the disk
     model used by the node:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml --limit SWF*</pre></div><p>
     If any errors occur, correct them. For instructions, see
     <a class="xref" href="#sec-input-swift-error" title="18.6.2.3. Interpreting Swift Input Model Validation Errors">Section 18.6.2.3, “Interpreting Swift Input Model Validation Errors”</a>.
    </p></li><li class="listitem "><p>
     Run the following playbook to ensure that all other server's host file are
     updated with the new server:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml --tags "generate_hosts_file"</pre></div></li><li class="listitem "><p>
     Run the <code class="literal">ardana-deploy.yml</code> playbook to rebalance the rings
     to include the node, deploy the rings, and configure the new node. Do not
     limit this to just the node (<span class="bold"><strong>swobj4</strong></span>) that
     you are adding:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-deploy.yml</pre></div></li><li class="listitem "><p>
     You may need to perform further rebalances of the rings. For instructions,
     see the "Weight Change Phase of Ring Rebalance" and the "Final Rebalance
     Phase" sections of <a class="xref" href="#change-swift-rings" title="9.5.5. Applying Input Model Changes to Existing Rings">Section 9.5.5, “Applying Input Model Changes to Existing Rings”</a>.
    </p><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-update-from-model-rebalance-rings.yml</pre></div></li></ol></div></div></div><div class="sect4" id="adding-proxy"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.5.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a Swift Proxy, Account, Container (PAC) Node</span> <a title="Permalink" class="permalink" href="#adding-proxy">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-add_swift_pac_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-add_swift_pac_node.xml</li><li><span class="ds-label">ID: </span>adding-proxy</li></ul></div></div></div></div><p>
  Steps for adding additional PAC nodes to your swift system.
 </p><p>
  This topic describes how to add additional swift proxy, account, and
  container (PAC) servers to an existing system.
 </p><div class="sect5" id="id-1.5.17.3.8.3.4.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">15.1.5.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a new node</span> <a title="Permalink" class="permalink" href="#id-1.5.17.3.8.3.4.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-add_swift_pac_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-add_swift_pac_node.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To add a new node to your cloud, you will need to add it to
   <code class="filename">servers.yml</code>, and then run the scripts that update your
   cloud configuration. To begin, access the <code class="filename">servers.yml</code>
   file by checking out the Git branch where you are required to make
   the changes:


  </p><p>
   Then, perform the following steps to add a new node:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Get the <code class="filename">servers.yml</code> file stored in Git:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/my_cloud/definition/data
git checkout site</pre></div></li><li class="listitem "><p>
     If not already done, set the weight-step attribute. For instructions, see
     <a class="xref" href="#swift-weight-att" title="9.5.2. Using the Weight-Step Attributes to Prepare for Ring Changes">Section 9.5.2, “Using the Weight-Step Attributes to Prepare for Ring Changes”</a>.
    </p></li><li class="listitem "><p>
     Add details of new nodes to the <code class="filename">servers.yml</code> file:
    </p><div class="verbatim-wrap"><pre class="screen">servers:
...
- id: swpac6
  role: SWPAC-ROLE
  server-group: &lt;server-group-name&gt;
  mac-addr: &lt;mac-address&gt;
  nic-mapping: &lt;nic-mapping-name&gt;
  ip-addr: &lt;ip-address&gt;
  ilo-ip: &lt;ilo-ip-address&gt;
  ilo-user: &lt;ilo-username&gt;
  ilo-password: &lt;ilo-password&gt;</pre></div><p>
     In the above example, only one new server
     <span class="bold"><strong>swpac6</strong></span> is added. However, you can add
     multiple servers by providing the server details in the
     <code class="filename">servers.yml</code> file.
    </p><p>
     In the entry-scale configurations there is no dedicated swift PAC cluster.
     Instead, there is a cluster using servers that have a role of
     <code class="literal">CONTROLLER-ROLE</code>. You cannot add additional nodes
     dedicated exclusively to swift PAC because that would change the
     <code class="literal">member-count</code> of the entire cluster. In that case, to
     create a dedicated swift PAC cluster, you will need to add it to the
     configuration files. For details on how to do this, see
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 11 “Modifying Example Configurations for Object Storage using Swift”, Section 11.7 “Creating a Swift Proxy, Account, and Container (PAC) Cluster”</span>.
    </p><p>
     If using a new PAC nodes you must add the PAC node's configuration details
     in the following yaml files:
    </p><div class="verbatim-wrap"><pre class="screen">control_plane.yml
disks_pac.yml
net_interfaces.yml
servers.yml
server_roles.yml</pre></div><p>
     You can see a good example of this in the example configurations for the
     mid-scale model in the <code class="literal">~/openstack/examples/mid-scale-kvm</code>
     directory.
    </p><p>
     The following steps assume that you have already created a dedicated swift
     PAC cluster and that it has two members
     (<span class="bold"><strong>swpac4</strong></span> and
     <span class="bold"><strong>swpac5</strong></span>).
    </p></li><li class="listitem "><p>
     Set the member count of the swift PAC cluster to match the number of nodes.
     For example, if you are adding <span class="bold"><strong>swpac6</strong></span> as
     the 6th swift PAC node, the member count should be increased from 5 to 6
     as shown in the following example:
    </p><div class="verbatim-wrap"><pre class="screen">control-planes:
    - name: control-plane-1
      control-plane-prefix: cp1

  . . .
  clusters:
  . . .
     - name: swpac
       cluster-prefix: swpac
       server-role: SWPAC-ROLE
       member-count: 6
   . . .</pre></div></li><li class="listitem "><p>
     Commit your changes:
    </p><div class="verbatim-wrap"><pre class="screen">git add -A
git commit -m "Add Node &lt;name&gt;"</pre></div><div id="id-1.5.17.3.8.3.4.4.4.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      Before you run any playbooks, remember that you need to export the
      encryption key in the following environment variable:
     </p><div class="verbatim-wrap"><pre class="screen">export HOS_USER_PASSWORD_ENCRYPT_KEY=<em class="replaceable ">ENCRYPTION_KEY</em></pre></div><p>
      For instructions, see <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 30 “Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Swift Only”</span>.
     </p></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Configure Cobbler to include the new node and reimage the node (if you are
     adding several nodes, use a comma-separated list for the
     <code class="literal">nodelist</code> argument):
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost cobbler-deploy.yml
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;server-id&gt;</pre></div><p>
     In the following example, the server id is
     <span class="bold"><strong>swpac6</strong></span> (mentioned in step 3):
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost cobbler-deploy.yml
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=swpac6</pre></div><div id="id-1.5.17.3.8.3.4.4.4.9.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      You must use the server id as it appears in the file
      <code class="filename">servers.yml</code> in the field
      <code class="literal">id</code>.
     </p></div></li><li class="listitem "><p>
     Review the <code class="literal">cloudConfig.yml</code> and
     <code class="literal">data/control_plane.yml</code> files to get the host prefix
     (for example, openstack) and the control plane name (for example, cp1). This
     gives you the hostname of the node. Configure the operating system:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostname&gt;</pre></div><p>
     For example, for <span class="bold"><strong>swpac6</strong></span>, the hostname is
     <span class="bold"><strong>ardana-cp1-c2-m3-mgmt</strong></span>:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit ardana-cp1-c2-m3-mgmt</pre></div></li><li class="listitem "><p>
     Validate that the disk drives of the new node are compatible with the disk
     model used by the node:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml</pre></div><p>
     If any errors occur, correct them. For instructions, see
     <a class="xref" href="#sec-input-swift-error" title="18.6.2.3. Interpreting Swift Input Model Validation Errors">Section 18.6.2.3, “Interpreting Swift Input Model Validation Errors”</a>.
    </p></li><li class="listitem "><p>
     Run the following playbook to ensure that all other server's host file are
     updated with the new server:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml --tags "generate_hosts_file"</pre></div></li><li class="listitem "><p>
     Run the <code class="literal">ardana-deploy.yml</code> playbook to rebalance the rings
     to include the node, deploy the rings, and configure the new node. Do not
     limit this to just the node (<span class="bold"><strong>swpac6</strong></span>) that
     you are adding:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-deploy.yml</pre></div></li><li class="listitem "><p>
     You may need to perform further rebalances of the rings. For instructions,
     see the "Weight Change Phase of Ring Rebalance" and the "Final Rebalance
     Phase" sections of <a class="xref" href="#change-swift-rings" title="9.5.5. Applying Input Model Changes to Existing Rings">Section 9.5.5, “Applying Input Model Changes to Existing Rings”</a>.
    </p></li></ol></div></div></div><div class="sect4" id="add-swift-disk"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.5.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding Additional Disks to a Swift Node</span> <a title="Permalink" class="permalink" href="#add-swift-disk">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-add_swift_disk.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-add_swift_disk.xml</li><li><span class="ds-label">ID: </span>add-swift-disk</li></ul></div></div></div></div><p>
  Steps for adding additional disks to any nodes hosting swift services.
 </p><p>
  You may have a need to add additional disks to a node for swift usage and we
  can show you how. These steps work for adding additional disks to swift
  object or proxy, account, container (PAC) nodes. It can also apply to adding
  additional disks to a controller node that is hosting the swift service, like
  you would see if you are using one of the entry-scale example models.
 </p><p>
  Read through the notes below before beginning the process.
 </p><p>
  You can add multiple disks at the same time, there is no need to do it one at
  a time.
 </p><div id="id-1.5.17.3.8.3.5.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Add the Same Number of Disks</h6><p>
   You must add the <span class="emphasis"><em>same</em></span> number of disks to each server
   that the disk model applies to. For example, if you have a single cluster of
   three swift servers and you want to increase capacity and decide to add two
   additional disks, you must add two to each of your three swift servers.
  </p></div><div class="sect5" id="id-1.5.17.3.8.3.5.7"><div class="titlepage"><div><div><h6 class="title"><span class="number">15.1.5.1.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding additional disks to your Swift servers</span> <a title="Permalink" class="permalink" href="#id-1.5.17.3.8.3.5.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-add_swift_disk.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-add_swift_disk.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Verify the general health of the swift system and that it is safe to
     rebalance your rings. See <a class="xref" href="#topic-ohx-j1t-4t" title="9.5.4. Determining When to Rebalance and Deploy a New Ring">Section 9.5.4, “Determining When to Rebalance and Deploy a New Ring”</a> for details
     on how to do this.
    </p></li><li class="step "><p>
     Perform the disk maintenance.
    </p><ol type="a" class="substeps "><li class="step " id="st-swift-add-disk-shutdown"><p>
       Shut down the first swift server you wish to add disks to.
      </p></li><li class="step "><p>
       Add the additional disks to the physical server. The disk drives that
       are added should be clean. They should either contain no partitions or a
       single partition the size of the entire disk. It should not contain a
       file system or any volume groups. Failure to comply will cause errors
       and the disk will not be added.
      </p><p>
       For more details, see <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 11 “Modifying Example Configurations for Object Storage using Swift”, Section 11.6 “Swift Requirements for Device Group Drives”</span>.
      </p></li><li class="step "><p>
       Power the server on.
      </p></li><li class="step "><p>
       While the server was shutdown, data that normally would have been placed
       on the server is placed elsewhere. When the server is rebooted, the
       swift replication process will move that data back onto the server.
       Monitor the replication process to determine when it is complete. See
       <a class="xref" href="#topic-ohx-j1t-4t" title="9.5.4. Determining When to Rebalance and Deploy a New Ring">Section 9.5.4, “Determining When to Rebalance and Deploy a New Ring”</a> for details on how to do this.
      </p></li><li class="step "><p>
       Repeat the steps from <a class="xref" href="#st-swift-add-disk-shutdown" title="Step 2.a">Step 2.a</a> for
       each of the swift servers you are adding the disks to, one at a time.
      </p><div id="id-1.5.17.3.8.3.5.7.2.2.2.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
         If the additional disks can be added to the swift servers online
         (for example, via hotplugging) then there is no need to perform the
         last two steps.
         
        </p></div></li></ol></li><li class="step "><p>
     On the Cloud Lifecycle Manager, update your cloud configuration with the details
     of your additional disks.
    </p><ol type="a" class="substeps "><li class="step "><p>
       Edit the disk configuration file that correlates to the type of server
       you are adding your new disks to.
      </p><p>
       Path to the typical disk configuration files:
      </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/disks_swobj.yml
~/openstack/my_cloud/definition/data/disks_swpac.yml
~/openstack/my_cloud/definition/data/disks_controller_*.yml</pre></div><p>
       Example showing the addition of a single new disk, indicated by the
       <code class="literal">/dev/sdd</code>, in bold:
      </p><div class="verbatim-wrap"><pre class="screen">device-groups:
  - name: swiftObject
    devices:
      - name: "/dev/sdb"
      - name: "/dev/sdc"
      <span class="bold"><strong>- name: "/dev/sdd"</strong></span>
    consumer:
      name: swift
      ...</pre></div><div id="id-1.5.17.3.8.3.5.7.2.3.2.1.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
         For more details on how the disk model works, see
         <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”</span>.
        </p></div></li><li class="step "><p>
       Configure the swift weight-step value in the
       <code class="literal">~/openstack/my_cloud/definition/data/swift/swift_config.yml</code>
       file. See <a class="xref" href="#swift-weight-att" title="9.5.2. Using the Weight-Step Attributes to Prepare for Ring Changes">Section 9.5.2, “Using the Weight-Step Attributes to Prepare for Ring Changes”</a> for details on how to do
       this.
      </p></li><li class="step "><p>
       Commit the changes to Git:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack
git commit -a -m "adding additional swift disks"</pre></div></li><li class="step "><p>
       Run the configuration processor:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
       Update your deployment directory:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li></ol></li><li class="step "><p>
     Run the <code class="literal">osconfig-run.yml</code> playbook against the swift
     nodes you have added disks to. Use the <code class="literal">--limit</code> switch
     to target the specific nodes:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostnames&gt;</pre></div><p>
     You can use a wildcard when specifying the hostnames with the
     <code class="literal">--limit</code> switch. If you added disks to all of the swift
     servers in your environment and they all have the same prefix (for
     example, <code class="literal">ardana-cp1-swobj...</code>) then you can use a
     wildcard like <code class="literal">ardana-cp1-swobj*</code>. If you only added
     disks to a set of nodes but not all of them, you can use a comma
     deliminated list and enter the hostnames of each of the nodes you added
     disks to.
    </p></li><li class="step "><p>
     Validate your swift configuration with this playbook which will also
     provide details of each drive being added:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml --extra-vars "drive_detail=yes"</pre></div></li><li class="step "><p>
     Verify that swift services are running on all of your servers:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-status.yml</pre></div></li><li class="step "><p>
     If everything looks okay with the swift status, then apply the changes to
     your swift rings with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-deploy.yml</pre></div></li><li class="step "><p>
     At this point your swift rings will begin rebalancing. You should wait
     until replication has completed or min-part-hours has elapsed (whichever
     is longer), as described in <a class="xref" href="#topic-ohx-j1t-4t" title="9.5.4. Determining When to Rebalance and Deploy a New Ring">Section 9.5.4, “Determining When to Rebalance and Deploy a New Ring”</a> and then
     follow the "Weight Change Phase of Ring Rebalance" process as described in
     <a class="xref" href="#change-swift-rings" title="9.5.5. Applying Input Model Changes to Existing Rings">Section 9.5.5, “Applying Input Model Changes to Existing Rings”</a>.
    </p></li></ol></div></div></div></div><div class="sect4" id="remove-swift-node"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.5.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Removing a Swift Node</span> <a title="Permalink" class="permalink" href="#remove-swift-node">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-removing_swift_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-removing_swift_node.xml</li><li><span class="ds-label">ID: </span>remove-swift-node</li></ul></div></div></div></div><p>
  Removal process for both swift Object and PAC nodes.
 </p><p>
  You can use this process when you want to remove one or more swift nodes
  permanently. This process applies to both swift Proxy, Account, Container
  (PAC) nodes and swift Object nodes.
 </p><div class="sect5" id="idg-all-operations-maintenance-swift-removing-swift-node-xml-6"><div class="titlepage"><div><div><h6 class="title"><span class="number">15.1.5.1.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting the Pass-through Attributes</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-swift-removing-swift-node-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-removing_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-removing_swift_node.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-swift-removing-swift-node-xml-6</li></ul></div></div></div></div><p>
   This process will remove the swift node's drives from the rings and rebalance
   their responsibilities among the remaining nodes in your cluster. Note that
   removal will not succeed if it causes the number of remaining disks in the
   cluster to decrease below the replica count of its rings.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Ensure that the weight-step attribute is set. See
     <a class="xref" href="#swift-weight-att" title="9.5.2. Using the Weight-Step Attributes to Prepare for Ring Changes">Section 9.5.2, “Using the Weight-Step Attributes to Prepare for Ring Changes”</a> for more details.
    </p></li><li class="listitem "><p>
     Add the pass-through definition to your input model, specifying the server
     ID (as opposed to the server name). It is easiest to include in your
     <code class="filename">~/openstack/my_cloud/definition/data/servers.yml</code> file
     since your server IDs are already listed in that file. For more
     information about pass-through, see <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 6 “Configuration Objects”, Section 6.17 “Pass Through”</span>.
    </p><p>
     Here is the format required, which can be inserted at the topmost level
     of indentation in your file (typically 2 spaces):
    </p><div class="verbatim-wrap"><pre class="screen">pass-through:
  servers:
    - id: <em class="replaceable ">server-id</em>
      data:
        <em class="replaceable ">subsystem</em>:
          <em class="replaceable ">subsystem-attributes</em></pre></div><p>
     Here is an example:
    </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  <span class="bold"><strong>pass-through:
    servers:
      - id: ccn-0001
        data:
          swift:
            drain: yes</strong></span></pre></div><p>
     If a pass-through definition already exists in any of your input model
     data files, just include the additional data for the server which you are
     removing instead of defining an entirely new pass-through block.
    </p><p>
     By setting this pass-through attribute, you indicate that the system
     should reduce the weight of the server's drives. The weight reduction is
     determined by the weight-step attribute as described in the previous step.
     This process is known as "draining", where you remove the swift data from
     the node in preparation for removing the node.
    </p></li><li class="listitem "><p>
     Commit your configuration to the local Git repository (see
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Use the playbook to create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the swift deploy playbook to perform the first ring rebuild. This will
     remove some of the partitions from all drives on the node you are
     removing:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-deploy.yml</pre></div></li><li class="listitem "><p>
     Wait until the replication has completed. For further details, see
     <a class="xref" href="#topic-ohx-j1t-4t" title="9.5.4. Determining When to Rebalance and Deploy a New Ring">Section 9.5.4, “Determining When to Rebalance and Deploy a New Ring”</a>
    </p></li><li class="listitem "><p>
     Determine whether all of the partitions have been removed from all drives
     on the swift node you are removing. You can do this by SSH'ing into the
     first account server node and using these commands:
    </p><div class="verbatim-wrap"><pre class="screen">cd /etc/swiftlm/cloud1/cp1/builder_dir/
sudo swift-ring-builder <em class="replaceable ">ring_name</em>.builder</pre></div><p>
     For example, if the node you are removing was part of the object-o ring
     the command would be:
    </p><div class="verbatim-wrap"><pre class="screen">sudo swift-ring-builder object-0.builder</pre></div><p>
     Check the output. You will need to know the IP address of the server being
     drained. In the example below, the number of partitions of the drives on
     192.168.245.3 has reached zero for the object-0 ring:
    </p><div class="verbatim-wrap"><pre class="screen">$ cd /etc/swiftlm/cloud1/cp1/builder_dir/
$ sudo swift-ring-builder object-0.builder
account.builder, build version 6
4096 partitions, 3.000000 replicas, 1 regions, 1 zones, 6 devices, 0.00 balance, 0.00 dispersion
The minimum number of hours before a partition can be reassigned is 16
The overload factor is 0.00% (0.000000)
Devices:    id  region  zone      ip address  port  replication ip  replication port      name weight partitions balance meta
             0       1     1   192.168.245.3  6002   192.168.245.3              6002     disk0   0.00          0   -0.00 padawan-ccp-c1-m1:disk0:/dev/sdc
             1       1     1   192.168.245.3  6002   192.168.245.3              6002     disk1   0.00          0   -0.00 padawan-ccp-c1-m1:disk1:/dev/sdd
             2       1     1   192.168.245.4  6002   192.168.245.4              6002     disk0  18.63       2048   -0.00 padawan-ccp-c1-m2:disk0:/dev/sdc
             3       1     1   192.168.245.4  6002   192.168.245.4              6002     disk1  18.63       2048   -0.00 padawan-ccp-c1-m2:disk1:/dev/sdd
             4       1     1   192.168.245.5  6002   192.168.245.5              6002     disk0  18.63       2048   -0.00 padawan-ccp-c1-m3:disk0:/dev/sdc
             5       1     1   192.168.245.5  6002   192.168.245.5              6002     disk1  18.63       2048   -0.00 padawan-ccp-c1-m3:disk1:/dev/sdd</pre></div></li><li class="listitem "><p>
     If the number of partitions is zero for the server on all rings, you can
     move to the next step, otherwise continue the ring rebalance cycle by
     repeating steps 7-9 until the weight has reached zero.
    </p></li><li class="listitem "><p>
     If the number of partitions is zero for the server on all rings, you can
     remove the swift nodes' drives from all rings. Edit the pass-through data
     you created in step #3 and set the <code class="literal">remove</code> attribute as
     shown in this example:
    </p><div class="verbatim-wrap"><pre class="screen">---
  product:
    version: 2

  pass-through:
    servers:
      - id: ccn-0001
        data:
          swift:
            <span class="bold"><strong>remove: yes</strong></span></pre></div></li><li class="listitem "><p>
     Commit your configuration to the local Git repository (see
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Run the swift deploy playbook to rebuild the rings by removing the server:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-deploy.yml</pre></div></li><li class="listitem "><p>
     At this stage, the server has been removed from the rings and the data
     that was originally stored on the server has been replicated in a balanced
     way to the other servers in the system. You can proceed to the next phase.
    </p></li></ol></div></div><div class="sect5" id="sec-swift-disable-node"><div class="titlepage"><div><div><h6 class="title"><span class="number">15.1.5.1.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To Disable Swift on a Node</span> <a title="Permalink" class="permalink" href="#sec-swift-disable-node">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-removing_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-removing_swift_node.xml</li><li><span class="ds-label">ID: </span>sec-swift-disable-node</li></ul></div></div></div></div><p>
   The next phase in this process will disable the swift service on the node.
   In this example, <span class="bold"><strong>swobj4</strong></span> is the node being
   removed from swift.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Stop swift services on the node using the
     <code class="literal">swift-stop.yml</code> playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-stop.yml --limit <span class="bold"><strong><em class="replaceable ">hostname</em></strong></span></pre></div><div id="id-1.5.17.3.8.3.6.5.3.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      When using the <code class="literal">--limit</code> argument, you must specify the
      full hostname (for example: <span class="emphasis"><em>ardana-cp1-swobj0004</em></span>) or
      use the wild card <code class="literal">*</code> (for example,
      <span class="emphasis"><em>*swobj4*</em></span>).
     </p></div><p>
     The following example uses the <code class="literal">swift-stop.yml</code> playbook
     to stop swift services on
     <span class="bold"><strong>ardana-cp1-swobj0004</strong></span>:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-stop.yml --limit <span class="bold"><strong>ardana-cp1-swobj0004</strong></span></pre></div></li><li class="listitem "><p>
     Remove the configuration files.
    </p><div class="verbatim-wrap"><pre class="screen">ssh ardana-cp1-swobj4-mgmt sudo rm -R /etc/swift</pre></div><div id="id-1.5.17.3.8.3.6.5.3.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      Do not run any other playbooks until you have finished the process
      described in <a class="xref" href="#sec-swift-remove-node-model" title="15.1.5.1.4.3. To Remove a Node from the Input Model">Section 15.1.5.1.4.3, “To Remove a Node from the Input Model”</a>. Otherwise,
      these playbooks may recreate <code class="filename">/etc/swift</code> and
      restart swift on <span class="emphasis"><em>swobj4</em></span>. If you accidentally run a
      playbook, repeat the process in <a class="xref" href="#sec-swift-disable-node" title="15.1.5.1.4.2. To Disable Swift on a Node">Section 15.1.5.1.4.2, “To Disable Swift on a Node”</a>.
     </p></div></li></ol></div></div><div class="sect5" id="sec-swift-remove-node-model"><div class="titlepage"><div><div><h6 class="title"><span class="number">15.1.5.1.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To Remove a Node from the Input Model</span> <a title="Permalink" class="permalink" href="#sec-swift-remove-node-model">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-removing_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-removing_swift_node.xml</li><li><span class="ds-label">ID: </span>sec-swift-remove-node-model</li></ul></div></div></div></div><p>
   Use the following steps to finish the process of removing the swift node.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Edit the <code class="literal">~/openstack/my_cloud/definition/data/servers.yml</code>
     file and remove the entry for the node
     (<span class="bold"><strong>swobj4</strong></span> in this example). In addition,
     remove the related entry you created in the pass-through section earlier
     in this process.
    </p></li><li class="listitem "><p>
     If this was a SWPAC node, reduce the member-count attribute by 1 in the
     <code class="literal">~/openstack/my_cloud/definition/data/control_plane.yml</code>
     file. For SWOBJ nodes, no such action is needed.
    </p></li><li class="listitem "><p>
     Commit your configuration to the local Git repository (see
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
     Using the <code class="literal">remove_deleted_servers</code> and
     <code class="literal">free_unused_addresses</code> switches is recommended
     to free up the resources associated with the removed node when
     running the configuration processor. For more information, see
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 7 “Other Topics”, Section 7.3 “Persisted Data”</span>.
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost config-processor-run.yml -e remove_deleted_servers="y" -e free_unused_addresses="y"</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Validate the changes you have made to the configuration files using the
     playbook below before proceeding further:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml --limit SWF*</pre></div><p>
     If any errors occur, correct them in your configuration files and repeat
     steps 3-5 again until no more errors occur before going to the next step.
    </p><p>
     For more details on how to interpret and resolve errors, see
     <a class="xref" href="#sec-input-swift-error" title="18.6.2.3. Interpreting Swift Input Model Validation Errors">Section 18.6.2.3, “Interpreting Swift Input Model Validation Errors”</a>
    </p></li><li class="listitem "><p>
     Remove the node from Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system remove --name=swobj4</pre></div></li><li class="listitem "><p>
     Run the Cobbler deploy playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="listitem "><p>
     The final step will depend on what type of swift node you are removing.
    </p><p>
     If the node was a SWPAC node, run the <code class="literal">ardana-deploy.yml</code>
     playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-deploy.yml</pre></div><p>
     If the node was a SWOBJ node (and not a SWPAC node), run the
     <code class="literal">swift-deploy.yml</code> playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-deploy.yml</pre></div></li><li class="listitem "><p>
     Wait until replication has finished. For more details, see
     <a class="xref" href="#topic-ohx-j1t-4t" title="9.5.4. Determining When to Rebalance and Deploy a New Ring">Section 9.5.4, “Determining When to Rebalance and Deploy a New Ring”</a>.
    </p></li><li class="listitem "><p>
     You may need to continue to rebalance the rings. For instructions, see
     <span class="bold"><strong>Final Rebalance Phase </strong></span> at
     <a class="xref" href="#change-swift-rings" title="9.5.5. Applying Input Model Changes to Existing Rings">Section 9.5.5, “Applying Input Model Changes to Existing Rings”</a>.
    </p></li></ol></div></div><div class="sect5" id="idg-all-operations-maintenance-swift-removing-swift-node-xml-9"><div class="titlepage"><div><div><h6 class="title"><span class="number">15.1.5.1.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Remove the Swift Node from Monitoring</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-swift-removing-swift-node-xml-9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-removing_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-removing_swift_node.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-swift-removing-swift-node-xml-9</li></ul></div></div></div></div><p>
   Once you have removed the swift node(s), the alarms against them will
   trigger so there are additional steps to take to resolve this issue.
  </p><p>
   Connect to each of the nodes in your cluster running the
   <code class="literal">monasca-api</code> service (as defined in
   <code class="filename">~/openstack/my_cloud/definition/data/control_plane.yml</code>)
   and use <code class="command">sudo vi /etc/monasca/agent/conf.d/host_alive.yaml</code>
   to delete all references to the swift node(s) you removed.
  </p><p>
   Once you have removed the references on each of your monasca API servers you
   then need to restart the monasca-agent on each of those servers with this
   command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo service openstack-monasca-agent restart</pre></div><p>
   With the swift node references removed and the monasca-agent restarted, you
   can then delete the corresponding alarm to finish this process. To do so we
   recommend using the monasca CLI which should be installed on each of your
   monasca API servers by default:
  </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-list --metric-dimensions hostname=<em class="replaceable ">swift node deleted</em></pre></div><p>
   You can then delete the alarm with this command:
  </p><div class="verbatim-wrap"><pre class="screen">monasca alarm-delete <em class="replaceable ">alarm ID</em></pre></div></div></div><div class="sect4" id="replace-swift-node"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.5.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replacing a swift Node</span> <a title="Permalink" class="permalink" href="#replace-swift-node">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-replacing_swift_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-replacing_swift_node.xml</li><li><span class="ds-label">ID: </span>replace-swift-node</li></ul></div></div></div></div><p>
  Maintenance steps for replacing a failed swift node in your environment.
 </p><p>
  This process is used when you want to replace a failed swift node in your
  cloud.
 </p><div id="id-1.5.17.3.8.3.7.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
   If it applies to the server, do not skip step 10. If you do, the system will
   overwrite the existing rings with new rings. This will not cause data loss,
   but, potentially, will move most objects in your system to new locations and
   may make data unavailable until the replication process has completed.
  </p></div><div class="sect5" id="id-1.5.17.3.8.3.7.5"><div class="titlepage"><div><div><h6 class="title"><span class="number">15.1.5.1.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How to replace a swift node in your environment</span> <a title="Permalink" class="permalink" href="#id-1.5.17.3.8.3.7.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-replacing_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-replacing_swift_node.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Power off the node.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-power-down.yml -e nodelist=<code class="literal">OLD_SWIFT_CONTROLLER_NODE</code></pre></div></li><li class="step "><p>
     Update your cloud configuration with the details of your replacement swift.
     node.
    </p><ol type="a" class="substeps "><li class="step "><p>
       Edit your <code class="literal">servers.yml</code> file to include the details
       (MAC address, IPMI user, password, and IP address (IPME) if these
       have changed) about your replacement swift node.
      </p><div id="id-1.5.17.3.8.3.7.5.2.3.2.1.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
        Do not change the server's IP address (that is, <code class="literal">ip-addr</code>).
       </p></div><p>
       Path to file:
      </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/definition/data/servers.yml</pre></div><p>
       Example showing the fields to edit, in bold:
      </p><div class="verbatim-wrap"><pre class="screen"> - id: swobj5
   role: SWOBJ-ROLE
   server-group: rack2
   <span class="bold"><strong>mac-addr: 8c:dc:d4:b5:cb:bd</strong></span>
   nic-mapping: HP-DL360-6PORT
   ip-addr: 10.243.131.10
   <span class="bold"><strong>ilo-ip: 10.1.12.88
   ilo-user: iLOuser
   ilo-password: iLOpass</strong></span>
   ...</pre></div></li><li class="step "><p>
       Commit the changes to Git:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack
<code class="prompt user">ardana &gt; </code>git commit -a -m "replacing a swift node"</pre></div></li><li class="step "><p>
       Run the configuration processor:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
       Update your deployment directory:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li></ol></li><li class="step "><p>
     Prepare SLES:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook prepare-sles-loader.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook prepare-sles-grub2.yml -e nodelist=<code class="literal">NEW REPLACEMENT NODE</code></pre></div></li><li class="step "><p>
     Update Cobbler and reimage your replacement swift node:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Obtain the name in Cobbler for your node you wish to remove. You will
       use this value to replace <code class="literal">&lt;node name&gt;</code> in future
       steps.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cobbler system list</pre></div></li><li class="step "><p>
       Remove the replaced swift node from Cobbler:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cobbler system remove --name &lt;node name&gt;</pre></div></li><li class="step "><p>
       Re-run the <code class="literal">cobbler-deploy.yml</code> playbook to add the
       replaced node:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step "><p>
       Reimage the node using this playbook:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node name&gt;</pre></div></li></ol></li><li class="step "><p>
      Wipe the disks on the <code class="literal">NEW REPLACEMENT NODE</code>.
      This action will not affect the OS partitions on the server.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit <code class="literal">NEW_REPLACEMENT_NODE</code></pre></div></li><li class="step "><p>
     Complete the deployment of your replacement swift node.
    </p><ol type="a" class="substeps "><li class="step "><p>
       Obtain the hostname for your new swift node. Use this value to replace
       <code class="literal">&lt;hostname&gt;</code> in future steps.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cat ~/openstack/my_cloud/info/server_info.yml</pre></div></li><li class="step "><p>
       Configure the operating system on your replacement swift node:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-run.yml -limit &lt;hostname&gt;</pre></div></li><li class="step "><p>
       If this is the swift ring builder server, restore the swift ring builder
       files to the <code class="literal">/etc/swiftlm/<em class="replaceable ">CLOUD-NAME</em>/<em class="replaceable ">CONTROL-PLANE-NAME</em>/builder_dir</code> directory. For
       more information and instructions, see
       <a class="xref" href="#topic-rtc-s3t-mt" title="18.6.2.4. Identifying the Swift Ring Building Server">Section 18.6.2.4, “Identifying the Swift Ring Building Server”</a> and
       <a class="xref" href="#topic-gbz-13t-mt" title="18.6.2.7. Recovering swift Builder Files">Section 18.6.2.7, “Recovering swift Builder Files”</a>.
      </p></li><li class="step "><p>
       Configure services on the node using the
       <code class="literal">ardana-deploy.yml</code> playbook. If you have used an
       encryption password when running the configuration processor, include
       the <code class="literal">--ask-vault-pass</code> argument.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-deploy.yml --ask-vault-pass --limit &lt;hostname&gt;</pre></div></li></ol></li></ol></div></div></div></div><div class="sect4" id="replacing-disks"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.1.5.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Replacing Drives in a swift Node</span> <a title="Permalink" class="permalink" href="#replacing-disks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-replacing_drives_swift_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-replacing_drives_swift_node.xml</li><li><span class="ds-label">ID: </span>replacing-disks</li></ul></div></div></div></div><p>
  Maintenance steps for replacing drives in a swift node.
 </p><p>
  This process is used when you want to remove a failed hard drive
  from swift node and replace it with a new one.
 </p><p>
  There are two different classes of drives in a swift node that needs to be
  replaced; the operating system disk drive (generally
  <span class="bold"><strong>/dev/sda</strong></span>) and storage disk drives. There are
  different procedures for the replacement of each class of drive to bring the
  node back to normal.
 </p><div class="sect5" id="id-1.5.17.3.8.3.8.5"><div class="titlepage"><div><div><h6 class="title"><span class="number">15.1.5.1.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To Replace the Operating System Disk Drive</span> <a title="Permalink" class="permalink" href="#id-1.5.17.3.8.3.8.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-replacing_drives_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-replacing_drives_swift_node.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   After the operating system disk drive is replaced, the node must be
   reimaged.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Update your Cobbler profile:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="listitem "><p>
     Reimage the node using this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;server name&gt;</pre></div><p>
     In the example below <span class="bold"><strong>swobj2</strong></span> server is
     reimaged:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=swobj2</pre></div></li><li class="listitem "><p>
     Review the <code class="literal">cloudConfig.yml</code> and
     <code class="literal">data/control_plane.yml</code> files to get the host prefix
     (for example, openstack) and the control plane name (for example, cp1). This
     gives you the hostname of the node. Configure the operating system:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;hostname&gt;</pre></div><p>
     In the following example, for <span class="bold"><strong>swobj2</strong></span>, the
     hostname is <span class="bold"><strong>ardana-cp1-swobj0002</strong></span>:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-run.yml -limit ardana-cp1-swobj0002*</pre></div></li><li class="listitem "><p>
     If this is the first server running the swift-proxy service, restore the
     swift Ring Builder files to the
     <code class="literal">/etc/swiftlm/<em class="replaceable ">CLOUD-NAME</em>/<em class="replaceable ">CONTROL-PLANE-NAME</em>/builder_dir</code> directory. For more
     information and instructions, see <a class="xref" href="#topic-rtc-s3t-mt" title="18.6.2.4. Identifying the Swift Ring Building Server">Section 18.6.2.4, “Identifying the Swift Ring Building Server”</a> and
     <a class="xref" href="#topic-gbz-13t-mt" title="18.6.2.7. Recovering swift Builder Files">Section 18.6.2.7, “Recovering swift Builder Files”</a>.
    </p></li><li class="listitem "><p>
     Configure services on the node using the <code class="literal">ardana-deploy.yml</code>
     playbook. If you have used an encryption password when running the
     configuration processor include the <code class="literal">--ask-vault-pass</code>
     argument.
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts ardana-deploy.yml --ask-vault-pass \
  --limit &lt;hostname&gt;</pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts ardana-deploy.yml --ask-vault-pass --limit ardana-cp1-swobj0002*</pre></div></li></ol></div></div><div class="sect5" id="id-1.5.17.3.8.3.8.6"><div class="titlepage"><div><div><h6 class="title"><span class="number">15.1.5.1.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">To Replace a Storage Disk Drive</span> <a title="Permalink" class="permalink" href="#id-1.5.17.3.8.3.8.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-replacing_drives_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-replacing_drives_swift_node.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   After a storage drive is replaced, there is no need to reimage the server.
   Instead, run the <code class="literal">swift-reconfigure.yml</code> playbook.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log onto the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Run the following commands:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml --limit &lt;hostname&gt;</pre></div><p>
     In following example, the server used is
     <span class="bold"><strong>swobj2</strong></span>:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml --limit ardana-cp1-swobj0002-mgmt</pre></div></li></ol></div></div></div></div></div><div class="sect2" id="mariadb-manual-update"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Updating MariaDB with Galera</span> <a title="Permalink" class="permalink" href="#mariadb-manual-update">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-mariadb-manual-update.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-mariadb-manual-update.xml</li><li><span class="ds-label">ID: </span>mariadb-manual-update</li></ul></div></div></div></div><p>
  Updating MariaDB with Galera must be done manually. Updates are not
  installed automatically. This is particularly an issue with upgrades to
  MariaDB 10.2.17 or higher from MariaDB 10.2.16 or earlier. See
  <a class="link" href="https://mariadb.com/kb/en/library/mariadb-10222-release-notes/" target="_blank">MariaDB
  10.2.22 Release Notes - Notable Changes</a>.
 </p><p>
  Using the CLI, update MariaDB with the following procedure:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Mark Galera as unmanaged:
   </p><div class="verbatim-wrap"><pre class="screen">crm resource unmanage galera</pre></div><p>
    Or put the whole cluster into maintenance mode:
   </p><div class="verbatim-wrap"><pre class="screen">crm configure property maintenance-mode=true</pre></div></li><li class="step "><p>
    Pick a node other than the one currently targeted by the loadbalancer and
    stop MariaDB on that node:
   </p><div class="verbatim-wrap"><pre class="screen">crm_resource --wait --force-demote -r galera -V</pre></div></li><li class="step "><p>
    Perform updates:
   </p><ol type="a" class="substeps "><li class="step "><p>
      Uninstall the old versions of MariaDB and the Galera wsrep provider.
     </p></li><li class="step "><p>
      Install the new versions of MariaDB and the Galera wsrep provider.
      Select the appropriate instructions at
      <a class="link" href="https://mariadb.com/kb/en/library/installing-mariadb-with-zypper/" target="_blank">Installing
      MariaDB with zypper</a>.
     </p></li><li class="step "><p>
      Change configuration options if necessary.
     </p></li></ol></li><li class="step "><p>
    Start MariaDB on the node.
   </p><div class="verbatim-wrap"><pre class="screen">crm_resource --wait --force-promote -r galera -V</pre></div></li><li class="step "><p>
    Run <code class="command">mysql_upgrade</code> with the
    <code class="literal">--skip-write-binlog</code> option.
   </p></li><li class="step "><p>
    On the other nodes, repeat the process detailed above: stop MariaDB,
    perform updates, start MariaDB, run <code class="command">mysql_upgrade</code>.
   </p></li><li class="step "><p>
    Mark Galera as managed:
   </p><div class="verbatim-wrap"><pre class="screen">crm resource manage galera</pre></div><p>
    Or take the cluster out of maintenance mode.
   </p></li></ol></div></div></div></div><div class="sect1" id="unplanned-maintenance"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Unplanned System Maintenance</span> <a title="Permalink" class="permalink" href="#unplanned-maintenance">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-unplanned_maintenance.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-unplanned_maintenance.xml</li><li><span class="ds-label">ID: </span>unplanned-maintenance</li></ul></div></div></div></div><p>
  Unplanned maintenance tasks for your cloud.
 </p><div class="sect2" id="whole-unplanned"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Whole Cloud Recovery Procedures</span> <a title="Permalink" class="permalink" href="#whole-unplanned">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-whole_unplanned.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-whole_unplanned.xml</li><li><span class="ds-label">ID: </span>whole-unplanned</li></ul></div></div></div></div><p>
  Unplanned maintenance procedures for your whole cloud.
 </p><div class="sect3" id="full-recovery"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Full Disaster Recovery</span> <a title="Permalink" class="permalink" href="#full-recovery">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery.xml</li><li><span class="ds-label">ID: </span>full-recovery</li></ul></div></div></div></div><p>
  In this disaster scenario, you have lost everything in your cloud. In other
  words, you have lost access to all data stored in the cloud that was not
  backed up to an external backup location, including:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Data in swift object storage
   </p></li><li class="listitem "><p>
    glance images
   </p></li><li class="listitem "><p>
    cinder volumes
   </p></li><li class="listitem "><p>
    Metering, Monitoring, and Logging (MML) data
   </p></li><li class="listitem "><p>
    Workloads running on compute resources
   </p></li></ul></div><p>
  In effect, the following recovery process creates a minimal new cloud with
  the existing identity information. Much of the operating state and data would
  have been lost, as would running workloads.
 </p><div id="id-1.5.17.4.3.3.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   We recommend backups external to your cloud for your data, including as much
   as possible of the types of resources listed above. Most workloads that were
   running could possibly be recreated with sufficient external backups.
  </p></div><div class="sect4" id="id-1.5.17.4.3.3.6"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.2.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Install and Set Up a Cloud Lifecycle Manager Node</span> <a title="Permalink" class="permalink" href="#id-1.5.17.4.3.3.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Before beginning the process of a full cloud recovery, you need to install
   and set up a Cloud Lifecycle Manager node as though you are creating a new cloud. There are
   several steps in that process:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Install the appropriate version of SUSE Linux Enterprise Server
    </p></li><li class="step "><p>
     Restore <code class="literal">passwd</code>, <code class="literal">shadow</code>, and
     <code class="literal">group</code> files. They have User ID (UID) and group ID (GID)
     content that will be used to set up the new cloud. If these are not
     restored immediately after installing the operating system, the cloud
     deployment will create new UIDs and GIDs, overwriting the existing
     content.
    </p></li><li class="step "><p>
     Install Cloud Lifecycle Manager software
    </p></li><li class="step "><p>
     Prepare the Cloud Lifecycle Manager, which includes installing the necessary packages
    </p></li><li class="step "><p>
     Initialize the Cloud Lifecycle Manager
    </p></li><li class="step "><p>
     Restore your OpenStack git repository
    </p></li><li class="step "><p>
     Adjust input model settings if the hardware setup has changed
    </p></li></ol></div></div><p>
   The following sections cover these steps in detail.
  </p></div><div class="sect4" id="id-1.5.17.4.3.3.7"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.2.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Install the Operating System</span> <a title="Permalink" class="permalink" href="#id-1.5.17.4.3.3.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Follow the instructions for installing SUSE Linux Enterprise Server in
   <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 15 “Installing the Cloud Lifecycle Manager server”</span>.
  </p></div><div class="sect4" id="id-1.5.17.4.3.3.8"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.2.1.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore files with UID and GID content</span> <a title="Permalink" class="permalink" href="#id-1.5.17.4.3.3.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div id="id-1.5.17.4.3.3.8.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    There is a risk that you may lose data completely. Restore the backups for
    <code class="filename">/etc/passwd</code>, <code class="filename">/etc/shadow</code>, and
    <code class="filename">/etc/group</code> immediately after installing SUSE Linux Enterprise Server.
   </p></div><p>
   Some backup files contain content that would no longer be valid if your
   cloud were to be freshly deployed in the next step of a whole cloud
   recovery. As a result, some of the backup must be restored before deploying
   a new cloud. Three kinds of backups are involved: passwd, shadow, and group.
   The following steps will restore those backups.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the server where the Cloud Lifecycle Manager will be installed.
    </p></li><li class="step "><p>
     Retrieve the Cloud Lifecycle Manager backups from the remote server, which were created and
     saved during <a class="xref" href="#manual-backup-setup" title="Manual Backup Setup">Procedure 17.1, “Manual Backup Setup”</a>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>scp <em class="replaceable ">USER@REMOTE_SERVER</em>:<em class="replaceable ">TAR_ARCHIVE</em></pre></div></li><li class="step "><p>
     Untar the TAR archives to overwrite the three locations:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       passwd
      </p></li><li class="listitem "><p>
       shadow
      </p></li><li class="listitem "><p>
       group
      </p></li></ul></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros \
--warning=none --overwrite --directory <em class="replaceable ">RESTORE_TARGET</em> -f <em class="replaceable ">BACKUP_TARGET</em>.tar.gz</pre></div><p>
     The following are examples. Use the actual <code class="filename">tar.gz</code>
     file names of the backups.
    </p><p>
     BACKUP_TARGET=<code class="filename">/etc/passwd</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros \
--warning=none --overwrite --directory /etc/ -f passwd.tar.gz</pre></div><p>
     BACKUP_TARGET=<code class="filename">/etc/shadow</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros \
--warning=none --overwrite --directory /etc/ -f shadow.tar.gz</pre></div><p>
     BACKUP_TARGET=<code class="filename">/etc/group</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros \
--warning=none --overwrite --directory /etc/ -f group.tar.gz</pre></div></li></ol></div></div></div><div class="sect4" id="id-1.5.17.4.3.3.9"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.2.1.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Install the Cloud Lifecycle Manager</span> <a title="Permalink" class="permalink" href="#id-1.5.17.4.3.3.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To ensure that you use the same version of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> that was previously
   loaded on your Cloud Lifecycle Manager, download and install the Cloud Lifecycle Manager software using the
   instructions from <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 15 “Installing the Cloud Lifecycle Manager server”, Section 15.5.2 “Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Extension”</span>.
  </p></div><div class="sect4" id="id-1.5.17.4.3.3.10"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.2.1.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prepare to deploy your cloud</span> <a title="Permalink" class="permalink" href="#id-1.5.17.4.3.3.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following is the general process for preparing to deploy a SUSE <span class="productname">OpenStack</span> Cloud. You
   may not need to perform all the steps, depending on your particular disaster
   recovery situation.
  </p><div id="id-1.5.17.4.3.3.10.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    When you install the <code class="literal">ardana cloud pattern</code> in the
    following process, the <code class="literal">ardana</code> user and
    <code class="literal">ardana</code> group will already exist in
    <code class="filename">/etc/passwd</code> and <code class="filename">/etc/group</code>. Do
    not re-create them.
   </p><p>
    When you run <code class="literal">ardana-init</code> in the following process,
    <code class="filename">/var/lib/ardana</code> is created as a deployer account using
    the account settings in <code class="filename">/etc/passwd</code> and
    <code class="filename">/etc/group</code> that were restored in the previous step.
   </p></div><div class="sect5" id="id-1.5.17.4.3.3.10.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">15.2.1.1.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prepare for Cloud Installation</span> <a title="Permalink" class="permalink" href="#id-1.5.17.4.3.3.10.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Review the <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 14 “Pre-Installation Checklist”</span> about recommended
     pre-installation tasks.
    </p></li><li class="step "><p>
     Prepare the Cloud Lifecycle Manager node. The Cloud Lifecycle Manager must be accessible either directly or via
     <code class="literal">ssh</code>, and have SUSE Linux Enterprise Server 12 SP4 installed. All nodes must
     be accessible to the Cloud Lifecycle Manager. If the nodes do not have direct access to
     online Cloud subscription channels, the Cloud Lifecycle Manager node will need to host the
     Cloud repositories.
    </p><ol type="a" class="substeps "><li class="step "><p>
       If you followed the installation instructions for Cloud Lifecycle Manager server (see <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 15 “Installing the Cloud Lifecycle Manager server”</span>), <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> software should already be
       installed. Double-check whether SUSE Linux Enterprise and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are properly
       registered at the SUSE Customer Center by starting YaST and running <span class="guimenu ">Software</span> › <span class="guimenu ">Product
       Registration</span>.
      </p><p>
       If you have not yet installed <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, do so by starting YaST and
       running <span class="guimenu ">Software</span> › <span class="guimenu ">Product
       Registration</span> › <span class="guimenu ">Select
       Extensions</span>. Choose <span class="guimenu "><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span>
       and follow the on-screen instructions. Make sure to register
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> during the installation process and to install the software
       pattern <code class="literal">patterns-cloud-ardana</code>.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo zypper -n in patterns-cloud-ardana</pre></div></li><li class="step "><p>
       Ensure the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> media repositories and updates repositories are
       made available to all nodes in your deployment. This can be accomplished
       either by configuring the Cloud Lifecycle Manager server as an SMT mirror as described in
       <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 16 “Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)”</span> or by syncing or mounting the
       Cloud and updates repositories to the Cloud Lifecycle Manager server as described in <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 17 “Software Repository Setup”</span>.
      </p></li><li class="step "><p>
       Configure passwordless <code class="command">sudo</code> for the user created when
       setting up the node (as described in <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 15 “Installing the Cloud Lifecycle Manager server”, Section 15.4 “Creating a User”</span>). Note that this is
       <span class="emphasis"><em>not</em></span> the user <code class="systemitem">ardana</code> that will be used later in this
       procedure. In the following we assume you named the user <code class="systemitem">cloud</code>. Run the command
       <code class="command">visudo</code> as user <code class="systemitem">root</code> and add the following line
       to the end of the file:
      </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable ">CLOUD</em> ALL = (root) NOPASSWD:ALL</pre></div><p>
       Make sure to replace <em class="replaceable ">CLOUD</em> with your user name
       choice.
      </p></li><li class="step "><p>
       Set the password for the user
       <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo passwd ardana</pre></div></li><li class="step "><p>
       Become the user <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>su - ardana</pre></div></li><li class="step "><p>
       Place a copy of the SUSE Linux Enterprise Server 12 SP4 <code class="filename">.iso</code> in the
       <code class="systemitem">ardana</code> home directory,
       <code class="filename">var/lib/ardana</code>, and rename it to
       <code class="filename">sles12sp4.iso</code>.
      </p></li><li class="step "><p>
       Install the templates, examples, and working model directories:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>/usr/bin/ardana-init</pre></div></li></ol></li></ol></div></div></div></div><div class="sect4" id="id-1.5.17.4.3.3.11"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.2.1.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore the remaining Cloud Lifecycle Manager content from a remote backup</span> <a title="Permalink" class="permalink" href="#id-1.5.17.4.3.3.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Retrieve the Cloud Lifecycle Manager backups from the remote server, which were created and
     saved during <a class="xref" href="#manual-backup-setup" title="Manual Backup Setup">Procedure 17.1, “Manual Backup Setup”</a>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>scp <em class="replaceable ">USER@REMOTE_SERVER</em>:<em class="replaceable ">TAR_ARCHIVE</em></pre></div></li><li class="step "><p>
     Untar the TAR archives to overwrite the remaining four required locations:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       home
      </p></li><li class="listitem "><p>
       ssh
      </p></li></ul></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros \
--warning=none --overwrite --directory <em class="replaceable ">RESTORE_TARGET</em> -f <em class="replaceable ">BACKUP_TARGET</em>.tar.gz</pre></div><p>
     The following are examples. Use the actual <code class="filename">tar.gz</code>
     file names of the backups.
    </p><p>
     BACKUP_TARGET=<code class="filename">/var/lib/ardana</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros \
--warning=none --overwrite --directory /var/lib/ -f home.tar.gz</pre></div><p>
     BACKUP_TARGET=/etc/ssh/
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros \
--warning=none --overwrite --directory /etc/ssh/ -f ssh.tar.gz</pre></div></li></ol></div></div></div><div class="sect4" id="id-1.5.17.4.3.3.12"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.2.1.1.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Re-deployment of controllers 1, 2 and 3</span> <a title="Permalink" class="permalink" href="#id-1.5.17.4.3.3.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Change back to the default ardana user.
    </p></li><li class="step "><p>
     Run the <code class="filename">cobbler-deploy.yml</code> playbook.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step "><p>
     Run the <code class="filename">bm-reimage.yml</code> playbook limited to the second
     and third controllers.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=controller2,controller3</pre></div><p>
     The names of controller2 and controller3. Use the
     <code class="filename">bm-power-status.yml</code> playbook to check the cobbler
     names of these nodes.
    </p></li><li class="step "><p>
     Run the <code class="filename">site.yml</code> playbook limited to the three
     controllers and localhost—in this example,
     <code class="literal">doc-cp1-c1-m1-mgmt</code>,
     <code class="literal">doc-cp1-c1-m2-mgmt</code>,
     <code class="literal">doc-cp1-c1-m3-mgmt</code>, and <code class="literal">localhost</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --limit \
doc-cp1-c1-m1-mgmt,doc-cp1-c1-m2-mgmt,doc-cp1-c1-m3-mgmt,localhost</pre></div></li><li class="step "><p>
     You can now perform the procedures to restore MariaDB and swift.
    </p></li></ol></div></div></div><div class="sect4" id="id-1.5.17.4.3.3.13"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.2.1.1.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore MariaDB from a remote backup</span> <a title="Permalink" class="permalink" href="#id-1.5.17.4.3.3.13">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the first node running the MariaDB service.
    </p></li><li class="step "><p>
     Retrieve the MariaDB backup that was created with
     <a class="xref" href="#mariadb-database-backup" title="17.3.2. MariaDB Database Backup">Section 17.3.2, “MariaDB Database Backup”</a>.
    </p></li><li class="step "><p>
     Create a temporary directory and extract the TAR archive (for example,
     <code class="filename">mydb.tar.gz</code>).
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>mkdir /tmp/mysql_restore; sudo tar -z --incremental \
--extract --ignore-zeros --warning=none --overwrite --directory /tmp/mysql_restore/ \
-f mydb.tar.gz</pre></div></li><li class="step "><p>
     Verify that the files have been restored on the controller.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo du -shx /tmp/mysql_restore/*
16K     /tmp/mysql_restore/aria_log.00000001
4.0K    /tmp/mysql_restore/aria_log_control
3.4M    /tmp/mysql_restore/barbican
8.0K    /tmp/mysql_restore/ceilometer
4.2M    /tmp/mysql_restore/cinder
2.9M    /tmp/mysql_restore/designate
129M    /tmp/mysql_restore/galera.cache
2.1M    /tmp/mysql_restore/glance
4.0K    /tmp/mysql_restore/grastate.dat
4.0K    /tmp/mysql_restore/gvwstate.dat
2.6M    /tmp/mysql_restore/heat
752K    /tmp/mysql_restore/horizon
4.0K    /tmp/mysql_restore/ib_buffer_pool
76M     /tmp/mysql_restore/ibdata1
128M    /tmp/mysql_restore/ib_logfile0
128M    /tmp/mysql_restore/ib_logfile1
12M     /tmp/mysql_restore/ibtmp1
16K     /tmp/mysql_restore/innobackup.backup.log
313M    /tmp/mysql_restore/keystone
716K    /tmp/mysql_restore/magnum
12M     /tmp/mysql_restore/mon
8.3M    /tmp/mysql_restore/monasca_transform
0       /tmp/mysql_restore/multi-master.info
11M     /tmp/mysql_restore/mysql
4.0K    /tmp/mysql_restore/mysql_upgrade_info
14M     /tmp/mysql_restore/nova
4.4M    /tmp/mysql_restore/nova_api
14M     /tmp/mysql_restore/nova_cell0
3.6M    /tmp/mysql_restore/octavia
208K    /tmp/mysql_restore/opsconsole
38M     /tmp/mysql_restore/ovs_neutron
8.0K    /tmp/mysql_restore/performance_schema
24K     /tmp/mysql_restore/tc.log
4.0K    /tmp/mysql_restore/test
8.0K    /tmp/mysql_restore/winchester
4.0K    /tmp/mysql_restore/xtrabackup_galera_info</pre></div></li><li class="step "><p>
     Stop <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services on the three controllers (using the hostnames
     of the controllers in your configuration).
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-stop.yml --limit \
doc-cp1-c1-m1-mgmt,doc-cp1-c1-m2-mgmt,doc-cp1-c1-m3-mgmt,localhost</pre></div></li><li class="step "><p>
     Delete the files in the <code class="filename">mysql</code> directory and copy the
     restored backup to that directory.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>cd /var/lib/mysql/
<code class="prompt user">root # </code>rm -rf ./*
<code class="prompt user">root # </code>cp -pr /tmp/mysql_restore/* ./</pre></div></li><li class="step "><p>
     Switch back to the <code class="literal">ardana</code> user when the copy is
     finished.
    </p></li></ol></div></div></div><div class="sect4" id="id-1.5.17.4.3.3.14"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.2.1.1.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore swift from a remote backup</span> <a title="Permalink" class="permalink" href="#id-1.5.17.4.3.3.14">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the first swift Proxy
     (<code class="literal">SWF-PRX--first-member</code>) node.
    </p><p>
     To find the first swift Proxy node:
    </p><ol type="a" class="substeps "><li class="step "><p>
       On the Cloud Lifecycle Manager
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd  ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-status.yml \
--limit SWF-PRX--first-member</pre></div><p>
       At the end of the output, you will see something like the following
       example:
      </p><div class="verbatim-wrap"><pre class="screen">...
Jun 18 20:01:49 ardana-qe102-cp1-c1-m1-mgmt swiftlm-uptime-mon[3985]: 'uptime-mon - INFO : Metric:keystone-get-token:max-latency: 0.679254770279 (at 1529352109.66)'
Jun 18 20:01:49 ardana-qe102-cp1-c1-m1-mgmt swiftlm-uptime-mon[3985]: 'uptime-mon - INFO : Metric:keystone-get-token:avg-latency: 0.679254770279 (at 1529352109.66)'

PLAY RECAP ********************************************************************
ardana-qe102-cp1-c1-m1 : ok=12 changed=0 unreachable=0 failed=0```</pre></div></li><li class="step "><p>
       Find the first node name and its IP address. For example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cat /etc/hosts | grep ardana-qe102-cp1-c1-m1</pre></div></li></ol></li><li class="step "><p>
     Retrieve (<code class="command">scp</code>) the swift backup that was created with
     <a class="xref" href="#swift-ring-backup" title="17.3.3. swift Ring Backup">Section 17.3.3, “swift Ring Backup”</a>.
    </p></li><li class="step "><p>
     Create a temporary directory and extract the TAR archive (for example,
     <code class="filename">swring.tar.gz</code>).
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>mkdir /tmp/swift_builder_dir_restore; sudo tar -z \
--incremental --extract --ignore-zeros --warning=none --overwrite --directory \
/tmp/swift_builder_dir_restore/  -f swring.tar.gz</pre></div></li><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Stop the swift service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-stop.yml</pre></div></li><li class="step "><p>
     Log back in to the first swift Proxy
     (<code class="literal">SWF-PRX--first-member</code>) node, which was determined in
     <a class="xref" href="#swift-nodes" title="Step 1">Step 1</a>.
    </p></li><li class="step "><p>
     Copy the restored files.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/* \
    /etc/swiftlm/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/</pre></div><p>
     For example
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/entry-scale-kvm/control-plane-1/builder_dir/* \
    /etc/swiftlm/entry-scale-kvm/control-plane-1/builder_dir/</pre></div></li><li class="step "><p>
     Log back in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Reconfigure the swift service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect4" id="id-1.5.17.4.3.3.15"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.2.1.1.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restart <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services</span> <a title="Permalink" class="permalink" href="#id-1.5.17.4.3.3.15">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Restart the MariaDB database
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts galera-bootstrap.yml</pre></div><p>
     On the deployer node, execute the
     <code class="filename">galera-bootstrap.yml</code> playbook which will determine
     the log sequence number, bootstrap the main node, and start the database
     cluster.
    </p><p>
     If this process fails to recover the database cluster, refer to
     <a class="xref" href="#mysql" title="15.2.3.1.2. Recovering the MariaDB Database">Section 15.2.3.1.2, “Recovering the MariaDB Database”</a>.
    </p></li><li class="step "><p>
     Restart <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services on the three controllers as in the
     following example.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml \
--limit doc-cp1-c1-m1-mgmt,doc-cp1-c1-m2-mgmt,doc-cp1-c1-m3-mgmt,localhost</pre></div></li><li class="step "><p>
     Reconfigure <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml</pre></div></li></ol></div></div></div></div></div><div class="sect2" id="recover-failed-boot-processes"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recover Start-up Processes</span> <a title="Permalink" class="permalink" href="#recover-failed-boot-processes">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-recover-boot-processes.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-recover-boot-processes.xml</li><li><span class="ds-label">ID: </span>recover-failed-boot-processes</li></ul></div></div></div></div><p>
  In this scenario, processes do not start. If those processes are not running,
  ansible start-up scripts will fail. On the deployer, use
  <code class="literal">Ansible</code> to check status on the control plane servers. The
  following checks and remedies address common causes of this condition.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    If disk space is low, determine the cause and remove anything that is no
    longer needed. Check disk space with the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible KEY-API -m shell -a 'df -h'</pre></div></li><li class="listitem "><p>
    Check that Network Time Protocol (NTP) is synchronizing clocks properly
    with the following command.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible resources -i hosts/verb_hosts \
-m shell -a "sudo ntpq -c peers"</pre></div></li><li class="listitem "><p>
    Check <code class="literal">keepalived</code>, the daemon that monitors services or
    systems and automatically fails over to a standby if problems occur.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible KEY-API -i hosts/verb_hosts \
-m shell -a "sudo systemctl status keepalived | head -8"</pre></div></li><li class="listitem "><p>
    Restart <code class="literal">keepalived</code> if necessary.
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Check RabbitMQ status first:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible KEY-API -i hosts/verb_hosts \
-m shell -a "sudo rabbitmqctl status | head -10"</pre></div></li><li class="step "><p>
      Restart RabbitMQ if necessary:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible KEY-API -i hosts/verb_hosts \
-m shell -a "sudo systemctl start rabbitmq-server"</pre></div></li><li class="step "><p>
      If RabbitMQ is running, restart <code class="literal">keepalived</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible KEY-API -i hosts/verb_hosts \
-m shell -a "sudo systemctl restart keepalived"</pre></div></li></ol></div></div></li><li class="listitem "><p>
    If RabbitMQ is up, is it clustered?
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible KEY-API -i hosts/verb_hosts \
-m shell -a "sudo rabbitmqctl cluster_status"</pre></div><p>
    Restart RabbitMQ cluster if necessary:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible_playbook -i hosts/verb_hosts rabbitmq-start.yml</pre></div></li><li class="listitem "><p>
    Check <code class="literal">Kafka</code> messaging:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible KEY-API -i hosts/verb_hosts \
-m shell -a "sudo systemctl status kafka | head -5"</pre></div></li><li class="listitem "><p>
    Check the <code class="literal">Spark</code> framework:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible KEY-API -i hosts/verb_hosts \
-m shell -a "sudo systemctl status spark-worker | head -8"
<code class="prompt user">ardana &gt; </code>ansible KEY-API -i hosts/verb_hosts \
-m shell -a "sudo systemctl status spark-master | head -8"</pre></div></li><li class="listitem "><p>
    If necessary, start <code class="literal">Spark</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts spark-start.yml
<code class="prompt user">ardana &gt; </code>ansible KEY-API -i hosts/verb_hosts -m shell -a \
"sudo systemctl start spark-master | head -8"</pre></div></li><li class="listitem "><p>
    Check <code class="literal">Zookeeper</code> centralized service:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible KEY-API -i hosts/verb_hosts \
-m shell -a "sudo systemctl status zookeeper| head -8"</pre></div></li><li class="listitem "><p>
    Check MariaDB:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible KEY-API -i hosts/verb_hosts
-m shell -a "sudo mysql -e 'show status;' | grep -e wsrep_incoming_addresses \
-e wsrep_local_state_comment "</pre></div></li></ul></div></div><div class="sect2" id="cont-ungplanned"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Unplanned Control Plane Maintenance</span> <a title="Permalink" class="permalink" href="#cont-ungplanned">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-cont_unplanned.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-cont_unplanned.xml</li><li><span class="ds-label">ID: </span>cont-ungplanned</li></ul></div></div></div></div><p>
  Unplanned maintenance tasks for controller nodes such as recovery from power
  failure.
 </p><div class="sect3" id="recover-downed-cluster"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.2.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restarting Controller Nodes After a Reboot</span> <a title="Permalink" class="permalink" href="#recover-downed-cluster">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-restart_controller.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-restart_controller.xml</li><li><span class="ds-label">ID: </span>recover-downed-cluster</li></ul></div></div></div></div><p>
  Steps to follow if one or more of your controller nodes lose network
  connectivity or power, which includes if the node is either rebooted or needs
  hardware maintenance.
 </p><p>
  When a controller node is rebooted, needs hardware maintenance, loses
  network connectivity or loses power, these steps will help you recover the
  node.
 </p><p>
  These steps may also be used if the Host Status (ping) alarm is triggered
  for one or more of your controller nodes.
 </p><div class="sect4" id="idg-all-operations-maintenance-controller-restart-controller-xml-7"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.2.3.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-controller-restart-controller-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-restart_controller.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-restart_controller.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-controller-restart-controller-xml-7</li></ul></div></div></div></div><p>
   The following conditions must be true in order to perform these steps
   successfully:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Each of your controller nodes should be powered on.
    </p></li><li class="listitem "><p>
     Each of your controller nodes should have network connectivity, verified
     by SSH connectivity from the Cloud Lifecycle Manager to them.
    </p></li><li class="listitem "><p>
     The operator who performs these steps will need access to the Cloud Lifecycle Manager.
    </p></li></ul></div></div><div class="sect4" id="mysql"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.2.3.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recovering the MariaDB Database</span> <a title="Permalink" class="permalink" href="#mysql">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-restart_controller.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-restart_controller.xml</li><li><span class="ds-label">ID: </span>mysql</li></ul></div></div></div></div><p>
   The recovery process for your MariaDB database cluster will depend on how
   many of your controller nodes need to be recovered. We will cover two
   scenarios:
  </p><p>
   <span class="bold"><strong>Scenario 1: Recovering one or two of your controller
   nodes but not the entire cluster</strong></span>
  </p><p>
   Follow these steps to recover one or two of your controller nodes but not the
   entire cluster, then use these steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Ensure the controller nodes have power and are booted to the command
     prompt.
    </p></li><li class="step "><p>
     If the MariaDB service is not started, start it with this command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo service mysql start</pre></div></li><li class="step "><p>
     If MariaDB fails to start, proceed to the next section which covers the
     bootstrap process.
    </p></li></ol></div></div><p>
   <span class="bold"><strong>Scenario 2: Recovering the entire controller cluster
   with the bootstrap playbook</strong></span>
  </p><p>
   If the scenario above failed or if you need to recover your entire control
   plane cluster, use the process below to recover the MariaDB database.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Make sure no <code class="literal">mysqld</code> daemon is running on any node in
     the cluster before you continue with the steps in this procedure. If there
     is a <code class="literal">mysqld</code> daemon running, then use the command below
     to shut down the daemon.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo systemctl stop mysql</pre></div><p>
     If the mysqld daemon does not go down following the service stop, then
     kill the daemon using <code class="literal">kill -9</code> before continuing.
    </p></li><li class="step "><p>
     On the deployer node, execute the
     <code class="filename">galera-bootstrap.yml</code> playbook which will
     automatically determine the log sequence number, bootstrap the main node,
     and start the database cluster.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts galera-bootstrap.yml</pre></div></li></ol></div></div></div><div class="sect4" id="hlm"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.2.3.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restarting Services on the Controller Nodes</span> <a title="Permalink" class="permalink" href="#hlm">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-restart_controller.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-restart_controller.xml</li><li><span class="ds-label">ID: </span>hlm</li></ul></div></div></div></div><p>
   From the Cloud Lifecycle Manager you should execute the
   <code class="literal">ardana-start.yml</code> playbook for each node that was brought
   down so the services can be started back up.
  </p><p>
   If you have a dedicated (separate) Cloud Lifecycle Manager node you can use this
   syntax:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit=&lt;hostname_of_node&gt;</pre></div><p>
   If you have a shared Cloud Lifecycle Manager/controller setup and need to restart
   services on this shared node, you can use <code class="literal">localhost</code> to
   indicate the shared node, like this:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit=&lt;hostname_of_node&gt;,localhost</pre></div><div id="id-1.5.17.4.5.3.7.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    If you leave off the <code class="literal">--limit</code> switch, the playbook will
    be run against all nodes.
   </p></div></div><div class="sect4" id="monasca"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.2.3.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restart the Monitoring Agents</span> <a title="Permalink" class="permalink" href="#monasca">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-restart_controller.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-restart_controller.xml</li><li><span class="ds-label">ID: </span>monasca</li></ul></div></div></div></div><p>
   As part of the recovery process, you should also restart the
   <code class="literal">monasca-agent</code> and these steps will show you how:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Stop the <code class="literal">monasca-agent</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-agent-stop.yml</pre></div></li><li class="step "><p>
     Restart the <code class="literal">monasca-agent</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-agent-start.yml</pre></div></li><li class="step "><p>
     You can then confirm the status of the <code class="literal">monasca-agent</code>
     with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</pre></div></li></ol></div></div></div></div><div class="sect3" id="recovering-controller-nodes"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.2.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recovering the Control Plane</span> <a title="Permalink" class="permalink" href="#recovering-controller-nodes">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-recovering_controller_nodes.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-recovering_controller_nodes.xml</li><li><span class="ds-label">ID: </span>recovering-controller-nodes</li></ul></div></div></div></div><p>
  If one or more of your controller nodes has experienced data or disk
  corruption due to power loss or hardware failure and you need perform
  disaster recovery, there are several scenarios for recovering your cloud.
 </p><div id="id-1.5.17.4.5.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   If you backed up the Cloud Lifecycle Manager manually after installation (see <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 38 “Post Installation Tasks”</span>, you will have a backup copy of
   <code class="filename">/etc/group</code>. When recovering a Cloud Lifecycle Manager node, manually copy
   the <code class="filename">/etc/group</code> file from a backup of the old Cloud Lifecycle Manager.
  </p></div><div class="sect4" id="pit-database-recovery"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.2.3.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Point-in-Time MariaDB Database Recovery</span> <a title="Permalink" class="permalink" href="#pit-database-recovery">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-pit_database_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-pit_database_recovery.xml</li><li><span class="ds-label">ID: </span>pit-database-recovery</li></ul></div></div></div></div><p>
  In this scenario, everything is still running (Cloud Lifecycle Manager, cloud controller nodes,
  and compute nodes) but you want to restore the MariaDB database to a
  previous state.
 </p><div class="sect5" id="id-1.5.17.4.5.4.4.3"><div class="titlepage"><div><div><h6 class="title"><span class="number">15.2.3.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore MariaDB manually</span> <a title="Permalink" class="permalink" href="#id-1.5.17.4.5.4.4.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-pit_database_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-pit_database_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Follow this procedure to manually restore MariaDB:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Stop the MariaDB cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts percona-stop.yml</pre></div></li><li class="step "><p>
     On all of the nodes running the MariaDB service, which should be all of
     your controller nodes, run the following command to purge the old
     database:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo rm -r /var/lib/mysql/*</pre></div></li><li class="step "><p>
     On the first node running the MariaDB service restore the backup with
     the command below. If you have already restored to a temporary directory,
     copy the files again.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cp -pr /tmp/mysql_restore/* /var/lib/mysql</pre></div></li><li class="step "><p>
     If you need to restore the files manually from SSH, follow these steps:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Log in to the first node running the MariaDB service.
      </p></li><li class="step "><p>
       Retrieve the MariaDB backup that was created with <a class="xref" href="#mariadb-database-backup" title="17.3.2. MariaDB Database Backup">Section 17.3.2, “MariaDB Database Backup”</a>.
       </p></li><li class="step "><p>
        Create a temporary directory and extract the TAR archive (for example,
       <code class="filename">mydb.tar.gz</code>).
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>mkdir /tmp/mysql_restore; sudo tar -z --incremental \
--extract --ignore-zeros --warning=none --overwrite --directory /tmp/mysql_restore/ \
-f mydb.tar.gz</pre></div></li><li class="step "><p>
        Verify that the files have been restored on the controller.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo du -shx /tmp/mysql_restore/*
16K     /tmp/mysql_restore/aria_log.00000001
4.0K    /tmp/mysql_restore/aria_log_control
3.4M    /tmp/mysql_restore/barbican
8.0K    /tmp/mysql_restore/ceilometer
4.2M    /tmp/mysql_restore/cinder
2.9M    /tmp/mysql_restore/designate
129M    /tmp/mysql_restore/galera.cache
2.1M    /tmp/mysql_restore/glance
4.0K    /tmp/mysql_restore/grastate.dat
4.0K    /tmp/mysql_restore/gvwstate.dat
2.6M    /tmp/mysql_restore/heat
752K    /tmp/mysql_restore/horizon
4.0K    /tmp/mysql_restore/ib_buffer_pool
76M     /tmp/mysql_restore/ibdata1
128M    /tmp/mysql_restore/ib_logfile0
128M    /tmp/mysql_restore/ib_logfile1
12M     /tmp/mysql_restore/ibtmp1
16K     /tmp/mysql_restore/innobackup.backup.log
313M    /tmp/mysql_restore/keystone
716K    /tmp/mysql_restore/magnum
12M     /tmp/mysql_restore/mon
8.3M    /tmp/mysql_restore/monasca_transform
0       /tmp/mysql_restore/multi-master.info
11M     /tmp/mysql_restore/mysql
4.0K    /tmp/mysql_restore/mysql_upgrade_info
14M     /tmp/mysql_restore/nova
4.4M    /tmp/mysql_restore/nova_api
14M     /tmp/mysql_restore/nova_cell0
3.6M    /tmp/mysql_restore/octavia
208K    /tmp/mysql_restore/opsconsole
38M     /tmp/mysql_restore/ovs_neutron
8.0K    /tmp/mysql_restore/performance_schema
24K     /tmp/mysql_restore/tc.log
4.0K    /tmp/mysql_restore/test
8.0K    /tmp/mysql_restore/winchester
4.0K    /tmp/mysql_restore/xtrabackup_galera_info</pre></div></li></ol></li><li class="step "><p>
     Log back in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Start the MariaDB service.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts galera-bootstrap.yml</pre></div></li><li class="step "><p>
     After approximately 10-15 minutes, the output of the
     <code class="literal">percona-status.yml</code> playbook should show all the
     MariaDB nodes in sync. MariaDB cluster status can be checked using
     this playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts percona-status.yml</pre></div><p>
     An example output is as follows:
    </p><div class="verbatim-wrap"><pre class="screen">TASK: [FND-MDB | status | Report status of "{{ mysql_service }}"] *************
  ok: [ardana-cp1-c1-m1-mgmt] =&gt; {
  "msg": "mysql is synced."
  }
  ok: [ardana-cp1-c1-m2-mgmt] =&gt; {
  "msg": "mysql is synced."
  }
  ok: [ardana-cp1-c1-m3-mgmt] =&gt; {
  "msg": "mysql is synced."
  }</pre></div></li></ol></div></div></div><div class="sect5" id="id-1.5.17.4.5.4.4.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">15.2.3.2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Point-in-Time Cassandra Recovery</span> <a title="Permalink" class="permalink" href="#id-1.5.17.4.5.4.4.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-pit_database_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-pit_database_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   A node may have been removed either due to an intentional action in the
   Cloud Lifecycle Manager Admin UI or as a result of a fatal hardware event that requires a
   server to be replaced. In either case, the entry for the failed or deleted
   node should be removed from Cassandra before a new node is brought up.
  </p><p>
   The following steps should be taken before enabling and deploying the
   replacement node.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Determine the IP address of the node that was removed or is being replaced.
    </p></li><li class="step "><p>
     On one of the functional Cassandra control plane nodes, log in as the
     <code class="literal">ardana</code> user.
    </p></li><li class="step "><p>
     Run the command <code class="command">nodetool status</code> to display a list of
     Cassandra nodes.
    </p></li><li class="step "><p>
     If the node that has been removed (no IP address matches that of the
     removed node) is not in the list, skip the next step.
    </p></li><li class="step "><p>
     If the node that was removed is still in the list, copy its node
     <em class="replaceable ">ID</em>.
    </p></li><li class="step "><p>
     Run the command <code class="command">nodetool removenode
     <em class="replaceable ">ID</em></code>.
    </p></li></ol></div></div><p>
   After any obsolete node entries have been removed, the replacement node can
   be deployed as usual (for more information, see <a class="xref" href="#cont-planned" title="15.1.2. Planned Control Plane Maintenance">Section 15.1.2, “Planned Control Plane Maintenance”</a>). The new Cassandra node will be able to join the
   cluster and replicate data.
  </p><p>
   For more information, please consult <a class="link" href="http://cassandra.apache.org/doc/latest/operating/topo_changes.html" target="_blank">the
   Cassandra documentation</a>.
  </p></div></div><div class="sect4" id="pit-swiftrings-recovery"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.2.3.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Point-in-Time swift Rings Recovery</span> <a title="Permalink" class="permalink" href="#pit-swiftrings-recovery">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-pit_swiftrings_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-pit_swiftrings_recovery.xml</li><li><span class="ds-label">ID: </span>pit-swiftrings-recovery</li></ul></div></div></div></div><p>
  In this situation, everything is still running (Cloud Lifecycle Manager, control plane nodes,
  and compute nodes) but you want to restore your swift rings to a previous
  state.
 </p><div id="id-1.5.17.4.5.4.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   This process restores swift rings only, not swift data.
  </p></div><div class="sect5" id="restore-swift-bu"><div class="titlepage"><div><div><h6 class="title"><span class="number">15.2.3.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from a swift backup</span> <a title="Permalink" class="permalink" href="#restore-swift-bu">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-pit_swiftrings_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-pit_swiftrings_recovery.xml</li><li><span class="ds-label">ID: </span>restore-swift-bu</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step " id="swift-nodes"><p>
     Log in to the first swift Proxy (<code class="literal">SWF-PRX--first-member</code>) node.
    </p><p>
     To find the first swift Proxy node:
    </p><ol type="a" class="substeps "><li class="step "><p>
       On the Cloud Lifecycle Manager
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd  ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-status.yml \
--limit SWF-PRX--first-member</pre></div><p>
       At the end of the output, you will see something like the following
       example:
      </p><div class="verbatim-wrap"><pre class="screen">...
Jun 18 20:01:49 ardana-qe102-cp1-c1-m1-mgmt swiftlm-uptime-mon[3985]: 'uptime-mon - INFO : Metric:keystone-get-token:max-latency: 0.679254770279 (at 1529352109.66)'
Jun 18 20:01:49 ardana-qe102-cp1-c1-m1-mgmt swiftlm-uptime-mon[3985]: 'uptime-mon - INFO : Metric:keystone-get-token:avg-latency: 0.679254770279 (at 1529352109.66)'

PLAY RECAP ********************************************************************
ardana-qe102-cp1-c1-m1 : ok=12 changed=0 unreachable=0 failed=0```</pre></div></li><li class="step "><p>
       Find the first node name and its IP address. For example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cat /etc/hosts | grep ardana-qe102-cp1-c1-m1</pre></div></li></ol></li><li class="step "><p>
     Retrieve (<code class="command">scp</code>) the swift backup that was created with
     <a class="xref" href="#swift-ring-backup" title="17.3.3. swift Ring Backup">Section 17.3.3, “swift Ring Backup”</a>.
    </p></li><li class="step "><p>
     Create a temporary directory and extract the TAR archive (for example,
     <code class="filename">swring.tar.gz</code>).
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>mkdir /tmp/swift_builder_dir_restore; sudo tar -z \
--incremental --extract --ignore-zeros --warning=none --overwrite --directory \
/tmp/swift_builder_dir_restore/  -f swring.tar.gz</pre></div></li><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Stop the swift service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-stop.yml</pre></div></li><li class="step "><p>
     Log back in to the first swift Proxy (<code class="literal">SWF-PRX--first-member</code>)
     node, which was determined in <a class="xref" href="#swift-nodes" title="Step 1">Step 1</a>.
    </p></li><li class="step "><p>
     Copy the restored files.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/* \
    /etc/swiftlm/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/</pre></div><p>For example</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/entry-scale-kvm/control-plane-1/builder_dir/* \
    /etc/swiftlm/entry-scale-kvm/control-plane-1/builder_dir/</pre></div></li><li class="step "><p>
     Log back in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Reconfigure the swift service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li></ol></div></div></div></div><div class="sect4" id="pit-lifecyclemanager-recovery"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.2.3.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Point-in-time Cloud Lifecycle Manager Recovery</span> <a title="Permalink" class="permalink" href="#pit-lifecyclemanager-recovery">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-pit_lifecyclemanager_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-pit_lifecyclemanager_recovery.xml</li><li><span class="ds-label">ID: </span>pit-lifecyclemanager-recovery</li></ul></div></div></div></div><p>
  In this scenario, everything is still running (Cloud Lifecycle Manager, controller nodes, and
  compute nodes) but you want to restore the Cloud Lifecycle Manager to a previous state.
 </p><div class="procedure " id="restore-swift-ssh-bu"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 15.1: </span><span class="name">Restoring from a Swift or SSH Backup </span><a title="Permalink" class="permalink" href="#restore-swift-ssh-bu">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="step "><p>
    Retrieve the Cloud Lifecycle Manager backups that were created with <a class="xref" href="#clm-data-backup" title="17.3.1. Cloud Lifecycle Manager Data Backup">Section 17.3.1, “Cloud Lifecycle Manager Data Backup”</a>. There are multiple backups; directories are
    handled differently than files.
    </p></li><li class="step "><p>
     Extract the TAR archives for each of the seven locations.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros
    --warning=none --overwrite --directory
    <em class="replaceable ">RESTORE_TARGET</em> -f
    <em class="replaceable ">BACKUP_TARGET</em>.tar.gz</pre></div><p>
     For example, with a directory such as
     <em class="replaceable ">BACKUP_TARGET</em>=<code class="filename">/etc/ssh/</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros \
--warning=none --overwrite --directory /etc/ssh/ -f ssh.tar.gz</pre></div><p>
     With a file such as
     <em class="replaceable ">BACKUP_TARGET</em>=<code class="filename">/etc/passwd</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros --warning=none --overwrite --directory /etc/ -f passwd.tar.gz</pre></div></li></ol></div></div></div><div class="sect4" id="lifecyclemanager-recovery"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.2.3.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cloud Lifecycle Manager Disaster Recovery</span> <a title="Permalink" class="permalink" href="#lifecyclemanager-recovery">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-lifecyclemanager_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-lifecyclemanager_recovery.xml</li><li><span class="ds-label">ID: </span>lifecyclemanager-recovery</li></ul></div></div></div></div><p>
  In this scenario everything is still running (controller nodes and compute
  nodes) but you have lost either a dedicated Cloud Lifecycle Manager or a shared
  Cloud Lifecycle Manager/controller node.
 </p><p>
  To ensure that you use the same version of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> that was previously
  loaded on your Cloud Lifecycle Manager, download and install the Cloud Lifecycle Manager software using the
  instructions from <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 15 “Installing the Cloud Lifecycle Manager server”, Section 15.5.2 “Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Extension”</span> before
  proceeding.
 </p><p>
  Prepare the Cloud Lifecycle Manager following the steps in the <code class="literal">Before You
  Start</code> section of <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 21 “Installing with the Install UI”</span>.
 </p><div class="sect5" id="id-1.5.17.4.5.4.7.5"><div class="titlepage"><div><div><h6 class="title"><span class="number">15.2.3.2.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from a remote backup</span> <a title="Permalink" class="permalink" href="#id-1.5.17.4.5.4.7.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-lifecyclemanager_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-lifecyclemanager_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
    Retrieve (with <code class="command">scp</code>) the Cloud Lifecycle Manager backups that were created
    with <a class="xref" href="#clm-data-backup" title="17.3.1. Cloud Lifecycle Manager Data Backup">Section 17.3.1, “Cloud Lifecycle Manager Data Backup”</a>. There are multiple backups;
    directories are handled differently than files.
    </p></li><li class="step "><p>
     Extract the TAR archives for each of the seven locations.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros
    --warning=none --overwrite --directory
    <em class="replaceable ">RESTORE_TARGET</em> -f
    <em class="replaceable ">BACKUP_TARGET</em>.tar.gz</pre></div><p>
     For example, with a directory such as
     <em class="replaceable ">BACKUP_TARGET</em>=<code class="filename">/etc/ssh/</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros \
--warning=none --overwrite --directory /etc/ssh/ -f ssh.tar.gz</pre></div><p>
     With a file such as
     <em class="replaceable ">BACKUP_TARGET</em>=<code class="filename">/etc/passwd</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros --warning=none --overwrite --directory /etc/ -f passwd.tar.gz</pre></div></li><li class="step "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready_deployment.yml</pre></div></li><li class="step "><p>
     When the Cloud Lifecycle Manager is restored, re-run the deployment to ensure
     the Cloud Lifecycle Manager is in the correct state:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --limit localhost</pre></div></li></ol></div></div></div></div><div class="sect4" id="onetwo-controller-recovery"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.2.3.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">One or Two Controller Node Disaster Recovery</span> <a title="Permalink" class="permalink" href="#onetwo-controller-recovery">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-onetwo_controller_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-onetwo_controller_recovery.xml</li><li><span class="ds-label">ID: </span>onetwo-controller-recovery</li></ul></div></div></div></div><p>
  This scenario makes the following assumptions:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Your Cloud Lifecycle Manager is still intact and working.
   </p></li><li class="listitem "><p>
    One or two of your controller nodes went down, but not the entire cluster.
   </p></li><li class="listitem "><p>
    The node needs to be rebuilt from scratch, not simply rebooted.
   </p></li></ul></div><div class="sect5" id="id-1.5.17.4.5.4.8.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">15.2.3.2.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Steps to recovering one or two controller nodes</span> <a title="Permalink" class="permalink" href="#id-1.5.17.4.5.4.8.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-onetwo_controller_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-onetwo_controller_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Ensure that your node has power and all of the hardware is functioning.
    </p></li><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Verify that all of the information in your
     <code class="literal">~/openstack/my_cloud/definition/data/servers.yml</code> file is
     correct for your controller node. You may need to replace the existing
     information if you had to either replacement your entire controller node
     or just pieces of it.
    </p></li><li class="listitem "><p>
     If you made changes to your <code class="literal">servers.yml</code> file then
     commit those changes to your local git:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "editing controller information"</pre></div></li><li class="listitem "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Ensure that Cobbler has the correct system information:
    </p><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       If you replaced your controller node with a completely new machine, you
       need to verify that Cobbler has the correct list of controller nodes:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cobbler system list</pre></div></li><li class="listitem "><p>
       Remove any controller nodes from Cobbler that no longer exist:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cobbler system remove --name=&lt;node&gt;</pre></div></li><li class="listitem "><p>
       Add the new node into Cobbler:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li></ol></div></li><li class="listitem "><p>
     Then you can image the node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node_name&gt;</pre></div><div id="id-1.5.17.4.5.4.8.4.2.8.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      If you do not know the <code class="literal">&lt;node name&gt;</code> already,
      you can get it by using <code class="command">sudo cobbler system list</code>.
     </p></div><p>
     Before proceeding, look at <span class="bold"><strong>info/server_info.yml</strong></span> to see if the assignment of
     the node you have added is what you expect. It may not be, as nodes will
     not be numbered consecutively if any have previously been removed. To
     prevent loss of data, the configuration processor retains data about
     removed nodes and keeps their ID numbers from being reallocated. For more
     information about how this works, see <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 7 “Other Topics”, Section 7.3 “Persisted Data”, Section 7.3.1 “Persisted Server Allocations”</span>.
    </p></li><li class="listitem "><p>
     Run the <code class="filename">wipe_disks.yml</code> playbook to ensure the
     non-OS partitions on your nodes are completely wiped prior to continuing with the
     installation.
    </p><div id="id-1.5.17.4.5.4.8.4.2.9.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      The <code class="filename">wipe_disks.yml</code> playbook is only meant to be run
      on systems immediately after running
      <code class="filename">bm-reimage.yml</code>. If used for any other situation, it
      may not wipe all of the expected partitions.
     </p></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts wipe_disks.yml --limit &lt;controller_node_hostname&gt;</pre></div></li><li class="listitem "><p>
     Complete the rebuilding of your controller node with the two playbooks
     below:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-run.yml -e rebuild=True --limit=&lt;controller_node_hostname&gt;
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-deploy.yml -e rebuild=True --limit=&lt;controller_node_hostname&gt;</pre></div></li></ol></div></div></div><div class="sect4" id="three-controller-recovery"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.2.3.2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Three Control Plane Node Disaster Recovery</span> <a title="Permalink" class="permalink" href="#three-controller-recovery">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-three_controller_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-three_controller_recovery.xml</li><li><span class="ds-label">ID: </span>three-controller-recovery</li></ul></div></div></div></div><p>
  In this scenario, all control plane nodes are down and need to be rebuilt or
  replaced. Restoring from a swift backup is not possible because swift is
  gone.
 </p><div class="sect5" id="id-1.5.17.4.5.4.9.3"><div class="titlepage"><div><div><h6 class="title"><span class="number">15.2.3.2.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from an SSH backup</span> <a title="Permalink" class="permalink" href="#id-1.5.17.4.5.4.9.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-three_controller_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-three_controller_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Deploy the control plane nodes, using the values for your control plane
     node hostnames:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --limit \
  <em class="replaceable ">CONTROL_PLANE_HOSTNAME1</em>,<em class="replaceable ">CONTROL_PLANE_HOSTNAME2</em>, \
  <em class="replaceable ">CONTROL_PLANE_HOSTNAME3</em> -e rebuild=True</pre></div><p>
     For example, if you were using the default values from the example model
     files, the command would look like this:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml \
--limit ardana-ccp-c1-m1-mgmt,ardana-ccp-c1-m2-mgmt,ardana-ccp-c1-m3-mgmt \
-e rebuild=True</pre></div><div id="id-1.5.17.4.5.4.9.3.2.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The <code class="literal">-e rebuild=True</code> is only used on a single control
      plane node when there are other controllers available to pull
      configuration data from. This causes the MariaDB database to be
      reinitialized, which is the only choice if there are no additional
      control nodes.
     </p></div></li><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Stop MariaDB:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts percona-stop.yml</pre></div></li><li class="step "><p>
     Retrieve the MariaDB backup that was created with <a class="xref" href="#mariadb-database-backup" title="17.3.2. MariaDB Database Backup">Section 17.3.2, “MariaDB Database Backup”</a>.
    </p></li><li class="step "><p>
     Create a temporary directory and extract the TAR archive (for example,
     <code class="filename">mydb.tar.gz</code>).
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>mkdir /tmp/mysql_restore; sudo tar -z --incremental \
--extract --ignore-zeros --warning=none --overwrite --directory /tmp/mysql_restore/ \
-f mydb.tar.gz</pre></div></li><li class="step "><p>
     Verify that the files have been restored on the controller.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo du -shx /tmp/mysql_restore/*
16K     /tmp/mysql_restore/aria_log.00000001
4.0K    /tmp/mysql_restore/aria_log_control
3.4M    /tmp/mysql_restore/barbican
8.0K    /tmp/mysql_restore/ceilometer
4.2M    /tmp/mysql_restore/cinder
2.9M    /tmp/mysql_restore/designate
129M    /tmp/mysql_restore/galera.cache
2.1M    /tmp/mysql_restore/glance
4.0K    /tmp/mysql_restore/grastate.dat
4.0K    /tmp/mysql_restore/gvwstate.dat
2.6M    /tmp/mysql_restore/heat
752K    /tmp/mysql_restore/horizon
4.0K    /tmp/mysql_restore/ib_buffer_pool
76M     /tmp/mysql_restore/ibdata1
128M    /tmp/mysql_restore/ib_logfile0
128M    /tmp/mysql_restore/ib_logfile1
12M     /tmp/mysql_restore/ibtmp1
16K     /tmp/mysql_restore/innobackup.backup.log
313M    /tmp/mysql_restore/keystone
716K    /tmp/mysql_restore/magnum
12M     /tmp/mysql_restore/mon
8.3M    /tmp/mysql_restore/monasca_transform
0       /tmp/mysql_restore/multi-master.info
11M     /tmp/mysql_restore/mysql
4.0K    /tmp/mysql_restore/mysql_upgrade_info
14M     /tmp/mysql_restore/nova
4.4M    /tmp/mysql_restore/nova_api
14M     /tmp/mysql_restore/nova_cell0
3.6M    /tmp/mysql_restore/octavia
208K    /tmp/mysql_restore/opsconsole
38M     /tmp/mysql_restore/ovs_neutron
8.0K    /tmp/mysql_restore/performance_schema
24K     /tmp/mysql_restore/tc.log
4.0K    /tmp/mysql_restore/test
8.0K    /tmp/mysql_restore/winchester
4.0K    /tmp/mysql_restore/xtrabackup_galera_info</pre></div></li><li class="step "><p>
     Log back in to the first controller node and move the following files:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ssh <em class="replaceable ">FIRST_CONTROLLER_NODE</em>
<code class="prompt user">ardana &gt; </code>sudo su
<code class="prompt user">root # </code>rm -rf /var/lib/mysql/*
<code class="prompt user">root # </code>cp -pr /tmp/mysql_restore/* /var/lib/mysql/</pre></div></li><li class="step "><p>
     Log back in to the Cloud Lifecycle Manager and bootstrap MariaDB:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts galera-bootstrap.yml</pre></div></li><li class="step "><p>
     Verify the status of MariaDB:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts percona-status.yml</pre></div></li></ol></div></div></div></div><div class="sect4" id="swiftrings-recovery"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.2.3.2.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">swift Rings Recovery</span> <a title="Permalink" class="permalink" href="#swiftrings-recovery">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-swiftrings_recovery.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-swiftrings_recovery.xml</li><li><span class="ds-label">ID: </span>swiftrings-recovery</li></ul></div></div></div></div><p>
  To recover the swift rings in the event of a disaster, follow the
  procedure that applies to your situation: either recover the rings with the
  manual swift backup and restore or use the SSH backup.
 </p><div class="sect5" id="id-1.5.17.4.5.4.10.3"><div class="titlepage"><div><div><h6 class="title"><span class="number">15.2.3.2.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from the swift deployment backup</span> <a title="Permalink" class="permalink" href="#id-1.5.17.4.5.4.10.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-swiftrings_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-swiftrings_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   See <a class="xref" href="#topic-gbz-13t-mt" title="18.6.2.7. Recovering swift Builder Files">Section 18.6.2.7, “Recovering swift Builder Files”</a>.
  </p></div><div class="sect5" id="id-1.5.17.4.5.4.10.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">15.2.3.2.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore from the SSH backup</span> <a title="Permalink" class="permalink" href="#id-1.5.17.4.5.4.10.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-swiftrings_recovery.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-swiftrings_recovery.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   In case you have lost all system disks of all object nodes and swift proxy
   nodes are corrupted, you can recover the rings from a copy of the swift
   rings was backed up previously. swift data is still available (the disks
   used by swift still need to be accessible).
  </p><p>
   Recover the rings with these steps.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to a swift proxy node.
    </p></li><li class="step "><p>
     Become root:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo su</pre></div></li><li class="step "><p>
     Create the temporary directory for your restored files:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mkdir /tmp/swift_builder_dir_restore/</pre></div></li><li class="step "><p>
     Retrieve (<code class="command">scp</code>) the swift backup that was created with
     <a class="xref" href="#swift-ring-backup" title="17.3.3. swift Ring Backup">Section 17.3.3, “swift Ring Backup”</a>.
    </p></li><li class="step "><p>
     Create a temporary directory and extract the TAR archive (for example,
     <code class="filename">swring.tar.gz</code>).
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>mkdir /tmp/swift_builder_dir_restore; sudo tar -z \
--incremental --extract --ignore-zeros --warning=none --overwrite --directory \
/tmp/swift_builder_dir_restore/  -f swring.tar.gz</pre></div><p>
     You now have the swift rings in
     <code class="literal">/tmp/swift_builder_dir_restore/</code>
    </p></li><li class="step "><p>
     If the SWF-PRX--first-member is already deployed, copy the contents of the
     restored directory
     (<code class="literal">/tmp/swift_builder_dir_restore/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/</code>)
     to
     <code class="literal">/etc/swiftlm/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/</code>
     on the SWF-PRX--first-member.
    </p></li><li class="step "><p>
      Then from the Cloud Lifecycle Manager run:
     </p></li><li class="step "><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/* \
/etc/swiftlm/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/</pre></div><p>For example</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/entry-scale-kvm/control-plane-1/builder_dir/* \
/etc/swiftlm/entry-scale-kvm/control-plane-1/builder_dir/</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li><li class="step "><p>
     If the SWF-ACC--first-member is<span class="bold"><strong> not </strong></span>deployed, from
     the Cloud Lifecycle Manager run these playbooks:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts guard-deployment.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts osconfig-run.yml --limit &lt;SWF-ACC[0]-hostname&gt;</pre></div></li><li class="step "><p>
     Copy the contents of the restored directory
     (<code class="literal">/tmp/swift_builder_dir_restore/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/</code>) to
     <code class="literal">/etc/swiftlm/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/</code>
     on the SWF-ACC[0].
    </p><p>
     Create the directories: <code class="literal">/etc/swiftlm/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/* \
/etc/swiftlm/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/</pre></div><p>For example:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cp -pr /tmp/swift_builder_dir_restore/entry-scale-kvm/control-plane-1/builder_dir/* \
/etc/swiftlm/entry-scale-kvm/control-plane-1/builder_dir/</pre></div></li><li class="step "><p>
     From the Cloud Lifecycle Manager, run the <code class="filename">ardana-deploy.yml</code>
     playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-deploy.yml</pre></div></li></ol></div></div></div></div></div></div><div class="sect2" id="unplanned-compute-maintenance"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Unplanned Compute Maintenance</span> <a title="Permalink" class="permalink" href="#unplanned-compute-maintenance">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute_maintenance.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute_maintenance.xml</li><li><span class="ds-label">ID: </span>unplanned-compute-maintenance</li></ul></div></div></div></div><p>
  Unplanned maintenance tasks including recovering compute nodes.
 </p><div class="sect3" id="recover-computenode"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.2.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recovering a Compute Node</span> <a title="Permalink" class="permalink" href="#recover-computenode">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-recover_compute_node.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-recover_compute_node.xml</li><li><span class="ds-label">ID: </span>recover-computenode</li></ul></div></div></div></div><p>
  If one or more of your compute nodes has experienced an issue such as power
  loss or hardware failure, then you need to perform disaster recovery. Here we
  provide different scenarios and how to resolve them to get your cloud
  repaired.
 </p><p>
  Typical scenarios in which you will need to recover a compute node include
  the following:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    The node has failed, either because it has shut down has a hardware
    failure, or for another reason.
   </p></li><li class="listitem "><p>
    The node is working but the <code class="literal">nova-compute</code> process is not
    responding, thus instances are working but you cannot manage them (for
    example to delete, reboot, and attach/detach volumes).
   </p></li><li class="listitem "><p>
    The node is fully operational but monitoring indicates a potential issue
    (such as disk errors) that require down time to fix.
   </p></li></ul></div><div class="sect4" id="idg-all-operations-maintenance-compute-recover-compute-node-xml-7"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.2.4.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">What to do if your compute node is down</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-compute-recover-compute-node-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-recover_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-recover_compute_node.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-compute-recover-compute-node-xml-7</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Compute node has power but is not powered
   on</strong></span>
  </p><p>
   If your compute node has power but is not powered on, use these steps to
   restore the node:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Obtain the name for your compute node in Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cobbler system list</pre></div></li><li class="step "><p>
     Power the node back up with this playbook, specifying the node name from
     Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-power-up.yml -e nodelist=&lt;node name&gt;</pre></div></li></ol></div></div><p>
   <span class="bold"><strong>Compute node is powered on but services are not
   running on it</strong></span>
  </p><p>
   If your compute node is powered on but you are unsure if services are
   running, you can use these steps to ensure that they are running:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Confirm the status of the compute service on the node with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-status.yml --limit &lt;hostname&gt;</pre></div></li><li class="step "><p>
     You can start the compute service on the node with this playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts nova-start.yml --limit &lt;hostname&gt;</pre></div></li></ol></div></div></div><div class="sect4" id="unplanned"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.2.4.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Scenarios involving disk failures on your compute nodes</span> <a title="Permalink" class="permalink" href="#unplanned">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-compute-recover_compute_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-compute-recover_compute_node.xml</li><li><span class="ds-label">ID: </span>unplanned</li></ul></div></div></div></div><p>
   Your compute nodes should have a minimum of two disks, one that is used for
   the operating system and one that is used as the data disk. These are
   defined during the installation of your cloud, in the
   <code class="literal">~/openstack/my_cloud/definition/data/disks_compute.yml</code>
   file on the Cloud Lifecycle Manager. The data disk(s) are where the
   <code class="literal">nova-compute</code> service lives. Recovery scenarios will
   depend on whether one or the other, or both, of these disks experienced
   failures.
  </p><p>
   <span class="bold"><strong>If your operating system disk failed but the data
   disk(s) are okay</strong></span>
  </p><p>
   If you have had issues with the physical volume that nodes your operating
   system you need to ensure that your physical volume is restored and then you
   can use the following steps to restore the operating system:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Source the administrator credentials:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc</pre></div></li><li class="step "><p>
     Obtain the hostname for your compute node, which you will use in
     subsequent commands when <code class="literal">&lt;hostname&gt;</code> is requested:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack host list | grep compute</pre></div></li><li class="step "><p>
     Obtain the status of the <code class="literal">nova-compute</code> service on that
     node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service list --host &lt;hostname&gt;</pre></div></li><li class="step "><p>
     You will likely want to disable provisioning on that node to ensure that
     <code class="literal">nova-scheduler</code> does not attempt to place any additional
     instances on the node while you are repairing it:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service set –disable --reason "node is being rebuilt" &lt;hostname&gt;</pre></div></li><li class="step "><p>
     Obtain the status of the instances on the compute node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list --host &lt;hostname&gt; --all-tenants</pre></div></li><li class="step "><p>
     Before continuing, you should either evacuate all of the instances off
     your compute node or shut them down. If the instances are booted from
     volumes, then you can use the <code class="literal">nova evacuate</code> or
     <code class="literal">nova host-evacuate</code> commands to do this. See
     <a class="xref" href="#liveInstMigration" title="15.1.3.3. Live Migration of Instances">Section 15.1.3.3, “Live Migration of Instances”</a> for more details on how to do this.
    </p><p>
     If your instances are not booted from volumes, you will need to stop the
     instances using the <code class="literal">openstack server stop</code> command. Because the
     <code class="literal">nova-compute</code> service is not running on the node you
     will not see the instance status change, but the <code class="literal">Task
     State</code> for the instance should change to
     <code class="literal">powering-off</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server stop &lt;instance_uuid&gt;</pre></div><p>
     Verify the status of each of the instances using these commands, verifying
     the <code class="literal">Task State</code> states <code class="literal">powering-off</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list --host &lt;hostname&gt; --all-tenants
<code class="prompt user">ardana &gt; </code>openstack server show &lt;instance_uuid&gt;</pre></div></li><li class="step "><p>
     At this point you should be ready with a functioning hard disk in the node
     that you can use for the operating system. Follow these steps:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Obtain the name for your compute node in Cobbler, which you will use in
       subsequent commands when <code class="literal">&lt;node_name&gt;</code> is
       requested:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cobbler system list</pre></div></li><li class="step "><p>
       Run the following playbook, ensuring that you specify only your UEFI
       SLES nodes using the nodelist. This playbook will reconfigure Cobbler
       for the nodes listed.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook prepare-sles-grub2.yml -e nodelist=node1[,node2,node3]</pre></div></li><li class="step "><p>
       Reimage the compute node with this playbook:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&lt;node name&gt;</pre></div></li></ol></li><li class="step "><p>
     Once reimaging is complete, use the following playbook to configure the
     operating system and start up services:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --limit &lt;hostname&gt;</pre></div></li><li class="step "><p>
     You should then ensure any instances on the recovered node are in an
     <code class="literal">ACTIVE</code> state. If they are not then use the
     <code class="literal">openstack server start</code> command to bring them to the
     <code class="literal">ACTIVE</code> state:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list --host &lt;hostname&gt; --all-tenants
<code class="prompt user">ardana &gt; </code>openstack server start &lt;instance_uuid&gt;</pre></div></li><li class="step "><p>
     Re-enable provisioning:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack compute service set –enable &lt;hostname&gt;</pre></div></li><li class="step "><p>
     Start any instances that you had stopped previously:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list --host &lt;hostname&gt; --all-tenants
<code class="prompt user">ardana &gt; </code>openstack server start &lt;instance_uuid&gt;</pre></div></li></ol></div></div><p>
   <span class="bold"><strong>If your data disk(s) failed but the operating system
   disk is okay OR if all drives failed</strong></span>
  </p><p>
   In this scenario your instances on the node are lost. First, follow steps 1
   to 5 and 8 to 9 in the previous scenario.
  </p><p>
   After that is complete, use the <code class="literal">openstack server rebuild</code>
   command to respawn your instances, which will also ensure that they receive
   the same IP address:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list --host &lt;hostname&gt; --all-tenants
<code class="prompt user">ardana &gt; </code>openstack server rebuild &lt;instance_uuid&gt;</pre></div></div></div></div><div class="sect2" id="storage-unplanned"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Unplanned Storage Maintenance</span> <a title="Permalink" class="permalink" href="#storage-unplanned">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-storage_unplanned.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-storage_unplanned.xml</li><li><span class="ds-label">ID: </span>storage-unplanned</li></ul></div></div></div></div><p>
  Unplanned maintenance tasks for storage nodes.
 </p><div class="sect3" id="swift-storage-unplanned"><div class="titlepage"><div><div><h4 class="title"><span class="number">15.2.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Unplanned swift Storage Maintenance</span> <a title="Permalink" class="permalink" href="#swift-storage-unplanned">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-swift_storage_unplanned.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-swift_storage_unplanned.xml</li><li><span class="ds-label">ID: </span>swift-storage-unplanned</li></ul></div></div></div></div><p>
  Unplanned maintenance tasks for swift storage nodes.
 </p><div class="sect4" id="recover-swiftnode"><div class="titlepage"><div><div><h5 class="title"><span class="number">15.2.5.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recovering a Swift Node</span> <a title="Permalink" class="permalink" href="#recover-swiftnode">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-recover_swift_node.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-recover_swift_node.xml</li><li><span class="ds-label">ID: </span>recover-swiftnode</li></ul></div></div></div></div><p>
  If one or more of your swift Object or PAC nodes has experienced an issue,
  such as power loss or hardware failure, and you need to perform disaster
  recovery then we provide different scenarios and how to resolve them to get
  your cloud repaired.
 </p><p>
  Typical scenarios in which you will need to repair a swift object or PAC
  node include:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    The node has either shut down or been rebooted.
   </p></li><li class="listitem "><p>
    The entire node has failed and needs to be replaced.
   </p></li><li class="listitem "><p>
    A disk drive has failed and must be replaced.
   </p></li></ul></div><div class="sect5" id="idg-all-operations-maintenance-swift-recover-swift-node-xml-5"><div class="titlepage"><div><div><h6 class="title"><span class="number">15.2.5.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">What to do if your Swift host has shut down or rebooted</span> <a title="Permalink" class="permalink" href="#idg-all-operations-maintenance-swift-recover-swift-node-xml-5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-recover_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-recover_swift_node.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-maintenance-swift-recover-swift-node-xml-5</li></ul></div></div></div></div><p>
   If your swift host has power but is not powered on, from the lifecycle
   manager you can run this playbook:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Obtain the name for your swift host in Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system list</pre></div></li><li class="listitem "><p>
     Power the node back up with this playbook, specifying the node name from
     Cobbler:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-power-up.yml -e nodelist=&lt;node name&gt;</pre></div></li></ol></div><p>
   Once the node is booted up, swift should start automatically. You can verify
   this with this playbook:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-status.yml</pre></div><p>
   Any alarms that have triggered due to the host going down should clear
   within 10 minutes. See <a class="xref" href="#alarmdefinitions" title="18.1.1. Alarm Resolution Procedures">Section 18.1.1, “Alarm Resolution Procedures”</a> if further
   assistance is needed with the alarms.
  </p></div><div class="sect5" id="replace"><div class="titlepage"><div><div><h6 class="title"><span class="number">15.2.5.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How to replace your Swift node</span> <a title="Permalink" class="permalink" href="#replace">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-recover_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-recover_swift_node.xml</li><li><span class="ds-label">ID: </span>replace</li></ul></div></div></div></div><p>
   If your swift node has irreparable damage and you need to replace the entire
   node in your environment, see <a class="xref" href="#replace-swift-node" title="15.1.5.1.5. Replacing a swift Node">Section 15.1.5.1.5, “Replacing a swift Node”</a> for
   details on how to do this.
  </p></div><div class="sect5" id="disk-replacement"><div class="titlepage"><div><div><h6 class="title"><span class="number">15.2.5.1.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How to replace a hard disk in your Swift node</span> <a title="Permalink" class="permalink" href="#disk-replacement">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-swift-recover_swift_node.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-swift-recover_swift_node.xml</li><li><span class="ds-label">ID: </span>disk-replacement</li></ul></div></div></div></div><p>
   If you need to do a hard drive replacement in your swift node, see
   <a class="xref" href="#replacing-disks" title="15.1.5.1.6. Replacing Drives in a swift Node">Section 15.1.5.1.6, “Replacing Drives in a swift Node”</a> for details on how to do this.
  </p></div></div></div></div></div><div class="sect1" id="maintenance-update"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cloud Lifecycle Manager Maintenance Update Procedure</span> <a title="Permalink" class="permalink" href="#maintenance-update">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-update_maintenance.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-update_maintenance.xml</li><li><span class="ds-label">ID: </span>maintenance-update</li></ul></div></div></div></div><div class="procedure " id="id-1.5.17.5.2"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 15.2: </span><span class="name">Preparing for Update </span><a title="Permalink" class="permalink" href="#id-1.5.17.5.2">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Ensure that the update repositories have been properly set up on all nodes.
    The easiest way to provide the required repositories on the Cloud Lifecycle Manager Server is
    to set up an SMT server as described in
    <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 16 “Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)”</span>. Alternatives to setting up an
    SMT server are described in <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 17 “Software Repository Setup”</span>.
   </p></li><li class="step "><p>
    Read the Release Notes for the security and maintenance updates that will
    be installed.
   </p></li><li class="step "><p>
    Have a backup strategy in place. For further information, see
    <a class="xref" href="#bura-overview" title="Chapter 17. Backup and Restore">Chapter 17, <em>Backup and Restore</em></a>.
   </p></li><li class="step "><p>
    Ensure that you have a known starting state by resolving any unexpected
    alarms.
   </p></li><li class="step "><p>
    Determine if you need to reboot your cloud after updating the software.
    Rebooting is highly recommended to ensure that all affected services are
    restarted. Reboot may be required after installing Linux kernel updates,
    but it can be skipped if the impact on running services is non-existent or
    well understood.
   </p></li><li class="step "><p>
    Review steps in <a class="xref" href="#add-network-node" title="15.1.4.1. Adding a Network Node">Section 15.1.4.1, “Adding a Network Node”</a> and
    <a class="xref" href="#rebootNodes" title="15.1.1.2. Rolling Reboot of the Cloud">Section 15.1.1.2, “Rolling Reboot of the Cloud”</a> to minimize the impact on existing
    workloads. These steps are critical when the neutron services are not
    provided via external SDN controllers.
   </p></li><li class="step "><p>
    Before the update, prepare your working loads by consolidating all of your
    instances to one or more Compute Nodes. After the update is complete on the
    evacuated Compute Nodes, reboot them and move the images from the remaining
    Compute Nodes to the newly booted ones. Then, update the remaining
    Compute Nodes.
   </p></li></ol></div></div><div class="sect2" id="perform-update"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Performing the Update</span> <a title="Permalink" class="permalink" href="#perform-update">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-update_maintenance.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-update_maintenance.xml</li><li><span class="ds-label">ID: </span>perform-update</li></ul></div></div></div></div><p>
     Before you proceed, get the status of all your services:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-status.yml</pre></div><p>
     If status check returns an error for a specific service, run the
     <code class="filename"><em class="replaceable ">SERVICE</em>-reconfigure.yml</code>
     playbook. Then run the
     <code class="filename"><em class="replaceable ">SERVICE</em>-status.yml</code>
     playbook to check that the issue has been resolved.
    </p><p>
     Update and reboot all nodes in the cloud one by one. Start with the
     deployer node, then follow the order recommended in
     <a class="xref" href="#rebootNodes" title="15.1.1.2. Rolling Reboot of the Cloud">Section 15.1.1.2, “Rolling Reboot of the Cloud”</a>.
    </p><div id="id-1.5.17.5.3.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The described workflow also covers cases in which the deployer node is
      also provisioned as an active cloud node.
     </p></div><p>
     To minimize the impact on the existing workloads, the node should first be
     prepared for an update and a subsequent reboot by following the steps
     leading up to stopping services listed in
     <a class="xref" href="#rebootNodes" title="15.1.1.2. Rolling Reboot of the Cloud">Section 15.1.1.2, “Rolling Reboot of the Cloud”</a>, such as migrating singleton agents on
     Control Nodes and evacuating Compute Nodes. Do not stop services running on
     the node, as they need to be running during the update.
    </p><div class="procedure " id="id-1.5.17.5.3.8"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 15.3: </span><span class="name">Update Instructions </span><a title="Permalink" class="permalink" href="#id-1.5.17.5.3.8">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Install all available security and maintenance updates on the deployer
       using the <code class="command">zypper patch</code> command.
      </p></li><li class="step "><p>
       Initialize the Cloud Lifecycle Manager and prepare the update playbooks.
      </p><ol type="a" class="substeps "><li class="step "><p>
         Run the <code class="systemitem">ardana-init</code> initialization script to
         update the deployer.
        </p></li><li class="step "><p>
         Redeploy cobbler:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li><li class="step "><p>
         Run the configuration processor:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
         Update your deployment directory:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li></ol></li><li class="step "><p>
       Installation and management of updates can be automated with the
       following playbooks:
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         <code class="filename">ardana-update-pkgs.yml</code>
        </p></li><li class="listitem "><p>
         <code class="filename">ardana-update.yml</code>
        </p></li><li class="listitem "><p>
         <code class="filename">ardana-update-status.yml</code>
        </p></li><li class="listitem "><p>
         <code class="filename">ardana-reboot.yml</code>
        </p></li></ul></div></li><li class="step "><p>
       Confirm version changes by running <code class="literal">hostnamectl</code>
       before and after running the <code class="literal">ardana-update-pkgs</code>
       playbook on each node.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>hostnamectl</pre></div><p>
       Notice that <code class="literal">Boot ID:</code> and <code class="literal">Kernel:</code>
       have changed.
      </p></li><li class="step "><p>
       By default, the <code class="filename">ardana-update-pkgs.yml</code> playbook
       will install patches and updates that do not require a system
       reboot. Patches and updates that <span class="bold"><strong>do</strong></span>
       require a system reboot will be installed later in this process.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-pkgs.yml \
--limit <em class="replaceable ">TARGET_NODE_NAME</em></pre></div><p>
       There may be a delay in the playbook output at the following task while
       updates are pulled from the deployer.
      </p><div class="verbatim-wrap"><pre class="screen">TASK: [ardana-upgrade-tools | pkg-update | Download and install
package updates] ***</pre></div></li><li class="step "><p>
       After running the <code class="filename">ardana-update-pkgs.yml</code> playbook
       to install patches and updates not requiring reboot, check the status of
       remaining tasks.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-status.yml \
--limit <em class="replaceable ">TARGET_NODE_NAME</em></pre></div></li><li class="step "><p>
       To install patches that require reboot, run the
       <code class="filename">ardana-update-pkgs.yml</code> playbook with the parameter
       <code class="literal">-e zypper_update_include_reboot_patches=true</code>.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-pkgs.yml \
--limit  <em class="replaceable ">TARGET_NODE_NAME</em> \
-e zypper_update_include_reboot_patches=true</pre></div><p>
	If the output of <code class="filename">ardana-update-pkgs.yml</code> indicates 
	that a reboot is required, run <code class="filename">ardana-reboot.yml</code> 
	<span class="emphasis"><em>after</em></span> completing the <code class="filename">ardana-update.yml</code> 
	step below. Running <code class="filename">ardana-reboot.yml</code>
	will cause cloud service interruption.
      </p><div id="id-1.5.17.5.3.8.8.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
        To update a single package (for example, apply a PTF on a single node
        or on all nodes), run <code class="command">zypper update
        <em class="replaceable ">PACKAGE</em></code>.
       </p><p>
        To install all package updates using <code class="command">zypper update</code>.
       </p></div></li><li class="step "><p>
       Update services:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update.yml \
--limit <em class="replaceable ">TARGET_NODE_NAME</em></pre></div></li><li class="step "><p>
       If indicated by the <code class="filename">ardana-update-status.yml</code>
       playbook, reboot the node.
      </p><p>
       There may also be a warning to reboot after running the
       <code class="filename">ardana-update-pkgs.yml</code>.
      </p><p>
       This check can be overridden by setting the
       <code class="literal">SKIP_UPDATE_REBOOT_CHECKS</code> environment variable or the
       <code class="literal">skip_update_reboot_checks</code> Ansible variable.
        </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts ardana-reboot.yml \
--limit <em class="replaceable ">TARGET_NODE_NAME</em></pre></div></li><li class="step "><p>
       To recheck pending system reboot status at a later time, run the
       following commands:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-status.yml \
--limit ardana-cp1-c1-m2 -e update_status_var=system-reboot</pre></div></li><li class="step "><p>
       The pending system reboot status can be reset by running:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-status.yml \
--limit ardana-cp1-c1-m2 \
-e update_status_var=system-reboot \
-e update_status_reset=true</pre></div></li><li class="step "><p>
       Multiple servers can be patched at the same time with
       <code class="filename">ardana-update-pkgs.yml</code> by setting the option
       <code class="literal">-e skip_single_host_checks=true</code>.
      </p><div id="id-1.5.17.5.3.8.13.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
        When patching multiple servers at the same time, take care not to
        compromise HA capability by updating an entire cluster (controller,
        database, monitor, logging) at the same time.
       </p></div><p>
       If multiple nodes are specified on the command line (with
       <code class="literal">--limit</code>), services on those servers will experience
       outages as the packages are shutdown and updated.  On Compute Nodes (or
       group of Compute Nodes) migrate the workload off if you plan to update
       it. The same applies to Control Nodes: move singleton services off of the
       control plane node that will be updated.
      </p><div id="id-1.5.17.5.3.8.13.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
        Do not reboot all of your controllers at the same time.
       </p></div></li><li class="step "><p>
       When the node comes up after the reboot, run the
       <code class="filename">spark-start.yml</code> file:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts spark-start.yml</pre></div></li><li class="step "><p>
       Verify that Spark is running on all Control Nodes:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts spark-status.yml</pre></div></li><li class="step "><p>
       After all nodes have been updated, check the status of all services:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-status.yml</pre></div></li></ol></div></div></div><div class="sect2" id="id-1.5.17.5.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Summary of the Update Playbooks</span> <a title="Permalink" class="permalink" href="#id-1.5.17.5.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-update_maintenance.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-update_maintenance.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.17.5.4.2.1"><span class="term ">ardana-update-pkgs.yml</span></dt><dd><p>
      Top-level playbook automates the installation of package updates on
      a single node. It also works for multiple nodes, if the single-node
      restriction is overridden by setting the SKIP_SINGLE_HOST_CHECKS
      environment variable <code class="literal">ardana-update-pkgs.yml -e
      skip_single_host_checks=true</code>.
     </p><p>
      Provide the following <code class="literal">-e</code> options to modify default
      behavior:
     </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
        <code class="literal">zypper_update_method</code> (default: patch)
       </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          <code class="literal">patch</code> installs all patches for the
          system. Patches are intended for specific bug and security fixes.
         </p></li><li class="listitem "><p>
          <code class="literal">update</code> installs all packages that have a
          higher version number than the installed packages.
         </p></li><li class="listitem "><p>
          <code class="literal">dist-upgrade</code> replaces each package installed with
          the version from the repository and deletes packages not available in
          the repositories.
         </p></li></ul></div></li><li class="listitem "><p>
        <code class="literal">zypper_update_repositories</code> (default: all) restricts
        the list of repositories used
       </p></li><li class="listitem "><p>
        <code class="literal">zypper_update_gpg_checks</code> (default: true) enables GPG
        checks. If set to <code class="literal">true</code>, checks if packages are
        correctly signed.
       </p></li><li class="listitem "><p>
        <code class="literal">zypper_update_licenses_agree</code> (default: false)
        automatically agrees with licenses. If set to <code class="literal">true</code>,
        zypper automatically accepts third party licenses.
       </p></li><li class="listitem "><p>
        <code class="literal">zypper_update_include_reboot_patches</code> (default:
        false) includes patches that require reboot. Setting this to
        <code class="literal">true</code> installs patches that require a reboot (such as
        kernel or glibc updates).
       </p></li></ul></div></dd><dt id="id-1.5.17.5.4.2.2"><span class="term ">ardana-update.yml</span></dt><dd><p>
      Top level playbook that automates the update of all the services. Runs
      on all nodes by default, or can be limited to a single node by adding
      <code class="literal">--limit <em class="replaceable ">nodename</em></code>.
     </p></dd><dt id="id-1.5.17.5.4.2.3"><span class="term ">ardana-reboot.yml</span></dt><dd><p>
      Top-level playbook that automates the steps required to reboot a node. It
      includes pre-boot and post-boot phases, which can be extended to include
      additional checks.
     </p></dd><dt id="id-1.5.17.5.4.2.4"><span class="term ">ardana-update-status.yml</span></dt><dd><p>
      This playbook can be used to check or reset the update-related status
      variables maintained by the update playbooks. The main reason for having
      this mechanism is to allow the update status to be checked at any point
      during the update procedure. It is also used heavily by the automation
      scripts to orchestrate installing maintenance updates on multiple nodes.
     </p></dd></dl></div></div></div><div class="sect1" id="upgrade-soc"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Upgrading Cloud Lifecycle Manager 8 to Cloud Lifecycle Manager 9</span> <a title="Permalink" class="permalink" href="#upgrade-soc">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-upgrade_maintenance.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-upgrade_maintenance.xml</li><li><span class="ds-label">ID: </span>upgrade-soc</li></ul></div></div></div></div><p>
   Before undertaking the upgrade from SUSE <span class="productname">OpenStack</span> Cloud (or HOS) 8 Cloud Lifecycle Manager to
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 Cloud Lifecycle Manager, you need to ensure that your
   existing SUSE <span class="productname">OpenStack</span> Cloud 8 Cloud Lifecycle Manager installation is up to date by following the
   <a class="link" href="https://documentation.suse.com/hpe-helion/8/html/hpe-helion-openstack-clm-all/system-maintenance.html#maintenance-update" target="_blank">https://documentation.suse.com/hpe-helion/8/html/hpe-helion-openstack-clm-all/system-maintenance.html#maintenance-update</a>.
 </p><p>
   Ensure you review the following resources:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <a class="link" href="https://documentation.suse.com/hpe-helion/8/html/hpe-helion-openstack-clm-all/system-maintenance.html#maintenance-update/" target="_blank">https://documentation.suse.com/hpe-helion/8/html/hpe-helion-openstack-clm-all/system-maintenance.html#maintenance-update/</a>
     </p></li><li class="listitem "><p>
       <a class="link" href="https://documentation.suse.com/soc/8/html/suse-openstack-cloud-clm-all/system-maintenance.html#maintenance-update" target="_blank">https://documentation.suse.com/soc/8/html/suse-openstack-cloud-clm-all/system-maintenance.html#maintenance-update</a>
     </p></li></ul></div><p>
   To confirm that all nodes have been successfully updated with no pending
   actions, run the <code class="filename">ardana-update-status.yml</code> playbook on
   the Cloud Lifecycle Manager deployer node as follows:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-status.yml</pre></div><div id="id-1.5.17.6.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     Ensure that all nodes have been updated, and that there are no pending
     update actions remaining to be completed. In particular, ensure that
     any nodes that need to be rebooted have been, using the documented reboot procedure.
   </p></div><div class="procedure " id="id-1.5.17.6.8"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 15.4: </span><span class="name">Running the Pre-Upgrade Validation Checks to Ensure that your Cloud is Ready for Upgrade </span><a title="Permalink" class="permalink" href="#id-1.5.17.6.8">#</a></h6></div><div class="procedure-contents"><ul class="procedure"><li class="step "><p>
       Once all nodes have been successfully updated, and there are no pending
       update actions remaining, you should be able to run the
       <code class="filename">ardana-pre-upgrade-validations.sh</code> script, as follows:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>./ardana-pre-upgrade-validations.sh
~/scratch/ansible/next/ardana/ansible ~/scratch/ansible/next/ardana/ansible

PLAY [Initialize an empty list of msgs] ***************************************

TASK: [set_fact ] *************************************************************
ok: [localhost]
...

PLAY RECAP ********************************************************************
...
localhost                  : ok=8    changed=5    unreachable=0    failed=0

msg: Please refer to /var/log/ardana-pre-upgrade-validations.log for the results of this run. Ensure that any messages in the file that have the words FAIL or WARN are resolved.</pre></div><p>
       The last line of output from the <code class="filename">ardana-pre-upgrade-validations.sh</code>
       script will tell you the name of its log file—in this case,
       <code class="filename">/var/log/ardana-pre-upgrade-validations.log</code>. If you
       look at the log file, you will see content similar to the following:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo cat /var/log/ardana-pre-upgrade-validations.log
ardana-cp-dbmqsw-m1*************************************************************
NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for /srv/node/disk0 is smaller than SLE 12 SP4 recommended 512. Some recommended XFS data integrity features may not be available after upgrade.
NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for /srv/node/disk1 is smaller than SLE 12 SP4 recommended 512. Some recommended XFS data integrity features may not be available after upgrade.
NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for /srv/node/disk2 is smaller than SLE 12 SP4 recommended 512. Some recommended XFS data integrity features may not be available after upgrade.

ardana-cp-dbmqsw-m2*************************************************************
NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for /srv/node/disk0 is smaller than SLE 12 SP4 recommended 512. Some recommended XFS data integrity features may not be available after upgrade.
NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for /srv/node/disk1 is smaller than SLE 12 SP4 recommended 512. Some recommended XFS data integrity features may not be available after upgrade.
NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for /srv/node/disk2 is smaller than SLE 12 SP4 recommended 512. Some recommended XFS data integrity features may not be available after upgrade.

ardana-cp-dbmqsw-m3*************************************************************
NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for /srv/node/disk0 is smaller than SLE 12 SP4 recommended 512. Some recommended XFS data integrity features may not be available after upgrade.
NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for /srv/node/disk1 is smaller than SLE 12 SP4 recommended 512. Some recommended XFS data integrity features may not be available after upgrade.
NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for /srv/node/disk2 is smaller than SLE 12 SP4 recommended 512. Some recommended XFS data integrity features may not be available after upgrade.

ardana-cp-mml-m1****************************************************************
SUCCESS: Keystone V2 ==&gt; V3 API config changes detected.
ardana-cp-mml-m2****************************************************************
SUCCESS: Keystone V2 ==&gt; V3 API config changes detected.
ardana-cp-mml-m3****************************************************************
SUCCESS: Keystone V2 ==&gt; V3 API config changes detected.
localhost***********************************************************************</pre></div><p>
       The report states the following:
     </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.17.6.8.2.6.1"><span class="term ">SUCCESS: Keystone V2 ==&gt; V3 API config changes detected.</span></dt><dd><p>
             This check confirms that your cloud has been updated with the
             necessary changes such that all services will be using Keystone V3 API.
             This means that there should be minimal interruption of service during
             the upgrade. This is important because the Keystone V2 API has been
             removed in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9.
           </p></dd><dt id="id-1.5.17.6.8.2.6.2"><span class="term ">NOTE: pre-upgrade-swift-checks: Swift XFS inode size of 256 for
           /srv/node/disk0 is smaller than the SUSE Linux Enterprise 12 SP4 recommendation of 512. Some
           recommended XFS data integrity features may not be available after upgrade.</span></dt><dd><p>
             This check will only report something if you have local swift
             configured and it is formatted with the SUSE Linux Enterprise 12 SP3 default XFS
             inode size of 256. In SUSE Linux Enterprise 12 SP4, the default XFS inode size for a
             newly-formatted XFS file system has been increased to 512, to
             allow room for enabling some additional XFS data-integrity features by default.
           </p></dd></dl></div></li></ul></div></div><div id="id-1.5.17.6.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     There will be no loss of functionality as regards the swift
     solution after the upgrade. The difference is that some additional
     XFS features will not be available on file systems which were formatted
     under SUSE Linux Enterprise 12 SP3 or earlier. These XFS features aid in the detection of,
     and recovery from, data corruption. They are enabled by default for XFS
     file systems formatted under SUSE Linux Enterprise 12 SP 4.
   </p></div><div class="procedure " id="id-1.5.17.6.10"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 15.5: </span><span class="name">Additional Pre-Upgrade Checks That Should Be Performed </span><a title="Permalink" class="permalink" href="#id-1.5.17.6.10">#</a></h6></div><div class="procedure-contents"><p>
     In addition to the automated upgrade checks above, there are some checks
     that should be performed manually.
   </p><ol class="procedure" type="1"><li class="step "><p>
       For each network interface device specified in the input model under
       <code class="filename">~/openstack/my_cloud/definition</code>, ensure that
       there is only one untagged VLAN. The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 Cloud Lifecycle Manager
       configuration processor will fail with an error if it detects this problem
       during the upgrade, so address this problem before starting the upgrade process.
     </p></li><li class="step "><p>
       If the deployer node is not a standalone system, but
       is instead co-located with the DB services, this can lead to
       potentially longer service disruptions during the upgrade process. To
       determine if this is the case, check if the deployer node (<code class="systemitem">OPS-LM--first-member</code>)
       is a member of the database nodes (<code class="literal">FND-MDB</code>). You can do this with the
       following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible -i hosts/verb_hosts 'FND-MDB:&amp;OPS-LM--first-member' --list-hosts</pre></div><p>
       If the output is:
     </p><div class="verbatim-wrap"><pre class="screen">       No hosts matched</pre></div><p>
       Then the deployer node is not co-located with the database nodes.
       Otherwise, if the command reports a hostname, then there may be
       additional interruptions to the database services during the upgrade.
     </p></li><li class="step "><p>
       Similarly, if the deployer is co-located with the database services,
       and you are also trying to run a local SMT service on the deployer
       node, you will run into issues trying to configure the SMT to enable and
       mirror the SUSE Linux Enterprise 12 SP4 and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 repositories.
     </p><p>
       In such cases, it is recommended that you run the SMT services on a
       different node, and NFS-import the <code class="filename">/srv/www/htdocs/repo</code> onto the deployer
       node, instead of trying to run the SMT services locally.
     </p></li></ol></div></div><div id="id-1.5.17.6.11" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Backup the Cloud Lifecycle Manager Configuration Settings</h6><p>
       The integrated backup solution in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 8 Cloud Lifecycle Manager, freezer, is no
       longer available in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 Cloud Lifecycle Manager. Therefore, we
       recommend doing a manual backup to a server that is not a member of the
       cloud, as per <a class="xref" href="#bura-overview" title="Chapter 17. Backup and Restore">Chapter 17, <em>Backup and Restore</em></a>.
     </p></div><div class="sect2" id="upgrade-overivew"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Migrating the Deployer Node Packages</span> <a title="Permalink" class="permalink" href="#upgrade-overivew">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-upgrade_maintenance.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-upgrade_maintenance.xml</li><li><span class="ds-label">ID: </span>upgrade-overivew</li></ul></div></div></div></div><p>
     The upgrade process first migrates the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 8 Cloud Lifecycle Manager deployer node
     to SUSE Linux Enterprise 12 SP4 and the SOC 9 Cloud Lifecycle Manager packages.
   </p><div id="id-1.5.17.6.12.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
       If the deployer node is not a dedicated node, but is instead a member
       of one of the cloud-control planes, then some services may restart with
       the SUSE Linux Enterprise 12 SP4 and SUSE <span class="productname">OpenStack</span> Cloud 9 CLM versions of the software during the migration.
       This may mean that:
     </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
           Some services fail to restart. This will be resolved when the
           appropriate <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 configuration changes are applied
           by running the <code class="filename">ardana-upgrade.yml</code> playbook, later
           during the upgrade process.
         </p></li><li class="listitem "><p>
           Other services may log excessive warnings about connectivity issues
           and backwards-compatibility warnings. This will be resolved
           when the relevant services are upgraded during the <code class="filename">ardana-upgrade.yml</code>
           playbook run.
         </p></li></ul></div></div><p>
     In order to upgrade the deployer node to be based on <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9
     Cloud Lifecycle Manager, you first need to migrate the system to SUSE Linux Enterprise 12 SP4 with the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9
     Cloud Lifecycle Manager product installed.
   </p><p>
     The process for migrating the deployer node differs somewhat, depending
     on whether your deployer node is registered with the SUSE Customer Center
     (or an SMT mirror), versus using locally-maintained repositories available
     at the relevant locations.
   </p><p>
     If your deployer node is registered with the SUSE Customer Center or an SMT, the migration
     process requires the <code class="literal">zypper-migration-plugin</code> package to be installed.
   </p><div class="procedure " id="id-1.5.17.6.12.7"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 15.6: </span><span class="name">Migrating an SCC/SMT Registered Deployer Node </span><a title="Permalink" class="permalink" href="#id-1.5.17.6.12.7">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         If you are using an SMT server to mirror the relevant repositories,
         then you need to enable mirroring of the relevant repositories. See
         <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 16 “Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)”, Section 16.3 “Setting up Repository Mirroring on the SMT Server”</span> for more information.
       </p><p>
         Ensure that the mirroring process has completed before proceeding.
       </p></li><li class="step "><p>
         Ensure that the <code class="literal">zypper-migration-plugin</code> package is
         installed; if not, install it:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo zypper install zypper-migration-plugin
Refreshing service 'SMT-http_smt_example_com'.
Loading repository data...
Reading installed packages...
'zypper-migration-plugin' is already installed.
No update candidate for 'zypper-migration-plugin-0.10-12.4.noarch'. The highest available version is already installed.
Resolving package dependencies...

Nothing to do.</pre></div></li><li class="step "><p>
         De-register the SUSE Linux Enterprise Server LTSS 12 SP3 x86_64
         extension (if enabled):
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo SUSEConnect --status-text
Installed Products:
------------------------------------------

  SUSE Linux Enterprise Server 12 SP3 LTSS
  (SLES-LTSS/12.3/x86_64)

  Registered

------------------------------------------

  SUSE Linux Enterprise Server 12 SP3
  (SLES/12.3/x86_64)

  Registered

------------------------------------------

  SUSE OpenStack Cloud 8
  (suse-openstack-cloud/8/x86_64)

  Registered

------------------------------------------


ardana &gt; sudo SUSEConnect -d -p SLES-LTSS/12.3/x86_64
Deregistering system from registration proxy https://smt.example.com/

Deactivating SLES-LTSS 12.3 x86_64 ...
-&gt; Refreshing service ...
-&gt; Removing release package ...
ardana &gt; sudo SUSEConnect --status-text
Installed Products:
------------------------------------------

  SUSE Linux Enterprise Server 12 SP3
  (SLES/12.3/x86_64)

  Registered

------------------------------------------

  SUSE OpenStack Cloud 8
  (suse-openstack-cloud/8/x86_64)

  Registered

------------------------------------------</pre></div></li><li class="step "><p>
         Disable any other SUSE Linux Enterprise 12 SP3 or SUSE <span class="productname">OpenStack</span> Cloud (or HOS) 8 Cloud Lifecycle Manager-related repositories.
         The zypper migration process should detect and disable most of these
         automatically, but in some cases it may not catch all of them, which
         can lead to a minor disruption later during the upgrade procedure. For
         example, to disable any repositories served from the
         <code class="filename">/srv/www/suse-12.3</code> directory or the SUSE-12-4 alias
         under <code class="systemitem">http://localhost:79/</code>, you could use the following commands:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>zypper repos --show-enabled-only --uri | grep -e dir:///srv/www/suse-12.3 -e http://localhost:79/SUSE-12-4/ | cut -d '|' -f 2
PTF
SLES12-SP3-LTSS-Updates
SLES12-SP3-Pool
SLES12-SP3-Updates
SUSE-OpenStack-Cloud-8-Pool
SUSE-OpenStack-Cloud-8-Updates
ardana &gt; for repo in $(zypper repos --show-enabled-only --uri | grep -e dir:///srv/www/suse-12.3 -e http://localhost:79/SUSE-12-4/ | cut -d '|' -f 2); do sudo zypper modifyrepo --disable "${repo}"; done
Repository 'PTF' has been successfully disabled.
Repository 'SLES12-SP3-LTSS-Updates' has been successfully disabled.
Repository 'SLES12-SP3-Pool' has been successfully disabled.
Repository 'SLES12-SP3-Updates' has been successfully disabled.
Repository 'SUSE-OpenStack-Cloud-8-Pool' has been successfully disabled.
Repository 'SUSE-OpenStack-Cloud-8-Updates' has been successfully disabled.</pre></div></li><li class="step "><p>
         Remove the PTF repository, which is based on SUSE Linux Enterprise 12 SP3 (a new one,
         based on SUSE Linux Enterprise 12 SP4, will be created during the upgrade process):
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>zypper repos | grep PTF
 2 | PTF                                                               | PTF                                      | No      | (r ) Yes  | Yes
ardana &gt; sudo zypper removerepo PTF
Removing repository 'PTF' ..............................................................................................[done]
Repository 'PTF' has been removed.</pre></div></li><li class="step "><p>
         Remove the Cloud media repository (if defined):
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>zypper repos | grep '[|] Cloud '
 1 | Cloud                          | SUSE OpenStack Cloud 8 DVD #1  | Yes     | (r ) Yes  | No
ardana &gt; sudo zypper removerepo Cloud
Removing repository 'SUSE OpenStack Cloud 8 DVD #1' ....................................................................[done]
Repository 'SUSE OpenStack Cloud 8 DVD #1' has been removed.</pre></div></li><li class="step "><p>
         Run the <code class="command">zypper migration</code> command, which should offer a single choice:
         namely, to upgrade to SUSE Linux Enterprise 12 SP4 and SUSE <span class="productname">OpenStack</span> Cloud 9 Cloud Lifecycle Manager. You need to accept the
         offered choice, then answer <code class="option">yes</code> to any prompts to disable obsoleted repositories.
         At that point, the <code class="command">zypper migration</code> command will run <code class="command">zypper dist-upgrade</code>,
         which will prompt you to agree with the proposed package changes. Finally,
         you will to agree with any new licenses. After this, the package upgrade
         of the deployer node will proceed. The output of the running <code class="command">zypper migration</code>
         should look something like the following:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo zypper migration

Executing 'zypper  refresh'

Repository 'SLES12-SP3-Pool' is up to date.
Repository 'SLES12-SP3-Updates' is up to date.
Repository 'SLES12-SP3-Pool' is up to date.
Repository 'SLES12-SP3-Updates' is up to date.
Repository 'SUSE-OpenStack-Cloud-8-Pool' is up to date.
Repository 'SUSE-OpenStack-Cloud-8-Updates' is up to date.
Repository 'OpenStack-Cloud-8-Pool' is up to date.
Repository 'OpenStack-Cloud-8-Updates' is up to date.
All repositories have been refreshed.

Executing 'zypper  --no-refresh patch-check --updatestack-only'

Loading repository data...
Reading installed packages...

0 patches needed (0 security patches)

Available migrations:

    1 | SUSE Linux Enterprise Server 12 SP4 x86_64
        SUSE OpenStack Cloud 9 x86_64


[num/q]: 1

Executing 'snapper create --type pre --cleanup-algorithm=number --print-number --userdata important=yes --description 'before online migration''

The config 'root' does not exist. Likely snapper is not configured.
See 'man snapper' for further instructions.
Upgrading product SUSE Linux Enterprise Server 12 SP4 x86_64.
Found obsolete repository SLES12-SP3-Updates
Disable obsolete repository SLES12-SP3-Updates [y/n] (y): y
... disabling.
Found obsolete repository SLES12-SP3-Pool
Disable obsolete repository SLES12-SP3-Pool [y/n] (y): y
... disabling.
Upgrading product SUSE OpenStack Cloud 9 x86_64.
Found obsolete repository OpenStack-Cloud-8-Pool
Disable obsolete repository OpenStack-Cloud-8-Pool [y/n] (y): y
... disabling.

Executing 'zypper --releasever 12.4 ref -f'

Warning: Enforced setting: $releasever=12.4
Forcing raw metadata refresh
Retrieving repository 'SLES12-SP4-Pool' metadata .......................................................................[done]
Forcing building of repository cache
Building repository 'SLES12-SP4-Pool' cache ............................................................................[done]
Forcing raw metadata refresh
Retrieving repository 'SLES12-SP4-Updates' metadata ....................................................................[done]
Forcing building of repository cache
Building repository 'SLES12-SP4-Updates' cache .........................................................................[done]
Forcing raw metadata refresh
Retrieving repository 'SUSE-OpenStack-Cloud-9-Pool' metadata ...........................................................[done]
Forcing building of repository cache
Building repository 'SUSE-OpenStack-Cloud-9-Pool' cache ................................................................[done]
Forcing raw metadata refresh
Retrieving repository 'SUSE-OpenStack-Cloud-9-Updates' metadata ........................................................[done]
Forcing building of repository cache
Building repository 'SUSE-OpenStack-Cloud-9-Updates' cache .............................................................[done]
Forcing raw metadata refresh
Retrieving repository 'OpenStack-Cloud-8-Updates' metadata .............................................................[done]
Forcing building of repository cache
Building repository 'OpenStack-Cloud-8-Updates' cache ..................................................................[done]
All repositories have been refreshed.

Executing 'zypper --releasever 12.4  --no-refresh  dist-upgrade --no-allow-vendor-change '

Warning: Enforced setting: $releasever=12.4
Warning: You are about to do a distribution upgrade with all enabled repositories. Make sure these repositories are compatible before you continue. See 'man zypper' for more information about this command.
Loading repository data...
Reading installed packages...
Computing distribution upgrade...

...

525 packages to upgrade, 14 to downgrade, 62 new, 5 to remove, 1 to change vendor, 1 to change arch.
Overall download size: 1.24 GiB. Already cached: 0 B. After the operation, additional 780.8 MiB will be used.
Continue? [y/n/...? shows all options] (y): y
...
    dracut: *** Generating early-microcode cpio image ***
    dracut: *** Constructing GenuineIntel.bin ****
    dracut: *** Store current command line parameters ***
    dracut: Stored kernel commandline:
    dracut:  rd.lvm.lv=ardana-vg/root
    dracut:  root=/dev/mapper/ardana--vg-root rootfstype=ext4 rootflags=rw,relatime,data=ordered
    dracut: *** Creating image file '/boot/initrd-4.4.180-94.127-default' ***
    dracut: *** Creating initramfs image file '/boot/initrd-4.4.180-94.127-default' done ***

Output of btrfsmaintenance-0.2-18.1.noarch.rpm %posttrans script:
    Refresh script btrfs-scrub.sh for monthly
    Refresh script btrfs-defrag.sh for none
    Refresh script btrfs-balance.sh for weekly
    Refresh script btrfs-trim.sh for none

There are some running programs that might use files deleted by recent upgrade. You may wish to check and restart some of them. Run 'zypper ps -s' to list these programs.</pre></div></li></ol></div></div><div class="procedure " id="id-1.5.17.6.12.8"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 15.7: </span><span class="name">Migrating a Deployer Node with Locally-Managed Repositories </span><a title="Permalink" class="permalink" href="#id-1.5.17.6.12.8">#</a></h6></div><div class="procedure-contents"><p>
       In this configuration, you need to manually migrate the system using
       <code class="command">zypper dist-upgrade</code>, according to the following steps:
     </p><ol class="procedure" type="1"><li class="step "><p>
         Disable any SUSE Linux Enterprise 12 SP3 or <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 8 Cloud Lifecycle Manager-related repositories.
         Leaving the SUSE Linux Enterprise 12 SP3 and/or SUSE <span class="productname">OpenStack</span> Cloud (or HOS) 8 Cloud Lifecycle Manager-related repositories
         enabled can lead to a minor disruption later during the upgrade procedure.
         For example, to disable any repositories served from the
         <code class="filename">/srv/www/suse-12.3</code> directory, or the SUSE-12-4
         alias under <code class="systemitem">http://localhost:79/</code>,  use the following commands:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>zypper repos --show-enabled-only --uri | grep -e dir:///srv/www/suse-12.3 -e http://localhost:79/SUSE-12-4/ | cut -d '|' -f 2
PTF
SLES12-SP3-LTSS-Updates
SLES12-SP3-Pool
SLES12-SP3-Updates
SUSE-OpenStack-Cloud-8-Pool
SUSE-OpenStack-Cloud-8-Updates
ardana &gt; for repo in $(zypper repos --show-enabled-only --uri | grep -e dir:///srv/www/suse-12.3 -e http://localhost:79/SUSE-12-4/ | cut -d '|' -f 2); do sudo zypper modifyrepo --disable "${repo}"; done
Repository 'PTF' has been successfully disabled.
Repository 'SLES12-SP3-LTSS-Updates' has been successfully disabled.
Repository 'SLES12-SP3-Pool' has been successfully disabled.
Repository 'SLES12-SP3-Updates' has been successfully disabled.
Repository 'SUSE-OpenStack-Cloud-8-Pool' has been successfully disabled.
Repository 'SUSE-OpenStack-Cloud-8-Updates' has been successfully disabled.</pre></div><div id="id-1.5.17.6.12.8.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
           The SLES12-SP3-LTSS-Updates repository should only be present if
           you have purchased the optional SUSE Linux Enterprise 12 SP3 LTSS support. Whether
           or not it is configured will not impact the upgrade process.
         </p></div></li><li class="step "><p>
         Remove the PTF repository, which is based on SUSE Linux Enterprise 12 SP3. A new one
         based on SUSE Linux Enterprise 12 SP4 will be created during the upgrade process.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>zypper repos | grep PTF
 2 | PTF                                                               | PTF                                      | Yes     | (r ) Yes  | Yes
ardana &gt; sudo zypper removerepo PTF
Removing repository 'PTF' ..............................................................................................[done]
Repository 'PTF' has been removed.</pre></div></li><li class="step "><p>
         Remove the Cloud media repository if defined.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>zypper repos | grep '[|] Cloud '
 1 | Cloud                          | SUSE OpenStack Cloud 8 DVD #1  | Yes     | (r ) Yes  | No
ardana &gt; sudo zypper removerepo Cloud
Removing repository 'SUSE OpenStack Cloud 8 DVD #1' ....................................................................[done]
Repository 'SUSE OpenStack Cloud 8 DVD #1' has been removed.</pre></div></li><li class="step "><p>
         Ensure the deployer node has access to the SUSE Linux Enterprise 12 SP4 and SUSE <span class="productname">OpenStack</span> Cloud 9 CLM
         repositories as documented in <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 17 “Software Repository Setup”</span>
         paying attention to the non-SMT based repository setup. When you run
         <code class="command">zypper repos --show-enabled-only</code>, the output should look similar to the following:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>zypper repos --show-enabled-only
#  | Alias                          | Name                           | Enabled | GPG Check | Refresh
---+--------------------------------+--------------------------------+---------+-----------+--------
 1 | Cloud                          | SUSE OpenStack Cloud 9 DVD #1  | Yes     | (r ) Yes  | No
 7 | SLES12-SP4-Pool                | SLES12-SP4-Pool                | Yes     | (r ) Yes  | No
 8 | SLES12-SP4-Updates             | SLES12-SP4-Updates             | Yes     | (r ) Yes  | Yes
 9 | SUSE-OpenStack-Cloud-9-Pool    | SUSE-OpenStack-Cloud-9-Pool    | Yes     | (r ) Yes  | No
10 | SUSE-OpenStack-Cloud-9-Updates | SUSE-OpenStack-Cloud-9-Updates | Yes     | (r ) Yes  | Yes</pre></div><div id="id-1.5.17.6.12.8.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
           The Cloud repository above is optional. Its content is equivalent to
           the SUSE-Openstack-Cloud-9-Pool repository.
         </p></div></li><li class="step "><p>
         Run the <code class="command">zypper dist-upgrade</code> command to upgrade the
         deployer node:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo zypper dist-upgrade

Warning: You are about to do a distribution upgrade with all enabled repositories. Make sure these repositories are compatible before you continue. See 'man zypper' for more information about this command.
Loading repository data...
Reading installed packages...
Computing distribution upgrade...

...

525 packages to upgrade, 14 to downgrade, 62 new, 5 to remove, 1 to change vendor, 1 to change arch.
Overall download size: 1.24 GiB. Already cached: 0 B. After the operation, additional 780.8 MiB will be used.
Continue? [y/n/...? shows all options] (y): y
...
    dracut: *** Generating early-microcode cpio image ***
    dracut: *** Constructing GenuineIntel.bin ****
    dracut: *** Store current command line parameters ***
    dracut: Stored kernel commandline:
    dracut:  rd.lvm.lv=ardana-vg/root
    dracut:  root=/dev/mapper/ardana--vg-root rootfstype=ext4 rootflags=rw,relatime,data=ordered
    dracut: *** Creating image file '/boot/initrd-4.4.180-94.127-default' ***
    dracut: *** Creating initramfs image file '/boot/initrd-4.4.180-94.127-default' done ***

Output of btrfsmaintenance-0.2-18.1.noarch.rpm %posttrans script:
    Refresh script btrfs-scrub.sh for monthly
    Refresh script btrfs-defrag.sh for none
    Refresh script btrfs-balance.sh for weekly
    Refresh script btrfs-trim.sh for none

There are some running programs that might use files deleted by recent upgrade. You may wish to check and restart some of them. Run 'zypper ps -s' to list these programs.</pre></div><div id="id-1.5.17.6.12.8.7.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
           You may need to run the <code class="command">zypper dist-upgrade</code> command more than
           once, if it determines that it needs to update the <code class="command">zypper</code>
           infrastructure on your system to be able to successfully
           <code class="literal">dist-upgrade</code> the node; the command will tell you if you need to run it again.
         </p></div></li></ol></div></div></div><div class="sect2" id="upgrade-deployer-node-config"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Upgrading the Deployer Node Configuration Settings</span> <a title="Permalink" class="permalink" href="#upgrade-deployer-node-config">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-upgrade_maintenance.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-upgrade_maintenance.xml</li><li><span class="ds-label">ID: </span>upgrade-deployer-node-config</li></ul></div></div></div></div><p>
      Now that the deployer node packages have been migrated to SUSE Linux Enterprise 12 SP4 and
      <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 Cloud Lifecycle Manager, we need to update the configuration
      settings to be <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 Cloud Lifecycle Manager based.
    </p><p>
      The first step is to run the <code class="command">ardana-init</code> command. This will:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          Add the PTF repository, creating it if needed.
        </p></li><li class="listitem "><p>
          Optionally add appropriate local repository references for any
          SMT-provided SUSE Linux Enterprise 12 SP4 and SUSE <span class="productname">OpenStack</span> Cloud 9 repositories.
        </p></li><li class="listitem "><p>
          Upgrade the deployer account <code class="literal">~/openstack</code> area to be
          based upon <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 Cloud Lifecycle Manager Ansible sources.
        </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
              This will import the new <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 Cloud Lifecycle Manager Ansible code into the Git
              repository on the Ardana branch, and then rebase the customer
              site branch on top of the updated Ardana branch.
            </p></li><li class="listitem "><p>
              Follow the directions to resolve any Git merge conflicts that may
              arise due to local changes that may have been made on the site branch:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ardana-init
...
 To continue installation copy your cloud layout to:
     /var/lib/ardana/openstack/my_cloud/definition

 Then execute the installation playbooks:
     cd /var/lib/ardana/openstack/ardana/ansible
     git add -A
     git commit -m 'My config'
     ansible-playbook -i hosts/localhost cobbler-deploy.yml
     ansible-playbook -i hosts/localhost bm-reimage.yml
     ansible-playbook -i hosts/localhost config-processor-run.yml
     ansible-playbook -i hosts/localhost ready-deployment.yml
     cd /var/lib/ardana/scratch/ansible/next/ardana/ansible
     ansible-playbook -i hosts/verb_hosts site.yml

 If you prefer to use the UI to install the product, you can
 do either of the following:
     - If you are running a browser on this machine, you can point
       your browser to http://localhost:9085 to start the install
       via the UI.
     - If you are running the browser on a remote machine, you will
       need to create an ssh tunnel to access the UI.  Please refer
       to the Ardana installation documentation for further details.</pre></div></li></ul></div></li></ul></div><div id="id-1.5.17.6.13.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
        As we are upgrading to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 Cloud Lifecycle Manager, we do not need
        to run the suggested <code class="filename">bm-reimage.yml</code> playbook.
      </p></div><div class="procedure " id="id-1.5.17.6.13.6"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 15.8: </span><span class="name">Updating the Bare-Metal Provisioning Configuration </span><a title="Permalink" class="permalink" href="#id-1.5.17.6.13.6">#</a></h6></div><div class="procedure-contents"><p>
        If you were previously using the <code class="literal">cobbler</code>-based integrated provisioning
        solution, then you will need to perform the following steps to import
        the SUSE Linux Enterprise 12 SP4 ISO and update the default provisioning distribution:
      </p><ol class="procedure" type="1"><li class="step "><p>
          Ensure there is a copy of the <code class="filename">SLE-12-SP4-Server-DVD-x86_64-GM-DVD1.iso</code>,
          named <code class="filename">sles12sp4.iso</code>, available in the <code class="filename">/var/lib/ardana</code> directory.
        </p></li><li class="step "><p>
          Ensure that any distribution entries in <code class="filename">servers.yml</code>
          (or whichever file holds the server node definitions) under
          <code class="filename">~/openstack/my_cloud/definition</code> are updated to
          specify <code class="literal">sles12sp4</code> if they are currently using <code class="literal">sles12sp3</code>.
        </p><div id="id-1.5.17.6.13.6.4.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
            The default distribution will now be <code class="literal">sles12sp4</code>, so if there are no
            specific distribution entries specified for the servers, then no change
            will be required.
          </p></div><p>
          If you have made any changes to the <code class="filename">~/openstack/my_cloud/definition</code>
          files, you will need to commit those changes, as follows:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition
 <code class="prompt user">ardana &gt; </code>git add -A
 <code class="prompt user">ardana &gt; </code>git commit -m "Update sles12sp3 distro entries to sles12sp4"</pre></div></li><li class="step "><p>
          Run the <code class="filename">cobbler-deploy.yml</code> playbook to import
          the SUSE Linux Enterprise 12 SP4 distribution as the new default distribution:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
 <code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.yml
 Enter the password that will be used to access provisioned nodes:
 confirm Enter the password that will be used to access provisioned nodes:

 PLAY [localhost] **************************************************************

 GATHERING FACTS ***************************************************************
 ok: [localhost]

 TASK: [pbstart.yml pb_start_playbook] *****************************************
 ok: [localhost] =&gt; {
     "msg": "Playbook started - cobbler-deploy.yml"
 }

 msg: Playbook started - cobbler-deploy.yml

 ...

 PLAY [localhost] **************************************************************

 TASK: [pbfinish.yml pb_finish_playbook] ***************************************
 ok: [localhost] =&gt; {
     "msg": "Playbook finished - cobbler-deploy.yml"
 }

 msg: Playbook finished - cobbler-deploy.yml

 PLAY RECAP ********************************************************************
 localhost                  : ok=92   changed=45   unreachable=0    failed=0</pre></div></li></ol></div></div><p>
      You are now ready to upgrade the input model to be compatible.
    </p><div class="procedure " id="id-1.5.17.6.13.8"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 15.9: </span><span class="name">Upgrading the Cloud Input Model </span><a title="Permalink" class="permalink" href="#id-1.5.17.6.13.8">#</a></h6></div><div class="procedure-contents"><p>
        At this point, there are some mandatory changes that will need to be made
        to the existing input model to permit the upgrade proceed. These mandatory
        changes represent:
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
            The removal of previously-deprecated service components;
          </p></li><li class="listitem "><p>
            The dropping of service components that are no longer supported;
          </p></li><li class="listitem "><p>
            That there can be only one untagged VLAN per network interface;
          </p></li><li class="listitem "><p>
            That there must be a <code class="literal">MANAGEMENT</code> network group.
          </p></li></ul></div><p>
        There are also some service components that have been made redundant
        and have no effect. These should be removed to quieten the associated
        <code class="filename">config-processor-run.yml</code> warnings.
      </p><p>
        For example, if you run the <code class="filename">configuration-processor-run.yml</code>
        playbook from the <code class="filename">~/openstack/ardana/ansible</code>
        directory before you made the necessary input model changes, you should
        see it fail with errors similar to those shown below—unless your input
        model doesn't deploy the problematic service component:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
 <code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
 Enter encryption key (press return for none):
 confirm Enter encryption key (press return for none):
 To change encryption key enter new key (press return for none):
 confirm To change encryption key enter new key (press return for none):

 PLAY [localhost] **************************************************************

 GATHERING FACTS ***************************************************************
 ok: [localhost]

 ...

             "################################################################################",
             "# The configuration processor failed.  ",
             "#   control-planes-2.0        WRN: cp:openstack-core: 'designate-pool-manager' has been deprecated and will be replaced by 'designate-worker'. The replacement component will be automatically deployed in a future release. You will then need to update the input model to remove this warning.",
             "",
             "#   control-planes-2.0        WRN: cp:openstack-core: 'manila-share' service component is deprecated. The 'manila-share' service component can be removed as manila share service will be deployed where manila-api is specified. This is not a deprecation for openstack-manila-share but just an entry deprecation in input model.",
             "",
             "#   control-planes-2.0        WRN: cp:openstack-core: 'designate-zone-manager' has been deprecated and will be replaced by 'designate-producer'. The replacement component will be automatically deployed in a future release. You will then need to update the input model to remove this warning.",
             "",
             "#   control-planes-2.0        WRN: cp:openstack-core: 'glance-registry' has been deprectated and is no longer deployed. Please update you input model to remove any 'glance-registry' service component specifications to remove this warning.",
             "",
             "#   control-planes-2.0        WRN: cp:mml: 'ceilometer-api' is no longer used by Ardana and will not be deployed. Please update your input model to remove this warning.",
             "",
             "#   control-planes-2.0        WRN: cp:sles-compute: 'neutron-lbaasv2-agent' has been deprecated and replaced by 'octavia' and will not be deployed in a future release. Please update your input model to remove this warning.",
             "",
             "#   control-planes-2.0        ERR: cp:common-service-components: Undefined component 'freezer-agent'",
             "#   control-planes-2.0        ERR: cp:openstack-core: Undefined component 'nova-console-auth'",
             "#   control-planes-2.0        ERR: cp:openstack-core: Undefined component 'heat-api-cloudwatch'",
             "#   control-planes-2.0        ERR: cp:mml: Undefined component 'freezer-api'",
             "################################################################################"
         ]
     }
 }

 TASK: [debug var=config_processor_result.stderr] ******************************
 ok: [localhost] =&gt; {
     "var": {
         "config_processor_result.stderr": "/usr/lib/python2.7/site-packages/ardana_configurationprocessor/cp/model/YamlConfigFile.py:95: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n  self._contents = yaml.load(''.join(lines))"
     }
 }

 TASK: [fail msg="Configuration processor run failed, see log output above for details"] ***
 failed: [localhost] =&gt; {"failed": true}
 msg: Configuration processor run failed, see log output above for details

 msg: Configuration processor run failed, see log output above for details

 FATAL: all hosts have already failed -- aborting

 PLAY RECAP ********************************************************************
            to retry, use: --limit @/var/lib/ardana/config-processor-run.retry

 localhost                  : ok=8    changed=5    unreachable=0    failed=1</pre></div><p>
        To resolve any errors and warnings like those shown above, you will
        need to perform the following actions:
      </p><ol class="procedure" type="1"><li class="step "><p>
          Remove any service component entries that are no longer valid from the
          <code class="filename">control_plane.yml</code> (or whichever file holds the
          control-plane definitions) under <code class="filename">~/openstack/my_cloud/definition</code>.
          This means that you have to comment out (or delete) any lines for the
          following service components, which are no longer available:
        </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
              <code class="literal">freezer-agent</code>
            </p></li><li class="listitem "><p>
              <code class="literal">freezer-api</code>
            </p></li><li class="listitem "><p>
              <code class="literal">heat-api-cloudwatch</code>
            </p></li><li class="listitem "><p>
              <code class="literal">nova-console-auth</code>
            </p></li></ul></div><div id="id-1.5.17.6.13.8.8.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
            This should resolve the errors that cause the <code class="filename">config-processor-run.yml</code>
            playbook to fail.
          </p></div></li><li class="step "><p>
          Similarly, remove any service components that are redundant and no
          longer required. This means that you should comment out (or delete)
          any lines for the following service components:
        </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
              <code class="literal">ceilometer-api</code>
            </p></li><li class="listitem "><p>
              <code class="literal">glance-registry</code>
            </p></li><li class="listitem "><p>
              <code class="literal">manila-share</code>
            </p></li><li class="listitem "><p>
              <code class="literal">neutron-lbaasv2-agent</code>
            </p></li></ul></div><div id="id-1.5.17.6.13.8.9.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
            This should resolve most of the warnings reported by the
            <code class="filename">config-processor-run.yml</code> playbook.
          </p></div><div id="id-1.5.17.6.13.8.9.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
            If you have deployed the <code class="systemitem">designate</code> service components
            (<code class="systemitem">designate-pool-manager</code> and <code class="systemitem">designate-zone-manager</code>) in your cloud,
            you will see warnings like those shown above, indicating that these
            service components have been deprecated.
          </p><p>
            You can switch to using the newer <code class="systemitem">designate-worker</code> and <code class="systemitem">designate-producer</code>
            service components, which will quieten these deprecation warnings
            produced by the <code class="filename">config-processor-run.yml</code> playbook run.
          </p><p>
            However, this is a procedure that should be perfomed after the
            upgrade has completed, as outlined in the <a class="xref" href="#post-upgrade-tasks" title="15.4.5. Post-Upgrade Tasks">Section 15.4.5, “Post-Upgrade Tasks”</a>
            section below.
          </p></div></li><li class="step "><p>
        Once you have made the necessary changes to your input model, if
        you run <code class="command">git diff</code> under the <code class="filename">~/openstack/my_cloud/definition</code>
        directory, you should see output similar to the following:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition
 <code class="prompt user">ardana &gt; </code>git diff
 diff --git a/my_cloud/definition/data/control_plane.yml b/my_cloud/definition/data/control_plane.yml
 index f7cfd84..2c1a73c 100644
 --- a/my_cloud/definition/data/control_plane.yml
 +++ b/my_cloud/definition/data/control_plane.yml
 @@ -32,7 +32,6 @@
          - NEUTRON-CONFIG-CP1
        common-service-components:
          - lifecycle-manager-target
 -        - freezer-agent
          - stunnel
          - monasca-agent
          - logging-rotate
 @@ -118,12 +117,10 @@
              - cinder-volume
              - cinder-backup
              - glance-api
 -            - glance-registry
              - nova-api
              - nova-placement-api
              - nova-scheduler
              - nova-conductor
 -            - nova-console-auth
              - nova-novncproxy
              - neutron-server
              - neutron-ml2-plugin
 @@ -137,7 +134,6 @@
              - horizon
              - heat-api
              - heat-api-cfn
 -            - heat-api-cloudwatch
              - heat-engine
              - ops-console-web
              - barbican-api
 @@ -151,7 +147,6 @@
              - magnum-api
              - magnum-conductor
              - manila-api
 -            - manila-share

          - name: mml
            cluster-prefix: mml
 @@ -164,9 +159,7 @@

              # freezer-api shares elastic-search with logging-server
              # so must be co-located with it
 -            - freezer-api

 -            - ceilometer-api
              - ceilometer-polling
              - ceilometer-agent-notification
              - ceilometer-common
 @@ -194,4 +187,3 @@
              - neutron-l3-agent
              - neutron-metadata-agent
              - neutron-openvswitch-agent
 -            - neutron-lbaasv2-agent</pre></div></li><li class="step "><p>
        If you are happy with these changes, commit them into
        the Git repository as follows:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/definition
 <code class="prompt user">ardana &gt; </code>git add -A
 <code class="prompt user">ardana &gt; </code>git commit -m "SOC 9 CLM Upgrade input model migration"</pre></div></li><li class="step "><p>
        Now you are ready to run the <code class="filename">config-processor-run.yml</code> playbook. If the
        necessary input model changes have been made, it will complete
        sucessfully:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
 <code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
 Enter encryption key (press return for none):
 confirm Enter encryption key (press return for none):
 To change encryption key enter new key (press return for none):
 confirm To change encryption key enter new key (press return for none):

 PLAY [localhost] **************************************************************

 GATHERING FACTS ***************************************************************
 ok: [localhost]

 ...
 PLAY RECAP ********************************************************************
 localhost                  : ok=24   changed=20   unreachable=0    failed=0</pre></div></li></ol></div></div></div><div class="sect2" id="upgrade-cloud-services"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Upgrading Cloud Services</span> <a title="Permalink" class="permalink" href="#upgrade-cloud-services">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-upgrade_maintenance.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-upgrade_maintenance.xml</li><li><span class="ds-label">ID: </span>upgrade-cloud-services</li></ul></div></div></div></div><p>
     The deployer node is now ready to be used to upgrade the remaining
     cloud nodes and running services.
   </p><div id="id-1.5.17.6.14.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
      If upgrading from Helion OpenStack 8, there is a manual file update that must
      be applied before continuing the upgrade process. In the file
      <code class="literal">/usr/share/ardana/ansible/roles/osconfig/tasks/check-product-status.yml</code>
      replace `command` with `shell` in the first ansible entry. The correct version
      of the file appears below.
     </p><div class="verbatim-wrap"><pre class="screen">- name: deployer-setup | check-product-status | Check HOS product installed
  shell: |-
    zypper info hpe-helion-openstack-release | grep "^Installed *: *Yes"
  ignore_errors: yes
  register: product_flavor_hos

- name: deployer-setup | check-product-status | Check SOC product availability
  become: yes
  zypper:
    name: "suse-openstack-cloud-release&gt;=8"
    state: present
  ignore_errors: yes
  register: product_flavor_soc

- name: deployer-setup | check-product-status | Provide help
  fail:
    msg: &gt;
      The deployer node does not have a Cloud Add-On product installed.
      In YaST select Software/Add-On Products to see an overview of installed
       add-on products and use "Add" to add the Cloud product.
  when:
    - product_flavor_soc|failed
    - product_flavor_hos|failed</pre></div><p>
      Changes to the <code class="literal">check-product-status.yml</code> file must be staged
      and committed via git.
   </p><div class="verbatim-wrap"><pre class="screen">git add -u
git commit -m "applying osconfig fix prior to HOS8 to SOC9 upgrade"</pre></div></div><div id="id-1.5.17.6.14.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
       The <code class="filename">ardana-upgrade.yml</code> playbook runs the upgrade process
       against all nodes in parallel, though some of the steps are serialised
       to run on only one node at a time to avoid triggering potentially
       problematic race conditions. As such, the playbook can take a long time to run.
     </p></div><div class="procedure " id="id-1.5.17.6.14.5"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 15.10: </span><span class="name">Generate the SUSE <span class="productname">OpenStack</span> Cloud 9 Cloud Lifecycle Manager Based Scratch Area </span><a title="Permalink" class="permalink" href="#id-1.5.17.6.14.5">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
        Generate the updated scratch area using the SUSE <span class="productname">OpenStack</span> Cloud 9 Cloud Lifecycle Manager Ansible sources:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml

PLAY [localhost] **************************************************************

GATHERING FACTS ***************************************************************
ok: [localhost]

...

PLAY RECAP ********************************************************************
localhost                  : ok=31   changed=16   unreachable=0    failed=0</pre></div></li><li class="step "><p>
         Confirm that there are no pending updates for the deployer node.
         This could happen if you are using an SMT to manage the repositories,
         and updates have been released through the official channels since
         the deployer node was migrated. To check for any pending Cloud Lifecycle Manager package
         updates, you can run the <code class="filename">ardana-update-pkgs.yml</code> playbook as follows:
       </p><div class="verbatim-wrap"><pre class="screen"> <code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-pkgs.yml --limit OPS-LM--first-member

PLAY [all] ********************************************************************

TASK: [setup] *****************************************************************
ok: [ardana-cp-dplyr-m1]

PLAY [localhost] **************************************************************

TASK: [pbstart.yml pb_start_playbook] *****************************************
ok: [localhost] =&gt; {
    "msg": "Playbook started - ardana-update-pkgs.yml"
}

...

TASK: [_ardana-update-status | Report update status] **************************
ok: [ardana-cp-dplyr-m1] =&gt; {
    "msg": "=====================================================================\nUpdate status for node ardana-cp-dplyr-m1:\n=====================================================================\nNo pending update actions on the ardana-cp-dplyr-m1 host\nwere collected or reset during this update run or persisted during\nprevious unsuccessful or incomplete update runs.\n\n====================================================================="
}

msg: =====================================================================
Update status for node ardana-cp-dplyr-m1:
=====================================================================
No pending update actions on the ardana-cp-dplyr-m1 host
were collected or reset during this update run or persisted during
previous unsuccessful or incomplete update runs.

=====================================================================

PLAY [localhost] **************************************************************

TASK: [pbfinish.yml pb_finish_playbook] ***************************************
ok: [localhost] =&gt; {
    "msg": "Playbook finished - ardana-update-pkgs.yml"
}

msg: Playbook finished - ardana-update-pkgs.yml

PLAY RECAP ********************************************************************
ardana-cp-dplyr-m1         : ok=98   changed=12   unreachable=0    failed=0
localhost                  : ok=6    changed=2    unreachable=0    failed=0</pre></div><div id="id-1.5.17.6.14.5.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
           If running the <code class="filename">ardana-update-pkgs.yml</code> playbook identifies that there
           were updates that needed to be installed on your deployer node, then you
           need to go back to running the <code class="command">ardana-init</code> command, followed by the
           <code class="filename">cobbler-deploy.yml</code> playbook, then the <code class="filename">config-processor-run.yml</code>
           playbook, and finally the <code class="filename">ready-deployment.yml</code> playbook, addressing
           any additional input model changes that may be needed. Then,
           repeat this step to check for any pending updates before
           continuing with the upgrade.
         </p></div></li><li class="step "><p>
         Double-check that there are no pending actions needed for the deployer node
         by running the <code class="filename">ardana-update-status.yml</code> playbook, as follows:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-status.yml --limit OPS-LM--first-member

PLAY [resources] **************************************************************

...

TASK: [_ardana-update-status | Report update status] **************************
ok: [ardana-cp-dplyr-m1] =&gt; {
    "msg": "=====================================================================\nUpdate status for node ardana-cp-dplyr-m1:\n=====================================================================\nNo pending update actions on the ardana-cp-dplyr-m1 host\nwere collected or reset during this update run or persisted during\nprevious unsuccessful or incomplete update runs.\n\n====================================================================="
}

msg: =====================================================================
Update status for node ardana-cp-dplyr-m1:
=====================================================================
No pending update actions on the ardana-cp-dplyr-m1 host
were collected or reset during this update run or persisted during
previous unsuccessful or incomplete update runs.

=====================================================================

PLAY RECAP ********************************************************************
ardana-cp-dplyr-m1         : ok=12   changed=0    unreachable=0    failed=0</pre></div></li><li class="step "><p>
         Having verified that there are no pending actions detected, it is
         safe to proceed with running the <code class="filename">ardana-upgrade.yml</code>
         playbook to upgrade the entire cloud:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-upgrade.yml
PLAY [all] ********************************************************************

...

TASK: [pbstart.yml pb_start_playbook] *****************************************
ok: [localhost] =&gt; {
    "msg": "Playbook started - ardana-upgrade.yml"
}

msg: Playbook started - ardana-upgrade.yml

...
...
...
...
...

TASK: [pbfinish.yml pb_finish_playbook] ***************************************
ok: [localhost] =&gt; {
    "msg": "Playbook finished - ardana-upgrade.yml"
}

msg: Playbook finished - ardana-upgrade.yml</pre></div></li></ol></div></div><p>
       The <code class="filename">ardana-upgrade.yml</code> playbook run will take a long time. The
       <code class="command">zypper dist-upgrade</code> phase is serialised across all of
       the nodes and usually takes between five and 10 minutes for each node. This
       is followed by the cloud service upgrade phase, which will take
       approximately the same amount of time as a full cloud deploy. During
       this time, the cloud should remain basically functional, though there
       may be brief interruptions to some services. However, it is recommended
       that any workload management tasks are avoided during this period.
     </p><div id="id-1.5.17.6.14.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
         Until the <code class="filename">ardana-upgrade.yml</code> playbook run has
         ompleted successfully, other playbooks such as the <code class="filename">ardana-status.yml</code>,
         may report status problems. This is because some services that are
         expected to be running may not be installed, enabled, or migrated yet.
       </p></div><p>
       The <code class="filename">ardana-upgrade.yml</code> playbook run may sometimes
       fail during the whole cloud upgrade phase, if a service (for example,
       the <code class="systemitem">monasca-thresh</code> service) is slow to restart. In such cases, it is
       safe to run the <code class="filename">ardana-upgrade.yml</code> playbook again,
       and in most cases it should continue past the stage that failed
       previously. However, if the same problem persists across multiple runs,
       contact your support team for assistance.
     </p><div id="id-1.5.17.6.14.9" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
         It is important to disable all SUSE Linux Enterprise 12 SP3 <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 8 Cloud Lifecycle Manager
         repositories before migrating the deployer to SUSE Linux Enterprise 12 SP4 <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
         9 Cloud Lifecycle Manager. If you did not do this, then the first time you
         run the <code class="filename">ardana-upgrade.yml</code> playbook, it may
         complain that there are pending updates for the deployer node.
         This will require you to repeat the earlier steps to upgrade the
          deployer node, starting with running the <code class="command">ardana-init</code>
          command. If this happens, repeat the steps as requested. Note that this
          does not represent a serious problem.
       </p></div><p>
     In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 Cloud Lifecycle Manager the LBaaS V2 legacy driver has been
     deprecated and removed. As part of the <code class="filename">ardana-upgrade.yml</code> playbook run,
     all existing LBaaS V2 load-balancers will be automatically migrated to being
     based on the Octavia Amphora provider. To enable creation of any new Octavia-
     based load-balancer instances, you need to ensure that an appropriate Amphora
     image is registered for use when creating instances, by following
     <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 43 “Configuring Load Balancer as a Service”</span>.
   </p><div id="id-1.5.17.6.14.11" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
       While running the <code class="filename">ardana-upgrade.yml</code> playbook, a point will be
       reached when the Neutron services are upgraded. As part of this upgrade,
       any existing LBaaS V2 load-balancer definitions will be migrated to
       Octavia Amphora-based load-balancer definitions.
     </p><p>
       After this migration of load-balancer definitions has completed,
       if a load-balancer failover is triggered, then the replacement load-
       balancer may fail to start, as an appropriate Octavia Amphora image
       for SUSE <span class="productname">OpenStack</span> Cloud 9 Cloud Lifecycle Manager will not yet be available.
     </p><p>
       However, once the Octavia Amphora image has been uploaded using the
       above instructions, then it will be possible to recover any failed
       load-balancers by re-triggering the failover: follow the instructions at
       <a class="link" href="https://docs.openstack.org/python-octaviaclient/latest/cli/index.html#loadbalancer-failover" target="_blank">https://docs.openstack.org/python-octaviaclient/latest/cli/index.html#loadbalancer-failover</a>.
     </p></div></div><div class="sect2" id="upgrade-reboot-nodes-kernel"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Rebooting the Nodes into the SUSE Linux Enterprise 12 SP4 Kernel</span> <a title="Permalink" class="permalink" href="#upgrade-reboot-nodes-kernel">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-upgrade_maintenance.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-upgrade_maintenance.xml</li><li><span class="ds-label">ID: </span>upgrade-reboot-nodes-kernel</li></ul></div></div></div></div><p>
     At this point, all of the cloud services have been upgraded, but
     the nodes are still running the SUSE Linux Enterprise 12 SP3 kernel. The final step in
     the upgrade workflow is to reboot all of the nodes in the
     cloud in a controlled fashion, to ensure that active services failover
     appropriately.
   </p><p>
     The recommended order for rebooting nodes is to start with the deployer.
     This requires special handling, since the Ansible-based automation cannot
     fully manage the reboot of the node that it is running on.
   </p><p>
     After that, we recommend rebooting the rest of the nodes in the control
     planes in a rolling-reboot fashion, ensuring that high-availability services
     remain available.
   </p><p>
     Finally, the compute nodes can be rebooted, either individually or
     in groups, as is appropriate to avoid interruptions to running workloads.
   </p><div id="id-1.5.17.6.15.6" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
       Do not reboot all your control plane nodes at the same time.
     </p></div><div class="procedure " id="id-1.5.17.6.15.7"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 15.11: </span><span class="name">Rebooting the Deployer Node </span><a title="Permalink" class="permalink" href="#id-1.5.17.6.15.7">#</a></h6></div><div class="procedure-contents"><p>
       The reboot of the deployer node requires additional steps, as the
       Ansible-based automation framework cannot fully automate the reboot of
       the node that runs the ansible-playbook commands.
     </p><ol class="procedure" type="1"><li class="step "><p>
         Run the <code class="filename">ardana-reboot.yml</code> playbook limited to the
         deployer node, either by name, or using the logical node identified
         <code class="literal">OPS-LM--first-member</code>, as follows:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit OPS-LM--first-member

PLAY [all] ********************************************************************

TASK: [setup] *****************************************************************
ok: [ardana-cp-dplyr-m1]

PLAY [localhost] **************************************************************

TASK: [pbstart.yml pb_start_playbook] *****************************************
ok: [localhost] =&gt; {
    "msg": "Playbook started - ardana-reboot.yml"
}

msg: Playbook started - ardana-reboot.yml

...

TASK: [ardana-reboot | Deployer node has to be rebooted manually] *************
failed: [ardana-cp-dplyr-m1] =&gt; {"failed": true}
msg: The deployer node needs to be rebooted manually. After reboot, resume by running the post-reboot playbook:
cd ~/scratch/ansible/next/ardana/ansible ansible-playbook -i hosts/verb_hosts _ardana-post-reboot.yml --limit ardana-cp-dplyr-m1

msg: The deployer node needs to be rebooted manually. After reboot, resume by running the post-reboot playbook:
cd ~/scratch/ansible/next/ardana/ansible ansible-playbook -i hosts/verb_hosts _ardana-post-reboot.yml --limit ardana-cp-dplyr-m1

FATAL: all hosts have already failed -- aborting

PLAY RECAP ********************************************************************
           to retry, use: --limit @/var/lib/ardana/ardana-reboot.retry

ardana-cp-dplyr-m1         : ok=8    changed=3    unreachable=0    failed=1
localhost                  : ok=7    changed=0    unreachable=0    failed=0</pre></div><p>
         The <code class="filename">ardana-reboot.yml</code> playbook will fail when run
         on a deployer node; this is expected. The reported failure message
         tells you what you need to do to complete the remaining steps of the
         reboot manually: namely, rebooting the node, then logging back in again
         to run the <code class="filename">_ardana-post-reboot.yml</code> playbook, to start any services
         that need to be running on the node.
       </p></li><li class="step "><p>
         Manually reboot the deployer node, for example with <code class="command">shutdown -r now</code>.
       </p></li><li class="step "><p>
         Once the deployer node has rebooted, you need to log in again and run
         the <code class="filename">_ardana-post-reboot.yml</code> playbook to complete
         the startup of any services that should be running on the deployer node, as follows:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook _ardana-post-reboot.yml --limit OPS-LM--first-member

PLAY [resources] **************************************************************

TASK: [Set pending_clm_update] ************************************************
skipping: [ardana-cp-dplyr-m1]

...

TASK: [pbfinish.yml pb_finish_playbook] ***************************************
ok: [localhost] =&gt; {
    "msg": "Playbook finished - ardana-status.yml"
}

msg: Playbook finished - ardana-status.yml

PLAY RECAP ********************************************************************
ardana-cp-dplyr-m1         : ok=26   changed=0    unreachable=0    failed=0
localhost                  : ok=19   changed=1    unreachable=0    failed=0</pre></div></li></ol></div></div><div class="procedure " id="id-1.5.17.6.15.8"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 15.12: </span><span class="name">Rebooting the Remaining Control Plane Nodes </span><a title="Permalink" class="permalink" href="#id-1.5.17.6.15.8">#</a></h6></div><div class="procedure-contents"><p>
       For the remaining nodes, you can use <code class="filename">ardana-reboot.yml</code>
       to fully automate the reboot process. However, it is recommended that you
       reboot the nodes in a rolling-reboot fashion, such that high-availability
       services continue to run without interruption. Similarly, to avoid
       interruption of service for any singleton services (such as the <code class="systemitem">cinder-volume</code>
       and <code class="systemitem">cinder-backup</code> services), they should be migrated off the intended
       node before it is rebooted, and then migrated back again afterwards.
     </p><ol class="procedure" type="1"><li class="step "><p>
         Use the <code class="command">ansible</code> command's <code class="option">--list-hosts</code> option to
         list the remaining nodes in the cloud that are neither the deployer
         nor a compute node:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible -i hosts/verb_hosts --list-hosts 'resources:!OPS-LM--first-member:!NOV-CMP'
    ardana-cp-dbmqsw-m1
    ardana-cp-dbmqsw-m2
    ardana-cp-dbmqsw-m3
    ardana-cp-osc-m1
    ardana-cp-osc-m2
    ardana-cp-mml-m1
    ardana-cp-mml-m2
    ardana-cp-mml-m3</pre></div></li><li class="step "><p>
         Use the following command to generate the set of <code class="command">ansible-playbook</code>
         commands that need to be run to reboot all the nodes sequentially:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>for node in $(ansible -i hosts/verb_hosts --list-hosts 'resources:!OPS-LM--first-member:!NOV-CMP'); do echo ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ${node} || break; done
ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-dbmqsw-m1
ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-dbmqsw-m2
ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-dbmqsw-m3
ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-osc-m1
ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-osc-m2
ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-mml-m1
ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-mml-m2
ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-mml-m3</pre></div><div id="id-1.5.17.6.15.8.4.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
          Do not reboot all your control-plane nodes at the same time.
        </p></div></li><li class="step "><p>
         To reboot a specific control-plane node, you can use the above
         <code class="command">ansible-playbook</code> commands as follows:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-mml-m3

PLAY [all] ********************************************************************

TASK: [setup] *****************************************************************
ok: [ardana-cp-mml-m3]

PLAY [localhost] **************************************************************

TASK: [pbstart.yml pb_start_playbook] *****************************************
ok: [localhost] =&gt; {
    "msg": "Playbook started - ardana-reboot.yml"
}

msg: Playbook started - ardana-reboot.yml



...

PLAY [localhost] **************************************************************

TASK: [pbfinish.yml pb_finish_playbook] ***************************************
ok: [localhost] =&gt; {
    "msg": "Playbook finished - ardana-status.yml"
}

msg: Playbook finished - ardana-status.yml

PLAY [localhost] **************************************************************

TASK: [pbfinish.yml pb_finish_playbook] ***************************************
ok: [localhost] =&gt; {
    "msg": "Playbook finished - ardana-reboot.yml"
}

msg: Playbook finished - ardana-reboot.yml

PLAY RECAP ********************************************************************
ardana-cp-mml-m3           : ok=389  changed=105  unreachable=0    failed=0
localhost                  : ok=27   changed=1    unreachable=0    failed=0</pre></div></li></ol></div></div><div id="id-1.5.17.6.15.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
       You can reboot more than one control-plane node at a time, but only if
       they are members of different control-plane clusters. For example, you
       could reboot one node out of each of the Openstack controller, database,
       swift, monitoring or logging clusters, so long as doing do only reboots
       one node out of each cluster at the same time.
     </p></div><p>
     When rebooting the first member of the control-plane cluster where
     monitoring services run, the <code class="systemitem">monasca-thresh</code> service can sometimes fail
     to start up in a timely fashion when the node is coming back up after
     being rebooted. This can cause <code class="filename">ardana-reboot.yml</code> to fail.
     See below for suggestions on how to handle this problem.
   </p><div class="procedure " id="id-1.5.17.6.15.11"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 15.13: </span><span class="name">Getting <code class="systemitem">monasca-thresh</code> Running After an <code class="filename">ardana-reboot.yml</code> Failure </span><a title="Permalink" class="permalink" href="#id-1.5.17.6.15.11">#</a></h6></div><div class="procedure-contents"><p>
         If the <code class="filename">ardana-reboot.yml</code> playbook failed because
         <code class="systemitem">monasca-thresh</code> didn't start up in a timely fashion
         after a reboot, you can retry starting the services on the node using
         the <code class="filename">_ardana-post-reboot.yml</code> playbook for the node.
         This is similar to the manual handling of the deployer reboot, since
         the node has already successfully rebooted onto the new kernel, and
         you just need to get the required services running again on the node.
       </p><p>
         It can sometimes take up to 15 minutes for the <code class="systemitem">monasca-thresh</code>
         service to successfully start in such cases.
       </p><ul class="procedure"><li class="step "><p>
         However, if the service still fails to start after that time, then
         you may need to force a restart of the <code class="systemitem">storm-nimbus</code> and
         <code class="systemitem">storm-supervisor</code> services on all nodes in the
         <code class="literal">MON-THR</code> node group, as follows:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible MON-THR -b -m shell -a "systemctl restart storm-nimbus"
ardana-cp-mml-m1 | success | rc=0 &gt;&gt;


ardana-cp-mml-m2 | success | rc=0 &gt;&gt;


ardana-cp-mml-m3 | success | rc=0 &gt;&gt;


ardana &gt; ansible MON-THR -b -m shell -a "systemctl restart storm-supervisor"
ardana-cp-mml-m1 | success | rc=0 &gt;&gt;


ardana-cp-mml-m2 | success | rc=0 &gt;&gt;


ardana-cp-mml-m3 | success | rc=0 &gt;&gt;


ardana &gt; ansible-playbook -i hosts/verb_hosts _ardana-post-reboot.yml --limit ardana-cp-mml-m1</pre></div></li></ul></div></div><p>
     If the <code class="literal">monasca-thresh</code> service still fails to start up,
     contact your support team.
   </p><p>
     To check which control plane nodes have not yet been rebooted onto
     the new kernel, you can use an Ansible command to run the command <code class="command">uname -r</code>
     on the target nodes, as follows:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible -i hosts/verb_hosts 'resources:!OPS-LM--first-member:!NOV-CMP' -m command -a 'uname -r'
ardana-cp-dbmqsw-m1 | success | rc=0 &gt;&gt;
4.12.14-95.57-default

ardana-cp-dbmqsw-m3 | success | rc=0 &gt;&gt;
4.12.14-95.57-default

ardana-cp-osc-m1 | success | rc=0 &gt;&gt;
4.12.14-95.57-default

ardana-cp-dbmqsw-m2 | success | rc=0 &gt;&gt;
4.12.14-95.57-default

ardana-cp-mml-m2 | success | rc=0 &gt;&gt;
4.12.14-95.57-default

ardana-cp-osc-m2 | success | rc=0 &gt;&gt;
4.12.14-95.57-default

ardana-cp-mml-m1 | success | rc=0 &gt;&gt;
4.12.14-95.57-default

ardana-cp-mml-m3 | success | rc=0 &gt;&gt;
4.12.14-95.57-default

ardana &gt; uname -r
4.12.14-95.57-default</pre></div><p>
     If any node's <code class="command">uname -r</code> value does not match the kernel
     that the deployer is running, you probably have not yet rebooted that node.
   </p><div class="procedure " id="id-1.5.17.6.15.16"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 15.14: </span><span class="name">Rebooting the Compute Nodes </span><a title="Permalink" class="permalink" href="#id-1.5.17.6.15.16">#</a></h6></div><div class="procedure-contents"><p>
       Finally, you need to reboot the compute nodes. Rebooting multiple
       compute nodes at the same time is possible, so long as doing so does
       not compromise the integrity of running workloads. We recommended
       that you migrate workloads off groups of compute nodes in a controlled fashion,
       enabling them to be rebooted together.
     </p><div id="id-1.5.17.6.15.16.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
         Do not reboot all of your compute nodes at the same time.
       </p></div><ol class="procedure" type="1"><li class="step "><p>
         To see all the compute nodes that are available to be rebooted, you
         can run the following command:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
    <code class="prompt user">ardana &gt; </code>ansible -i hosts/verb_hosts --list-hosts NOV-CMP
    ardana-cp-slcomp0001
    ardana-cp-slcomp0002
...
    ardana-cp-slcomp0080</pre></div></li><li class="step "><p>
         Reboot the compute nodes, individually or in groups, using the
         <code class="filename">ardana-reboot.yml</code> playbook as follows:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
    <code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reboot.yml --limit ardana-cp-slcomp0001,ardana-cp-slcomp0002

PLAY [all] ********************************************************************

TASK: [setup] *****************************************************************
ok: [ardana-cp-slcomp0001]
ok: [ardana-cp-slcomp0002]

PLAY [localhost] **************************************************************

TASK: [pbstart.yml pb_start_playbook] *****************************************
ok: [localhost] =&gt; {
    "msg": "Playbook started - ardana-reboot.yml"
}

msg: Playbook started - ardana-reboot.yml

...

PLAY [localhost] **************************************************************

TASK: [pbfinish.yml pb_finish_playbook] ***************************************
ok: [localhost] =&gt; {
    "msg": "Playbook finished - ardana-status.yml"
}

msg: Playbook finished - ardana-status.yml

PLAY [localhost] **************************************************************

TASK: [pbfinish.yml pb_finish_playbook] ***************************************
ok: [localhost] =&gt; {
    "msg": "Playbook finished - ardana-reboot.yml"
}

msg: Playbook finished - ardana-reboot.yml

PLAY RECAP ********************************************************************
ardana-cp-slcomp0001       : ok=120  changed=11   unreachable=0    failed=0
ardana-cp-slcomp0002       : ok=120  changed=11   unreachable=0    failed=0
localhost                  : ok=27   changed=1    unreachable=0    failed=0</pre></div></li></ol></div></div><div id="id-1.5.17.6.15.17" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
       You must ensure that there is sufficient unused workload capacity to
       host any migrated workload or Amphora instances that may be running on
       the targeted compute nodes.
     </p><p>
       When rebooting multiple compute nodes at the same time, consider
       manually migrating any running workloads and Amphora instances off the
       target nodes in advance, to avoid any potential risk of workload or
       service interruption.
     </p></div></div><div class="sect2" id="post-upgrade-tasks"><div class="titlepage"><div><div><h3 class="title"><span class="number">15.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Post-Upgrade Tasks</span> <a title="Permalink" class="permalink" href="#post-upgrade-tasks">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-upgrade_maintenance.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-upgrade_maintenance.xml</li><li><span class="ds-label">ID: </span>post-upgrade-tasks</li></ul></div></div></div></div><p>
     After the cloud has been upgraded to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 Cloud Lifecycle Manager,
     if <code class="systemitem">designate</code> was previously configured, then the deprecated service
     components, <code class="literal">designate-zone-manager</code> and
     <code class="literal">designate-pool-manager</code>, were being used.
   </p><p>
     They will continue to operate correctly under <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 Cloud Lifecycle Manager,
     but we recommend that you migrate to using the newer <code class="systemitem">designate-worker</code>
     <code class="systemitem">designate-producer</code> service components instead by
     following the procedure documented in <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 25 “DNS Service Installation Overview”, Section 25.4 “Migrate Zone/Pool to Worker/Producer after Upgrade”</span>.
   </p><div class="procedure " id="id-1.5.17.6.16.4"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 15.15: </span><span class="name">Cleanup Orphaned Packages </span><a title="Permalink" class="permalink" href="#id-1.5.17.6.16.4">#</a></h6></div><div class="procedure-contents"><ul class="procedure"><li class="step "><p>
         After migrating the deployer node, there are a small number of
         packages that were installed that are no longer required—such as
         the <code class="systemitem">ceilometer</code> and <code class="systemitem">freezer</code> <code class="literal">virtualenv</code> (venv) packages.
       </p><p>
         You can safely remove these packages with the following command:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>zypper packages --orphaned
Loading repository data...
Reading installed packages...
S | Repository | Name                             | Version                           | Arch
--+------------+----------------------------------+-----------------------------------+-------
i | @System    | python-flup                      | 1.0.3.dev_20110405-2.10.52        | noarch
i | @System    | python-happybase                 | 0.9-1.64                          | noarch
i | @System    | venv-openstack-ceilometer-x86_64 | 9.0.8~dev7-12.24.2                | noarch
i | @System    | venv-openstack-freezer-x86_64    | 5.0.0.0~xrc2~dev2-10.22.1         | noarch
ardana&gt; sudo zypper remove venv-openstack-ceilometer-x86_64 venv-openstack-freezer-x86_64
Loading repository data...
Reading installed packages...
Resolving package dependencies...

The following 2 packages are going to be REMOVED:
  venv-openstack-ceilometer-x86_64 venv-openstack-freezer-x86_64

2 packages to remove.
After the operation, 79.0 MiB will be freed.
Continue? [y/n/...? shows all options] (y): y
(1/2) Removing venv-openstack-ceilometer-x86_64-9.0.8~dev7-12.24.2.noarch ..................................................................[done]
Additional rpm output:
/usr/lib/python2.7/site-packages/ardana_packager/indexer.py:148: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  return yaml.load(f)


(2/2) Removing venv-openstack-freezer-x86_64-5.0.0.0~xrc2~dev2-10.22.1.noarch ..............................................................[done]
Additional rpm output:
/usr/lib/python2.7/site-packages/ardana_packager/indexer.py:148: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  return yaml.load(f)</pre></div></li></ul></div></div><div class="procedure " id="id-1.5.17.6.16.5"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 15.16: </span><span class="name">Delete freezer Containers from swift </span><a title="Permalink" class="permalink" href="#id-1.5.17.6.16.5">#</a></h6></div><div class="procedure-contents"><p>
       The freezer service has been deprecated and removed from <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 Cloud Lifecycle Manager,
       but the backups that the freezer service created before you upgraded
       will still be consuming space in your Swift Object store.
     </p><p>
       Therefore, once you have completed the upgrade successfully, you can
       safely delete the containers that freezer used to hold the database and
       ring backups, freeing up that space.
     </p><ul class="procedure"><li class="step "><p>
         Using the credentials in the <code class="filename">backup.osrc</code> file,
         found on the deployer node in the Ardana account's home directory,
         run the following commands:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>. ~/backup.osrc
<code class="prompt user">ardana &gt; </code>swift list
freezer_database_backups
freezer_rings_backups
ardana&gt; swift delete --all
freezer_database_backups/data/tar/ardana-cp-dbmqsw-m2-host_freezer_mysql_backup/1598505404/1_1598548599/segments/000000021
freezer_database_backups/data/tar/ardana-cp-dbmqsw-m2-host_freezer_mysql_backup/1598505404/2_1598605266/data1
...
freezer_database_backups/data/tar/ardana-cp-dbmqsw-m2-host_freezer_mysql_backup/1598505404/0_1598505404/segments/000000001
freezer_database_backups
freezer_rings_backups/metadata/tar/ardana-cp-dbmqsw-m1-host_freezer_swift_builder_dir_backup/1598548636/0_1598548636/metadata
...
freezer_rings_backups/data/tar/ardana-cp-dbmqsw-m1-host_freezer_swift_builder_dir_backup/1598548636/0_1598548636/data
freezer_rings_backups</pre></div></li></ul></div></div></div></div><div class="sect1" id="deploy-ptf"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cloud Lifecycle Manager Program Temporary Fix (PTF) Deployment</span> <a title="Permalink" class="permalink" href="#deploy-ptf">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-deploy-ptf.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-deploy-ptf.xml</li><li><span class="ds-label">ID: </span>deploy-ptf</li></ul></div></div></div></div><p>
  Occasionally, in order to fix a given issue, SUSE will provide a set of
  packages known as a Program Temporary Fix (PTF). Such a PTF is fully
  supported by SUSE until the Maintenance Update containing a permanent fix has
  been released via the regular Update repositories. Customers running PTF
  fixes will be notified through the related Service Request when a permanent
  patch for a PTF has been released.
 </p><p>
  Use the following steps to deploy a PTF:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    When SUSE has developed a PTF, you will receive a URL for that PTF. You
    should download the packages from the location provided by SUSE Support
    to a temporary location on the Cloud Lifecycle Manager. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>tmpdir=`mktemp -d`
<code class="prompt user">ardana &gt; </code>cd $tmpdir
<code class="prompt user">ardana &gt; </code>wget --no-directories --recursive --reject "index.html*"\
--user=<em class="replaceable ">USER_NAME</em> \
--ask-password \
--no-parent https://ptf.suse.com/54321aaaa...dddd12345/cloud8/042171/x86_64/20181030/</pre></div></li><li class="step "><p>
    Remove any old data from the PTF repository, such as a listing for a PTF
    repository from a migration or when previous product patches were
    installed.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo rm -rf /srv/www/suse-12.4/x86_64/repos/PTF/*</pre></div></li><li class="step "><p>
    Move packages from the temporary download location to the PTF repository
    directory on the CLM Server. This example is for a neutron PTF.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo mkdir -p /srv/www/suse-12.4/x86_64/repos/PTF/
<code class="prompt user">ardana &gt; </code>sudo mv $tmpdir/*
   /srv/www/suse-12.4/x86_64/repos/PTF/
<code class="prompt user">ardana &gt; </code>sudo chown --recursive root:root /srv/www/suse-12.4/x86_64/repos/PTF/*
<code class="prompt user">ardana &gt; </code>rmdir $tmpdir</pre></div></li><li class="step "><p>
    Create or update the repository metadata:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo /usr/local/sbin/createrepo-cloud-ptf
Spawning worker 0 with 2 pkgs
Workers Finished
Saving Primary metadata
Saving file lists metadata
Saving other metadata</pre></div></li><li class="step "><p>
    Refresh the PTF repository before installing package updates on the Cloud Lifecycle Manager
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo zypper refresh --force --repo PTF
Forcing raw metadata refresh
Retrieving repository 'PTF' metadata
..........................................[d
one]
Forcing building of repository cache
Building repository 'PTF' cache ..........................................[done]
Specified repositories have been refreshed.</pre></div></li><li class="step "><p>
    The PTF shows as available on the deployer.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo zypper se --repo PTF
Loading repository data...
Reading installed packages...

S | Name                          | Summary                                 | Type
--+-------------------------------+-----------------------------------------+--------
  | python-neutronclient          | Python API and CLI for OpenStack neutron | package
i | venv-openstack-neutron-x86_64 | Python virtualenv for OpenStack neutron | package</pre></div></li><li class="step "><p>
    Install the PTF venv packages on the Cloud Lifecycle Manager
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo zypper dup  --from PTF
Refreshing service
Loading repository data...
Reading installed packages...
Computing distribution upgrade...

The following package is going to be upgraded:
  venv-openstack-neutron-x86_64

The following package has no support information from its vendor:
  venv-openstack-neutron-x86_64

1 package to upgrade.
Overall download size: 64.2 MiB. Already cached: 0 B. After the operation, additional 6.9 KiB will be used.
Continue? [y/n/...? shows all options] (y): y
Retrieving package venv-openstack-neutron-x86_64-11.0.2-13.8.1.042171.0.PTF.102473.noarch ... (1/1),  64.2 MiB ( 64.6 MiB unpacked)
Retrieving: venv-openstack-neutron-x86_64-11.0.2-13.8.1.042171.0.PTF.102473.noarch.rpm ....[done]
Checking for file conflicts: ..............................................................[done]
(1/1) Installing: venv-openstack-neutron-x86_64-11.0.2-13.8.1.042171.0.PTF.102473.noarch ....[done]
Additional rpm output:
warning
warning: /var/cache/zypp/packages/PTF/noarch/venv-openstack-neutron-x86_64-11.0.2-13.8.1.042171.0.PTF.102473.noarch.rpm: Header V3 DSA/SHA1 Signature, key ID b37b98a9: NOKEY</pre></div></li><li class="step "><p>
    Validate the venv tarball has been installed into the deployment directory:(note:the packages file under that dir shows the registered tarballs that will be used for the services, which should align with the installed venv RPM)
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls -la /opt/ardana_packager/ardana-9/sles_venv/x86_64
total 898952
drwxr-xr-x 2 root root     4096 Oct 30 16:10 .
...
-rw-r--r-- 1 root root 67688160 Oct 30 12:44 neutron-20181030T124310Z.tgz &lt;&lt;&lt;
-rw-r--r-- 1 root root 64674087 Aug 14 16:14 nova-20180814T161306Z.tgz
-rw-r--r-- 1 root root 45378897 Aug 14 16:09 octavia-20180814T160839Z.tgz
-rw-r--r-- 1 root root     1879 Oct 30 16:10 packages
-rw-r--r-- 1 root root 27186008 Apr 26  2018 swift-20180426T230541Z.tgz</pre></div></li><li class="step "><p>
    Install the non-venv PTF packages on the Compute Node
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update-pkgs.yml --extra-vars '{"zypper_update_method": "update", "zypper_update_repositories": ["PTF"]}' --limit comp0001-mgmt</pre></div><p>
    When it has finished, you can see that the upgraded package
    has been installed on <code class="literal">comp0001-mgmt</code>.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo zypper se --detail python-neutronclient
Loading repository data...
Reading installed packages...

S | Name                 | Type     | Version                         | Arch   | Repository
--+----------------------+----------+---------------------------------+--------+--------------------------------------
i | python-neutronclient | package  | 6.5.1-4.361.042171.0.PTF.102473 | noarch | PTF
  | python-neutronclient | package  | 6.5.0-4.361                     | noarch | SUSE-OPENSTACK-CLOUD-x86_64-GM-DVD1</pre></div></li><li class="step "><p>
    Running the ardana update playbook will distribute the PTF venv packages to
    the cloud server. Then you can find them loaded in the virtual environment
    directory with the other venvs.
   </p><p>
    The Compute Node before running the update playbook:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls -la /opt/stack/venv
total 24
drwxr-xr-x  9 root root 4096 Jul 18 15:47 neutron-20180718T154642Z
drwxr-xr-x  9 root root 4096 Aug 14 16:13 neutron-20180814T161306Z
drwxr-xr-x 10 root root 4096 May 28 09:30 nova-20180528T092954Z
drwxr-xr-x 10 root root 4096 Aug 14 16:13 nova-20180814T161306Z</pre></div></li><li class="step "><p>
    Run the update.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-update.yml --limit comp0001-mgmt</pre></div><p>
    When it has finished, you can see that an additional virtual environment
    has been installed.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls -la /opt/stack/venv
total 28
drwxr-xr-x  9 root root 4096 Jul 18 15:47 neutron-20180718T154642Z
drwxr-xr-x  9 root root 4096 Aug 14 16:13 neutron-20180814T161306Z
drwxr-xr-x  9 root root 4096 Oct 30 12:43 neutron-20181030T124310Z &lt;&lt;&lt; New venv installed
drwxr-xr-x 10 root root 4096 May 28 09:30 nova-20180528T092954Z
drwxr-xr-x 10 root root 4096 Aug 14 16:13 nova-20180814T161306Z</pre></div></li><li class="step "><p>
    The PTF may also have <code class="literal">RPM</code> package updates in addition to
    venv updates. To complete the update, follow the instructions at <a class="xref" href="#perform-update" title="15.3.1. Performing the Update">Section 15.3.1, “Performing the Update”</a>
   </p></li></ol></div></div></div><div class="sect1" id="database-maintenance"><div class="titlepage"><div><div><h2 class="title"><span class="number">15.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Periodic OpenStack Maintenance Tasks</span> <a title="Permalink" class="permalink" href="#database-maintenance">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-database_maintenance.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-database_maintenance.xml</li><li><span class="ds-label">ID: </span>database-maintenance</li></ul></div></div></div></div><p>
    Heat-manage helps manage Heat specific database operations. The associated
    database should be periodically purged to save space. The following should
    be setup as a cron job on the servers where the heat service is running at
    <code class="literal">/etc/cron.weekly/local-cleanup-heat</code>
    with the following content:
  </p><div class="verbatim-wrap"><pre class="screen">  #!/bin/bash
  su heat -s /bin/bash -c "/usr/bin/heat-manage purge_deleted -g days 14" || :</pre></div><p>
     nova-manage db archive_deleted_rows command will move deleted rows
     from production tables to shadow tables. Including
     <code class="literal">--until-complete</code> will make the command run continuously
     until all deleted rows are archived. It is recommended to setup this task
     as <code class="literal">/etc/cron.weekly/local-cleanup-nova</code>
     on the servers where the nova service is running, with the
     following content:
  </p><div class="verbatim-wrap"><pre class="screen">  #!/bin/bash
  su nova -s /bin/bash -c "/usr/bin/nova-manage db archive_deleted_rows --until-complete" || :</pre></div></div></div><div class="chapter " id="manage-ops-console"><div class="titlepage"><div><div><h1 class="title"><span class="number">16 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Operations Console</span> <a title="Permalink" class="permalink" href="#manage-ops-console">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-managing-ops-console.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-managing-ops-console.xml</li><li><span class="ds-label">ID: </span>manage-ops-console</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#using-opsconsole"><span class="number">16.1 </span><span class="name">Using the Operations Console</span></a></span></dt><dt><span class="section"><a href="#opsconsole-alarm-definitions"><span class="number">16.2 </span><span class="name">Alarm Definition</span></a></span></dt><dt><span class="section"><a href="#opsconsole-alarm-explorer"><span class="number">16.3 </span><span class="name">Alarm Explorer</span></a></span></dt><dt><span class="section"><a href="#opsconsole-compute-hosts"><span class="number">16.4 </span><span class="name">Compute Hosts</span></a></span></dt><dt><span class="section"><a href="#opsconsole-compute-instances"><span class="number">16.5 </span><span class="name">Compute Instances</span></a></span></dt><dt><span class="section"><a href="#opsconsole-compute-alarm-summary"><span class="number">16.6 </span><span class="name">Compute Summary</span></a></span></dt><dt><span class="section"><a href="#opsconsole-idg-all-operations-opsconsole-en-logging-xml-1"><span class="number">16.7 </span><span class="name">Logging</span></a></span></dt><dt><span class="section"><a href="#opsconsole-my-dashboard-overview"><span class="number">16.8 </span><span class="name">My Dashboard</span></a></span></dt><dt><span class="section"><a href="#opsconsole-networking-alarm-summary"><span class="number">16.9 </span><span class="name">Networking Alarm Summary</span></a></span></dt><dt><span class="section"><a href="#opsconsole-dashboard-overview"><span class="number">16.10 </span><span class="name">Central Dashboard</span></a></span></dt></dl></div></div><p>
   Often referred to as the Ops Console, you can use this web-based graphical
   user interface (GUI) to view data about your cloud infrastructure and ensure
   your cloud is operating correctly.
  </p><div class="sect1" id="using-opsconsole"><div class="titlepage"><div><div><h2 class="title"><span class="number">16.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Operations Console</span> <a title="Permalink" class="permalink" href="#using-opsconsole">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-using.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-using.xml</li><li><span class="ds-label">ID: </span>using-opsconsole</li></ul></div></div></div></div><div class="sect2" id="opsconsole-overview"><div class="titlepage"><div><div><h3 class="title"><span class="number">16.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Operations Console Overview</span> <a title="Permalink" class="permalink" href="#opsconsole-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-overview.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-overview.xml</li><li><span class="ds-label">ID: </span>opsconsole-overview</li></ul></div></div></div></div><p>
  Often referred to as the Ops Console, you can use this web-based graphical
  user interface (GUI) to view data about your cloud infrastructure and ensure
  your cloud is operating correctly.
 </p><p>
  You can use the Operations Console for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 to view
  data about your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> infrastructure in a web-based graphical user
  interface (GUI) and ensure your cloud is operating correctly. By logging on
  to the console, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> administrators can manage data in the following
  ways: <span class="bold"><strong>Triage alarm notifications</strong></span>.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Alarm Definitions and notifications now have their own screens and are
    collected under the <span class="bold"><strong>Alarm Explorer</strong></span> menu
    item which can be accessed from the Central Dashboard. Central Dashboard
    now allows you to customize the view in the following ways:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Rename or re-configure existing alarm cards to include services
      different from the defaults
     </p></li><li class="listitem "><p>
      Create a new alarm card with the services you want to select
     </p></li><li class="listitem "><p>
      Reorder alarm cards using drag and drop
     </p></li><li class="listitem "><p>
      View all alarms that have no service dimension now grouped in an
      <span class="bold"><strong>Uncategorized Alarms</strong></span> card
     </p></li><li class="listitem "><p>
      View all alarms that have a service dimension that does not match any of
      the other cards -now grouped in an <span class="bold"><strong>Other
      Alarms</strong></span> card
     </p></li></ul></div></li><li class="listitem "><p>
    You can also easily access alarm data for a specific component. On the
    Summary page for the following components, a link is provided to an alarms
    screen specifically for that component:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Compute Instances: <a class="xref" href="#opsconsole-manage-compute" title="16.1.3. Managing Compute Hosts">Section 16.1.3, “Managing Compute Hosts”</a>
     </p></li><li class="listitem "><p>
      Object Storage: <a class="xref" href="#opsconsole-swift-alarmsumm" title="16.1.4.4. Alarm Summary">Section 16.1.4.4, “Alarm Summary”</a>
     </p></li></ul></div></li></ul></div><div class="sect3" id="opsconsole-monitor-env"><div class="titlepage"><div><div><h4 class="title"><span class="number">16.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitor the environment by giving priority to alarms that take precedence.</span> <a title="Permalink" class="permalink" href="#opsconsole-monitor-env">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-overview.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-overview.xml</li><li><span class="ds-label">ID: </span>opsconsole-monitor-env</li></ul></div></div></div></div><p>
   Alarm Explorer now allows you to manage alarms in the following ways:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Refine the monitoring environment by creating new alarms to specify a
     combination of metrics, services, and hosts that match the triggers unique
     to an environment
    </p></li><li class="listitem "><p>
     Filter alarms in one place using an enumerated filter box instead of
     service badges
    </p></li><li class="listitem "><p>
     Specify full alarm IDs as dimension key-value pairs in the form of
     <span class="bold"><strong>dimension=value</strong></span>
    </p></li></ul></div></div><div class="sect3" id="opsconsole-support-changes"><div class="titlepage"><div><div><h4 class="title"><span class="number">16.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Support Changes</span> <a title="Permalink" class="permalink" href="#opsconsole-support-changes">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-overview.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-overview.xml</li><li><span class="ds-label">ID: </span>opsconsole-support-changes</li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     To resolve scalability issues, plain text search through alarm sets is no
     longer supported
    </p></li></ul></div><p>
   The Business Logic Layer of Operations Console is a middleware component that
   serves as a single point of contact for the user interface to communicate
   with OpenStack services such as monasca, nova, and others.
  </p></div></div><div class="sect2" id="opsconsole-connect"><div class="titlepage"><div><div><h3 class="title"><span class="number">16.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Connecting to the Operations Console</span> <a title="Permalink" class="permalink" href="#opsconsole-connect">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-connect.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-connect.xml</li><li><span class="ds-label">ID: </span>opsconsole-connect</li></ul></div></div></div></div><p>
  Instructions for accessing the Operations Console through a web browser.
 </p><p>
  To connect to Operations Console, perform the following:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Ensure your login has the required access credentials:
    <a class="xref" href="#opsconsole-access-creds" title="16.1.2.1. Required Access Credentials">Section 16.1.2.1, “Required Access Credentials”</a>
   </p></li><li class="listitem "><p>
    Connect through a browser: <a class="xref" href="#opsconsole-command" title="16.1.2.2. Connect Through a Browser">Section 16.1.2.2, “Connect Through a Browser”</a>
   </p></li><li class="listitem "><p>
    Optionally use a Hostname OR virtual IP address to access Operations Console:
    <a class="xref" href="#opsconsole-find-nameIP" title="16.1.2.3. Optionally use a Hostname OR virtual IP address to access Operations Console">Section 16.1.2.3, “Optionally use a Hostname OR virtual IP address to access Operations Console”</a>
   </p></li></ul></div><p>
  Operations Console will always be accessed over port 9095.
 </p><div class="sect3" id="opsconsole-access-creds"><div class="titlepage"><div><div><h4 class="title"><span class="number">16.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Required Access Credentials</span> <a title="Permalink" class="permalink" href="#opsconsole-access-creds">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-connect.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-connect.xml</li><li><span class="ds-label">ID: </span>opsconsole-access-creds</li></ul></div></div></div></div><p>
   In previous versions of Operations Console you were required to have only the
   password for the Administrator account (admin by default). Now the
   Administrator user account must also have all of the following credentials:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /></colgroup><thead><tr><th>Project</th><th>Domain</th><th>Role</th><th>Description</th></tr></thead><tbody><tr><td>*All projects*</td><td>*not specific*</td><td>Admin</td><td>Admin role on at least one project</td></tr><tr><td>*All projects*</td><td>*not specific*</td><td>Admin</td><td>Admin role in default domain</td></tr><tr><td>Admin</td><td>default</td><td>Admin or monasca-user</td><td>Admin or monasca-user role on admin project</td></tr></tbody></table></div><div id="id-1.5.18.3.3.6.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    If your login account has administrator role on the administrator project,
    then you only need to make sure you have the administrator role on the
    default domain.
   </p></div><p>
   <span class="bold"><strong>Administrator account</strong></span>
  </p><p>
   During installation, an administrator account called
   <code class="systemitem">admin</code> is created by default.
  </p><p>
   <span class="bold"><strong>Administrator password</strong></span>
  </p><p>
   During installation, an administrator password is randomly created by
   default. It is not recommend that you change the default password.
  </p><p>
   To find the randomized password:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To display the password, log on to the Cloud Lifecycle Manager and run:
    </p><div class="verbatim-wrap"><pre class="screen">cat ~/service.osrc</pre></div></li></ol></div></div><div class="sect3" id="opsconsole-command"><div class="titlepage"><div><div><h4 class="title"><span class="number">16.1.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Connect Through a Browser</span> <a title="Permalink" class="permalink" href="#opsconsole-command">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-connect.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-connect.xml</li><li><span class="ds-label">ID: </span>opsconsole-command</li></ul></div></div></div></div><p>
   The following instructions will show you how to find the URL to access
   Operations Console. You will use SSH, also known as Secure Socket Shell, which provides
   administrators with a secure way to access a remote computer.
  </p><p>
   <span class="bold"><strong>To access Operations Console:</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="listitem "><p>
     Locate the URL or IP address for the Operations Console with the following
     command:
    </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc &amp;&amp; openstack endpoint list | grep opsconsole | grep admin</pre></div><p>
     <span class="bold"><strong>Sample output:</strong></span>
    </p><div class="verbatim-wrap"><pre class="screen">| 8ef10dd9c00e4abdb18b5b22adc93e87 | region1 | opsconsole | opsconsole | True | admin | https://192.168.24.169:9095/api/v1/</pre></div><p>
     To access Operations Console, in the sample output, remove everything after port
     9095 (api/v1/) and in a browser, type:
    </p><div class="verbatim-wrap"><pre class="screen">https://192.168.24.169:9095</pre></div></li></ol></div></div><div class="sect3" id="opsconsole-find-nameIP"><div class="titlepage"><div><div><h4 class="title"><span class="number">16.1.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Optionally use a Hostname OR virtual IP address to access Operations Console</span> <a title="Permalink" class="permalink" href="#opsconsole-find-nameIP">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-connect.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-connect.xml</li><li><span class="ds-label">ID: </span>opsconsole-find-nameIP</li></ul></div></div></div></div><div id="id-1.5.18.3.3.8.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    If you can access Operations Console using the above instructions, then you can
    skip this section. These steps provide an alternate way to access Operations Console
    if the above steps do not work for you.
   </p></div><p>
   To find your hostname OR IP address:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Navigate to and open in a text editor the following file:
    </p><div class="verbatim-wrap"><pre class="screen">network_groups.yml</pre></div></li><li class="listitem "><p>
     Find the following entry:
    </p><div class="verbatim-wrap"><pre class="screen">external-name</pre></div></li><li class="listitem "><p>
     If your administrator set a hostname value in the external-name field, you
     will use that hostname when logging in to Operations Console. or example, in a
     browser you would type:
    </p><div class="verbatim-wrap"><pre class="screen">https://VIP:9095</pre></div></li><li class="listitem "><p>
     If your administrator did not set a hostname value, then to determine the
     IP address to use, from your Cloud Lifecycle Manager, run:
    </p><div class="verbatim-wrap"><pre class="screen">grep HZN-WEB /etc/hosts</pre></div><p>
     The output of that command will show you the virtual IP address you should
     use. For example, in a browser you would type:
    </p><div class="verbatim-wrap"><pre class="screen">https://VIP:9095</pre></div></li></ol></div></div></div><div class="sect2" id="opsconsole-manage-compute"><div class="titlepage"><div><div><h3 class="title"><span class="number">16.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Compute Hosts</span> <a title="Permalink" class="permalink" href="#opsconsole-manage-compute">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-manage-compute-instances.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-manage-compute-instances.xml</li><li><span class="ds-label">ID: </span>opsconsole-manage-compute</li></ul></div></div></div></div><p>
  Operations Console (Ops Console) provides a graphical interface for you to
  add and delete compute hosts.
 </p><p>
  As your deployment grows and changes, you may need to add more compute hosts
  to increase your capacity for VMs, or delete a host to reallocate hardware
  for a different use. To accomplish these tasks, in previous versions of
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> you had to use the command line to update configuration files and
  run ansible playbooks. Now Operations Console provides a
  graphical interface for you to complete the same tasks quickly using menu
  items in the console.
 </p><div id="id-1.5.18.3.4.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   Do not refresh the Operations Console page or open Operations Console in another window
   during the following tasks. If you do, you will not see any notifications or
   be able to review the error log for more information. This would make
   troubleshooting difficult since you would not know the error that was
   encountered, or why it occurred.
  </p></div><p>
  Use Operations Console to perform the following tasks:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Create a Compute Host: <a class="xref" href="#opsconsole-create-hosts" title="16.1.3.1. Create a Compute Host">Section 16.1.3.1, “Create a Compute Host”</a>
   </p></li><li class="listitem "><p>
    Deactivate a Compute Host: <a class="xref" href="#opsconsole-deactivate" title="16.1.3.2. Deactivate a Compute Host">Section 16.1.3.2, “Deactivate a Compute Host”</a>
   </p></li><li class="listitem "><p>
    Activate a Compute Host: <a class="xref" href="#opsconsole-activate" title="16.1.3.3. Activate a Compute Host">Section 16.1.3.3, “Activate a Compute Host”</a>
   </p></li><li class="listitem "><p>
    Delete a Compute Host: <a class="xref" href="#opsconsole-delete" title="16.1.3.4. Delete a Compute Host">Section 16.1.3.4, “Delete a Compute Host”</a>
   </p></li></ul></div><div id="id-1.5.18.3.4.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   To use Operations Console, you need to have the correct permissions and
   know the URL or VIP connected to Operations Console during installation.
  </p></div><div class="sect3" id="opsconsole-create-hosts"><div class="titlepage"><div><div><h4 class="title"><span class="number">16.1.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create a Compute Host</span> <a title="Permalink" class="permalink" href="#opsconsole-create-hosts">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-manage-compute-instances.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-manage-compute-instances.xml</li><li><span class="ds-label">ID: </span>opsconsole-create-hosts</li></ul></div></div></div></div><p>
   If you need to create additional compute hosts for more virtual machine
   capacity, you can do this easily on the Compute Hosts screen.
  </p><p>
   To add a compute host:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To open Operations Console, in a browser, enter either the URL or Virtual
     IP connected to Operations Console.
    </p><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">https://myardana.test:9095
https://VIP:9095</pre></div></li><li class="listitem "><p>
     On the <span class="bold"><strong>Home</strong></span> screen, click the menu
     represented by 3 horizontal lines (<span class="inlinemediaobject"><img xmlns="" src="images/media-opsconsole-OpsConsoleBurgerMenu.png" width="" alt="Three-Line Icon" /></span>).
    </p></li><li class="listitem "><p>
     From the menu that slides in on the left side, click
     <span class="bold"><strong>Compute</strong></span>, and
     then <span class="bold"><strong>Compute Hosts</strong></span>.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Compute Hosts</strong></span> page, click
     <span class="bold"><strong>Create Host</strong></span>.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Add &amp; Activate Compute Host</strong></span>
     tab that slides in from the right, enter the following information:
    </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.18.3.4.8.4.5.2.1"><span class="term ">Host ID</span></dt><dd><p>
        Cloud Lifecycle Manager model's server ID
       </p></dd><dt id="id-1.5.18.3.4.8.4.5.2.2"><span class="term ">Host Role</span></dt><dd><p>
        Defined in the Cloud Lifecycle Manager model and cannot be modified in Operations Console
       </p></dd><dt id="id-1.5.18.3.4.8.4.5.2.3"><span class="term ">Host Group</span></dt><dd><p>
        Defined in the Cloud Lifecycle Manager model and cannot be modified in Operations Console
       </p></dd><dt id="id-1.5.18.3.4.8.4.5.2.4"><span class="term ">Host NIC Mapping</span></dt><dd><p>
        Defined in the Cloud Lifecycle Manager model and cannot be modified in Operations Console
       </p></dd><dt id="id-1.5.18.3.4.8.4.5.2.5"><span class="term ">Encryption Key</span></dt><dd><p>
        If the configuration is encrypted, enter the encryption key here
       </p></dd></dl></div></li><li class="listitem "><p>
     Click <span class="guimenu ">Create Host</span>, and in the
     confirmation screen that opens, click
     <span class="guimenu ">Confirm</span>.
    </p></li><li class="listitem "><p>
     Wait for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to complete the pre deployment steps. This process can
     take up to 2 minutes.
    </p></li><li class="listitem "><p>
     If pre-deployment is successful, you will see a notification that
     deployment has started.
    </p><div id="id-1.5.18.3.4.8.4.8.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      If you receive a notice that pre-deployment did not complete
      successfully, read the notification explaining at which step the error
      occured. You can click on the error notification and see the ansible log
      for the configuration processor playbook. Then you can click
      <span class="bold"><strong>Create Host</strong></span>
      in step 4 again and correct the mistake.
     </p></div></li><li class="listitem "><p>
     Wait for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to complete the deployments steps. This process can take
     up to 20 minutes.
    </p></li><li class="listitem "><p>
     If deployment is successful, you will see a notification and a new entry
     will appear in the compute hosts table.
    </p><div id="id-1.5.18.3.4.8.4.10.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      If you receive a notice that deployment did not complete successfully,
      read the notification explaining at which step the error occured. You can
      click on the error notification for more details.
     </p></div></li></ol></div></div><div class="sect3" id="opsconsole-deactivate"><div class="titlepage"><div><div><h4 class="title"><span class="number">16.1.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deactivate a Compute Host</span> <a title="Permalink" class="permalink" href="#opsconsole-deactivate">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-manage-compute-instances.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-manage-compute-instances.xml</li><li><span class="ds-label">ID: </span>opsconsole-deactivate</li></ul></div></div></div></div><p>
   If you have multiple compute hosts and for debugging reasons you want to
   disable them all except one, you may need to deactivate and then activate a
   compute host. If you want to delete a host, you will also have to deactivate
   it first. This can be done easily in the Operations Console.
  </p><div id="id-1.5.18.3.4.9.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    The host must be in the following state:
    <span class="bold"><strong>ACTIVATED</strong></span>
   </p></div><p>
   To deactivate a compute host:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To open Operations Console, in a browser, enter either the URL or Virtual
     IP connected to Operations Console.
    </p><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">https://myardana.test:9095
https://VIP:9095</pre></div></li><li class="listitem "><p>
     On the <span class="bold"><strong>Home</strong></span> screen, click the menu
     represented by 3 horizontal lines (<span class="inlinemediaobject"><img xmlns="" src="images/media-opsconsole-OpsConsoleBurgerMenu.png" width="" alt="Three-Line Icon" /></span>).
    </p></li><li class="listitem "><p>
     From the menu that slides in on the left side, click
     <span class="bold"><strong>Compute</strong></span>, and
     then <span class="bold"><strong>Compute Hosts</strong></span>.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Compute Hosts</strong></span> page, in the row for
     the host you want to deactivate, click the details button
     (<span class="inlinemediaobject"><img xmlns="" src="images/media-opsconsole-DetailDots.png" width="" alt="Ellipsis Icon" /></span>).
    </p></li><li class="listitem "><p>
     Click <span class="bold"><strong>Deactivate</strong></span>, and in the confirmation
     screen that opens, click <span class="bold"><strong>Confirm</strong></span>.
    </p></li><li class="listitem "><p>
     Wait for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to complete the operation. This process can take up to 2
     minutes.
    </p></li><li class="listitem "><p>
     If deactivation is successful, you will see a notification and in the
     compute hosts table the <span class="bold"><strong>STATE</strong></span> will change
     to <span class="bold"><strong>DEACTIVATED</strong></span>.
    </p><div id="id-1.5.18.3.4.9.5.7.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      If you receive a notice that the operation did not complete successfully,
      read the notification explaining at which step the error occured. You can
      click on the link in the error notification for more details. In the
      compute hosts table the <span class="bold"><strong>STATE</strong></span> will
      remain
      <span class="bold"><strong>ACTIVATED</strong></span>.
     </p></div></li></ol></div></div><div class="sect3" id="opsconsole-activate"><div class="titlepage"><div><div><h4 class="title"><span class="number">16.1.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Activate a Compute Host</span> <a title="Permalink" class="permalink" href="#opsconsole-activate">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-manage-compute-instances.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-manage-compute-instances.xml</li><li><span class="ds-label">ID: </span>opsconsole-activate</li></ul></div></div></div></div><div id="id-1.5.18.3.4.10.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    The host must be in the following state:
    <span class="bold"><strong>DEACTIVATED</strong></span>
   </p></div><p>
   To activate a compute host:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To open Operations Console, in a browser, enter either the URL or Virtual
     IP connected to Operations Console.
    </p><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">https://myardana.test:9095
https://VIP:9095</pre></div></li><li class="listitem "><p>
     On the <span class="bold"><strong>Home</strong></span> screen, click the menu
     represented by 3 horizontal lines (<span class="inlinemediaobject"><img xmlns="" src="images/media-opsconsole-OpsConsoleBurgerMenu.png" width="" alt="Three-Line Icon" /></span>).
    </p></li><li class="listitem "><p>
     From the menu that slides in on the left side, click
     <span class="bold"><strong>Compute</strong></span>, and
     then <span class="bold"><strong>Compute Hosts</strong></span>.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Compute Hosts</strong></span> page, in the row for
     the host you want to activate, click the details button
     (<span class="inlinemediaobject"><img xmlns="" src="images/media-opsconsole-DetailDots.png" width="" alt="Ellipsis Icon" /></span>).
    </p></li><li class="listitem "><p>
     Click <span class="bold"><strong>Activate</strong></span>, and in the confirmation
     screen that opens, click <span class="bold"><strong>Confirm</strong></span>.
    </p></li><li class="listitem "><p>
     Wait for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to complete the operation. This process can take up to 2
     minutes.
    </p></li><li class="listitem "><p>
     If activation is successful, you will see a notification and in the
     compute hosts table the <span class="bold"><strong>STATE</strong></span> will change
     to <span class="bold"><strong>ACTIVATED</strong></span>.
    </p><div id="id-1.5.18.3.4.10.4.7.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      If you receive a notice that the operation did not complete successfully,
      read the notification explaining at which step the error occured. You can
      click on the link in the error notification for more details. In the
      compute hosts table the <span class="bold"><strong>STATE</strong></span> will
      remain
      <span class="bold"><strong>DEACTIVATED</strong></span>.
     </p></div></li></ol></div></div><div class="sect3" id="opsconsole-delete"><div class="titlepage"><div><div><h4 class="title"><span class="number">16.1.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Delete a Compute Host</span> <a title="Permalink" class="permalink" href="#opsconsole-delete">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-manage-compute-instances.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-manage-compute-instances.xml</li><li><span class="ds-label">ID: </span>opsconsole-delete</li></ul></div></div></div></div><p>
   If you need to scale down the size of your current deployment to use the
   hardware for other purposes, you may want to delete a compute host.
  </p><div id="id-1.5.18.3.4.11.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    Complete the following steps before deleting a host:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      host must be in the following state:
      <span class="bold"><strong>DEACTIVATED</strong></span>
     </p></li><li class="listitem "><p>
      Optionally you can migrate the instance off the host to be deleted. To do
      this, complete the following sections in
      <a class="xref" href="#remove-compute-node" title="15.1.3.5. Removing a Compute Node">Section 15.1.3.5, “Removing a Compute Node”</a>:
     </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
        Disable provisioning on the compute host.
       </p></li><li class="listitem "><p>
        Use live migration to move any instances on this host to other hosts.
       </p></li></ol></div></li></ul></div></div><p>
   To delete a compute host:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To open Operations Console, in a browser, enter either the URL or Virtual
     IP connected to Operations Console.
    </p><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">https://myardana.test:9095
https://VIP:9095</pre></div></li><li class="listitem "><p>
     On the <span class="bold"><strong>Home</strong></span> screen, click the menu
     represented by 3 horizontal lines (<span class="inlinemediaobject"><img xmlns="" src="images/media-opsconsole-OpsConsoleBurgerMenu.png" width="" alt="Three-Line Icon" /></span>).
    </p></li><li class="listitem "><p>
     From the menu that slides in on the left side, click
     <span class="bold"><strong>Compute</strong></span>, and
     then <span class="bold"><strong>Compute Hosts</strong></span>.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Compute Hosts</strong></span> page, in the row for
     the host you want to delete, click the details button
     (<span class="inlinemediaobject"><img xmlns="" src="images/media-opsconsole-DetailDots.png" width="" alt="Ellipsis Icon" /></span>).
    </p></li><li class="listitem "><p>
     Click <span class="bold"><strong>Delete</strong></span>, and if the configuration is
     encrypted, enter the encryption key.
    </p></li><li class="listitem "><p>
     in the confirmation screen that opens, click
     <span class="bold"><strong>Confirm</strong></span>.
    </p></li><li class="listitem "><p>
     In the compute hosts table you will see the
     <span class="bold"><strong>STATE</strong></span> change to
     <span class="bold"><strong>Deleting</strong></span>.
    </p></li><li class="listitem "><p>
     Wait for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> to complete the operation. This process can take up to 2
     minutes.
    </p></li><li class="listitem "><p>
     If deletion is successful, you will see a notification and in the compute
     hosts table the host will not be listed.
    </p><div id="id-1.5.18.3.4.11.5.9.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      If you receive a notice that the operation did not complete successfully,
      read the notification explaining at which step the error occured. You can
      click on the link in the error notification for more details. In the
      compute hosts table the <span class="bold"><strong>STATE</strong></span> will
      remain <span class="bold"><strong>DEACTIVATED</strong></span>.
     </p></div></li></ol></div></div><div class="sect3" id="opsconsole-more-info"><div class="titlepage"><div><div><h4 class="title"><span class="number">16.1.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="#opsconsole-more-info">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-manage-compute-instances.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-manage-compute-instances.xml</li><li><span class="ds-label">ID: </span>opsconsole-more-info</li></ul></div></div></div></div><p>
   For more information on how to complete these tasks through the command
   line, see the following topics:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <a class="xref" href="#adding-compute-nodes" title="15.1.3.4. Adding Compute Node">Section 15.1.3.4, “Adding Compute Node”</a>
    </p></li><li class="listitem "><p>
     <a class="xref" href="#remove-compute-node" title="15.1.3.5. Removing a Compute Node">Section 15.1.3.5, “Removing a Compute Node”</a>
    </p></li></ul></div></div></div><div class="sect2" id="opsconsole-manage-swift"><div class="titlepage"><div><div><h3 class="title"><span class="number">16.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Managing Swift Performance</span> <a title="Permalink" class="permalink" href="#opsconsole-manage-swift">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-manage-swift.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-manage-swift.xml</li><li><span class="ds-label">ID: </span>opsconsole-manage-swift</li></ul></div></div></div></div><p>
  In Operations Console you can monitor your swift cluster to ensure long-term
  data protection as well as sufficient performance.
 </p><p>
  OpenStack swift is an object storage solution with a focus on availability.
  While there are various mechanisms inside swift to protect stored data and
  ensure a high availability, you must still closely monitor your swift cluster
  to ensure long-term data protection as well as sufficient performance. The
  best way to manage swift is to collect useful data that will detect possible
  performance impacts early on.
 </p><p>
  The new Object Summary Dashboard in Operations Console provides
  an overview of your swift environment.
 </p><div id="id-1.5.18.3.5.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   If swift is not installed and configured, you will not be able to access
   this dashboard. The swift endpoint must be present in keystone for the
   Object Summary to be present in the menu.
  </p></div><p>
  In Operations Console's object storage dashboard, you can easily review the
  following information:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Performance Summary: <a class="xref" href="#opsconsole-swift-summary" title="16.1.4.1. Performance Summary">Section 16.1.4.1, “Performance Summary”</a>
   </p></li><li class="listitem "><p>
    Inventory Summary: <a class="xref" href="#opsconsole-swift-inventory" title="16.1.4.2. Inventory Summary">Section 16.1.4.2, “Inventory Summary”</a>
   </p></li><li class="listitem "><p>
    Capacity Summary: <a class="xref" href="#opsconsole-swift-capacity" title="16.1.4.3. Capacity Summary">Section 16.1.4.3, “Capacity Summary”</a>
   </p></li><li class="listitem "><p>
    Alarm Summary: <a class="xref" href="#opsconsole-swift-alarmsumm" title="16.1.4.4. Alarm Summary">Section 16.1.4.4, “Alarm Summary”</a>
   </p></li></ul></div><div class="sect3" id="opsconsole-swift-summary"><div class="titlepage"><div><div><h4 class="title"><span class="number">16.1.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Performance Summary</span> <a title="Permalink" class="permalink" href="#opsconsole-swift-summary">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-manage-swift.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-manage-swift.xml</li><li><span class="ds-label">ID: </span>opsconsole-swift-summary</li></ul></div></div></div></div><p>
   View a comprehensive summary of current performance values.
  </p><p>
   To access the object storage performance dashboard:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To open Operations Console, in a browser enter either the URL or
     Virtual IP connected to Operations Console.
    </p><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">https://myardana.test:9095
https://VIP:9095</pre></div></li><li class="listitem "><p>
     On the <span class="bold"><strong>Home</strong></span> screen, click the menu
     represented by 3 horizontal lines (<span class="inlinemediaobject"><img xmlns="" src="images/media-opsconsole-OpsConsoleBurgerMenu.png" width="" alt="Three-Line Icon" /></span>).
    </p></li><li class="listitem "><p>
     In the menu, click <span class="guimenu ">Storage</span> › <span class="guimenu ">Object Storage Summary</span>.
    </p></li></ol></div><p>
   Performance data includes:
  </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.18.3.5.8.6.1"><span class="term ">Healthcheck Latency from monasca</span></dt><dd><p>
      This latency is the average time it takes for swift to respond to a
      healthcheck, or ping, request. The swiftlm-uptime monitor program reports
      the value. A large difference between average and maximum may indicate a
      problem with one node.
     </p></dd><dt id="id-1.5.18.3.5.8.6.2"><span class="term ">Operational Latency from monasca</span></dt><dd><p>
      Operational latency is the average time it takes for swift to respond to
      an upload, download, or object delete request. The swiftlm-uptime monitor
      program reports the value. A large difference between average and maximum
      may indicate a problem with one node.
     </p></dd><dt id="id-1.5.18.3.5.8.6.3"><span class="term ">Service Availability</span></dt><dd><p>
      This is the availability over the last 24 hours as a percentage.
     </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
        <span class="bold"><strong>100%</strong></span> - No outages in the last 24 hours
       </p></li><li class="listitem "><p>
        <span class="bold"><strong>50%</strong></span> - swift was unavailable for a
        total of 12 hours in the last 24-hour period
       </p></li></ul></div></dd><dt id="id-1.5.18.3.5.8.6.4"><span class="term ">Graph of Performance Over Time</span></dt><dd><p>
      Create a visual representation of performance data to see when swift
      encountered longer-than-normal response times.
     </p><p>
      To create a graph:
     </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
        Choose the length of time you want to graph in
        <span class="guimenu ">Date Range</span>.
        This sets the length of time for the x-axis which
        counts backwards until it reaches the present time. In the example
        below, 1 day is selected, and so the x axis shows performance starting
        from 24 hours ago (-24) until the present time.
       </p></li><li class="listitem "><p>
        Look at the y-axis to understand the range of response times. The first
        number is the smallest value in the data collected from the backend,
        and the last number is the longest amount of time it took swift to
        respond to a request. In the example below, the shortest time for a
        response from swift was 16.1 milliseconds.
       </p></li><li class="listitem "><p>
        Look for spikes which represent longer than normal response times. In
        the example below, swift experienced long response times 21 hours ago
        and again 1 hour ago.
       </p></li><li class="listitem "><p>
        Look for the latency value at the present time. The line running across
        the x-axis at 16.1 milliseconds shows you what the response time is
        currently.
       </p></li></ol></div></dd></dl></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-opsconsole-ObjStorageSumm2.png" target="_blank"><img src="images/media-opsconsole-ObjStorageSumm2.png" width="" /></a></div></div></div><div class="sect3" id="opsconsole-swift-inventory"><div class="titlepage"><div><div><h4 class="title"><span class="number">16.1.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Inventory Summary</span> <a title="Permalink" class="permalink" href="#opsconsole-swift-inventory">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-manage-swift.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-manage-swift.xml</li><li><span class="ds-label">ID: </span>opsconsole-swift-inventory</li></ul></div></div></div></div><p>
   Monitor details about all the swift resources deployed in your cloud.
  </p><p>
   To access the object storage inventory screen:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To open Operations Console, in a browser enter either the URL or
     Virtual IP connected to Operations Console.
    </p><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">https://myardana.test:9095
https://VIP:9095</pre></div></li><li class="listitem "><p>
     On the <span class="guimenu ">Home</span> screen, click the menu
     represented by 3 horizontal lines (<span class="inlinemediaobject"><img xmlns="" src="images/media-opsconsole-OpsConsoleBurgerMenu.png" width="" alt="Three-Line Icon" /></span>).
    </p></li><li class="listitem "><p>
     In the menu, click
     <span class="guimenu ">Storage</span> › <span class="guimenu ">Object Storage Summary</span>.
    </p></li><li class="listitem "><p>
     On the <span class="guimenu ">Summary</span> page, click
     <span class="guimenu ">Inventory Summary</span>.
    </p></li></ol></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-opsconsole-ObjStorageInventory1.png" target="_blank"><img src="images/media-opsconsole-ObjStorageInventory1.png" width="" /></a></div></div><p>
   General swift metrics are available for the following attributes:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="guimenu ">Time to replicate</span>: The average time in
     seconds it takes all hosts to complete a replication cycle.
    </p></li><li class="listitem "><p>
     <span class="guimenu ">Oldest replication</span>: The time in seconds
     that has elapsed since the object replication process completed its last
     replication cycle.
    </p></li><li class="listitem "><p>
     <span class="guimenu ">Async Pending</span>: This is the number of
     failed requests to add an entry in the container server's database.There
     is one async queue per swift disk, and a cron job queries all swift
     servers to calculate the total. When an object is uploaded into swift, and
     it is successfully stored, a request is sent to the container server to
     add a new entry for the object in the database. If the container update
     fails, the request is stored in what swift calls an Async Pending Queue.
    </p><div id="id-1.5.18.3.5.9.7.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      On a public cloud deployment, this value can reach millions. If it
      continues to grow, it means that the container updates are not keeping up
      with the requests. It is also normal for it this number to grow if a node
      hosting the swift container service is down.
     </p></div></li><li class="listitem "><p>
     <span class="guimenu ">Total number of alarms</span>: This number
     includes all nodes that host swift services, including proxy, account,
     container, and object storage services.
    </p></li><li class="listitem "><p>
     <span class="guimenu ">Total nodes</span>: This number includes all
     nodes that host swift services, including proxy, account, container, and
     object storage services. The number in the colored box represents the
     number of alarms in that state. The following colors are used to show the
     most severe alarm triggered on all nodes:
    </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.18.3.5.9.7.5.2.1"><span class="term ">Green</span></dt><dd><p>
        Indicates all alarms are in a known and untriggered state. For example,
        if there are 5 nodes and they are all known with no alarms, you will
        see the number 5 in the green box, and a zero in all the other colored
        boxes.
       </p></dd><dt id="id-1.5.18.3.5.9.7.5.2.2"><span class="term ">Yellow</span></dt><dd><p>
        Indicates that some low or medium alarms have been triggered but no
        critical or high alarms. For example, if there are 5 nodes, and there
        are 3 nodes with untriggered alarms and 2 nodes with medium severity
        alarms, you will see the number 3 in the green box, the number 2 in the
        yellow box, and zeros in all the other colored boxes.
       </p></dd><dt id="id-1.5.18.3.5.9.7.5.2.3"><span class="term ">Red</span></dt><dd><p>
        Indicates at least one critical or high severity alarm has been
        triggered on a node. For example, if there are 5 nodes, and there are 3
        nodes with untriggered alarms, 1 node with a low severity, and 1 node
        with a critical alarm, you will see the number 3 in the green box, the
        number 1 in the yellow box, the number 1 in the red box,and a zero in
        the gray box.
       </p></dd><dt id="id-1.5.18.3.5.9.7.5.2.4"><span class="term ">Gray</span></dt><dd><p>
        Indicates that all alarms on the nodes are unknown. For example, if
        there are 5 nodes with no data reported, you will see the number 5 in
        the gray box, and zeros in all the other colored boxes.
       </p></dd></dl></div></li><li class="listitem "><p>
     <span class="guimenu ">Cluster breakdown of nodes</span>: In the
     example screen above, the cluster consists of 2 nodes named SWPAC and
     SWOBJ. Click a node name to bring up more detailed information about
     that node.
    </p></li></ul></div></div><div class="sect3" id="opsconsole-swift-capacity"><div class="titlepage"><div><div><h4 class="title"><span class="number">16.1.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Capacity Summary</span> <a title="Permalink" class="permalink" href="#opsconsole-swift-capacity">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-manage-swift.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-manage-swift.xml</li><li><span class="ds-label">ID: </span>opsconsole-swift-capacity</li></ul></div></div></div></div><p>
   Use this screen to view the size of the file system space on all nodes and
   disk drives assigned to swift. Also shown is the remaining space available
   and the total size of all file systems used by swift. Values are given in
   megabytes (MB).
  </p><p>
   To access the object storage alarm summary screen:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To open Operations Console, in a browser enter either the URL or
     Virtual IP connected to Operations Console.
    </p><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>https://myardana.test:9095
https://VIP:9095</pre></div></li><li class="listitem "><p>
     On the <span class="guimenu ">Home</span> screen, click the menu
     represented by 3 horizontal lines (<span class="inlinemediaobject"><img xmlns="" src="images/media-opsconsole-OpsConsoleBurgerMenu.png" width="" alt="Three-Line Icon" /></span>).
    </p></li><li class="listitem "><p>
     In the menu, click <span class="guimenu ">Storage</span> › <span class="guimenu ">Object Storage Summary</span>.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Summary</strong></span> page, click
     <span class="bold"><strong>Capacity Summary</strong></span>.
    </p></li></ol></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-opsconsole-ObjStorageCapacity.png" target="_blank"><img src="images/media-opsconsole-ObjStorageCapacity.png" width="" /></a></div></div></div><div class="sect3" id="opsconsole-swift-alarmsumm"><div class="titlepage"><div><div><h4 class="title"><span class="number">16.1.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Alarm Summary</span> <a title="Permalink" class="permalink" href="#opsconsole-swift-alarmsumm">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-manage-swift.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-manage-swift.xml</li><li><span class="ds-label">ID: </span>opsconsole-swift-alarmsumm</li></ul></div></div></div></div><p>
   Use this page to quickly see the most recent alarms and triage all alarms
   related to object storage.
  </p><p>
   To access the object storage alarm summary screen:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To open Operations Console, in a browser enter either the URL or
     Virtual IP connected to Operations Console.
    </p><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>https://myardana.test:9095
https://VIP:9095</pre></div></li><li class="listitem "><p>
     On the <span class="guimenu ">Home</span> screen, click the menu
     represented by 3 horizontal lines (<span class="inlinemediaobject"><img xmlns="" src="images/media-opsconsole-OpsConsoleBurgerMenu.png" width="" alt="Three-Line Icon" /></span>).
    </p></li><li class="listitem "><p>
     In the menu, click <span class="guimenu ">Storage</span> › <span class="guimenu ">Object Storage Summary</span>.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Summary</strong></span> page, click
     <span class="bold"><strong>Alarm Summary</strong></span>.
    </p></li></ol></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-opsconsole-OpsConsoleSwiftAlarmSumm.png" target="_blank"><img src="images/media-opsconsole-OpsConsoleSwiftAlarmSumm.png" width="" /></a></div></div><p>
   Each row has a checkbox to allow you to select multiple alarms and set the
   same condition on them.
  </p><p>
   The <span class="guimenu ">State</span> column displays a graphical
   indicator representing the state of each alarm:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Green indicator: OK. Good operating state.
    </p></li><li class="listitem "><p>
     Yellow indicator: Warning. Low severity, not requiring
     immediate action.
    </p></li><li class="listitem "><p>
     Red indicator: Alarm. Varying severity levels and must be
     addressed.
    </p></li><li class="listitem "><p>
     Gray indicator: Undetermined.
    </p></li></ul></div><p>
   The <span class="guimenu ">Alarm</span> column identifies the alarm by
   the name it was given when it was originally created.
  </p><p>
   The <span class="guimenu ">Last Check</span> column displays the date and
   time the most recent occurrence of the alarm.
  </p><p>
   The <span class="guimenu ">Dimension</span> column describes the
   components to check in order to clear the alarm.
  </p><p>
   The last column, depicted by three dots, reveals an
   <span class="guimenu ">Actions</span> menu that allows
   you to choose:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="guimenu ">View Details</span>, which opens a separate window that shows
     all the information from the table view and the alarm history.
    </p><p>
     Comments can be updated by clicking <span class="guimenu ">Update Comment</span>.
     Click <span class="guimenu ">View Alarm Definition</span> to go
     to the <span class="guimenu ">Alarm Definition</span> tab showing that specific alarm
     definition.
    </p></li></ul></div></div></div><div class="sect2" id="opsconsole-charts"><div class="titlepage"><div><div><h3 class="title"><span class="number">16.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Visualizing Data in Charts</span> <a title="Permalink" class="permalink" href="#opsconsole-charts">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-charts.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-charts.xml</li><li><span class="ds-label">ID: </span>opsconsole-charts</li></ul></div></div></div></div><p>
  Operations Console allows you to create a new chart and select the time range
  and the metric you want to chart, based on monasca metrics.
 </p><p>
  Present data in a pictorial or graphical format to enable administrators and
  decision makers to grasp difficult concepts or identify new patterns.
 </p><p>
  <span class="bold"><strong>Create new time-series graphs from
  <span class="emphasis"><em>My Dashboard</em></span>.</strong></span>
 </p><p>
  <span class="emphasis"><em>My Dashboard</em></span> also allows you to customize the view in
  the following ways:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Include alarm cards from the Central Dashboard
   </p></li><li class="listitem "><p>
    Customize graphs in new ways
   </p></li><li class="listitem "><p>
    Reorder items using drag and drop
   </p></li></ul></div><p>
  <span class="bold"><strong>Plan for future storage</strong></span>
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Track capacity over time to predict with some degree of reliability the
    amount of additional storage needed.
   </p></li></ul></div><p>
  Charts and graphs provide a quick way to visualize large amounts of complex
  data. It is especially useful when trying to find relationships and
  understand your data, which could include thousands or even millions of
  variables. You can create a new chart in Operations Console from
  <span class="bold"><strong>My Dashboard</strong></span>.
 </p><p>
  The charts in Operations Console are based on monasca data. When you create a
  new chart you will be able to select the time range and the metric you want
  to chart. The list of Metrics you can choose from is equivalent to using the
  <span class="bold"><strong>monasca metric-name-list</strong></span> on the command
  line. After you select a metric, you can then specify a dimension, which is
  derived from the <span class="bold"><strong>monasca metric-list –name
  &lt;metric_name&gt;</strong></span> command line results. The dimension list
  changes based on the selected metric.
 </p><p>
  This topic provides instructions on how to create a basic chart, and how to
  create a chart specifically to visualize your cinder capacity.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Create a Chart: <a class="xref" href="#opsconsole-create-charts" title="16.1.5.1. Create a Chart">Section 16.1.5.1, “Create a Chart”</a>
   </p></li><li class="listitem "><p>
    Chart cinder Capacity: <a class="xref" href="#opsconsole-cinder-chart" title="16.1.5.2. Chart cinder Capacity">Section 16.1.5.2, “Chart cinder Capacity”</a>
   </p></li></ul></div><div class="sect3" id="opsconsole-create-charts"><div class="titlepage"><div><div><h4 class="title"><span class="number">16.1.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create a Chart</span> <a title="Permalink" class="permalink" href="#opsconsole-create-charts">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-charts.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-charts.xml</li><li><span class="ds-label">ID: </span>opsconsole-create-charts</li></ul></div></div></div></div><p>
   Create a chart to visually display data for up to 6 metrics over a period of
   time.
  </p><p>
   To create a chart:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To open Operations Console, in a browser, enter either the URL or Virtual
     IP connected to Operations Console.
    </p><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">https://myardana.test:9095
https://VIP:9095</pre></div></li><li class="listitem "><p>
     On the <span class="bold"><strong>Home</strong></span> screen, click the menu
     represented by 3 horizontal lines (<span class="inlinemediaobject"><img xmlns="" src="images/media-opsconsole-OpsConsoleBurgerMenu.png" width="" alt="Three-Line Icon" /></span>).
    </p></li><li class="listitem "><p>
     From the menu that slides in on the left, select
     <span class="bold"><strong>Home</strong></span>, and then
     select <span class="bold"><strong>My Dashboard</strong></span>.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>My Dashboard</strong></span> screen, select
     <span class="bold"><strong>Create New Chart</strong></span>.
    </p></li><li class="listitem "><p>
     On the <span class="bold"><strong>Add New Time Series Chart</strong></span> screen,
     in <span class="bold"><strong>Chart Definition</strong></span> complete any of the
     optional fields:
    </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.18.3.6.13.4.5.2.1"><span class="term ">Name</span></dt><dd><p>
        Short description of chart.
       </p></dd><dt id="id-1.5.18.3.6.13.4.5.2.2"><span class="term ">Time Range</span></dt><dd><p>
        Specifies the interval between metric collection. The default is
        <span class="bold"><strong>1 hour</strong></span>. Can be set to hours
        (1,2,4,8,24) or days (7,30,45).
       </p></dd><dt id="id-1.5.18.3.6.13.4.5.2.3"><span class="term ">Chart Update Rate</span></dt><dd><p>
        Collects metric data and adds it to the chart at the specified
        interval. The default is <span class="bold"><strong>1 minute</strong></span>. Can
        be set to minutes (1,5,10,30) or 1 hour.
       </p></dd><dt id="id-1.5.18.3.6.13.4.5.2.4"><span class="term ">Chart Type</span></dt><dd><p>
        Determines how the data is displayed. The default type is
        <span class="bold"><strong>Line</strong></span>. Can be set to the following
        values:
       </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-opsconsole-LineChartIcon.png" target="_blank"><img src="images/media-opsconsole-LineChartIcon.png" width="" /></a></div></div><p>
          Line
         </p></li><li class="listitem "><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-opsconsole-BarChartIcon.png" target="_blank"><img src="images/media-opsconsole-BarChartIcon.png" width="" /></a></div></div><p>
          Bar
         </p></li><li class="listitem "><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-opsconsole-StackedBarIcon.png" target="_blank"><img src="images/media-opsconsole-StackedBarIcon.png" width="" /></a></div></div><p>
          Stacked Bar
         </p></li><li class="listitem "><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-opsconsole-AreaIcon.png" target="_blank"><img src="images/media-opsconsole-AreaIcon.png" width="" /></a></div></div><p>
          Area
         </p></li><li class="listitem "><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-opsconsole-AreaStackedIcon.png" target="_blank"><img src="images/media-opsconsole-AreaStackedIcon.png" width="" /></a></div></div><p>
          Stacked Area
         </p></li></ul></div></dd><dt id="id-1.5.18.3.6.13.4.5.2.5"><span class="term ">Chart Size</span></dt><dd><p>
        This controls the visual display of the chart width as it appears on
        <span class="bold"><strong>My Dashboard</strong></span>. The default is
        <span class="bold"><strong>Small</strong></span>. This field can be set to
        <span class="bold"><strong>Small</strong></span> to display it at 50% or
        <span class="bold"><strong>Large</strong></span> for 100%.
       </p></dd></dl></div></li><li class="listitem "><p>
     On the <span class="bold"><strong>Add New Time Series Chart</strong></span> screen,
     in <span class="bold"><strong>Added Chart Data</strong></span> complete the
     following fields:
    </p><div class="variablelist "><dl class="variablelist"><dt id="id-1.5.18.3.6.13.4.6.2.1"><span class="term ">Metric</span></dt><dd><p>
        In monasca, a metric is a multi-dimensional description that consists
        of the following fields: name, dimensions, timestamp, value and
        value_meta. The pre-populated list is equivalent to using the
        <span class="bold"><strong>monasca metric-name-list</strong></span> on the
        command line.
       </p></dd><dt id="id-1.5.18.3.6.13.4.6.2.2"><span class="term ">Dimension</span></dt><dd><p>
        The set of unique dimensions that are defined for a specific metric.
        Dimensions are a dictionary of key-value pairs. This pre-populated list
        is equivalent to using the
        <span class="bold"><strong>monasca
        metric-list –name &lt;metric_name&gt;</strong></span> on the command line.
       </p></dd><dt id="id-1.5.18.3.6.13.4.6.2.3"><span class="term ">Function</span></dt><dd><p>
        Operations Console uses monasca to provide the results of all
        mathematical functions. monasca in turns uses Graphite to perform the
        mathematical calculations and return the results. The default is
        <span class="bold"><strong>AVG</strong></span>. The
        <span class="bold"><strong>Function</strong></span>
        field can be set to
        <span class="bold"><strong>AVG</strong></span>
        (default), <span class="bold"><strong>MIN</strong></span>,
        <span class="bold"><strong>MAX</strong></span>. and
        <span class="bold"><strong>COUNT</strong></span>.
        For more information on these functions, see the Graphite
        documentation at
        <a class="link" href="http://www.aosabook.org/en/graphite.html" target="_blank">http://www.aosabook.org/en/graphite.html</a>.
       </p></dd></dl></div></li><li class="listitem "><p>
     Click <span class="bold"><strong>Add Data To Chart</strong></span>. To add another
     metric to the chart, repeat the previous step until all metrics are added.
     The maximum you can have in one chart is 6 metrics.
    </p></li><li class="listitem "><p>
     To create the chart, click <span class="bold"><strong>Create New
     Chart</strong></span>.
    </p></li></ol></div><p>
   After you click <span class="bold"><strong>Create New Chart</strong></span>, you will
   be returned to <span class="emphasis"><em>My Dashboard</em></span> where the new chart will be
   shown. From the <span class="emphasis"><em>My Dashboard</em></span> screen you can use the menu
   in the top-right corner of the card to
   delete or edit the chart. You can also select an option to create a
   comma-delimited file of the data in the chart.
  </p></div><div class="sect3" id="opsconsole-cinder-chart"><div class="titlepage"><div><div><h4 class="title"><span class="number">16.1.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Chart cinder Capacity</span> <a title="Permalink" class="permalink" href="#opsconsole-cinder-chart">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-charts.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-charts.xml</li><li><span class="ds-label">ID: </span>opsconsole-cinder-chart</li></ul></div></div></div></div><p>
   To visualize the use of storage capacity over time, you can create a chart
   that graphs the total block storage backend capacity. To find out how much
   of that total is being used, you can also create a chart that graphs the
   available block storage capacity.
  </p><p>
   <span class="bold"><strong>Visualizing cinder:</strong></span>
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Chart Total Capacity: <a class="xref" href="#opsconsole-cinder-chart-total" title="16.1.5.3. Chart Total Capacity">Section 16.1.5.3, “Chart Total Capacity”</a>
    </p></li><li class="listitem "><p>
     Chart Available Capacity: <a class="xref" href="#opsconsole-cinder-chart-avail" title="16.1.5.4. Chart Available Capacity">Section 16.1.5.4, “Chart Available Capacity”</a>
    </p></li></ul></div><div id="id-1.5.18.3.6.14.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    The total and free capacity values are based on the available capacity
    reported by the cinder backend. Be aware that some backends can be
    configured to thinly provision.
   </p></div></div><div class="sect3" id="opsconsole-cinder-chart-total"><div class="titlepage"><div><div><h4 class="title"><span class="number">16.1.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Chart Total Capacity</span> <a title="Permalink" class="permalink" href="#opsconsole-cinder-chart-total">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-charts.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-charts.xml</li><li><span class="ds-label">ID: </span>opsconsole-cinder-chart-total</li></ul></div></div></div></div><p>
   To chart the total block-storage backend capacity:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to Operations Console.
    </p></li><li class="listitem "><p>
     Follow the steps in the previous instructions to start creating a chart.
    </p></li><li class="listitem "><p>
     To chart the total backend capacity, on the <span class="bold"><strong>Add New
     Time Series Chart</strong></span> screen, in <span class="bold"><strong>Chart
     Definition</strong></span> use the following settings:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Field</th><th>Setting</th></tr></thead><tbody><tr><td>Metrics</td><td>cinderlm.cinder.backend.total.size</td></tr><tr><td>Dimension</td><td><p>any hostname. If multiple backends are available, select any one. The backends will all return the same metric data.</p></td></tr></tbody></table></div></li><li class="listitem "><p>
     Add the data to the chart and click
     <span class="bold"><strong>Create</strong></span>.
    </p></li></ol></div><p>
   Example of a cinder Total Capacity Chart:
  </p></div><div class="sect3" id="opsconsole-cinder-chart-avail"><div class="titlepage"><div><div><h4 class="title"><span class="number">16.1.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Chart Available Capacity</span> <a title="Permalink" class="permalink" href="#opsconsole-cinder-chart-avail">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-charts.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-charts.xml</li><li><span class="ds-label">ID: </span>opsconsole-cinder-chart-avail</li></ul></div></div></div></div><p>
   To chart the available block-storage backend capacity:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to Operations Console.
    </p></li><li class="listitem "><p>
     Follow the steps in the previous instructions to start creating a chart.
    </p></li><li class="listitem "><p>
     To chart the available backend capacity, on the <span class="bold"><strong>Add
     New Time Series Chart</strong></span> screen, in <span class="bold"><strong>Chart
     Definition</strong></span> use the following settings:
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Field</th><th>Setting</th></tr></thead><tbody><tr><td>Metrics</td><td>cinderlm.cinder.backend.total.avail</td></tr><tr><td>Dimension</td><td><p>any hostname. If multiple backends are available, select any one. The backends will all return the same metric data.</p></td></tr></tbody></table></div></li><li class="listitem "><p>
     Add the data to the chart and click
     <span class="bold"><strong>Create</strong></span>.
    </p></li></ol></div><p>
   Example of a chart showing cinder Available Capacity:
  </p><div id="id-1.5.18.3.6.16.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    The source data for the Capacity Summary pages is only refreshed at the top
    of each hour. This affects the latency of the displayed data on those
    pages.
   </p></div></div></div><div class="sect2" id="opsconsole-help"><div class="titlepage"><div><div><h3 class="title"><span class="number">16.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Getting Help with the Operations Console</span> <a title="Permalink" class="permalink" href="#opsconsole-help">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-help.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-help.xml</li><li><span class="ds-label">ID: </span>opsconsole-help</li></ul></div></div></div></div><p>
  On each of the Operations Console pages there is a help menu that you can
  click on to take you to a help page specific to the console you are
  currently viewing.
 </p><p>
  To reach the help page:
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Click the help menu option in the upper-right corner of the page, depicted
    by the question mark seen in the screenshot below.
   </p></li><li class="listitem "><p>
    Click the <span class="guimenu ">Get Help For This Page</span> link which will
    open the help page in a new tab in your browser.
   </p></li></ol></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-opsconsole-OpsConsoleGetHelp.png" target="_blank"><img src="images/media-opsconsole-OpsConsoleGetHelp.png" width="" /></a></div></div></div></div><div class="sect1" id="opsconsole-alarm-definitions"><div class="titlepage"><div><div><h2 class="title"><span class="number">16.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Alarm Definition</span> <a title="Permalink" class="permalink" href="#opsconsole-alarm-definitions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-alarm-definitions.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-alarm-definitions.xml</li><li><span class="ds-label">ID: </span>opsconsole-alarm-definitions</li></ul></div></div></div></div><p>
  The <span class="guimenu ">Alarm Definition</span> section under
  <span class="guimenu ">Monitoring</span> allows you to define alarms that are useful
  in generating
  notifications and metrics required by your organization. By default, alarm
  definitions are sorted by name and in a table format.
 </p><div class="sect2" id="id-1.5.18.4.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">16.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Filter and Sort</span> <a title="Permalink" class="permalink" href="#id-1.5.18.4.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-alarm-definitions.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-alarm-definitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The search feature allows you to search and filter alarm entries by name
   and description.
  </p><p>
   The check box above the top left of the table is used to select all alarm
   definitions on the current page.
  </p><p>
   To sort the table, click the desired column header. To reverse the
   sort order, click the column again.
  </p></div><div class="sect2" id="id-1.5.18.4.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">16.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create Alarm Definitions</span> <a title="Permalink" class="permalink" href="#id-1.5.18.4.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-alarm-definitions.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-alarm-definitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The <span class="guimenu ">Create Alarm Definition</span> button next to the search bar
   allows you to create a new alarm definition.
  </p><p>
   To create a new alarm definition:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Click <span class="guimenu ">Create Alarm Definition</span> to open the
     <span class="guimenu ">Create Alarm Definition</span> dialog.
    </p></li><li class="listitem "><p>
     In the Create Alarm Definition window, type a name for the alarm in the
     <span class="guimenu ">Name</span> text field. The name is mandatory and can be up to
     255 characters long. The name can include letters, numbers, and special
     characters.
    </p></li><li class="listitem "><p>
     Provide a short description of the alarm in the
     <span class="guimenu ">Description</span> text field (optional).
    </p></li><li class="listitem "><p>
     Select the desired severity level of the alarm from the
     <span class="guimenu ">Severity</span> drop-down box. The severity level is
     subjective, so choose
     the level appropriate for prioritizing the handling of alarms when they
     occur.
    </p></li><li class="listitem "><p>
     Although not required, in order to specify how to receive notifications,
     you must be able to select the method(s) of notification (Email, Web, API,
     etc.) from the list of options in the Alarm Notifications area. If none
     are available to choose from, you must first configure them in the
     Notifications Methods window. Refer to the Notification Methods help page
     for further instructions.
    </p></li><li class="listitem "><p>
     To enable notifications for the alarm, enable the check box next to the
     desired alarm notification method.
    </p></li><li class="listitem "><p>
     Apply the following rules to your alarm by using the Alarm Expression
     form:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="guimenu ">Function</span>: determines the output value from a supplied
       input value.
      </p></li><li class="listitem "><p>
       <span class="guimenu ">Metric</span>: applies a pre-defined means of measuring
       whatever aspect of the alarm.
      </p></li><li class="listitem "><p>
       <span class="guimenu ">Dimension(s)</span>: identifies which aspect (Hostname,
       Region, and Service) of the alarm you want to monitor.
      </p></li><li class="listitem "><p>
       <span class="guimenu ">Comparator</span>: specifies the operator for how
       you want the alarm to trigger.
      </p></li><li class="listitem "><p>
       <span class="guimenu ">Threshold</span>: determines the numeric threshold associated
       with the operator you specified.
      </p></li></ul></div></li><li class="listitem "><p>
     <span class="guimenu ">Match By</span> (optional): group results by a specific
     dimension that is not part of the Dimension(s) solution.
    </p></li><li class="listitem "><p>
     To save the changes and add the new alarm definition to the table, click
     <span class="guimenu ">Create Alarm Definition</span>.
    </p></li></ol></div></div></div><div class="sect1" id="opsconsole-alarm-explorer"><div class="titlepage"><div><div><h2 class="title"><span class="number">16.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Alarm Explorer</span> <a title="Permalink" class="permalink" href="#opsconsole-alarm-explorer">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-alarm_explorer.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-alarm_explorer.xml</li><li><span class="ds-label">ID: </span>opsconsole-alarm-explorer</li></ul></div></div></div></div><p>
  This page displays the alarms for all services and appliances. By default,
  alarms are sorted by their state.
 </p><div class="sect2" id="id-1.5.18.5.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">16.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Filter and Sort</span> <a title="Permalink" class="permalink" href="#id-1.5.18.5.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-alarm_explorer.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-alarm_explorer.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Using the <span class="guimenu ">Filter Alarms</span> button, you can filter the
   alarms by their IDs and dimensions. The <span class="guimenu ">Filter Alarms</span>
   dialog lets you configure a filtering rule using the <span class="guimenu ">Alarm
   ID</span> field and options in the <span class="guimenu ">Dimension(s)</span>
   section.
  </p><p>
   You can display the alarms by grid, list or table views by selecting the
   corresponding icons next to the <span class="guimenu ">Sort By</span> control.
  </p><p>
   To sort the alarm list, click the desired column header. To reverse the
   sort order, click the column again.
  </p></div><div class="sect2" id="id-1.5.18.5.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">16.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Alarm Table</span> <a title="Permalink" class="permalink" href="#id-1.5.18.5.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-alarm_explorer.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-alarm_explorer.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Each row has a checkbox to allow you to select multiple alarms and set the
   same condition on them.
  </p><p>
   The <span class="bold"><strong>Status</strong></span> column displays a graphical
   indicator that shows the state of each alarm:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Green indicator: OK. Good operating state.
    </p></li><li class="listitem "><p>
     Yellow indicator: Warning. Low severity, not requiring
     immediate action.
    </p></li><li class="listitem "><p>
     Red indicator: Alarm. Varying severity levels and must be
     addressed.
    </p></li><li class="listitem "><p>
     Gray indicator: Unknown.
    </p></li></ul></div><p>
   The <span class="bold"><strong>Alarm</strong></span> column identifies the alarm by
   the name it was given when it was originally created.
  </p><p>
   The <span class="bold"><strong>Last Check</strong></span> column displays the date and
   time the most recent occurrence of the alarm.
  </p><p>
   The <span class="bold"><strong>Dimension</strong></span> column describes the
   components to check in order to clear the alarm.
  </p></div><div class="sect2" id="id-1.5.18.5.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">16.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Notification Methods</span> <a title="Permalink" class="permalink" href="#id-1.5.18.5.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-alarm_explorer.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-alarm_explorer.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The <span class="guimenu ">Notification Methods</span> section of the Alarm Explorer
   allows you to define notification methods that are used by the alarms. By
   default, notification methods are sorted by name.
  </p><div class="sect3" id="id-1.5.18.5.5.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">16.3.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Filter and Sort</span> <a title="Permalink" class="permalink" href="#id-1.5.18.5.5.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-alarm_explorer.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-alarm_explorer.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    The filter bar allows you to filter the notification methods by specifying a
    filter criteria. You can sort the available notification methods by clicking
    on the desired column header in the table.
   </p></div><div class="sect3" id="id-1.5.18.5.5.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">16.3.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create Notification Methods</span> <a title="Permalink" class="permalink" href="#id-1.5.18.5.5.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-alarm_explorer.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-alarm_explorer.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The <span class="guimenu ">Create Notification Methods</span> button beside the search
   bar allows you to create a new notification method.
  </p><p>
   To create a new notification method:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Click the <span class="guimenu ">Create Notification Method</span> button.
    </p></li><li class="step "><p>
     In the <span class="guimenu ">Create Notification Method</span> window, specify a
     name for the notification in the <span class="guimenu ">Name</span> text field. The
     name is required, and it can be up to 255 characters in length, consisting
     of letters, numbers, or special characters.
    </p></li><li class="step "><p>
     Select a <span class="guimenu ">Type</span> in the drop down and select the desired
     option:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <span class="guimenu ">Web Hook</span> allows you to enter in an internet
       address, also referred to as a <span class="emphasis"><em>Web Hook</em></span>.
      </p></li><li class="listitem "><p>
       <span class="guimenu ">Email</span> allows you to enter in an email address. For
       this method to work you need to have a SMTP server specified.
      </p></li><li class="listitem "><p>
       <span class="guimenu ">PagerDuty</span> allows you to enter in a PagerDuty
       address.
      </p></li></ul></div></li><li class="step "><p>
     In the <span class="guimenu ">Address/Key</span> text field, provide the required
     values.
    </p></li><li class="step "><p>
     Press <span class="guimenu ">Create Notification Method</span>, and you should see
     the created notification method in the table.
    </p></li></ol></div></div></div></div></div><div class="sect1" id="opsconsole-compute-hosts"><div class="titlepage"><div><div><h2 class="title"><span class="number">16.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute Hosts</span> <a title="Permalink" class="permalink" href="#opsconsole-compute-hosts">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-compute_hosts.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-compute_hosts.xml</li><li><span class="ds-label">ID: </span>opsconsole-compute-hosts</li></ul></div></div></div></div><p>
  This <span class="guimenu ">Compute Hosts</span> page in the
  <span class="guimenu ">Compute</span> section allows you to view your Compute Host
  resources.
 </p><div class="sect2" id="id-1.5.18.6.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">16.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Filter and Sort</span> <a title="Permalink" class="permalink" href="#id-1.5.18.6.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-compute_hosts.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-compute_hosts.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The dedicated bar at the top of the page bar lets you filter alarm entries
   using the available filtering options.
   </p><div class="figure" id="id-1.5.18.6.3.3"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/opsconsole-compute_hosts.png" target="_blank"><img src="images/opsconsole-compute_hosts.png" width="" alt="Compute Hosts" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 16.1: </span><span class="name">Compute Hosts </span><a title="Permalink" class="permalink" href="#id-1.5.18.6.3.3">#</a></h6></div></div><p>
    Click the <span class="guimenu ">Filter</span> icon to select one of
   the available options:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      <span class="guimenu ">Any Column</span> enables plain search across all columns
     </p></li><li class="listitem "><p>
      <span class="guimenu ">Status</span> filters alarm entries by status.
     </p></li><li class="listitem "><p>
      <span class="guimenu ">Type</span> enables filtering by host type, including
      Hyper-V, KVM, ESXi, and VMWare vCenter server.
     </p></li><li class="listitem "><p>
      <span class="guimenu ">State</span> filters alarm entries by nova state (for
      example, Activated, Activating, Imported, etc.).
     </p></li><li class="listitem "><p>
      <span class="guimenu ">Alarm State</span> filters entries bay status of the alarms
      that are triggered on the host.
     </p></li><li class="listitem "><p>
      <span class="guimenu ">Cluster</span> returns a filtered list of configured
      clusters that Compute Hosts belong to.
     </p></li></ul></div><p>
   The alarm entries can be sorted by clicking on the appropriate column
   header, such as <span class="guimenu ">Name</span>, <span class="guimenu ">Status</span>,
   <span class="guimenu ">Type</span>, <span class="guimenu ">State</span>, etc.
  </p><p>
   To view detailed information (including alarm counts and utilization
   metrics) about a specific host in the list, click in the
   host's name in the list.
  </p></div></div><div class="sect1" id="opsconsole-compute-instances"><div class="titlepage"><div><div><h2 class="title"><span class="number">16.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute Instances</span> <a title="Permalink" class="permalink" href="#opsconsole-compute-instances">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-compute_instances.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-compute_instances.xml</li><li><span class="ds-label">ID: </span>opsconsole-compute-instances</li></ul></div></div></div></div><p>
  This Operations Console page allows you to monitor your Compute instances.
 </p><div class="sect2" id="id-1.5.18.7.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">16.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Search and Sort</span> <a title="Permalink" class="permalink" href="#id-1.5.18.7.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-compute_instances.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-compute_instances.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The search bar allows you to filter the alarm definitions you want to view.
   Type and Status are examples of alarm criteria that can be specified.
   Additionally, you can filter by typing in text similar to searching by
   keywords.
  </p><p>
   The checkbox allows you to select (or deselect) a group of alarm definitions
   to delete:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="guimenu ">Select Visible</span> allows you to delete the selected alarm
     definitions from the table.
    </p></li><li class="listitem "><p>
     <span class="guimenu ">Select All</span> allows you to delete all the alarms from the
     table.
    </p></li><li class="listitem "><p>
     <span class="guimenu ">Clear Selection</span> allows you to clear all the selections
     currently selected from the table.
    </p></li></ul></div><p>
   You can display the alarm definitions by grid, list or table views by
   selecting the corresponding icons next to the <span class="guimenu ">Sort By</span>
   control.
  </p><p>
   The <span class="guimenu ">Sort By</span> control contains a drop-down list of ways by
   which you can sort the compute nodes. Alternatively, you can also sort using
   the column headers in the table.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="guimenu ">Sort by Name</span> displays the compute instances by the name
     assigned to it when it was created.
    </p></li><li class="listitem "><p>
     <span class="guimenu ">Sort by State</span> displays the compute instances by their
     current state.
    </p></li><li class="listitem "><p>
     <span class="guimenu ">Sort by Status</span> displays the compute instances by their
     current status.
    </p></li><li class="listitem "><p>
     <span class="guimenu ">Sort by Host</span> displays the compute instances by their
     host.
    </p></li><li class="listitem "><p>
     <span class="guimenu ">Sort by Image</span> displays the compute instances by the
     image being used.
    </p></li><li class="listitem "><p>
     <span class="guimenu ">Sort by IP Address</span> displays the compute instances by
     their IP address.
    </p></li></ul></div></div></div><div class="sect1" id="opsconsole-compute-alarm-summary"><div class="titlepage"><div><div><h2 class="title"><span class="number">16.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute Summary</span> <a title="Permalink" class="permalink" href="#opsconsole-compute-alarm-summary">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-compute_summary.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-compute_summary.xml</li><li><span class="ds-label">ID: </span>opsconsole-compute-alarm-summary</li></ul></div></div></div></div><p>
  The <span class="guimenu ">Compute Summary</span> page in the <span class="guimenu ">Compute</span>
  section gives you access to inventory, capacity, and alarm summaries.
 </p><div class="sect2" id="id-1.5.18.8.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">16.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Inventory Summary</span> <a title="Permalink" class="permalink" href="#id-1.5.18.8.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-compute_summary.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-compute_summary.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The <span class="guimenu ">Inventory Summary</span> section provides an overview of
   compute alarms by status. These alarms are grouped by control plane. There
   is also information on resource usage for each compute host. Here you can
   also see alarms triggered on individual compute hosts.
  </p><div class="figure" id="id-1.5.18.8.3.3"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/opsconsole-compute_inventory_summary.png" target="_blank"><img src="images/opsconsole-compute_inventory_summary.png" width="" alt="Compute Summary" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 16.2: </span><span class="name">Compute Summary </span><a title="Permalink" class="permalink" href="#id-1.5.18.8.3.3">#</a></h6></div></div></div><div class="sect2" id="id-1.5.18.8.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">16.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Capacity Summary</span> <a title="Permalink" class="permalink" href="#id-1.5.18.8.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-compute_summary.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-compute_summary.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="guimenu ">Capacity Summary</span> offers an overview of the utilization
   of physical resources and allocation of virtual resources among compute
   nodes. Here you will also find a break-down of CPU, memory, and storage
   usage across all compute resources in the cloud.
  </p></div><div class="sect2" id="id-1.5.18.8.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">16.6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute Summary</span> <a title="Permalink" class="permalink" href="#id-1.5.18.8.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-compute_summary.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-compute_summary.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
  The <span class="guimenu ">Compute Summary</span> show overviews of new alarms
  as well as a list of all alarms that can be filtered and sorted. For more
  information on filtering alarms, see <a class="xref" href="#opsconsole-alarm-explorer" title="16.3. Alarm Explorer">Section 16.3, “Alarm Explorer”</a>.
 </p></div><div class="sect2" id="opsconsole-CS-Ops-Appliances"><div class="titlepage"><div><div><h3 class="title"><span class="number">16.6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Appliances</span> <a title="Permalink" class="permalink" href="#opsconsole-CS-Ops-Appliances">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-compute_summary.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-compute_summary.xml</li><li><span class="ds-label">ID: </span>opsconsole-CS-Ops-Appliances</li></ul></div></div></div></div><p>
  This page displays details of an appliance.
 </p><p>
  <span class="bold"><strong>Search and Sort</strong></span>
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    The search bar allows you to filter the appliances you want to view.
    <span class="bold"><strong>Role</strong></span> and
    <span class="bold"><strong>Status</strong></span> are examples of criteria that can
    be specified. Additionally, you can filter by selecting
    <span class="bold"><strong>Any Column</strong></span> and typing in text similar to
    searching by keywords.
   </p></li><li class="listitem "><p>
    You can sort using the column headers in the table.
   </p></li></ul></div><p>
  <span class="bold"><strong>Actions</strong></span>
 </p><p>
  Click the Action icon (three dots) to view details of an appliance.
 </p></div><div class="sect2" id="opsconsole-CS-OPS-block-storage-summary"><div class="titlepage"><div><div><h3 class="title"><span class="number">16.6.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Block Storage Summary</span> <a title="Permalink" class="permalink" href="#opsconsole-CS-OPS-block-storage-summary">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-compute_summary.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-compute_summary.xml</li><li><span class="ds-label">ID: </span>opsconsole-CS-OPS-block-storage-summary</li></ul></div></div></div></div><p>
  This page displays the alarms that have triggered since the timeframe
  indicated.
 </p><p>
  <span class="bold"><strong>Search and Sort</strong></span>
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    The search bar allows you to filter the alarms you want to view.
    <span class="bold"><strong>State</strong></span> and
    <span class="bold"><strong>Service</strong></span> are examples of criteria that can
    be specified. Additionally, you can filter by typing in text similar to
    searching by keywords.
   </p></li><li class="listitem "><p>
    You can sort alarm entries using the column headers in the table.
   </p></li></ul></div><p>
  <span class="bold"><strong>New Alarms: Block Storage</strong></span>
 </p><p>
  The New Alarms section shows you the alarms that have triggered since the
  timeframe indicated. You can select the timeframe using the Configure control with
  options ranging from the Last Minute to Last 30 Days. This section refreshes
  every 60 seconds.
 </p><p>
  The new alarms will be separated into the following categories:
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Category</th><th>Description</th></tr></thead><tbody><tr><td>Critical</td><td>Open alarms, identified by red indicator.</td></tr><tr><td>Warning</td><td>Open alarms, identified by yellow indicator.</td></tr><tr><td>Unknown</td><td>
      <p>
       Open alarms, identified by gray indicator. Unknown will be the status of
       an alarm that has stopped receiving a metric. This can be caused by the
       following conditions:
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         An alarm exists for a service or component that is not installed in
         the environment.
        </p></li><li class="listitem "><p>
         An alarm exists for a virtual machine or node that previously existed
         but has been removed without the corresponding alarms being removed.
        </p></li><li class="listitem "><p>
         There is a gap between the last reported metric and the next metric.
        </p></li></ul></div>
     </td></tr><tr><td>Open</td><td>Complete list of open alarms.</td></tr><tr><td>Total</td><td>
      <p>
       Complete list of alarms, may include Acknowledged and Resolved alarms.
      </p>
     </td></tr></tbody></table></div><p>
  <span class="bold"><strong>More Information</strong></span>
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="#opsconsole-alarm-explorer" title="16.3. Alarm Explorer">Section 16.3, “Alarm Explorer”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#alarmdefinitions" title="18.1.1. Alarm Resolution Procedures">Section 18.1.1, “Alarm Resolution Procedures”</a>
   </p></li></ul></div></div></div><div class="sect1" id="opsconsole-idg-all-operations-opsconsole-en-logging-xml-1"><div class="titlepage"><div><div><h2 class="title"><span class="number">16.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Logging</span> <a title="Permalink" class="permalink" href="#opsconsole-idg-all-operations-opsconsole-en-logging-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-logging.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-logging.xml</li><li><span class="ds-label">ID: </span>opsconsole-idg-all-operations-opsconsole-en-logging-xml-1</li></ul></div></div></div></div><p>
  This page displays the link to the Logging Interface, known as Kibana.
 </p><div id="id-1.5.18.9.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Accessing Kibana</h6><p>
   The Kibana logging interface only runs on the management network. You need
   to have access to that network to be able to use Kibana.
  </p></div><div class="sect2" id="id-1.5.18.9.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">16.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">View Logging Interface</span> <a title="Permalink" class="permalink" href="#id-1.5.18.9.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-logging.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-logging.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   To access the logging interface, click the <span class="guimenu ">Launch Logging
   Interface</span> button, which will open the interface in a new window.
  </p><p>
   For more details about the logging interface, see
   <a class="xref" href="#centralized-logging" title="13.2. Centralized Logging Service">Section 13.2, “Centralized Logging Service”</a>.
  </p></div></div><div class="sect1" id="opsconsole-my-dashboard-overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">16.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">My Dashboard</span> <a title="Permalink" class="permalink" href="#opsconsole-my-dashboard-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-my_dashboard_overview.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-my_dashboard_overview.xml</li><li><span class="ds-label">ID: </span>opsconsole-my-dashboard-overview</li></ul></div></div></div></div><p>
  This page allows you to customize the dashboard by mixing and matching graphs
  and alarm cards.
 </p><p>
  <span class="emphasis"><em>My Dashboard</em></span> allows you to customize the dashboard by
  mixing and matching
  graphs and alarm cards. Since different operators may be interested in
  different metrics and alarms, the configuration for this page is tied to the
  login account used to access Operations Console. Charts available here are based on
  metrics collected by the monasca monitoring component.
 </p></div><div class="sect1" id="opsconsole-networking-alarm-summary"><div class="titlepage"><div><div><h2 class="title"><span class="number">16.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networking Alarm Summary</span> <a title="Permalink" class="permalink" href="#opsconsole-networking-alarm-summary">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-networking_summary.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-networking_summary.xml</li><li><span class="ds-label">ID: </span>opsconsole-networking-alarm-summary</li></ul></div></div></div></div><p>
  This page displays the alarms for the Networking (neutron), DNS, Firewall,
  and Load Balancing services. By default, alarms are sorted by State.
 </p><div class="sect2" id="id-1.5.18.11.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">16.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Filter and Sort</span> <a title="Permalink" class="permalink" href="#id-1.5.18.11.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-networking_summary.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-networking_summary.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The filter bar allows you to filter the alarms by the available criteria,
   including <span class="guimenu ">Dimension</span>, <span class="guimenu ">State</span>, and
   <span class="guimenu ">Service</span>. The dimension filter accepts key/value pairs,
   while the State filter provides a selection of valid values.
  </p><p>
   You can sort alarm entries using the column headers in the table.
  </p></div><div class="sect2" id="id-1.5.18.11.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">16.9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Alarm Table</span> <a title="Permalink" class="permalink" href="#id-1.5.18.11.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-networking_summary.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-networking_summary.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   You can select one or multiple alarms using the check box next to
   each entry.
  </p><p>
   The <span class="bold"><strong>State</strong></span> column displays a graphical
   indicator that shows the state of each alarm:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Green indicator: OK. Good operating state.
    </p></li><li class="listitem "><p>
     Yellow indicator: Warning. Low severity, not requiring
     immediate action.
    </p></li><li class="listitem "><p>
     Red indicator: Alarm. Varying severity levels and must be
     addressed.
    </p></li><li class="listitem "><p>
     Gray square (or gray indicator): Undetermined.
    </p></li></ul></div><p>
   The <span class="bold"><strong>Alarm</strong></span> column identifies the alarm by
   its name.
  </p><p>
   The <span class="bold"><strong>Last Check</strong></span> column displays the date and
   time the most recent occurrence of the alarm.
  </p><p>
   The <span class="bold"><strong>Dimension</strong></span> column shows the
   components to check in order to clear the alarm.
  </p><p>
   The last column, depicted by three dots, reveals an
   <span class="bold"><strong>Actions</strong></span> menu gives you access to the
   following options:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="guimenu ">View Details</span> opens a separate window with the
     information from the table view and the alarm history.
    </p></li><li class="listitem "><p>
     <span class="guimenu ">View Alarm Definition</span> allows you to view and edit the
     selected alarm definition.
    </p></li><li class="listitem "><p>
     <span class="guimenu ">Delete</span> is used to delete the currently selected alarm
     entry.
    </p></li></ul></div></div></div><div class="sect1" id="opsconsole-dashboard-overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">16.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Central Dashboard</span> <a title="Permalink" class="permalink" href="#opsconsole-dashboard-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-opsconsole_dashboard.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-opsconsole_dashboard.xml</li><li><span class="ds-label">ID: </span>opsconsole-dashboard-overview</li></ul></div></div></div></div><p>
  This page displays a high level overview of all cloud resources and their
  alarm status.
 </p><div class="sect2" id="id-1.5.18.12.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">16.10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Central Dashboard</span> <a title="Permalink" class="permalink" href="#id-1.5.18.12.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-opsconsole_dashboard.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-opsconsole_dashboard.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-opsconsole-OpsConsoleDashboard.png" target="_blank"><img src="images/media-opsconsole-OpsConsoleDashboard.png" width="" /></a></div></div></div><div class="sect2" id="id-1.5.18.12.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">16.10.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">New Alarms</span> <a title="Permalink" class="permalink" href="#id-1.5.18.12.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-opsconsole_dashboard.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-opsconsole_dashboard.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The New Alarms section shows you the alarms that have triggered since the
   timeframe indicated. You can select the timeframe using the
   <span class="guimenu ">View</span> control with options ranging from the Last Minute to
   Last 30 Days. This section refreshes every 60 seconds.
  </p><p>
   The new alarms will be separated into the following categories:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="guimenu ">Critical</span> - Open alarms, identified by red indicator.
    </p></li><li class="listitem "><p>
     <span class="guimenu ">Warning</span> - Open alarms, identified by yellow indicator.
    </p></li><li class="listitem "><p>
     <span class="guimenu ">Unknown</span> - Open alarms, identified by gray indicator.
     Unknown will be the status of an alarm that has stopped receiving a
     metric. This can be caused by the following conditions:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       An alarm exists for a service or component that is not installed in the
       environment.
      </p></li><li class="listitem "><p>
       An alarm exists for a virtual machine or node that previously existed
       but has been removed without the corresponding alarms being removed.
      </p></li><li class="listitem "><p>
       There is a gap between the last reported metric and the next metric.
      </p></li></ul></div></li><li class="listitem "><p>
     <span class="guimenu ">Open</span> - Complete list of open alarms.
    </p></li><li class="listitem "><p>
     <span class="guimenu ">Total</span> - Complete list of alarms, may include
     Acknowledged and Resolved alarms.
    </p></li></ul></div></div><div class="sect2" id="id-1.5.18.12.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">16.10.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Alarm Summary</span> <a title="Permalink" class="permalink" href="#id-1.5.18.12.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-ops-console-opsconsole_dashboard.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-ops-console-opsconsole_dashboard.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Each service or group of services have a dedicated card displaying related
   alarms.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="guimenu ">Critical</span> - Open alarms, identified by red indicator.
    </p></li><li class="listitem "><p>
     <span class="guimenu ">Warning</span> - Open alarms, identified by yellow indicator.
    </p></li><li class="listitem "><p>
     <span class="guimenu ">Unknown</span> - Open alarms, identified by gray indicator.
     Unknown will be the status of an alarm that has stopped receiving a
     metric. This can be caused by the following conditions:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       An alarm exists for a service or component that is not installed in the
       environment.
      </p></li><li class="listitem "><p>
       An alarm exists for a virtual machine or node that previously existed
       but has been removed without the corresponding alarms being removed.
      </p></li><li class="listitem "><p>
       There is a gap between the last reported metric and the next metric.
      </p></li></ul></div></li><li class="listitem "><p>
     <span class="guimenu ">Open</span> - Complete list of open alarms.
    </p></li><li class="listitem "><p>
     <span class="guimenu ">Total</span> - Complete list of alarms, may include
     Acknowledged and Resolved alarms.
    </p></li></ul></div></div></div></div><div class="chapter " id="bura-overview"><div class="titlepage"><div><div><h1 class="title"><span class="number">17 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Backup and Restore</span> <a title="Permalink" class="permalink" href="#bura-overview">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/bura-bura_overview.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-bura_overview.xml</li><li><span class="ds-label">ID: </span>bura-overview</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#topic-bxm-gxr-st"><span class="number">17.1 </span><span class="name">Manual Backup Overview</span></a></span></dt><dt><span class="section"><a href="#topic-jsc-qps-qt"><span class="number">17.2 </span><span class="name">Enabling Backups to a Remote Server</span></a></span></dt><dt><span class="section"><a href="#bura-manual-backup"><span class="number">17.3 </span><span class="name">Manual Backup and Restore Procedures</span></a></span></dt><dt><span class="section"><a href="#full-recovery-test"><span class="number">17.4 </span><span class="name">Full Disaster Recovery Test</span></a></span></dt></dl></div></div><p>
  The following sections cover backup and restore operations. Before installing
  your cloud, there are several things you must do so that you achieve the
  backup and recovery results you need. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> comes with playbooks and
  procedures to recover the control plane from various disaster scenarios.
 </p><p>
  As of SUSE <span class="productname">OpenStack</span> Cloud 9, Freezer (a distributed backup restore and disaster recovery
  service) is no longer supported; backup and restore are manual operations.
 </p><p>
  Consider <a class="xref" href="#topic-jsc-qps-qt" title="17.2. Enabling Backups to a Remote Server">Section 17.2, “Enabling Backups to a Remote Server”</a> in case you lose cloud servers
  that back up and restore services.
 </p><p>
  The following features are supported:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    File system backup using a point-in-time snapshot.
   </p></li><li class="listitem "><p>
    Strong encryption: AES-256-CFB.
   </p></li><li class="listitem "><p>
    MariaDB database backup with LVM snapshot.
   </p></li><li class="listitem "><p>
    Restoring your data from a previous backup.
   </p></li><li class="listitem "><p>
    Low storage requirement: backups are stored as compressed files.
   </p></li><li class="listitem "><p>
    Flexible backup (both incremental and differential).
   </p></li><li class="listitem "><p>
    Data is archived in GNU Tar format for file-based incremental backup and
    restore.
   </p></li><li class="listitem "><p>
    When a key is provided, Open SSL is used to encrypt data (AES-256-CFB).
   </p></li></ul></div><div class="sect1" id="topic-bxm-gxr-st"><div class="titlepage"><div><div><h2 class="title"><span class="number">17.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Manual Backup Overview</span> <a title="Permalink" class="permalink" href="#topic-bxm-gxr-st">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/bura-manual_services.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-manual_services.xml</li><li><span class="ds-label">ID: </span>topic-bxm-gxr-st</li></ul></div></div></div></div><p>
  This section covers manual backup and some restore processes. Full
  documentation for restore operations is in <a class="xref" href="#unplanned-maintenance" title="15.2. Unplanned System Maintenance">Section 15.2, “Unplanned System Maintenance”</a>.To back up outside the cluster,
  refer to <a class="xref" href="#topic-jsc-qps-qt" title="17.2. Enabling Backups to a Remote Server">Section 17.2, “Enabling Backups to a Remote Server”</a>. Backups of the following types
  of resources are covered:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="formalpara-title">Cloud Lifecycle Manager Data. </span>
     All important information on the Cloud Lifecycle Manager
    </p></li><li class="listitem "><p><span class="formalpara-title">MariaDB database that is part of the Control Plane. </span>
     The MariaDB database contains most of the data needed to restore
     services. MariaDB supports full back up and recovery for all services.
     Logging data in Elasticsearch is not backed up. swift objects are
     not backed up because of the redundant nature of swift.
    </p></li><li class="listitem "><p><span class="formalpara-title">swift Rings used in the swift storage deployment. </span>
     swift rings are backed up so that you can recover more quickly than
     rebuilding with swift. swift can rebuild the rings without this backup
     data, but automatically rebuilding the rings is slower than restoring
     from a backup.
    </p></li><li class="listitem "><p><span class="formalpara-title">Audit Logs. </span>
     Audit Logs are backed up to provide retrospective information and
     statistical data for performance and security purposes.
    </p></li></ul></div><p>
  The following services will be backed up. Specifically, the data needed to
  restore the services is backed up. This includes databases and
  configuration-related files.
 </p><div id="id-1.5.19.7.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   Data content for some services is not backed up, as indicated below.
  </p></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    ceilometer. There is no backup of metrics data.
   </p></li><li class="listitem "><p>
    cinder. There is no backup of the volumes.
   </p></li><li class="listitem "><p>
    glance. There is no backup of the images.
   </p></li><li class="listitem "><p>
    heat
   </p></li><li class="listitem "><p>
    horizon
   </p></li><li class="listitem "><p>
    keystone
   </p></li><li class="listitem "><p>
    neutron
   </p></li><li class="listitem "><p>
    nova. There is no backup of the images.
   </p></li><li class="listitem "><p>
    swift. There is no backup of the objects. swift has its own high
    availability and redundancy. swift rings are backed up. Although swift
    can rebuild the rings itself, restoring from backup is faster.
   </p></li><li class="listitem "><p>
    Operations Console
   </p></li><li class="listitem "><p>
    monasca. There is no backup of the metrics.
   </p></li></ul></div></div><div class="sect1" id="topic-jsc-qps-qt"><div class="titlepage"><div><div><h2 class="title"><span class="number">17.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling Backups to a Remote Server</span> <a title="Permalink" class="permalink" href="#topic-jsc-qps-qt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/bura-cloud_control_plane_backup.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-cloud_control_plane_backup.xml</li><li><span class="ds-label">ID: </span>topic-jsc-qps-qt</li></ul></div></div></div></div><p>
   We recommend that you set up a remote server to store your backups, so that
   you can restore the control plane nodes. This may be necessary if you lose
   all of your control plane nodes at the same time.
  </p><div id="id-1.5.19.8.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    A remote backup server must be set up before proceeding.
   </p></div><p>
   You do not have to restore from the remote server if only one or two control
   plane nodes are lost. In that case, the control planes can be recovered from
   the data on a remaining control plane node following the restore procedures
   in <a class="xref" href="#recovering-controller-nodes" title="15.2.3.2. Recovering the Control Plane">Section 15.2.3.2, “Recovering the Control Plane”</a>.
  </p><div class="sect2" id="secure-ssh-bu"><div class="titlepage"><div><div><h3 class="title"><span class="number">17.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Securing your SSH backup server</span> <a title="Permalink" class="permalink" href="#secure-ssh-bu">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/bura-cloud_control_plane_backup.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-cloud_control_plane_backup.xml</li><li><span class="ds-label">ID: </span>secure-ssh-bu</li></ul></div></div></div></div><p>
   You can do the following to harden an SSH server:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Disable root login
    </p></li><li class="listitem "><p>
     Move SSH to a non-default port (the default SSH port is 22)
    </p></li><li class="listitem "><p>
     Disable password login (only allow RSA keys)
    </p></li><li class="listitem "><p>
     Disable SSH v1
    </p></li><li class="listitem "><p>
     Authorize Secure File Transfer Protocol (SFTP) only for the designated
     backup user (disable SSH shell)
    </p></li><li class="listitem "><p>
     Firewall SSH traffic to ensure it comes from the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> address range
    </p></li><li class="listitem "><p>
     Install a <a class="link" href="https://www.fail2ban.org" target="_blank">Fail2Ban</a> solution
    </p></li><li class="listitem "><p>
     Restrict users who are allowed to SSH
    </p></li><li class="listitem "><p>
     Additional suggestions are available online
    </p></li></ul></div><p>
   Remove the key pair generated earlier on the backup server; the only thing
   needed is <code class="filename">.ssh/authorized_keys</code>. You can remove the
   <code class="filename">.ssh/id_rsa</code> and <code class="filename">.ssh/id_rsa.pub</code>
   files. Be sure to save a backup of them.
  </p></div><div class="sect2" id="id-1.5.19.8.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">17.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">General tips</span> <a title="Permalink" class="permalink" href="#id-1.5.19.8.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/bura-cloud_control_plane_backup.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-cloud_control_plane_backup.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Provide adequate space in the directory that is used for backup.
    </p></li><li class="listitem "><p>
     Monitor the space left on that directory.
    </p></li><li class="listitem "><p>
     Keep the system up to date on that server.
    </p></li></ul></div></div></div><div class="sect1" id="bura-manual-backup"><div class="titlepage"><div><div><h2 class="title"><span class="number">17.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Manual Backup and Restore Procedures</span> <a title="Permalink" class="permalink" href="#bura-manual-backup">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/bura-manual_backup.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-manual_backup.xml</li><li><span class="ds-label">ID: </span>bura-manual-backup</li></ul></div></div></div></div><p>
  Each backup requires the following steps:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Create a snapshot.
   </p></li><li class="step "><p>
    Mount the snapshot.
   </p></li><li class="step "><p>
    Generate a TAR archive and save it.
   </p></li><li class="step "><p>
    Unmount and delete the snapshot.
   </p></li></ol></div></div><div class="sect2" id="clm-data-backup"><div class="titlepage"><div><div><h3 class="title"><span class="number">17.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cloud Lifecycle Manager Data Backup</span> <a title="Permalink" class="permalink" href="#clm-data-backup">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/bura-manual_backup.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-manual_backup.xml</li><li><span class="ds-label">ID: </span>clm-data-backup</li></ul></div></div></div></div><p>
   The following procedure is used for each of the seven
   <em class="replaceable ">BACKUP_TARGETS</em> (list below). Incremental backup
   instructions follow the full backup procedure. For both full and incremental
   backups, the last step of the procedure is to unmount and delete the
   snapshot after the TAR archive has been created and saved. A new snapshot
   must be created every time a backup is created.
  </p><div class="procedure " id="manual-backup-setup"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 17.1: </span><span class="name">Manual Backup Setup </span><a title="Permalink" class="permalink" href="#manual-backup-setup">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Create a snapshot on the Cloud Lifecycle Manager in (<code class="literal">ardana-vg</code>), the
     location where all Cloud Lifecycle Manager data is stored.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo lvcreate --size 2G --snapshot --permission r \
--name lvm_clm_snapshot /dev/ardana-vg/root</pre></div><div id="id-1.5.19.9.4.3.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      If you have stored additional data or files in your
      <code class="literal">ardana-vg</code> directory, you may need more space than the
      2G indicated for the <code class="literal">size</code> parameter. In this
      situation, create a preliminary TAR archive with the
      <code class="command">tar</code> command on the directory before creating a
      snapshot. Set the <code class="literal">size</code> snapshot parameter larger than
      the size of the archive.
     </p></div></li><li class="step "><p>
     Mount the snapshot
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo mkdir /var/tmp/clm_snapshot
<code class="prompt user">ardana &gt; </code>sudo mount -o ro /dev/ardana-vg/lvm_clm_snapshot /var/tmp/clm_snapshot</pre></div></li><li class="step "><p>
     Generate a TAR archive (does not apply to incremental backups) with an
     appropriate
     <code class="filename"><em class="replaceable ">BACKUP_TAR_ARCHIVE_NAME</em>.tar.gz</code>
     backup file for each of the following <em class="replaceable ">BACKUP_TARGETS</em>.
    </p><p>
     <span class="bold"><strong>Backup Targets</strong></span>
    </p><div class="itemizedlist " id="clm-backup-targets"><ul class="itemizedlist"><li class="listitem "><p>
       home
      </p></li><li class="listitem "><p>
       ssh
      </p></li><li class="listitem "><p>
       shadow
      </p></li><li class="listitem "><p>
       passwd
      </p></li><li class="listitem "><p>
       group
      </p></li></ul></div><p>
     The backup TAR archive should contain only the necessary data; nothing
     extra. Some of the archives will be stored as directories, others as
     files. The backup commands are slightly different for each type.
    </p><p>
     If the <em class="replaceable ">BACKUP_TARGET</em>
     is a directory, then that directory must be appended to
     <code class="filename">/var/tmp/clm_snapshot/</code><em class="replaceable ">TARGET_DIR</em>. If the <em class="replaceable ">BACKUP_TARGET</em>
     is a file, then its parent directory must be appended to <code class="filename">/var/tmp/clm_snapshot/</code>.
    </p><p>
     In the commands that follow, replace
     <em class="replaceable ">BACKUP_TARGET</em> with the appropriate
     <em class="replaceable ">BACKUP_PATH</em> (replacement table is below).
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar --create -z --warning=none --no-check-device \
--one-file-system --preserve-permissions --same-owner --seek \
--ignore-failed-read --file <em class="replaceable ">BACKUP_TAR_ARCHIVE_NAME</em>.tar.gz -C \
/var/tmp/clm_snapshot<em class="replaceable ">TARGET_DIR</em>|<em class="replaceable ">BACKUP_TARGET_WITHOUT_LEADING_DIR</em></pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       If <em class="replaceable ">BACKUP_TARGET</em> is a directory, replace
       <em class="replaceable ">TARGET_DIR</em> with
       <em class="replaceable ">BACKUP_PATH</em>.
      </p><p>
       For example, where
       <em class="replaceable ">BACKUP_PATH</em>=<code class="filename">/etc/ssh/</code> (a
       directory):
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar --create -z --warning=none --no-check-device \
--one-file-system --preserve-permissions --same-owner --seek \
--ignore-failed-read --file ssh.tar.gz -C /var/tmp/clm_snapshot/etc/ssh .</pre></div></li><li class="listitem "><p>
       If <em class="replaceable ">BACKUP_TARGET</em> is a file (not a directory),
       replace <em class="replaceable ">TARGET_DIR</em> with the parent directory
       of <em class="replaceable ">BACKUP_PATH</em>.
      </p><p>
       For example, where
       <em class="replaceable ">BACKUP_PATH</em>=<code class="filename">/etc/passwd</code>
       (a file):
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar --create -z --warning=none --no-check-device \
--one-file-system --preserve-permissions --same-owner --seek \
--ignore-failed-read --file passwd.tar.gz -C /var/tmp/clm_snapshot/etc/passwd</pre></div></li></ul></div></li><li class="step "><p>
     Save the TAR archive to the remote server.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>scp <em class="replaceable ">TAR_ARCHIVE</em> <em class="replaceable ">USER@REMOTE_SERVER</em></pre></div></li><li class="step "><p>
     Use the following commands to unmount and delete a snapshot.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo umount -l -f /var/tmp/clm_snapshot; rm -rf /var/tmp/clm_snapshot
<code class="prompt user">ardana &gt; </code>sudo lvremove -f /dev/ardana-vg/lvm_clm_snapshot</pre></div></li></ol></div></div><p>
   The table below shows Cloud Lifecycle Manager <code class="literal">backup_targets</code> and their
   respective <code class="literal">backup_paths</code>.
  </p><div class="table" id="id-1.5.19.9.4.5"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 17.1: </span><span class="name">Cloud Lifecycle Manager Backup Paths </span><a title="Permalink" class="permalink" href="#id-1.5.19.9.4.5">#</a></h6></div><div class="table-contents"><table class="table" summary="Cloud Lifecycle Manager Backup Paths" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th>
       <p>
        backup_name
       </p>
      </th><th>
       <p>
        backup_path
       </p>
      </th></tr></thead><tbody><tr><td>
       <p>
        home_backup
       </p>
      </td><td>
       <p>
        /var/lib/ardana (file)
       </p>
      </td></tr><tr><td>
       <p>
        etc_ssh_backup
       </p>
      </td><td>
       <p>
        /etc/ssh/ (directory)
       </p>
      </td></tr><tr><td>
       <p>
        shadow_backup
       </p>
      </td><td>
       <p>
        /etc/shadow (file)
       </p>
      </td></tr><tr><td>
       <p>
        passwd_backup
       </p>
      </td><td>
       <p>
        /etc/passwd (file)
       </p>
      </td></tr><tr><td>
       <p>
        group_backup
       </p>
      </td><td>
       <p>
        /etc/group (file)
       </p>
      </td></tr></tbody></table></div></div><div class="sect3" id="id-1.5.19.9.4.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">17.3.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cloud Lifecycle Manager Incremental Backup</span> <a title="Permalink" class="permalink" href="#id-1.5.19.9.4.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/bura-manual_backup.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-manual_backup.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    Incremental backups require a <code class="literal">meta</code> file. If you use the
    incremental backup option, a meta file must be included in the
    <code class="command">tar</code> command in the initial backup and whenever you do an
    incremental backup. A copy of the original <code class="literal">meta</code> file
    should be stored in each backup. The <code class="literal">meta</code> file is used
    to determine the incremental changes from the previous backup, so it is
    rewritten with each incremental backup.
   </p><p>
    Versions are useful for incremental backup because they provide a way
    to differentiate between each backup. Versions are included in the
    <code class="command">tar</code> command.
   </p><p>
    Every incremental backup requires creating and mounting a separate
    snapshot. After the TAR archive is created, the snapshot is unmounted and
    deleted.
   </p><p>
    To prepare for incremental backup, follow the steps in <a class="xref" href="#manual-backup-setup" title="Manual Backup Setup">Procedure 17.1, “Manual Backup Setup”</a> with the following differences in the
    commands for generating a <code class="literal">tar</code> archive.
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      First time full backup
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar --create -z --warning=none --no-check-device \
--one-file-system --preserve-permissions --same-owner --seek \
--ignore-failed-read <span class="bold"><strong>--listed-incremental=PATH_TO_YOUR_META</strong></span> \
--file <em class="replaceable ">BACKUP_TAR_ARCHIVE_NAME</em>.tar.gz -C \
/var/tmp/clm_snapshot<em class="replaceable ">TARGET_DIR</em>|<em class="replaceable ">BACKUP_TARGET_WITHOUT_LEADING_DIR</em></pre></div><p>
      For example, where
      <em class="replaceable ">BACKUP_PATH</em>=<code class="filename">/etc/ssh/</code>
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar --create -z --warning=none --no-check-device \
--one-file-system --preserve-permissions --same-owner --seek --ignore-failed-read \
<span class="bold"><strong>--listed-incremental=mysshMeta</strong></span> --file ssh.tar.gz -C \
/var/tmp/clm_snapshot/etc/ssh .</pre></div></li><li class="listitem "><p>
      Incremental backup
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar --create -z --warning=none --no-check-device \
--one-file-system --preserve-permissions --same-owner --seek \
--ignore-failed-read <span class="bold"><strong>--listed-incremental=PATH_TO_YOUR_META</strong></span>\
--file <em class="replaceable ">BACKUP_TAR_ARCHIVE_NAME</em><span class="bold"><strong>_VERSION</strong></span>.tar.gz -C \
/var/tmp/clm_snapshot<em class="replaceable ">TARGET_DIR</em>|<em class="replaceable ">BACKUP_TARGET_WITHOUT_LEADING_DIR</em></pre></div><p>
      For example, where
      <em class="replaceable ">BACKUP_PATH</em>=<code class="filename">/etc/ssh/</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar --create -z --warning=none --no-check-device \
--one-file-system --preserve-permissions --same-owner --seek --ignore-failed-read \
<span class="bold"><strong>--listed-incremental=mysshMeta</strong></span> --file \
<span class="bold"><strong>ssh_v1.tar.gz</strong></span> -C \
/var/tmp/clm_snapshot/etc/ssh .</pre></div></li></ul></div><p>
    After creating an incremental backup, use the following commands to unmount
    and delete a snapshot.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo umount -l -f /var/tmp/clm_snapshot; rm -rf /var/tmp/clm_snapshot
<code class="prompt user">ardana &gt; </code>sudo lvremove -f /dev/ardana-vg/lvm_clm_snapshot</pre></div></div><div class="sect3" id="manual-backup-encryption"><div class="titlepage"><div><div><h4 class="title"><span class="number">17.3.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Encryption</span> <a title="Permalink" class="permalink" href="#manual-backup-encryption">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/bura-manual_backup.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-manual_backup.xml</li><li><span class="ds-label">ID: </span>manual-backup-encryption</li></ul></div></div></div></div><p>
    When a key is provided, Open SSL is used to encrypt data
    (AES-256-CFB). Backup files can be encrypted with the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo openssl enc -aes-256-cfb -pass file:<em class="replaceable ">ENCRYPT_PASS_FILE_PATH</em> -in \
<em class="replaceable ">YOUR_BACKUP_TAR_ARCHIVE_NAME</em>.tar.gz -out <em class="replaceable ">YOUR_BACKUP_TAR_ARCHIVE_NAME</em>.tar.gz.enc</pre></div><p>
    For example, using the <code class="filename">ssh.tar.gz</code> generated above:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo openssl enc  -aes-256-cfb -pass file:myEncFile -in ssh.tar.gz  -out ssh.tar.gz.enc</pre></div></div></div><div class="sect2" id="mariadb-database-backup"><div class="titlepage"><div><div><h3 class="title"><span class="number">17.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">MariaDB Database Backup</span> <a title="Permalink" class="permalink" href="#mariadb-database-backup">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/bura-manual_backup.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-manual_backup.xml</li><li><span class="ds-label">ID: </span>mariadb-database-backup</li></ul></div></div></div></div><p>
   When backing up MariaDB, the following process must be performed on
   <span class="bold"><strong>all nodes</strong></span> in the cluster. It is similar to
   the backup procedure above for the Cloud Lifecycle Manager (see <a class="xref" href="#manual-backup-setup" title="Manual Backup Setup">Procedure 17.1, “Manual Backup Setup”</a>). The difference is the addition of SQL
   commands, which are run with the <code class="filename">create_db_snapshot.yml</code>
   playbook.
  </p><p>
   Create the <code class="filename">create_db_snapshot.yml</code> file in
   <code class="filename">~/scratch/ansible/next/ardana/ansible/</code> on the deployer
   with the following content:
  </p><div class="verbatim-wrap"><pre class="screen">- hosts: FND-MDB
vars:
 - snapshot_name: lvm_mysql_snapshot
 - lvm_target: /dev/ardana-vg/mysql

 tasks:
 - name: Cleanup old snapshots
   become: yes
   shell: |
    lvremove -f /dev/ardana-vg/{{ snapshot_name }}
   ignore_errors: True

 - name: Create snapshot
   become: yes
   shell: |
    lvcreate --size 2G --snapshot --permission r --name {{ snapshot_name }} {{ lvm_target }}
   register: snapshot_st
   ignore_errors: True

 - fail:
     msg: "Fail to create snapshot on  {{ lvm_target }}"
   when: snapshot_st.rc != 0</pre></div><div id="id-1.5.19.9.5.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     Verify the validity of the <code class="literal">lvm_target</code> variable
     (which refers to the actual database LVM volume) before proceeding with
     the backup.
    </p></div><p>
    <span class="bold"><strong>Doing the MariaDB backup</strong></span>
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     We recommend storing the MariaDB version with your backup. The following
     command saves the MariaDB version as
     <em class="replaceable ">MARIADB_VER</em>.
    </p><div class="verbatim-wrap"><pre class="screen">mysql -V | grep -Eo '(\S+?)-MariaDB' &gt; <em class="replaceable ">MARIADB_VER</em></pre></div></li><li class="step "><p>
     Open a MariaDB client session on all controllers.
    </p></li><li class="step "><p>
     Run the command to spread <code class="literal">read lock</code> on all controllers
     and keep the MariaDB session open.
    </p><div class="verbatim-wrap"><pre class="screen">&gt;&gt; FLUSH TABLES WITH READ LOCK;</pre></div></li><li class="step "><p>
     Open a new terminal and run the
     <code class="filename">create_db_snapshot.yml</code> playbook created above.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts create_db_snapshot.yml</pre></div></li><li class="step "><p>
     Go back to the open MariaDB session and run the command to flush the lock on
     all controllers.
    </p><div class="verbatim-wrap"><pre class="screen">&gt;&gt; UNLOCK TABLES;</pre></div></li><li class="step "><p>
     Mount the snapshot
    </p><div class="verbatim-wrap"><pre class="screen">dbnode&gt;&gt; mkdir /var/tmp/mysql_snapshot
dbnode&gt;&gt; sudo mount -o ro /dev/ardana-vg/lvm_mysql_snapshot  /var/tmp/mysql_snapshot</pre></div></li><li class="step "><p>
     On each database node, generate a TAR archive with an appropriate
     <code class="filename"><em class="replaceable ">BACKUP_TAR_ARCHIVE_NAME</em>.tar.gz</code>
     backup file for the <em class="replaceable ">BACKUP_TARGET</em>.
    </p><p>
     The <code class="literal">backup_name</code> is <code class="literal">mysql_backup</code> and
     the <code class="literal">backup_path</code>
     (<em class="replaceable ">BACKUP_TARGET</em>) is
     <code class="filename">/var/lib/mysql/</code>.
    </p><div class="verbatim-wrap"><pre class="screen">dbnode&gt;&gt; sudo tar --create -z --warning=none --no-check-device \
--one-file-system --preserve-permissions --same-owner --seek --ignore-failed-read \
--file mydb.tar.gz /var/tmp/mysql_snapshot/var/lib/mysql .</pre></div></li><li class="step "><p>
     Unmount and delete the MariaDB snapshot on each database node.
    </p><div class="verbatim-wrap"><pre class="screen">dbnode&gt;&gt; sudo  umount -l -f /var/tmp/mysql_snapshot; \
sudo rm -rf /var/tmp/mysql_snapshot; sudo lvremove -f /dev/ardana-vg/lvm_mysql_snapshot</pre></div></li></ol></div></div><div class="sect3" id="id-1.5.19.9.5.8"><div class="titlepage"><div><div><h4 class="title"><span class="number">17.3.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Incremental MariaDB Database Backup</span> <a title="Permalink" class="permalink" href="#id-1.5.19.9.5.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/bura-manual_backup.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-manual_backup.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    Incremental backups require a <code class="literal">meta</code> file. If you use the
    incremental backup option, a meta file must be included in the
    <code class="command">tar</code> command in the initial backup and whenever you do an
    incremental backup. A copy of the original <code class="literal">meta</code> file
    should be stored in each backup. The <code class="literal">meta</code> file is used
    to determine the incremental changes from the previous backup, so it is
    rewritten with each incremental backup.
   </p><p>
    Versions are useful for incremental backup because they provide a way
    to differentiate between each backup. Versions are included in the
    <code class="command">tar</code> command.
   </p><p>
    To prepare for incremental backup, follow the steps in the previous section
    except for the <code class="command">tar</code> commands. Incremental backup
    <code class="command">tar</code> commands must have additional information.
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      First time MariaDB database full backup
     </p><div class="verbatim-wrap"><pre class="screen">dbnode&gt;&gt; sudo tar --create -z --warning=none --no-check-device \
--one-file-system --preserve-permissions --same-owner --seek \
--ignore-failed-read <span class="bold"><strong>--listed-incremental=PATH_TO_YOUR_DB_META</strong></span> \
--file mydb.tar.gz -C /var/tmp/mysql_snapshot/var/lib/mysql .</pre></div><p>
      For example, where
      <em class="replaceable ">BACKUP_PATH</em>=<code class="filename">/var/lib/mysql/</code>:
     </p><div class="verbatim-wrap"><pre class="screen">dbnode&gt;&gt; sudo tar --create -z --warning=none --no-check-device \
--one-file-system --preserve-permissions --same-owner --seek --ignore-failed-read \
<span class="bold"><strong>--listed-incremental=mydbMeta</strong></span> --file mydb.tar.gz -C \
/var/tmp/mysql_snapshot/var/lib/mysql .</pre></div></li><li class="listitem "><p>
      Incremental MariaDB database backup
     </p><div class="verbatim-wrap"><pre class="screen">dbnode&gt;&gt; sudo tar --create -z --warning=none --no-check-device \
--one-file-system --preserve-permissions --same-owner --seek \
--ignore-failed-read <span class="bold"><strong>--listed-incremental=PATH_TO_YOUR_META</strong></span>\
--file <em class="replaceable ">BACKUP_TAR_ARCHIVE_NAME</em><span class="bold"><strong>_VERSION</strong></span>.tar.gz -C \
/var/tmp/clm_snapshot<em class="replaceable ">TARGET_DIR</em></pre></div><p>
      For example, where
      <em class="replaceable ">BACKUP_PATH</em>=<code class="filename">/var/lib/mysql/</code>:
     </p><div class="verbatim-wrap"><pre class="screen">dbnode&gt;&gt; sudo tar --create -z --warning=none --no-check-device \
--one-file-system --preserve-permissions --same-owner --seek --ignore-failed-read \
<span class="bold"><strong>--listed-incremental=mydbMeta</strong></span> --file \
<span class="bold"><strong>mydb_v1.tar.gz</strong></span> -C /var/tmp/mysql_snapshot/var/lib/mysql .</pre></div></li></ul></div><p>
    After creating and saving the TAR archive, unmount and delete the snapshot.
   </p><div class="verbatim-wrap"><pre class="screen">dbnode&gt;&gt; sudo  umount -l -f /var/tmp/mysql_snapshot; \
sudo rm -rf /var/tmp/mysql_snapshot; sudo lvremove -f /dev/ardana-vg/lvm_mysql_snapshot</pre></div></div><div class="sect3" id="id-1.5.19.9.5.9"><div class="titlepage"><div><div><h4 class="title"><span class="number">17.3.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">MariaDB Database Encryption</span> <a title="Permalink" class="permalink" href="#id-1.5.19.9.5.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/bura-manual_backup.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-manual_backup.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p> Encrypt your MariaDB database backup following the instructions
     in <a class="xref" href="#manual-backup-encryption" title="17.3.1.2. Encryption">Section 17.3.1.2, “Encryption”</a>
     </p></li><li class="step "><p>
      Upload your <em class="replaceable ">BACKUP_TARGET</em>.tar.gz to your
      preferred remote server.
     </p></li></ol></div></div></div></div><div class="sect2" id="swift-ring-backup"><div class="titlepage"><div><div><h3 class="title"><span class="number">17.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">swift Ring Backup</span> <a title="Permalink" class="permalink" href="#swift-ring-backup">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/bura-manual_backup.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-manual_backup.xml</li><li><span class="ds-label">ID: </span>swift-ring-backup</li></ul></div></div></div></div><p>
   The following procedure is used to back up swift rings. It is similar to
   the Cloud Lifecycle Manager backup (see <a class="xref" href="#manual-backup-setup" title="Manual Backup Setup">Procedure 17.1, “Manual Backup Setup”</a>).
  </p><div id="id-1.5.19.9.6.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    The steps must be performed only on the building server (For more
    information, see <a class="xref" href="#topic-rtc-s3t-mt" title="18.6.2.4. Identifying the Swift Ring Building Server">Section 18.6.2.4, “Identifying the Swift Ring Building Server”</a>.).
   </p></div><p>
   The <code class="literal">backup_name</code> is
   <code class="literal">swift_builder_dir_backup</code> and the
   <code class="literal">backup_path</code> is <code class="filename">/etc/swiftlm/</code>.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Create a snapshot
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo lvcreate --size 2G --snapshot --permission r \
--name lvm_root_snapshot /dev/ardana-vg/root</pre></div></li><li class="step "><p>
     Mount the snapshot
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>mkdir /var/tmp/root_snapshot; sudo mount -o ro \
/dev/ardana-vg/lvm_root_snapshot /var/tmp/root_snapshot</pre></div></li><li class="step "><p>
     Create the TAR archive
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar --create -z --warning=none --no-check-device \
--one-file-system --preserve-permissions --same-owner --seek --ignore-failed-read \
--file swring.tar.gz -C /var/tmp/root_snapshot/etc/swiftlm .</pre></div></li><li class="step "><p>
     Upload your <code class="filename">swring.tar.gz</code> TAR archive to your preferred remote server.
    </p></li><li class="step "><p>
     Unmount and delete the snapshot
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo umount -l -f /var/tmp/root_snapshot; sudo rm -rf \
/var/tmp/root_snapshot; sudo lvremove -f /dev/ardana-vg/lvm_root_snapshot</pre></div></li></ol></div></div></div><div class="sect2" id="manual-audit-log-bur"><div class="titlepage"><div><div><h3 class="title"><span class="number">17.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Audit Log Backup and Restore</span> <a title="Permalink" class="permalink" href="#manual-audit-log-bur">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/bura-manual_backup.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-manual_backup.xml</li><li><span class="ds-label">ID: </span>manual-audit-log-bur</li></ul></div></div></div></div><div class="sect3" id="id-1.5.19.9.7.2"><div class="titlepage"><div><div><h4 class="title"><span class="number">17.3.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Audit Log Backup</span> <a title="Permalink" class="permalink" href="#id-1.5.19.9.7.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/bura-manual_backup.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-manual_backup.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The following procedure is used to back up Audit Logs. It is similar to the
   Cloud Lifecycle Manager backup (see <a class="xref" href="#manual-backup-setup" title="Manual Backup Setup">Procedure 17.1, “Manual Backup Setup”</a>). The steps must be
   performed on all nodes; there will be a backup TAR archive for each
   node. Before performing the following steps, run through <a class="xref" href="#topic-enable-audit-logs" title="13.2.7.2. Enable Audit Logging">Section 13.2.7.2, “Enable Audit Logging”</a> .
  </p><p>
   The <code class="literal">backup_name</code> is
   <code class="literal">audit_log_backup</code> and the
   <code class="literal">backup_path</code> is <code class="filename">/var/audit</code>.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Create a snapshot
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo lvcreate --size 2G --snapshot --permission r --name \
lvm_root_snapshot /dev/ardana-vg/root</pre></div></li><li class="step "><p>
     Mount the snapshot
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>mkdir /var/tmp/root_snapshot; sudo mount -o ro \
/dev/ardana-vg/lvm_root_snapshot /var/tmp/root_snapshot</pre></div></li><li class="step "><p>
     Create the TAR archive
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar --create -z --warning=none --no-check-device \
--one-file-system --preserve-permissions --same-owner --seek --ignore-failed-read \
--file audit.tar.gz -C /var/tmp/root_snapshot/var/audit .</pre></div></li><li class="step "><p>
     Upload your <code class="filename">audit.tar.gz</code> TAR archive to your
     preferred remote server.
    </p></li><li class="step "><p>
     Unmount and delete a snapshot
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo umount -l -f /var/tmp/root_snapshot; sudo rm -rf \
/var/tmp/root_snapshot; sudo lvremove -f /dev/ardana-vg/lvm_root_snapshot</pre></div></li></ol></div></div></div><div class="sect3" id="id-1.5.19.9.7.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">17.3.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Audit Logs Restore</span> <a title="Permalink" class="permalink" href="#id-1.5.19.9.7.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/bura-manual_backup.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>bura-manual_backup.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    Restore the Audit Logs backup with the following commands
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Retrieve the Audit Logs TAR archive
     </p></li><li class="step "><p>
      Extract the TAR archive to the proper backup location
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros \
--warning=none --overwrite --directory /var/audit/  -f audit.tar.gz</pre></div></li></ol></div></div></div></div></div><div class="sect1" id="full-recovery-test"><div class="titlepage"><div><div><h2 class="title"><span class="number">17.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Full Disaster Recovery Test</span> <a title="Permalink" class="permalink" href="#full-recovery-test">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span>full-recovery-test</li></ul></div></div></div></div><p>
  Full Disaster Recovery Test
 </p><div class="sect2" id="id-1.5.19.10.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">17.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">High Level View of the Recovery Process</span> <a title="Permalink" class="permalink" href="#id-1.5.19.10.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Back up the control plane using the manual backup procedure
    </p></li><li class="step "><p>
     Backup the Cassandra Database
    </p></li><li class="step "><p>
     Re-install Controller 1 with the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> ISO
    </p></li><li class="step "><p>
     Use manual restore steps to recover deployment data (and model)
    </p></li><li class="step "><p>
     Re-install <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> on Controllers 1, 2, 3
    </p></li><li class="step "><p>
     Recover the backup of the MariaDB database
    </p></li><li class="step "><p>
     Recover the Cassandra Database
    </p></li><li class="step "><p>
     Verify testing
    </p></li></ol></div></div></div><div class="sect2" id="id-1.5.19.10.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">17.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Description of the testing environment</span> <a title="Permalink" class="permalink" href="#id-1.5.19.10.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The testing environment is similar to the Entry Scale model.
  </p><p>
   It uses five servers: three Control Nodes and two Compute Nodes.
  </p><p>
   The controller node has three disks. The first is reserved for the system;
   the others are used for swift.
  </p><div id="id-1.5.19.10.4.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    For this Disaster Recovery test, data has been saved on disks 2 and 3
    of the swift controllers, which allows for swift objects to be restored
    the recovery. If these disks were also wiped, swift data would be lost,
    but the procedure would not change. The only difference is that glance
    images would be lost and would have to be uploaded again.
   </p><p>
    Unless specified otherwise, all commands should be executed on controller
    1, which is also the deployer node.
   </p></div></div><div class="sect2" id="id-1.5.19.10.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">17.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Pre-Disaster testing</span> <a title="Permalink" class="permalink" href="#id-1.5.19.10.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   In order to validate the procedure after recovery, we need to create some
   workloads.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Source the service credential file
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc</pre></div></li><li class="step "><p>
     Copy an image to the platform and create a glance image with it. In this
     example, Cirros is used
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack image create --disk-format raw --container-format \
bare --public --file ~/cirros-0.3.5-x86_64-disk.img cirros</pre></div></li><li class="step "><p>
     Create a network
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network create test_net</pre></div></li><li class="step "><p>
     Create a subnet
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack subnet create 07c35d11-13f9-41d4-8289-fa92147b1d44 192.168.42.0/24 --name test_subnet</pre></div></li><li class="step "><p>
     Create some instances
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server create server_1 --image 411a0363-7f4b-4bbc-889c-b9614e2da52e --flavor m1.small --nic net-id=07c35d11-13f9-41d4-8289-fa92147b1d44
<code class="prompt user">ardana &gt; </code>openstack server create server_2 --image 411a03...e2da52e --flavor m1.small --nic net-id=07c35d...147b1d44
<code class="prompt user">ardana &gt; </code>openstack server create server_3 --image 411a03...e2da52e --flavor m1.small --nic net-id=07c35d...147b1d44
<code class="prompt user">ardana &gt; </code>openstack server create server_4 --image 411a03...e2da52e --flavor m1.small --nic net-id=07c35d...147b1d44
<code class="prompt user">ardana &gt; </code>openstack server create server_5 --image 411a03...e2da52e --flavor m1.small --nic net-id=07c35d...147b1d44
<code class="prompt user">ardana &gt; </code>openstack server list</pre></div></li><li class="step "><p>
     Create containers and objects
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack object create container_1 ~/service.osrc
var/lib/ardana/service.osrc

<code class="prompt user">ardana &gt; </code>openstack object create container_1 ~/backup.osrc
swift upload container_1 ~/backup.osrc

<code class="prompt user">ardana &gt; </code>openstack object list container_1
var/lib/ardana/backup.osrc
var/lib/ardana/service.osrc</pre></div></li></ol></div></div></div><div class="sect2" id="id-1.5.19.10.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">17.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Preparation of the test backup server</span> <a title="Permalink" class="permalink" href="#id-1.5.19.10.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="sect3" id="id-1.5.19.10.6.2"><div class="titlepage"><div><div><h4 class="title"><span class="number">17.4.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Preparation to store backups</span> <a title="Permalink" class="permalink" href="#id-1.5.19.10.6.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    In this example, backups are stored on the server
    <code class="literal">192.168.69.132</code>
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Connect to the backup server
     </p></li><li class="step "><p>
      Create the user
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>useradd <em class="replaceable ">BACKUPUSER</em> --create-home --home-dir /mnt/backups/</pre></div></li><li class="step "><p>
      Switch to that user
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>su <em class="replaceable ">BACKUPUSER</em></pre></div></li><li class="step "><p>
      Create the SSH keypair
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">backupuser &gt; </code>ssh-keygen -t rsa
&gt; # Leave the default for the first question and do not set any passphrase
&gt; Generating public/private rsa key pair.
&gt; Enter file in which to save the key (/mnt/backups//.ssh/id_rsa):
&gt; Created directory '/mnt/backups//.ssh'.
&gt; Enter passphrase (empty for no passphrase):
&gt; Enter same passphrase again:
&gt; Your identification has been saved in /mnt/backups//.ssh/id_rsa
&gt; Your public key has been saved in /mnt/backups//.ssh/id_rsa.pub
&gt; The key fingerprint is:
&gt; a9:08:ae:ee:3c:57:62:31:d2:52:77:a7:4e:37:d1:28 backupuser@padawan-ccp-c0-m1-mgmt
&gt; The key's randomart image is:
&gt; +---[RSA 2048]----+
&gt; |          o      |
&gt; |   . . E + .     |
&gt; |  o . . + .      |
&gt; | o +   o +       |
&gt; |  + o o S .      |
&gt; | . + o o         |
&gt; |  o + .          |
&gt; |.o .             |
&gt; |++o              |
&gt; +-----------------+</pre></div></li><li class="step "><p>
      Add the public key to the list of the keys authorized to connect to that
      user on this server
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">backupuser &gt; </code>cat /mnt/backups/.ssh/id_rsa.pub &gt;&gt; /mnt/backups/.ssh/authorized_keys</pre></div></li><li class="step "><p>
      Print the private key. This will be used for the backup configuration
      (ssh_credentials.yml file)
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">backupuser &gt; </code>cat /mnt/backups/.ssh/id_rsa

&gt; -----BEGIN RSA PRIVATE KEY-----
&gt; MIIEogIBAAKCAQEAvjwKu6f940IVGHpUj3ffl3eKXACgVr3L5s9UJnb15+zV3K5L
&gt; BZuor8MLvwtskSkgdXNrpPZhNCsWSkryJff5I335Jhr/e5o03Yy+RqIMrJAIa0X5
&gt; ...
&gt; ...
&gt; ...
&gt; iBKVKGPhOnn4ve3dDqy3q7fS5sivTqCrpaYtByJmPrcJNjb2K7VMLNvgLamK/AbL
&gt; qpSTZjicKZCCl+J2+8lrKAaDWqWtIjSUs29kCL78QmaPOgEvfsw=
&gt; -----END RSA PRIVATE KEY-----</pre></div></li></ol></div></div></div><div class="sect3" id="id-1.5.19.10.6.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">17.4.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Preparation to store Cassandra backups</span> <a title="Permalink" class="permalink" href="#id-1.5.19.10.6.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    In this example, backups will be stored on the server
    <code class="literal">192.168.69.132</code>, in the
    <code class="filename">/mnt/backups/cassandra_backups/</code> directory.
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Create a directory on the backup server to store Cassandra backups.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">backupuser &gt; </code>mkdir /mnt/backups/cassandra_backups</pre></div></li><li class="step "><p>
      Copy the private SSH key from the backup server to all controller nodes.
     </p><p>
      Replace <em class="replaceable ">CONTROLLER</em> with each control node e.g.
      doc-cp1-c1-m1-mgmt, doc-cp1-c1-m2-mgmt etc
     </p></li><li class="step "><p>
      Log in to each controller node and copy the private SSH key to
      <code class="filename">.ssh</code> directory of the root user.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code> sudo cp /var/lib/ardana/.ssh/id_rsa_backup /root/.ssh/</pre></div></li><li class="step "><p>
      Verify that you can SSH to the backup server as backupuser using the
      private key.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ssh -i ~/.ssh/id_rsa_backup backupuser@192.168.69.132</pre></div></li></ol></div></div></div></div><div class="sect2" id="id-1.5.19.10.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">17.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Perform Backups for disaster recovery test</span> <a title="Permalink" class="permalink" href="#id-1.5.19.10.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="sect3" id="id-1.5.19.10.7.2"><div class="titlepage"><div><div><h4 class="title"><span class="number">17.4.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Execute backup of Cassandra</span> <a title="Permalink" class="permalink" href="#id-1.5.19.10.7.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
    Create the following <code class="filename">cassandra-backup-extserver.sh</code>
    script on all controller nodes.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>cat &gt; ~/cassandra-backup-extserver.sh &lt;&lt; EOF
#!/bin/sh

# backup user
BACKUP_USER=backupuser
# backup server
BACKUP_SERVER=192.168.69.132
# backup directory
BACKUP_DIR=/mnt/backups/cassandra_backups/

# Setup variables
DATA_DIR=/var/cassandra/data/data
NODETOOL=/usr/bin/nodetool

# example: cassandra-snp-2018-06-26-1003
SNAPSHOT_NAME=cassandra-snp-\$(date +%F-%H%M)
HOST_NAME=\$(/bin/hostname)_

# Take a snapshot of Cassandra database
\$NODETOOL snapshot -t \$SNAPSHOT_NAME monasca

# Collect a list of directories that make up the snapshot
SNAPSHOT_DIR_LIST=\$(find \$DATA_DIR -type d -name \$SNAPSHOT_NAME)
for d in \$SNAPSHOT_DIR_LIST
  do
    # copy snapshot directories to external server
    rsync -avR -e "ssh -i /root/.ssh/id_rsa_backup" \$d \$BACKUP_USER@\$BACKUP_SERVER:\$BACKUP_DIR/\$HOST_NAME\$SNAPSHOT_NAME
  done

\$NODETOOL clearsnapshot monasca
EOF</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>chmod +x ~/cassandra-backup-extserver.sh</pre></div><p>
    Execute following steps on all the controller nodes
   </p><div id="id-1.5.19.10.7.2.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     The <code class="filename">/usr/local/sbin/cassandra-backup-extserver.sh</code>
     script should be executed on all three controller nodes at the same time
     (within seconds of each other) for a successful backup.
    </p></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Edit the
      <code class="filename">/usr/local/sbin/cassandra-backup-extserver.sh</code> script
     </p><p>
      Set <em class="replaceable ">BACKUP_USER</em> and
      <em class="replaceable ">BACKUP_SERVER</em> to the desired backup user (for
      example, <code class="systemitem">backupuser</code>) and
      desired backup server (for example, <code class="literal">192.168.68.132</code>),
      respectively.
     </p><div class="verbatim-wrap"><pre class="screen">BACKUP_USER=backupuser
BACKUP_SERVER=192.168.69.132
BACKUP_DIR=/mnt/backups/cassandra_backups/</pre></div></li><li class="step "><p>
      Execute <code class="filename">~/cassandra-backup-extserver.sh</code> on on all
      controller nodes which are also Cassandra nodes.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>~/cassandra-backup-extserver.sh

Requested creating snapshot(s) for [monasca] with snapshot name [cassandra-snp-2018-06-28-0251] and options {skipFlush=false}
Snapshot directory: cassandra-snp-2018-06-28-0251
sending incremental file list
created directory /mnt/backups/cassandra_backups//doc-cp1-c1-m1-mgmt_cassandra-snp-2018-06-28-0251
/var/
/var/cassandra/
/var/cassandra/data/
/var/cassandra/data/data/
/var/cassandra/data/data/monasca/

...
...
...

/var/cassandra/data/data/monasca/measurements-e29033d0488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/mc-72-big-Summary.db
/var/cassandra/data/data/monasca/measurements-e29033d0488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/mc-72-big-TOC.txt
/var/cassandra/data/data/monasca/measurements-e29033d0488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/schema.cql
sent 173,691 bytes  received 531 bytes  116,148.00 bytes/sec
total size is 171,378  speedup is 0.98
Requested clearing snapshot(s) for [monasca]</pre></div></li><li class="step "><p>
      Verify the Cassandra backup directory on the backup server.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">backupuser &gt; </code>ls -alt /mnt/backups/cassandra_backups
total 16
drwxr-xr-x 4 backupuser users 4096 Jun 28 03:06 .
drwxr-xr-x 3 backupuser users 4096 Jun 28 03:06 doc-cp1-c1-m2-mgmt_cassandra-snp-2018-06-28-0306
drwxr-xr-x 3 backupuser users 4096 Jun 28 02:51 doc-cp1-c1-m1-mgmt_cassandra-snp-2018-06-28-0251
drwxr-xr-x 8 backupuser users 4096 Jun 27 20:56 ..

$backupuser@backupserver&gt; du -shx /mnt/backups/cassandra_backups/*
6.2G    /mnt/backups/cassandra_backups/doc-cp1-c1-m1-mgmt_cassandra-snp-2018-06-28-0251
6.3G    /mnt/backups/cassandra_backups/doc-cp1-c1-m2-mgmt_cassandra-snp-2018-06-28-0306</pre></div></li></ol></div></div></div><div class="sect3" id="id-1.5.19.10.7.3"><div class="titlepage"><div><div><h4 class="title"><span class="number">17.4.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Execute backup of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <a title="Permalink" class="permalink" href="#id-1.5.19.10.7.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Back up the Cloud Lifecycle Manager using the procedure at
      <a class="xref" href="#clm-data-backup" title="17.3.1. Cloud Lifecycle Manager Data Backup">Section 17.3.1, “Cloud Lifecycle Manager Data Backup”</a>
     </p></li><li class="step "><p>
      Back up the MariaDB database using the procedure at
      <a class="xref" href="#mariadb-database-backup" title="17.3.2. MariaDB Database Backup">Section 17.3.2, “MariaDB Database Backup”</a>
     </p></li><li class="step "><p>
      Back up swift rings using the procedure at
      <a class="xref" href="#swift-ring-backup" title="17.3.3. swift Ring Backup">Section 17.3.3, “swift Ring Backup”</a>
     </p></li></ol></div></div><div class="sect4" id="id-1.5.19.10.7.3.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">17.4.5.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore the first controller</span> <a title="Permalink" class="permalink" href="#id-1.5.19.10.7.3.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Log in to the Cloud Lifecycle Manager.
      </p></li><li class="step "><p>
       Retrieve the Cloud Lifecycle Manager backups that were created with <a class="xref" href="#clm-data-backup" title="17.3.1. Cloud Lifecycle Manager Data Backup">Section 17.3.1, “Cloud Lifecycle Manager Data Backup”</a>. There are multiple backups; directories are
       handled differently than files.
      </p></li><li class="step "><p>
       Extract the TAR archives for each of the seven locations.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros \
--warning=none --overwrite --directory <em class="replaceable ">RESTORE_TARGET</em> \
-f <em class="replaceable ">BACKUP_TARGET</em>.tar.gz</pre></div><p>
       For example, with a directory such as
       <em class="replaceable ">BACKUP_TARGET</em>=<code class="filename">/etc/ssh/</code>
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros \
--warning=none --overwrite --directory /etc/ssh/ -f ssh.tar.gz</pre></div><p>
       With a file such as <em class="replaceable ">BACKUP_TARGET</em>=/etc/passwd
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo tar -z --incremental --extract --ignore-zeros --warning=none --overwrite --directory /etc/ -f passwd.tar.gz</pre></div></li></ol></div></div></div><div class="sect4" id="id-1.5.19.10.7.3.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">17.4.5.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Re-deployment of controllers 1, 2 and 3</span> <a title="Permalink" class="permalink" href="#id-1.5.19.10.7.3.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Change back to the default ardana user.
      </p></li><li class="step "><p>
       Run the <code class="filename">cobbler-deploy.yml</code> playbook.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost cobbler-deploy.xml</pre></div></li><li class="step "><p>
       Run the <code class="filename">bm-reimage.yml</code> playbook limited to the
       second and third controllers.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=controller2,controller3</pre></div><p>
       The names of controller2 and controller3. Use the
       <code class="filename">bm-power-status.yml</code> playbook to check the cobbler
       names of these nodes.
      </p></li><li class="step "><p>
       Run the <code class="filename">site.yml</code> playbook limited to the three
       controllers and localhost—in this example,
       <code class="literal">doc-cp1-c1-m1-mgmt</code>,
       <code class="literal">doc-cp1-c1-m2-mgmt</code>,
       <code class="literal">doc-cp1-c1-m3-mgmt</code>, and <code class="literal">localhost</code>
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts site.yml --limit \
doc-cp1-c1-m1-mgmt,doc-cp1-c1-m2-mgmt,doc-cp1-c1-m3-mgmt,localhost</pre></div></li></ol></div></div></div><div class="sect4" id="id-1.5.19.10.7.3.5"><div class="titlepage"><div><div><h5 class="title"><span class="number">17.4.5.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore Databases</span> <a title="Permalink" class="permalink" href="#id-1.5.19.10.7.3.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="sect5" id="id-1.5.19.10.7.3.5.2"><div class="titlepage"><div><div><h6 class="title"><span class="number">17.4.5.2.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore MariaDB database</span> <a title="Permalink" class="permalink" href="#id-1.5.19.10.7.3.5.2">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
        Log in to the <span class="bold"><strong>first controller node</strong></span>.
       </p></li><li class="step "><p>
        Retrieve the MariaDB backup that was created with <a class="xref" href="#mariadb-database-backup" title="17.3.2. MariaDB Database Backup">Section 17.3.2, “MariaDB Database Backup”</a>.
       </p></li><li class="step "><p>
        Create a temporary directory and extract the TAR archive (for example,
        <code class="filename">mydb.tar.gz</code>).
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>mkdir /tmp/mysql_restore; sudo tar -z --incremental \
--extract --ignore-zeros --warning=none --overwrite --directory /tmp/mysql_restore/ \
-f mydb.tar.gz</pre></div></li><li class="step "><p>
        Verify that the files have been restored on the controller.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo du -shx /tmp/mysql_restore/*
16K     /tmp/mysql_restore/aria_log.00000001
4.0K    /tmp/mysql_restore/aria_log_control
3.4M    /tmp/mysql_restore/barbican
8.0K    /tmp/mysql_restore/ceilometer
4.2M    /tmp/mysql_restore/cinder
2.9M    /tmp/mysql_restore/designate
129M    /tmp/mysql_restore/galera.cache
2.1M    /tmp/mysql_restore/glance
4.0K    /tmp/mysql_restore/grastate.dat
4.0K    /tmp/mysql_restore/gvwstate.dat
2.6M    /tmp/mysql_restore/heat
752K    /tmp/mysql_restore/horizon
4.0K    /tmp/mysql_restore/ib_buffer_pool
76M     /tmp/mysql_restore/ibdata1
128M    /tmp/mysql_restore/ib_logfile0
128M    /tmp/mysql_restore/ib_logfile1
12M     /tmp/mysql_restore/ibtmp1
16K     /tmp/mysql_restore/innobackup.backup.log
313M    /tmp/mysql_restore/keystone
716K    /tmp/mysql_restore/magnum
12M     /tmp/mysql_restore/mon
8.3M    /tmp/mysql_restore/monasca_transform
0       /tmp/mysql_restore/multi-master.info
11M     /tmp/mysql_restore/mysql
4.0K    /tmp/mysql_restore/mysql_upgrade_info
14M     /tmp/mysql_restore/nova
4.4M    /tmp/mysql_restore/nova_api
14M     /tmp/mysql_restore/nova_cell0
3.6M    /tmp/mysql_restore/octavia
208K    /tmp/mysql_restore/opsconsole
38M     /tmp/mysql_restore/ovs_neutron
8.0K    /tmp/mysql_restore/performance_schema
24K     /tmp/mysql_restore/tc.log
4.0K    /tmp/mysql_restore/test
8.0K    /tmp/mysql_restore/winchester
4.0K    /tmp/mysql_restore/xtrabackup_galera_info</pre></div></li><li class="step "><p>
        Stop <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services on the three controllers (using the
        hostnames of the controllers in your configuration).
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-stop.yml --limit \
doc-cp1-c1-m1-mgmt,doc-cp1-c1-m2-mgmt,doc-cp1-c1-m3-mgmt,localhost</pre></div></li><li class="step "><p>
        Delete the files in the <code class="filename">mysql</code> directory and copy
        the restored backup to that directory.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>cd /var/lib/mysql/
<code class="prompt user">root # </code>rm -rf ./*
<code class="prompt user">root # </code>cp -pr /tmp/mysql_restore/* ./</pre></div></li><li class="step "><p>
        Switch back to the <code class="literal">ardana</code> user when the copy is finished.
       </p></li></ol></div></div></div><div class="sect5" id="id-1.5.19.10.7.3.5.3"><div class="titlepage"><div><div><h6 class="title"><span class="number">17.4.5.2.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restore Cassandra database</span> <a title="Permalink" class="permalink" href="#id-1.5.19.10.7.3.5.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
      Create a script called
      <code class="filename">cassandra-restore-extserver.sh</code> on all controller
      nodes
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>cat &gt; ~/cassandra-restore-extserver.sh &lt;&lt; EOF
#!/bin/sh

# backup user
BACKUP_USER=backupuser
# backup server
BACKUP_SERVER=192.168.69.132
# backup directory
BACKUP_DIR=/mnt/backups/cassandra_backups/

# Setup variables
DATA_DIR=/var/cassandra/data/data
NODETOOL=/usr/bin/nodetool

HOST_NAME=\$(/bin/hostname)_

#Get snapshot name from command line.
if [ -z "\$*"  ]
then
  echo "usage \$0 &lt;snapshot to restore&gt;"
  exit 1
fi
SNAPSHOT_NAME=\$1

# restore
rsync -av -e "ssh -i /root/.ssh/id_rsa_backup" \$BACKUP_USER@\$BACKUP_SERVER:\$BACKUP_DIR/\$HOST_NAME\$SNAPSHOT_NAME/ /

# set ownership of newley restored files
chown -R cassandra:cassandra \$DATA_DIR/monasca/*

# Get a list of snapshot directories that have files to be restored.
RESTORE_LIST=\$(find \$DATA_DIR -type d -name \$SNAPSHOT_NAME)

# use RESTORE_LIST to move snapshot files back into place of database.
for d in \$RESTORE_LIST
do
  cd \$d
  mv * ../..
  KEYSPACE=\$(pwd | rev | cut -d '/' -f4 | rev)
  TABLE_NAME=\$(pwd | rev | cut -d '/' -f3 |rev | cut -d '-' -f1)
  \$NODETOOL refresh \$KEYSPACE \$TABLE_NAME
done
cd
# Cleanup snapshot directories
\$NODETOOL clearsnapshot \$KEYSPACE
EOF</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>chmod +x ~/cassandra-restore-extserver.sh</pre></div><p>
      Execute following steps on all the controller nodes.
     </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
        Edit the <code class="filename">~/cassandra-restore-extserver.sh</code> script.
       </p><p>
        Set
        <em class="replaceable ">BACKUP_USER</em>,<em class="replaceable ">BACKUP_SERVER</em>
        to the desired backup user (for example,
        <code class="systemitem">backupuser</code>) and the
        desired backup server (for example, <code class="literal">192.168.68.132</code>),
        respectively.
       </p><div class="verbatim-wrap"><pre class="screen">BACKUP_USER=backupuser
BACKUP_SERVER=192.168.69.132
BACKUP_DIR=/mnt/backups/cassandra_backups/</pre></div></li><li class="step "><p>
        Execute <code class="filename">~/cassandra-restore-extserver.sh</code>
        <em class="replaceable ">SNAPSHOT_NAME</em>
       </p><p>
        Find <em class="replaceable ">SNAPSHOT_NAME</em> from listing of
        /mnt/backups/cassandra_backups. All the directories have the format
        <em class="replaceable ">HOST</em>_<em class="replaceable ">SNAPSHOT_NAME</em>.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls -alt /mnt/backups/cassandra_backups
total 16
drwxr-xr-x 4 backupuser users 4096 Jun 28 03:06 .
drwxr-xr-x 3 backupuser users 4096 Jun 28 03:06 doc-cp1-c1-m2-mgmt_cassandra-snp-2018-06-28-0306</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>~/cassandra-restore-extserver.sh cassandra-snp-2018-06-28-0306

receiving incremental file list
./
var/
var/cassandra/
var/cassandra/data/
var/cassandra/data/data/
var/cassandra/data/data/monasca/
var/cassandra/data/data/monasca/alarm_state_history-e6bbdc20488d11e8bdabc32666406af1/
var/cassandra/data/data/monasca/alarm_state_history-e6bbdc20488d11e8bdabc32666406af1/snapshots/
var/cassandra/data/data/monasca/alarm_state_history-e6bbdc20488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/
var/cassandra/data/data/monasca/alarm_state_history-e6bbdc20488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/manifest.json
var/cassandra/data/data/monasca/alarm_state_history-e6bbdc20488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/mc-37-big-CompressionInfo.db
var/cassandra/data/data/monasca/alarm_state_history-e6bbdc20488d11e8bdabc32666406af1/snapshots/cassandra-snp-2018-06-28-0306/mc-37-big-Data.db
...
...
...
/usr/bin/nodetool clearsnapshot monasca</pre></div></li></ol></div></div></div><div class="sect5" id="id-1.5.19.10.7.3.5.4"><div class="titlepage"><div><div><h6 class="title"><span class="number">17.4.5.2.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restart <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services</span> <a title="Permalink" class="permalink" href="#id-1.5.19.10.7.3.5.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h6><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
        Restart the MariaDB database
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts galera-bootstrap.yml</pre></div><p>
        On the deployer node, execute the
        <code class="filename">galera-bootstrap.yml</code> playbook which will determine
        the log sequence number, bootstrap the main node, and start the
        database cluster.
       </p><p>
        If this process fails to recover the database cluster, refer to
        <a class="xref" href="#mysql" title="15.2.3.1.2. Recovering the MariaDB Database">Section 15.2.3.1.2, “Recovering the MariaDB Database”</a>.
       </p></li><li class="step "><p>
        Restart <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services on the three controllers as in
        the following example.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml \
--limit doc-cp1-c1-m1-mgmt,doc-cp1-c1-m2-mgmt,doc-cp1-c1-m3-mgmt,localhost</pre></div></li><li class="step "><p>
        Reconfigure <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml</pre></div></li></ol></div></div></div></div><div class="sect4" id="id-1.5.19.10.7.3.6"><div class="titlepage"><div><div><h5 class="title"><span class="number">17.4.5.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Post restore testing</span> <a title="Permalink" class="permalink" href="#id-1.5.19.10.7.3.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-maintenance-controller-full_recovery_test.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-maintenance-controller-full_recovery_test.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Source the service credential file
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>source ~/service.osrc</pre></div></li><li class="step "><p>
       swift
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack container list
container_1
volumebackups

<code class="prompt user">ardana &gt; </code>openstack object list container_1
var/lib/ardana/backup.osrc
var/lib/ardana/service.osrc

<code class="prompt user">ardana &gt; </code>openstack object save container_1 /tmp/backup.osrc</pre></div></li><li class="step "><p>
       neutron
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack network list
+--------------------------------------+---------------------+--------------------------------------+
| ID                                   | Name                | Subnets                              |
+--------------------------------------+---------------------+--------------------------------------+
| 07c35d11-13f9-41d4-8289-fa92147b1d44 | test-net             | 02d5ca3b-1133-4a74-a9ab-1f1dc2853ec8|
+--------------------------------------+---------------------+--------------------------------------+</pre></div></li><li class="step "><p>
       glance
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack image list
+--------------------------------------+----------------------+--------+
| ID                                   | Name                 | Status |
+--------------------------------------+----------------------+--------+
| 411a0363-7f4b-4bbc-889c-b9614e2da52e | cirros-0.4.0-x86_64  | active |
+--------------------------------------+----------------------+--------+
<code class="prompt user">ardana &gt; </code>openstack image save --file /tmp/cirros f751c39b-f1e3-4f02-8332-3886826889ba
<code class="prompt user">ardana &gt; </code>ls -lah /tmp/cirros
-rw-r--r-- 1 ardana ardana 12716032 Jul  2 20:52 /tmp/cirros</pre></div></li><li class="step "><p>
       nova
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openstack server list

<code class="prompt user">ardana &gt; </code>openstack server create server_6 --image 411a0363-7f4b-4bbc-889c-b9614e2da52e  --flavor m1.small --nic net-id=07c35d11-13f9-41d4-8289-fa92147b1d44
+-------------------------------------+------------------------------------------------------------+
| Field                               | Value                                                      |
+-------------------------------------+------------------------------------------------------------+
| OS-DCF:diskConfig                   | MANUAL                                                     |
| OS-EXT-AZ:availability_zone         |                                                            |
| OS-EXT-SRV-ATTR:host                | None                                                       |
| OS-EXT-SRV-ATTR:hypervisor_hostname | None                                                       |
| OS-EXT-SRV-ATTR:instance_name       |                                                            |
| OS-EXT-STS:power_state              | NOSTATE                                                    |
| OS-EXT-STS:task_state               | scheduling                                                 |
| OS-EXT-STS:vm_state                 | building                                                   |
| OS-SRV-USG:launched_at              | None                                                       |
| OS-SRV-USG:terminated_at            | None                                                       |
| accessIPv4                          |                                                            |
| accessIPv6                          |                                                            |
| addresses                           |                                                            |
| adminPass                           | iJBoBaj53oUd                                               |
| config_drive                        |                                                            |
| created                             | 2018-07-02T21:02:01Z                                       |
| flavor                              | m1.small (2)                                               |
| hostId                              |                                                            |
| id                                  | ce7689ff-23bf-4fe9-b2a9-922d4aa9412c                       |
| image                               | cirros-0.4.0-x86_64 (f751c39b-f1e3-4f02-8332-3886826889ba) |
| key_name                            | None                                                       |
| name                                | server_6                                                   |
| progress                            | 0                                                          |
| project_id                          | cca416004124432592b2949a5c5d9949                           |
| properties                          |                                                            |
| security_groups                     | name='default'                                             |
| status                              | BUILD                                                      |
| updated                             | 2018-07-02T21:02:01Z                                       |
| user_id                             | 8cb1168776d24390b44c3aaa0720b532                           |
| volumes_attached                    |                                                            |
+-------------------------------------+------------------------------------------------------------+

<code class="prompt user">ardana &gt; </code>openstack server list
+--------------------------------------+----------+--------+---------------------------------+---------------------+-----------+
| ID                                   | Name     | Status | Networks                        | Image               | Flavor    |
+--------------------------------------+----------+--------+---------------------------------+---------------------+-----------+
| ce7689ff-23bf-4fe9-b2a9-922d4aa9412c | server_6 | ACTIVE | n1=1.1.1.8                      | cirros-0.4.0-x86_64 | m1.small  |

<code class="prompt user">ardana &gt; </code>openstack server delete ce7689ff-23bf-4fe9-b2a9-922d4aa9412c</pre></div></li></ol></div></div></div></div></div></div></div><div class="chapter " id="idg-all-operations-troubleshooting-troubleshooting-issues-xml-1"><div class="titlepage"><div><div><h1 class="title"><span class="number">18 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting Issues</span> <a title="Permalink" class="permalink" href="#idg-all-operations-troubleshooting-troubleshooting-issues-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-troubleshooting_issues.xml" title="Edit the source file for this section">Edit source</a></h1><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-troubleshooting_issues.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-troubleshooting-issues-xml-1</li></ul></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#general-troubleshooting"><span class="number">18.1 </span><span class="name">General Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#troubleshooting-controlplane"><span class="number">18.2 </span><span class="name">Control Plane Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#ts-compute"><span class="number">18.3 </span><span class="name">Troubleshooting Compute service</span></a></span></dt><dt><span class="section"><a href="#neutron-troubleshooting"><span class="number">18.4 </span><span class="name">Network Service Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#troubleshooting-glance"><span class="number">18.5 </span><span class="name">Troubleshooting the Image (glance) Service</span></a></span></dt><dt><span class="section"><a href="#troubleshooting-storage"><span class="number">18.6 </span><span class="name">Storage Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#monitoring-logging-usage-reporting"><span class="number">18.7 </span><span class="name">Monitoring, Logging, and Usage Reporting Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#troubleshooting-orchestration"><span class="number">18.8 </span><span class="name">Orchestration Troubleshooting</span></a></span></dt><dt><span class="section"><a href="#troubleshooting-tools"><span class="number">18.9 </span><span class="name">Troubleshooting Tools</span></a></span></dt></dl></div></div><p>
  Troubleshooting and support processes for solving issues in your environment.
 </p><p>
  This section contains troubleshooting tasks for your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> cloud.
 </p><div class="sect1" id="general-troubleshooting"><div class="titlepage"><div><div><h2 class="title"><span class="number">18.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">General Troubleshooting</span> <a title="Permalink" class="permalink" href="#general-troubleshooting">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-general_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-general_troubleshooting.xml</li><li><span class="ds-label">ID: </span>general-troubleshooting</li></ul></div></div></div></div><p>
  General troubleshooting procedures for resolving your cloud issues including
  steps for resolving service alarms and support contact information.
 </p><p>
  Before contacting support to help you with a problem on SUSE <span class="productname">OpenStack</span> Cloud, we recommend
  gathering as much information as possible about your system and the
  problem. For this purpose, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> ships with a tool called
  <code class="command">supportconfig</code>. It gathers system information such as the
  current kernel version being used, the hardware, RPM database, partitions,
  and other items. <code class="command">supportconfig</code> also collects the most
  important log files. This information assists support staff to identify and
  solve your problem.
 </p><p>
  Always run <code class="command">supportconfig</code> on the Cloud Lifecycle Manager and on the
  Control Node(s). If a Compute Node or a Storage Node is part of the problem, run
  <code class="command">supportconfig</code> on the affected node as well. For details on
  how to run <code class="command">supportconfig</code>, see
  <a class="link" href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-admin/#cha-adm-support" target="_blank">https://documentation.suse.com/sles/15-SP1/single-html/SLES-admin/#cha-adm-support</a>.
 </p><div class="sect2" id="alarmdefinitions"><div class="titlepage"><div><div><h3 class="title"><span class="number">18.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Alarm Resolution Procedures</span> <a title="Permalink" class="permalink" href="#alarmdefinitions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-alarm_resolutions.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-alarm_resolutions.xml</li><li><span class="ds-label">ID: </span>alarmdefinitions</li></ul></div></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> provides a monitoring solution based on OpenStack’s monasca
  service. This service provides monitoring and metrics for all OpenStack
  components, as well as much of the underlying system. By default, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  comes with a set of alarms that provide coverage of the primary systems. In
  addition, you can define alarms based on threshold values for any metrics
  defined in the system. You can view alarm information in the Operations
  Console. You can also receive or deliver this information to others by
  configuring email or other mechanisms. Alarms provide information about
  whether a component failed and is affecting the system, and also what
  condition triggered the alarm.
 </p><p>
  Here is a list of the included service-specific alarms and the recommended
  troubleshooting steps. We have organized these alarms by the section of the
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Operations Console, they are organized in as well as the
  <code class="literal">service</code> dimension defined.
 </p><div class="sect3" id="compute-alarmdefinitions"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute Alarms</span> <a title="Permalink" class="permalink" href="#compute-alarmdefinitions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-compute_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span>compute-alarmdefinitions</li></ul></div></div></div></div><p>
  These alarms show under the Compute section of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Operations Console.
 </p><div class="sect4" id="id-1.5.20.4.5.4.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: COMPUTE</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.4.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-compute_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: HTTP Status</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span> This is a <code class="literal">nova-api</code> health check.
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
        </p>
       </td><td valign="top">Restart the <code class="literal">nova-api</code> process on the affected
     node. Review the <code class="literal">nova-api.log</code> files. Try to connect
     locally to the http port that is found in the dimension field of the alarm
     to see if the connection is accepted.</td></tr><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: Host Status</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span>: Alarms when the specified host is down or not reachable.
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> The host is down, has been rebooted, or has network
         connectivity issues.
        </p>
       </td><td valign="top">If it is a single host, attempt to restart the system. If it is
     multiple hosts, investigate networking issues.</td></tr><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: Process Bound Check</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span>: <code class="literal">process_name=nova-api</code> This alarm
         checks that the number of processes found is in a predefined range.
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> Process crashed or too many processes running
        </p>
       </td><td valign="top">Stop all the processes and restart the nova-api process on the
       affected host.  Review the system and nova-api logs.</td></tr><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: Process Check</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span>: Separate alarms for each of these nova services,
         specified by the <code class="literal">component</code> dimension:
        </p>
        <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
           nova-api
          </p></li><li class="listitem "><p>
           nova-cert
          </p></li><li class="listitem "><p>
           nova-compute
          </p></li><li class="listitem "><p>
           nova-conductor
          </p></li><li class="listitem "><p>
           nova-scheduler
          </p></li><li class="listitem "><p>
           nova-novncproxy
          </p></li></ul></div>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> Process specified by the <code class="literal">component</code>
         dimension has crashed on the host specified by the
         <code class="literal">hostname</code> dimension.
        </p>
       </td><td valign="top">
        <p>
         Restart the process on the affected node using these steps:
        </p>
        <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
           Log in to the Cloud Lifecycle Manager.
          </p></li><li class="step "><p>
           Use the nova start playbook against the affected node:
          </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts nova-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
        <p>
         Review the associated logs. The logs will be in the format of
         <code class="literal">&lt;service&gt;.log</code>, such as
         <code class="literal">nova-compute.log</code> or
         <code class="literal">nova-scheduler.log</code>.
        </p>
       </td></tr><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: nova.heartbeat</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span>: Check that all services are sending heartbeats.
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> Process for service specified in the alarm has crashed
         or is hung and not reporting its status to the database. Alternatively
         it may be the service is fine but an issue with messaging or the
         database which means the status is not being updated correctly.
        </p>
       </td><td valign="top">Restart the affected service. If the service is reporting OK the
     issue may be with RabbitMQ or MySQL. In that case, check the alarms for
     those services.</td></tr><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span>: Service log directory consuming more disk than its quota.
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> This could be due to a service set to
         <code class="literal">DEBUG</code> instead of <code class="literal">INFO</code> level.
         Another reason could be due to a repeating error message filling up
         the log files. Finally, it could be due to log rotate not configured
         properly so old log files are not being deleted properly.
        </p>
       </td><td valign="top">Find the service that is consuming too much disk space. Look at the
     logs. If <code class="literal">DEBUG</code> log entries exist, set the logging level
     to <code class="literal">INFO</code>. If the logs are repeatedly logging an error
     message, do what is needed to resolve the error. If old log files exist,
     configure log rotate to remove them. You could also choose to remove old
     log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.5.20.4.5.4.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: IMAGE-SERVICE in Compute section</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.4.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-compute_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: HTTP Status</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span>: Separate alarms for
         each of these glance services, specified by the
         <code class="literal">component</code> dimension:
        </p>
        <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          glance-api
         </p></li><li class="listitem "><p>
          glance-registry
         </p></li></ul></div>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> API is unresponsive.
        </p>
       </td><td valign="top">
        <p>
         Restart the process on the affected node using these steps:
        </p>
        <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use the glance start playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts glance-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p></td></tr><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span>: Service log directory consuming more disk than its quota.
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> This could be due to a service set to
         <code class="literal">DEBUG</code> instead of <code class="literal">INFO</code>
         level. Another reason could be due to a repeating error message
         filling up the log files. Finally, it could be due to log rotate not
         configured properly so old log files are not being deleted properly.
        </p>
       </td><td valign="top">Find the service that is consuming too much disk space. Look at
       the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
       level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
       error message, do what is needed to resolve the error. If old log files
       exist, configure log rotate to remove them. You could also choose to
       remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.5.20.4.5.4.5"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: BAREMETAL in Compute section</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.4.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-compute_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-compute_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: Process Check</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span> Alarms when the
         specified process is not running: <code class="literal">process_name = ironic-api</code>
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> The ironic API is unresponsive.
        </p>
       </td><td valign="top">
        <p>
        Restart the <code class="literal">ironic-api</code> process with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the affected host via SSH.
         </p></li><li class="step "><p>
          Restart the <code class="literal">ironic-api</code> process with this command:
         </p><div class="verbatim-wrap"><pre class="screen">sudo service ironic-api restart</pre></div></li></ol></div></div>
       </td></tr><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: Process Check</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span> Alarms when the
         specified process is not running: <code class="literal">process_name = ironic-conductor</code>
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> The
         <code class="literal">ironic-conductor</code> process has crashed.
        </p>
       </td><td valign="top">
        <p>
        Restart the <code class="literal">ironic-conductor</code> process with these
        steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Source your <code class="literal">admin</code> user credentials:
         </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc</pre></div></li><li class="step "><p>
          Locate the <code class="literal">messaging_deployer</code> VM:
         </p><div class="verbatim-wrap"><pre class="screen">openstack server list --all-tenants | grep mess</pre></div></li><li class="step "><p>
          SSH to the <code class="literal">messaging_deployer</code> VM:
         </p><div class="verbatim-wrap"><pre class="screen">sudo -u ardana ssh &lt;IP_ADDRESS&gt;</pre></div></li><li class="step "><p>
          Stop the <code class="literal">ironic-conductor</code> process by using this
          playbook:
         </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-stop.yml</pre></div></li><li class="step "><p>
          Start the process back up again, effectively restarting it, by using
          this playbook:
         </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-start.yml</pre></div></li></ol></div></div>
       </td></tr><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: HTTP Status</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span> Alarms when the
         specified HTTP endpoint is down or not reachable.
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> The API is unresponsive.
        </p>
       </td><td valign="top">
        <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Source your <code class="literal">admin</code> user credentials:
         </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc</pre></div></li><li class="step "><p>
          Locate the <code class="literal">messaging_deployer</code> VM:
         </p><div class="verbatim-wrap"><pre class="screen">openstack server list --all-tenants | grep mess</pre></div></li><li class="step "><p>
          SSH to the <code class="literal">messaging_deployer</code> VM:
         </p><div class="verbatim-wrap"><pre class="screen">sudo -u ardana ssh &lt;IP_ADDRESS&gt;</pre></div></li><li class="step "><p>
          Stop the <code class="literal">ironic-api</code> process by using this
          playbook:
         </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-stop.yml</pre></div></li><li class="step "><p>
          Start the process back up again, effectively restarting it, by using
          this playbook:
         </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-start.yml</pre></div></li></ol></div></div>
       </td></tr><tr><td valign="top">
        <p>
         <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
        </p>
        <p>
         <span class="bold"><strong>Description:</strong></span> Service log directory
         consuming more disk than its quota.
        </p>
        <p>
         <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
         service set to <code class="literal">DEBUG</code> instead of
         <code class="literal">INFO</code> level. Another reason could be due to a
         repeating error message filling up the log files. Finally, it could be
         due to log rotate not configured properly so old log files are not
         being deleted properly.
        </p>
       </td><td valign="top">Find the service that is consuming too much disk space. Look at
       the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
       level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
       error message, do what is needed to resolve the error. If old log files
       exist, configure log rotate to remove them. You could also choose to
       remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div></div><div class="sect3" id="storage-alarmdefinitions"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Storage Alarms</span> <a title="Permalink" class="permalink" href="#storage-alarmdefinitions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-storage_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-storage_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span>storage-alarmdefinitions</li></ul></div></div></div></div><p>
  These alarms show under the Storage section of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Operations Console.
 </p><div class="sect4" id="id-1.5.20.4.5.5.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: OBJECT-STORAGE</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.5.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-storage_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-storage_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1" width="99%"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: swiftlm-scan monitor</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if
        <code class="literal">swiftlm-scan</code> cannot execute a monitoring task.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The
        <code class="literal">swiftlm-scan</code> program is used to monitor and measure
        a number of metrics. If it is unable to monitor or measure something,
        it raises this alarm.
       </p>
      </td><td valign="top">
       <p>
        Click on the alarm to examine the <code class="literal">Details</code> field and
        look for a <code class="literal">msg</code> field. The text may explain the error
        problem. To view/confirm this, you can also log into the host specified
        by the <code class="literal">hostname</code> dimension, and then run this
        command:
       </p>
<div class="verbatim-wrap"><pre class="screen">sudo swiftlm-scan | python -mjson.tool</pre></div>
       <p>
        The <code class="literal">msg</code> field is contained in the
        <code class="literal">value_meta</code> item.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: swift account replicator last</strong></span>
        completed in 12 hours
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if an
        <code class="literal">account-replicator</code> process did not complete a
        replication cycle within the last 12 hours.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This can indicate that
        the <code class="literal">account-replication</code> process is stuck.
       </p>
      </td><td valign="top">
       <p>
        Another cause of this problem may be that a file system may be corrupt.
        Look for sign of this in these logs on the affected node:
       </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/swift/swift.log
/var/log/kern.log</pre></div>
       <p>
        The file system may need to be wiped, contact Sales Engineering for advice
        on the best way to do that if needed. You can then reformat the file
        system with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the swift deploy playbook against the affected node, which will
          format the wiped file system:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-deploy.yml
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: swift container replicator last</strong></span>
        completed in 12 hours
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if a
        container-replicator process did not complete a replication cycle
        within the last 12 hours
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This can indicate that
        the container-replication process is stuck.
       </p>
      </td><td valign="top">
       <p>
        SSH to the affected host and restart the process with this command:
       </p>
<div class="verbatim-wrap"><pre class="screen">sudo systemctl restart swift-container-replicator</pre></div>
       <p>
        Another cause of this problem may be that a file system may be corrupt.
        Look for sign of this in these logs on the affected node:
       </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/swift/swift.log
/var/log/kern.log</pre></div>
       <p>
        The file system may need to be wiped, contact Sales Engineering for advice
        on the best way to do that if needed. You can then reformat the file
        system with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the swift deploy playbook against the affected node, which will
          format the wiped file system:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-deploy.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: swift object replicator last</strong></span>
        completed in 24 hours
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if an
        object-replicator process did not complete a replication cycle within
        the last 24 hours
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This can indicate that
        the object-replication process is stuck.
       </p>
      </td><td valign="top">
       <p>
        SSH to the affected host and restart the process with this command:
       </p>
<div class="verbatim-wrap"><pre class="screen">sudo systemctl restart swift-account-replicator</pre></div>
       <p>
        Another cause of this problem may be that a file system may be corrupt.
        Look for sign of this in these logs on the affected node:
       </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/swift/swift.log
/var/log/kern.log</pre></div>
       <p>
        The file system may need to be wiped, contact Sales Engineering for advice
        on the best way to do that if needed. You can then reformat the file
        system with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the swift deploy playbook against the affected node, which will
          format the wiped file system:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-deploy.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: swift configuration file</strong></span>
        ownership
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if
        files/directories in <code class="literal">/etc/swift</code> are not owned by
        swift.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> For files in
        <code class="literal">/etc/swift</code>, somebody may have manually edited or
        created a file.
       </p>
      </td><td valign="top">
       <p>
        For files in <code class="literal">/etc/swift</code>, use this command to change
        the file ownership:
       </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo chown swift.swift /etc/swift/, /etc/swift/*</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: swift data filesystem ownership</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if files or
        directories in <code class="literal">/srv/node</code> are not owned by swift.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> For directories in
        <code class="literal">/srv/node/*</code>, it may happen that the root partition
        was reimaged or reinstalled and the UID assigned to the swift user
        change. The directories and files would then not be owned by the UID
        assigned to the swift user.
       </p>
      </td><td valign="top">
       <p>
        For directories and files in <code class="filename">/srv/node/*</code>, compare
        the swift UID of this system and other systems and the UID of the owner
        of <code class="filename">/srv/node/*</code>. If possible, make the UID of the
        swift user match the directories or files. Otherwise, change the
        ownership of all files and directories under the
        <code class="filename">/srv/node</code> path using a similar <code class="command">chown
        swift.swift</code> command as above.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Drive URE errors detected</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if
        <code class="literal">swift-drive-audit</code> reports an unrecoverable read
        error on a drive used by the swift service.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> An unrecoverable read
        error occurred when swift attempted to access a directory.
       </p>
      </td><td valign="top">
       <p>
        The UREs reported only apply to file system metadata (that is,
        directory structures). For UREs in object files, the swift system
        automatically deletes the file and replicates a fresh copy from one of
        the other replicas.
       </p>
       <p>
        UREs are a normal feature of large disk drives. It does not mean that
        the drive has failed. However, if you get regular UREs on a specific
        drive, then this may indicate that the drive has indeed failed and
        should be replaced.
       </p>
       <p>
        You can use standard XFS repair actions to correct the UREs in the file
        system.
       </p>
       <p>
        If the XFS repair fails, you should wipe the GPT table as follows
        (where &lt;drive_name&gt; is replaced by the actual drive name):
       </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo dd if=/dev/zero of=/dev/sd&lt;drive_name&gt; \
bs=$((1024*1024)) count=1</pre></div>
       <p>
        Then follow the steps below which will reformat the drive, remount it,
        and restart swift services on the affected node.
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the swift reconfigure playbook, specifying the affected node:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts _swift-configure.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
       <p>
        It is safe to reformat drives containing swift data because swift
        maintains other copies of the data (usually, swift is configured to
        have three replicas of all data).
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: swift service</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if a swift
        process, specified by the <code class="literal">component</code> field, is not
        running.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> A daemon specified by
        the <code class="literal">component</code> dimension on the host specified by the
        <code class="literal">hostname</code> dimension has stopped running.
       </p>
      </td><td valign="top">
       <p>
        Examine the <code class="filename">/var/log/swift/swift.log</code> file for
        possible error messages related the swift process. The process in
        question is listed in the alarm dimensions in the
        <code class="literal">component</code> dimension.
       </p>
       <p>
        Restart swift processes by running the
        <code class="filename">swift-start.yml</code> playbook, with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the swift start playbook against the affected host:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: swift filesystem mount point</strong></span>
        status
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if a file
        system/drive used by swift is not correctly mounted.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The device specified by
        the <code class="literal">device</code> dimension is not correctly mounted at the
        mountpoint specified by the <code class="literal">mount</code> dimension.
       </p>
       <p>
        The most probable cause is that the drive has failed or that it had a
        temporary failure during the boot process and remained unmounted.
       </p>
       <p>
        Other possible causes are a file system corruption that prevents the
        device from being mounted.
       </p>
      </td><td valign="top">
       <p>
        Reboot the node and see if the file system remains unmounted.
       </p>
       <p>
        If the file system is corrupt, see the process used for the "Drive URE
        errors" alarm to wipe and reformat the drive.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: swift uptime-monitor status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if the
        swiftlm-uptime-monitor has errors using keystone (<code class="literal">keystone-get-token</code>),
        swift (<code class="literal">rest-api</code>) or swift's healthcheck.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The
        swiftlm-uptime-monitor cannot get a token from keystone or cannot get a
        successful response from the swift Object-Storage API.
       </p>
      </td><td valign="top">
       <p>
        Check that the keystone service is running:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check the status of the keystone service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-status.yml</pre></div></li><li class="step "><p>
          If it is not running, start the service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts keystone-start.yml</pre></div></li><li class="step "><p>
          Contact the support team if further assistance troubleshooting the
          keystone service is needed.
         </p></li></ol></div></div>
       <p>
        Check that swift is running:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check the status of the keystone service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-status.yml</pre></div></li><li class="step "><p>
          If it is not running, start the service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-start.yml</pre></div></li></ol></div></div>
       <p>
        Restart the swiftlm-uptime-monitor as follows:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log into the first server running the swift-proxy-server service. Use
          this playbook below to determine whcih host this is:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-status.yml
--limit SWF-PRX[0]</pre></div></li><li class="step "><p>
          Restart the swiftlm-uptime-monitor with this command:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo systemctl restart swiftlm-uptime-monitor</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: swift keystone server connect</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if a socket cannot
        be opened to the keystone service (used for token validation)
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The Identity service
        (keystone) server may be down. Another possible cause is that the
        network between the host reporting the problem and the keystone server
        or the <code class="literal">haproxy</code> process is not forwarding requests to
        keystone.
       </p>
      </td><td valign="top">
       <p>
        The <code class="literal">URL</code> dimension contains the name of the virtual
        IP address. Use cURL or a similar program to confirm that a connection
        can or cannot be made to the virtual IP address. Check that
        <code class="literal">haproxy</code> is running. Check that the keystone service
        is working.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: swift service listening on ip</strong></span>
        and port
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when a swift
        service is not listening on the correct port or ip.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The swift service may be
        down.
       </p>
      </td><td valign="top">
       <p>
        Verify the status of the swift service on the affected host, as
        specified by the <code class="literal">hostname</code> dimension.
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the swift status playbook to confirm status:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-status.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
       <p>
        If an issue is determined, you can stop and restart the swift service
        with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Stop the swift service on the affected host:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-stop.yml \
--limit &lt;hostname&gt;</pre></div></li><li class="step "><p>
          Restart the swift service on the affected host:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: swift rings checksum</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if the swift rings
        checksums do not match on all hosts.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The swift ring files
        must be the same on every node. The files are located in
        <code class="filename">/etc/swift/*.ring.gz</code>.
       </p>
       <p>
        If you have just changed any of the rings and you are still deploying
        the change, it is normal for this alarm to trigger.
       </p>
      </td><td valign="top">
       <p>
        If you have just changed any of your swift rings, if you wait until the
        changes complete then this alarm will likely clear on its own. If it
        does not, then continue with these steps.
       </p>
       <p>
        Use <code class="command">sudo swift-recon --md5</code> to find which node has
        outdated rings.
       </p>
       <p>
        Run the <code class="filename">swift-reconfigure.yml</code> playbook, using the
        steps below. This deploys the same set of rings to every node.
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the swift start playbook against the affected host:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: swift memcached server connect</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if a socket cannot
        be opened to the specified memcached server.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The server may be down.
        The memcached daemon running the server may have stopped.
       </p>
      </td><td valign="top">
       <p>
        If the server is down, restart it.
       </p>
       <p>
        If memcached has stopped, you can restart it by using the
        <code class="filename">memcached-start.yml</code> playbook, using the steps
        below. If this fails, rebooting the node will restart the process.
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the memcached start playbook against the affected host:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts memcached-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
       <p>
        If the server is running and memcached is running, there may be a
        network problem blocking port 11211.
       </p>
       <p>
        If you see sporadic alarms on different servers, the system may be
        running out of resources. Contact Sales Engineering for advice.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: swift individual disk usage
        exceeds 80%</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when a disk drive
        used by swift exceeds 80% utilization.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Generally all disk
        drives will fill roughly at the same rate. If an individual disk drive
        becomes filled faster than other drives it can indicate a problem with
        the replication process.
       </p>
      </td><td valign="top">
       <p>
        If many or most of your disk drives are 80% full, you need to add more
        nodes to your system or delete existing objects.
       </p>
       <p>
        If one disk drive is noticeably (more than 30%) more utilized than the
        average of other disk drives, check that swift processes are working on
        the server (use the steps below) and also look for alarms related to
        the host. Otherwise continue to monitor the situation.
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the swift status:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-status.yml</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: swift individual disk usage exceeds
        90%</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when a disk drive
        used by swift exceeds 90% utilization.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Generally all disk
        drives will fill roughly at the same rate. If an individual disk drive
        becomes filled faster than other drives it can indicate a problem with
        the replication process.
       </p>
      </td><td valign="top">
       <p>
        If one disk drive is noticeably (more than 30%) more utilized than the
        average of other disk drives, check that swift processes are working on
        the server, using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the swift status:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-status.yml</pre></div></li></ol></div></div>
       <p>
        Also look for alarms related to the host. An individual disk drive
        filling can indicate a problem with the replication process.
       </p>
       <p>
        Restart swift on that host using the <code class="literal">--limit</code>
        argument to target the host:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Stop the swift service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-stop.yml \
--limit &lt;hostname&gt;</pre></div></li><li class="step "><p>
          Start the swift service back up:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
       <p>
        If the utilization does not return to similar values as other disk
        drives, you can reformat the disk drive. You should only do this if the
        average utilization of all disk drives is less than 80%. To format a
        disk drive contact Sales Engineering for instructions.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: swift total disk usage exceeds
        80%</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the average
        disk utilization of swift disk drives exceeds 80% utilization.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The number and size of
        objects in your system is beginning to fill the available disk space.
        Account and container storage is included in disk utilization. However,
        this generally consumes 1-2% of space compared to objects, so object
        storage is the dominate consumer of disk space.
       </p>
      </td><td valign="top">
       <p>
        You need to add more nodes to your system or delete existing objects to
        remain under 80% utilization.
       </p>
       <p>
        If you delete a project/account, the objects in that account are not
        removed until a week later by the <code class="literal">account-reaper</code>
        process, so this is not a good way of quickly freeing up space.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: swift total disk usage exceeds
        90%</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the average
        disk utilization of swift disk drives exceeds 90% utilization.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The number and size of
        objects in your system is beginning to fill the available disk space.
        Account and container storage is included in disk utilization. However,
        this generally consumes 1-2% of space compared to objects, so object
        storage is the dominate consumer of disk space.
       </p>
      </td><td valign="top">
       <p>
        If your disk drives are 90% full, you must immediately stop all
        applications that put new objects into the system. At that point you
        can either delete objects or add more servers.
       </p>
       <p>
        Using the steps below, set the <code class="literal">fallocate_reserve</code>
        value to a value higher than the currently available space on disk
        drives. This will prevent more objects being created.
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Edit the configuration files below and change the value for
          <code class="literal">fallocate_reserve</code> to a value higher than the
          currently available space on the disk drives:
         </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/swift/account-server.conf.j2
~/openstack/my_cloud/config/swift/container-server.conf.j2
~/openstack/my_cloud/config/swift/object-server.conf.j2</pre></div></li><li class="step "><p>
          Commit the changes to git:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "changing swift fallocate_reserve value"</pre></div></li><li class="step "><p>
          Run the configuration processor:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
          Update your deployment directory:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
          Run the swift reconfigure playbook to deploy the change:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li></ol></div></div>
       <p>
        If you allow your file systems to become full, you will be unable to
        delete objects or add more nodes to the system. This is because the
        system needs some free space to handle the replication process when
        adding nodes. With no free space, the replication process cannot work.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: swift service per-minute
        availability</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if the swift
        service reports unavailable for the previous minute.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The
        <code class="literal">swiftlm-uptime-monitor</code> service runs on the first
        proxy server. It monitors the swift endpoint and reports latency data.
        If the endpoint stops reporting, it generates this alarm.
       </p>
      </td><td valign="top">
       <p>
        There are many reasons why the endpoint may stop running. Check:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          Is <code class="literal">haproxy</code> running on the control nodes?
         </p></li><li class="listitem "><p>
          Is <code class="literal">swift-proxy-server</code> running on the swift proxy
          servers?
         </p></li></ul></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: swift rsync connect</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if a socket cannot
        be opened to the specified rsync server
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The rsync daemon on the
        specified node cannot be contacted. The most probable cause is that the
        node is down. The rsync service might also have been stopped on the
        node.
       </p>
      </td><td valign="top">
       <p>
        Reboot the server if it is down.
       </p>
       <p>
        Attempt to restart rsync with this command:
       </p>
<div class="verbatim-wrap"><pre class="screen">systemctl restart rsync.service</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: swift smart array controller
        status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if there is a
        failure in the Smart Array.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The Smart Array or Smart
        HBA controller has a fault or a component of the controller (such as a
        battery) is failed or caching is disabled.
       </p>
       <p>
        The HPE Smart Storage Administrator (HPE SSA) CLI component will have
        to be installed for SSACLI status to be reported. HPE-specific binaries
        that are not based on open source are distributed directly from and
        supported by HPE. To download and install the SSACLI utility, please
        refer to:
        <a class="link" href="https://support.hpe.com/hpsc/swd/public/detail?swItemId=MTX_3d16386b418a443388c18da82f" target="_blank">https://support.hpe.com/hpsc/swd/public/detail?swItemId=MTX_3d16386b418a443388c18da82f</a>
       </p>
      </td><td valign="top">
       <p>
        Log in to the reported host and run these commands to find out the
        status of the controllers:
       </p>
<div class="verbatim-wrap"><pre class="screen">sudo hpssacli
=&gt; controller show all detail</pre></div>
       <p>
        For hardware failures (such as failed battery), replace the failed
        component. If the cache is disabled, reenable the cache.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: swift physical drive status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if there is a
        failure in the Physical Drive.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span>A disk drive on the
        server has failed or has warnings.
       </p>
      </td><td valign="top">
       <p>
        Log in to the reported and run these commands to find out the status of
        the drive:
       </p>
<div class="verbatim-wrap"><pre class="screen">sudo hpssacli
=&gt; ctrl slot=1 pd all show</pre></div>
       <p>
        Replace any broken drives.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: swift logical drive status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if there is a
        failure in the Logical Drive.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> A LUN on the server is
        degraded or has failed.
       </p>
      </td><td valign="top">
       <p>
        Log in to the reported host and run these commands to find out the
        status of the LUN:
       </p>
<div class="verbatim-wrap"><pre class="screen">sudo hpssacli
=&gt; ctrl slot=1 ld all show
=&gt; ctrl slot=1 pd all show</pre></div>
       <p>
        Replace any broken drives.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> If the
        <code class="literal">service</code> dimension is
        <code class="literal">object-store</code>, see the description of the "swift
        Service" alarm for possible causes.
       </p>
      </td><td valign="top">
       <p>
        If the <code class="literal">service</code> dimension is
        <code class="literal">object-storage</code>, see the description of the "swift
        Service" alarm for possible mitigation tasks.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        HTTP endpoint is down or not reachable.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> If the
        <code class="literal">service</code> dimension is
        <code class="literal">object-store</code>, see the description of the "swift host
        socket connect" alarm for possible causes.
       </p>
      </td><td valign="top">
       <p>
        If the <code class="literal">service</code> dimension is
        <code class="literal">object-storage</code>, see the description of the "swift
        host socket connect" alarm for possible mitigation tasks.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to <code class="literal">DEBUG</code> instead of
        <code class="literal">INFO</code> level. Another reason could be due to a
        repeating error message filling up the log files. Finally, it could be
        due to log rotate not configured properly so old log files are not
        being deleted properly.
       </p>
      </td><td valign="top">
       <p>
        Find the service that is consuming too much disk space. Look at the
        logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
        level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
        error message, do what is needed to resolve the error. If old log files
        exist, configure log rotate to remove them. You could also choose to
        remove old log files by hand after backing them up if needed.
       </p>
      </td></tr></tbody></table></div></div><div class="sect4" id="id-1.5.20.4.5.5.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: BLOCK-STORAGE in Storage section</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.5.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-storage_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-storage_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Separate alarms for each
        of these cinder services, specified by the <code class="literal">component</code>
        dimension:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          cinder-api
         </p></li><li class="listitem "><p>
          cinder-backup
         </p></li><li class="listitem "><p>
          cinder-scheduler
         </p></li><li class="listitem "><p>
          cinder-volume
         </p></li></ul></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node. Review the associated logs.
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the <code class="filename">cinder-start.yml</code> playbook to start the
          process back up:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts cinder-start.yml
--limit &lt;hostname&gt;</pre></div><div id="id-1.5.20.4.5.5.4.2.1.4.1.2.2.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
           The <code class="literal">--limit &lt;hostname&gt;</code> switch is optional.
           If it is included, then the <code class="literal">&lt;hostname&gt;</code> you
           should use is the host where the alarm was raised.
          </p></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name=cinder-backup</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Alert may be incorrect if the service has migrated. Validate that the
        service is intended to be running on this node before restarting the
        service. Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name=cinder-scheduler</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node. Review the associated logs.
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run the <code class="filename">cinder-start.yml</code> playbook to start the
          process back up:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts cinder-start.yml \
--limit &lt;hostname&gt;</pre></div><div id="id-1.5.20.4.5.5.4.2.1.4.3.2.2.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
           The <code class="literal">--limit &lt;hostname&gt;</code> switch is optional.
           If it is included, then the <code class="literal">&lt;hostname&gt;</code> you
           should use is the host where the alarm was raised.
          </p></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name=cinder-volume</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span>Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Alert may be incorrect if the service has migrated. Validate that the
        service is intended to be running on this node before restarting the
        service. Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: cinder backup running
        &lt;hostname&gt; check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> cinder backup singleton
        check.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Backup process is one of
        the following:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          It is running on a node it should not be on
         </p></li><li class="listitem "><p>
          It is not running on a node it should be on
         </p></li></ul></div>
      </td><td valign="top">
       <p>
        Run the <code class="filename">cinder-migrate-volume.yml</code> playbook to
        migrate the volume and back up to the correct node:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run this playbook to migrate the service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts cinder-migrate-volume.yml</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: cinder volume running
        &lt;hostname&gt; check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> cinder volume singleton
        check.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The
        <code class="literal">cinder-volume</code> process is either:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          running on a node it should not be on, or
         </p></li><li class="listitem "><p>
          not running on a node it should be on
         </p></li></ul></div>
      </td><td valign="top">
       <p>
        Run the <code class="filename">cinder-migrate-volume.yml</code> playbook to
        migrate the volume and backup to correct node:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run this playbook to migrate the service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts cinder-migrate-volume.yml</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Storage faulty lun check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if local LUNs on
        your HPE servers using smartarray are not OK.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> A LUN on the server is
        degraded or has failed.
       </p>
      </td><td valign="top">
       <p>
        Log in to the reported host and run these commands to find out the
        status of the LUN:
       </p>
<div class="verbatim-wrap"><pre class="screen">sudo hpssacli
=&gt; ctrl slot=1 ld all show
=&gt; ctrl slot=1 pd all show</pre></div>
       <p>
        Replace any broken drives.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Storage faulty drive check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if the local disk
        drives on your HPE servers using smartarray are not OK.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> A disk drive on the
        server has failed or has warnings.
       </p>
      </td><td valign="top">
       <p>
        Log in to the reported and run these commands to find out the status of
        the drive:
       </p>
<div class="verbatim-wrap"><pre class="screen">sudo hpssacli
=&gt; ctrl slot=1 pd all show</pre></div>
       <p>
        Replace any broken drives.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to <code class="literal">DEBUG</code> instead of
        <code class="literal">INFO</code> level. Another reason could be due to a
        repeating error message filling up the log files. Finally, it could be
        due to log rotate not configured properly so old log files are not
        being deleted properly.
       </p>
      </td><td valign="top">
       <p>
        Find the service that is consuming too much disk space. Look at the
        logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
        level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
        error message, do what is needed to resolve the error. If old log files
        exist, configure log rotate to remove them. You could also choose to
        remove old log files by hand after backing them up if needed.
       </p>
      </td></tr></tbody></table></div></div></div><div class="sect3" id="networking-alarmdefinitions"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.1.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networking Alarms</span> <a title="Permalink" class="permalink" href="#networking-alarmdefinitions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-networking_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-networking_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span>networking-alarmdefinitions</li></ul></div></div></div></div><p>
  These alarms show under the Networking section of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Operations Console.
 </p><div class="sect4" id="id-1.5.20.4.5.6.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: NETWORKING</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.6.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-networking_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-networking_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running. Separate alarms for each of these neutron
        services, specified by the <code class="literal">component</code> dimension:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         ipsec/charon
        </p></li><li class="listitem "><p>
         neutron-openvswitch-agent
        </p></li><li class="listitem "><p>
         neutron-l3-agent
        </p></li><li class="listitem "><p>
         neutron-dhcp-agent
        </p></li><li class="listitem "><p>
         neutron-metadata-agent
        </p></li><li class="listitem "><p>
         neutron-server
        </p></li><li class="listitem "><p>
         neutron-vpn-agent
        </p></li></ul></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
       Restart the process on the affected node:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Cloud Lifecycle Manager.
        </p></li><li class="step "><p>
         Check the status of the networking status:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts neutron-status.yml</pre></div></li><li class="step "><p>
         Make note of the failed service names and the affected hosts which you
         will use to review the logs later.
        </p></li><li class="step "><p>
         Using the affected hostname(s) from the previous output, run the
         neutron start playbook to restart the services:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-start.yml \
--limit &lt;hostname&gt;</pre></div><div id="id-1.5.20.4.5.6.3.2.1.4.1.2.2.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
          You can pass multiple hostnames with
          <code class="literal">--limit</code> option by separating them with a colon
          <code class="literal">:</code>.
         </p></div></li><li class="step "><p>
         Check the status of the networking service again:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts neutron-status.yml</pre></div></li><li class="step "><p>
         Once all services are back up, you can SSH to the affected host(s) and
         review the logs in the location below for any errors around the time
         that the alarm triggered:
        </p><div class="verbatim-wrap"><pre class="screen">/var/log/neutron/&lt;service_name&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = neutron-rootwrap</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
       Currently <code class="literal">neutron-rootwrap</code> is only used to run
       <code class="literal">ovsdb-client</code>. To restart this process, use these
       steps:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         SSH to the affected host(s).
        </p></li><li class="step "><p>
         Restart the process:
        </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl restart neutron-openvswitch-agent</pre></div></li><li class="step "><p>
         Review the logs at the location below for errors:
        </p><div class="verbatim-wrap"><pre class="screen">/var/log/neutron/neutron-openvswitch-agent.log</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> neutron api health check
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process is stuck if the
        <code class="literal">neutron-server</code> Process Check is not OK.
       </p>
      </td><td valign="top">
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         SSH to the affected host(s).
        </p></li><li class="step "><p>
         Run this command to restart the <code class="literal">neutron-server</code>
         process:
        </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl restart neutron-server</pre></div></li><li class="step "><p>
         Review the logs at the location below for errors:
        </p><div class="verbatim-wrap"><pre class="screen">/var/log/neutron/neutron-server.log</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> neutron api health check
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The node
        crashed. Alternatively, only connectivity might have been lost if the
        local node HTTP Status is OK or UNKNOWN.
       </p>
      </td><td valign="top">Reboot the node if it crashed or diagnose the networking
      connectivity failures between the local and remote nodes. Review the
      logs.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Directory Log Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to <code class="literal">DEBUG</code> instead of
        <code class="literal">INFO</code> level. Another reason could be due to a
        repeating error message filling up the log files. Finally, it could be
        due to log rotate not configured properly so old log files are not
        being deleted properly.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.5.20.4.5.6.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: DNS in Networking section</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.6.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-networking_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-networking_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1" width="99%"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = designate-zone-manager</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
       Restart the process on the affected node using these steps:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Cloud Lifecycle Manager.
        </p></li><li class="step "><p>
         Use the designate start playbook against the affected node:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts designate-start.yml \
--limit 'DES-ZMG'</pre></div></li></ol></div></div>
      <p>
       Review the log located at:
      </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/designate/designate-zone-manager.log</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = designate-pool-manager</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
       Restart the process on the affected node using these steps:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Cloud Lifecycle Manager.
        </p></li><li class="step "><p>
         Use the designate start playbook against the affected node:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts designate-start.yml \
--limit 'DES-PMG'</pre></div></li></ol></div></div>
      <p>
       Review the log located at:
      </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/designate/designate-pool-manager.log</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = designate-central</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
       Restart the process on the affected node using these steps:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Cloud Lifecycle Manager.
        </p></li><li class="step "><p>
         Use the designate start playbook against the affected node:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts designate-start.yml \
--limit 'DES-CEN'</pre></div></li></ol></div></div>
      <p>
       Review the log located at:
      </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/designate/designate-central.log</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = designate-api</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
       Restart the process on the affected node using these steps:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Cloud Lifecycle Manager.
        </p></li><li class="step "><p>
         Use the designate start playbook against the affected node:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts designate-start.yml \
--limit 'DES-API'</pre></div></li></ol></div></div>
      <p>
       Review the log located at:
      </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/designate/designate-api.log</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = designate-mdns</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
       Restart the process on the affected node using these steps:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Cloud Lifecycle Manager.
        </p></li><li class="step "><p>
         Use the designate start playbook against the affected node:
        </p><div class="verbatim-wrap"><pre class="screen">         <code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts designate-start.yml \
--limit 'DES-MDN'</pre></div></li></ol></div></div>
      <p>
       Review the log located at:
      </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/designate/designate-mdns.log</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> <code class="literal">component =
        designate-api</code> This alarm will also have the
        <code class="literal">api_endpoint</code> and
        <code class="literal">monitored_host_types</code> dimensions defined. The likely
        cause and mitigation steps are the same for both.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The API is unresponsive.
       </p>
      </td><td valign="top">
       <p>
       Restart the process on the affected node using these steps:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Cloud Lifecycle Manager.
        </p></li><li class="step "><p>
         Use the designate start playbook against the affected node:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts designate-start.yml \
--limit 'DES-API,DES-CEN'</pre></div></li></ol></div></div>
      <p>
       Review the logs located at:
      </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/designate/designate-api.log
/var/log/designate/designate-central.log</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Directory Log Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to <code class="literal">DEBUG</code> instead of
        <code class="literal">INFO</code> level. Another reason could be due to a
        repeating error message filling up the log files. Finally, it could be
        due to log rotate not configured properly so old log files are not
        being deleted properly.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.5.20.4.5.6.5"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: BIND in Networking section</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.6.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-networking_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-networking_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name = pdns_server</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
       Restart the process on the affected node using these steps:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Cloud Lifecycle Manager.
        </p></li><li class="step "><p>
         Use the PowerDNS start playbook against the affected node:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts bind-start.yml</pre></div></li></ol></div></div>
      <p>
       Review the log located at, querying against <code class="literal">process =
       pdns_server</code>:
      </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/syslog</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name = named</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
       Restart the process on the affected node using these steps:
      </p>
      <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Log in to the Cloud Lifecycle Manager.
        </p></li><li class="step "><p>
         Use the Bind start playbook against the affected node:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts bind-start.yml</pre></div></li></ol></div></div>
      <p>
       Review the log located at, querying against <code class="literal">process =
       named</code>:
      </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/syslog</pre></div>
      </td></tr></tbody></table></div></div></div><div class="sect3" id="identity-alarmdefinitions"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.1.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Identity Alarms</span> <a title="Permalink" class="permalink" href="#identity-alarmdefinitions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-identity_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-identity_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span>identity-alarmdefinitions</li></ul></div></div></div></div><p>
  These alarms show under the Identity section of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Operations Console.
 </p><div class="sect4" id="id-1.5.20.4.5.7.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: IDENTITY-SERVICE</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.7.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-identity_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-identity_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> This check is contacting
        the keystone public endpoint directly.
       </p>
<div class="verbatim-wrap"><pre class="screen">component=keystone-api
api_endpoint=public</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The keystone service is
        down on the affected node.
       </p>
      </td><td valign="top">
       <p>
        Restart the keystone service on the affected node:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use the keystone start playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts keystone-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> This check is contacting
        the keystone admin endpoint directly
       </p>
<div class="verbatim-wrap"><pre class="screen">component=keystone-api
api_endpoint=admin</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The keystone service is
        down on the affected node.
       </p>
      </td><td valign="top">
       <p>
        Restart the keystone service on the affected node:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use the keystone start playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts keystone-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> This check is contacting
        the keystone admin endpoint via the virtual IP address (HAProxy)
       </p>
<div class="verbatim-wrap"><pre class="screen">component=keystone-api
monitored_host_type=vip</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The keystone service is
        unreachable via the virtual IP address.
       </p>
      </td><td valign="top">
       <p>
        If neither the <code class="literal">api_endpoint=public</code> or
        <code class="literal">api_endpoint=admin</code> alarms are triggering at the same
        time then there is likely a problem with haproxy.
       </p>
       <p>
        You can restart the haproxy service with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use this playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts FND-CLU-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Separate alarms for each
        of these glance services, specified by the <code class="literal">component</code>
        dimension:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          keystone-main
         </p></li><li class="listitem "><p>
          keystone admin
         </p></li></ul></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        You can restart the keystone service with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use this playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts keystone-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
       <p>
        Review the logs in <code class="literal">/var/log/keystone</code> on the affected
        node.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to <code class="literal">DEBUG</code> instead of
        <code class="literal">INFO</code> level. Another reason could be due to a
        repeating error message filling up the log files. Finally, it could be
        due to log rotate not configured properly so old log files are not
        being deleted properly.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div></div><div class="sect3" id="telemetry-alarmdefinitions"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.1.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Telemetry Alarms</span> <a title="Permalink" class="permalink" href="#telemetry-alarmdefinitions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-telemetry_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-telemetry_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span>telemetry-alarmdefinitions</li></ul></div></div></div></div><p>
  These alarms show under the Telemetry section of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Operations Console.
 </p><div class="sect4" id="id-1.5.20.4.5.8.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: TELEMETRY</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.8.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-telemetry_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-telemetry_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the
        <code class="literal">ceilometer-agent-notification</code> process is not
        running.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process has crashed.
       </p>
      </td><td valign="top">
       <p>
        Review the logs on the alarming host in the following location for the
        cause:
       </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/ceilometer/ceilometer-agent-notification-json.log</pre></div>
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use the ceilometer start playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ceilometer-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the
        <code class="literal">ceilometer-polling</code> process is not running.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process has crashed.
       </p>
      </td><td valign="top">
       <p>
        Review the logs on the alarming host in the following location for the
        cause:
       </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/ceilometer/ceilometer-polling-json.log</pre></div>
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use the ceilometer start playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ceilometer-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr></tbody></table></div></div><div class="sect4" id="id-1.5.20.4.5.8.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: METERING in Telemetry section</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.8.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-telemetry_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-telemetry_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to <code class="literal">DEBUG</code> instead of
        <code class="literal">INFO</code> level. Another reason could be due to a
        repeating error message filling up the log files. Finally, it could be
        due to log rotate not configured properly so old log files are not
        being deleted properly.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
       the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
       level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
       error message, do what is needed to resolve the error. If old log files
       exist, configure log rotate to remove them. You could also choose to
       remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.5.20.4.5.8.5"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: KAFKA in Telemetry section</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.8.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-telemetry_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-telemetry_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Kafka Persister Metric Consumer Lag</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the Persister
        consumer group is not keeping up with the incoming messages on the
        metric topic.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> There is a slow down in
        the system or heavy load.
       </p>
      </td><td valign="top">
       <p>
        Verify that all of the monasca-persister services are up with these
        steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager
         </p></li><li class="step "><p>
          Verify that all of the <code class="literal">monasca-persister</code> services
          are up with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags monasca-persister</pre></div></li></ol></div></div>
       <p>
        Look for high load in the various systems. This alert can fire for
        multiple topics or on multiple hosts. Determining which alarms are
        firing can help diagnose likely causes. For example, if the alarm is
        alerting all on one machine it could be the machine. If one topic
        across multiple machines it is likely the consumers of that topic, etc.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Kafka Alarm Transition Consumer Lag</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        consumer group is not keeping up with the incoming messages on the
        alarm state transition topic.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> There is a slow down in
        the system or heavy load.
       </p>
      </td><td valign="top">
       <p>
        Check that monasca-thresh and monasca-notification are up.
       </p>
       <p>
        Look for high load in the various systems. This alert can fire for
        multiple topics or on multiple hosts. Which alarms are firing can help
        diagnose likely causes. For example:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          If all alarms are on the same machine, the machine could be at fault.
         </p></li><li class="listitem "><p>
          If one topic is shared across multiple machines, the consumers of
          that topic are likely at fault.
         </p></li></ul></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Kafka Kronos Consumer Lag</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the Kronos
        consumer group is not keeping up with the incoming messages on the
        metric topic.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> There is a slow down in
        the system or heavy load.
       </p>
      </td><td valign="top">
       <p>
        Look for high load in the various systems. This alert can fire for
        multiple topics or on multiple hosts. Which alarms are firing can help
        diagnose likely causes. For example:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          If all alarms are on the same machine, the machine could be at fault.
         </p></li><li class="listitem "><p>
          If one topic is shared across multiple machines, the consumers of
          that topic are likely at fault.
         </p></li></ul></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = kafka.Kafka</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span>
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Stop the kafka service with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags kafka</pre></div></li><li class="step "><p>
          Start the kafka service back up with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags kafka</pre></div></li></ol></div></div>
       <p>
        Review the logs in <code class="filename">/var/log/kafka/server.log</code>
       </p>
      </td></tr></tbody></table></div></div><div class="sect4" id="id-1.5.20.4.5.8.6"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: LOGGING in Telemetry section</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.8.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-telemetry_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-telemetry_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Beaver Memory Usage</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Beaver is using more
        memory than expected. This may indicate that it cannot forward messages
        and its queue is filling up. If you continue to see this, see the
        troubleshooting guide.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Overloaded system or
        services with memory leaks.
       </p>
      </td><td valign="top">Log on to the reporting host to investigate high memory users.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Audit Log Partition Low Watermark</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> The
        <code class="literal">/var/audit</code> disk space usage has crossed low
        watermark. If the high watermark is reached, logrotate will be run to
        free up disk space. If needed, adjust:
       </p>
       <div class="verbatim-wrap"><pre class="screen">var_audit_low_watermark_percent</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to DEBUG instead of INFO level. Another reason could be due
        to a repeating error message filling up the log files. Finally, it
        could be due to log rotate not configured properly so old log files are
        not being deleted properly.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
       the logs. If DEBUG log entries exist, set the logging level to INFO. If
       the logs are repeatedly logging an error message, do what is needed to
       resolve the error. If old log files exist, configure log rotate to
       remove them. You could also choose to remove old log files by hand after
       backing them up if needed.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Audit Log Partition High Watermark</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> The
        <code class="literal">/var/audit</code> volume is running low on disk space.
        Logrotate will be run now to free up space. If needed, adjust:
       </p>
       <div class="verbatim-wrap"><pre class="screen">var_audit_high_watermark_percent</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to DEBUG instead of INFO level. Another reason could be due
        to a repeating error message filling up the log files. Finally, it
        could be due to log rotate not configured properly so old log files are
        not being deleted properly.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
       the logs. If DEBUG log entries exist, set the logging level to INFO. If
       the logs are repeatedly logging an error message, do what is needed to
       resolve the error. If old log files exist, configure log rotate to
       remove them. You could also choose to remove old log files by hand after
       backing them up if needed.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Elasticsearch Unassigned Shards</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> component =
        elasticsearch; Elasticsearch unassigned shards count is greater than
        0.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Environment could be
        misconfigured.
       </p>
      </td><td valign="top">
       <p>
        To find the unassigned shards, run the following command on the Cloud Lifecycle Manager
        from the <code class="filename">~/scratch/ansible/next/ardana/ansible</code>
        directory:
       </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible -i hosts/verb_hosts LOG-SVR[0] -m shell -a \
"curl localhost:9200/_cat/shards?pretty -s" | grep UNASSIGNED</pre></div>
       <p>
        This shows which shards are unassigned, like this:
       </p>
<div class="verbatim-wrap"><pre class="screen">logstash-2015.10.21 4 p UNASSIGNED ... 10.240.75.10 NodeName</pre></div>
       <p>
        The last column shows the name that Elasticsearch uses for the node
        that the unassigned shards are on. To find the actual host name, run:
       </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible -i hosts/verb_hosts LOG-SVR[0] -m shell -a \
"curl localhost:9200/_nodes/_all/name?pretty -s"</pre></div>
       <p>
        When you find the host name, take the following steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Make sure the node is not out of disk space, and free up space if
          needed.
         </p></li><li class="step "><p>
          Restart the node (use caution, as this may affect other services as
          well).
         </p></li><li class="step "><p>
          Make sure all versions of Elasticsearch are the same:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible -i hosts/verb_hosts LOG-SVR -m shell -a \
"curl localhost:9200/_nodes/_local/name?pretty -s" | grep version</pre></div></li><li class="step "><p>
          Contact customer support.
         </p></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Elasticsearch Number of Log Entries</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Elasticsearch Number of
        Log Entries: <code class="literal">component = elasticsearch;</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The number of log
        entries may get too large.
       </p>
      </td><td valign="top">Older versions of Kibana (version 3 and earlier) may hang if the
       number of log entries is too large (for example, above 40,000), and the
       page size would need to be small enough (about 20,000 results), because
       if it is larger (for example, 200,000), it may hang the browser, but
       Kibana 4 should not have this issue.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Elasticsearch Field Data Evictions</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Elasticsearch Field
        Data Evictions count is greater than 0: <code class="literal">component =
        elasticsearch</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Field Data Evictions may
        be found even though it is nowhere near the limit set.
       </p>
      </td><td valign="top">
       <p>
        The <code class="literal">elasticsearch_indices_fielddata_cache_size</code> is
        set to <code class="literal">unbounded</code> by default. If this is set by the
        user to a value that is insufficient, you may need to increase this
        configuration parameter or set it to <code class="literal">unbounded</code> and
        run a reconfigure using the steps below:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Edit the configuration file below and change the value for
          <code class="literal">elasticsearch_indices_fielddata_cache_size</code> to your
          desired value:
         </p><div class="verbatim-wrap"><pre class="screen">~/openstack/my_cloud/config/logging/main.yml</pre></div></li><li class="step "><p>
          Commit the changes to git:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -a -m "Elasticsearch fielddata cache size"</pre></div></li><li class="step "><p>
          Run the configuration processor:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
          Update your deployment directory:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
          Run the Logging reconfigure playbook to deploy the change:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts kronos-reconfigure.yml</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to <code class="literal">DEBUG</code> instead of
        <code class="literal">INFO</code> level. Another reason could be due to a
        repeating error message filling up the log files. Finally, it could be
        due to log rotate not configured properly so old log files are not
        being deleted properly.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
       the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
       level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
       error message, do what is needed to resolve the error. If old log files
       exist, configure log rotate to remove them. You could also choose to
       remove old log files by hand after backing them up if needed.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Separate alarms for each
        of these logging services, specified by the
        <code class="literal">process_name</code> dimension:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          elasticsearch
         </p></li><li class="listitem "><p>
          logstash
         </p></li><li class="listitem "><p>
          beaver
         </p></li><li class="listitem "><p>
          apache2
         </p></li><li class="listitem "><p>
          kibana
         </p></li></ul></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process has crashed.
       </p>
      </td><td valign="top">
       <p>
        On the affected node, attempt to restart the process.
       </p>
       <p>
        If the <code class="command">elasticsearch</code> process has crashed, use:
       </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo systemctl restart elasticsearch</pre></div>
       <p>
        If the logstash process has crashed, use:
       </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo systemctl restart logstash</pre></div>
       <p>
        The rest of the processes can be restarted using similar commands,
        listed here:
       </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo systemctl restart beaver
<code class="prompt user">ardana &gt; </code>sudo systemctl restart apache2
<code class="prompt user">ardana &gt; </code>sudo systemctl restart kibana</pre></div>
      </td></tr></tbody></table></div></div><div class="sect4" id="id-1.5.20.4.5.8.7"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.5.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: MONASCA-TRANSFORM in Telemetry section</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.8.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-telemetry_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-telemetry_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> <code class="literal">process_name =
        pyspark</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Service process has
        crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart process on affected node. Review logs.
       </p>
       <p>
        Child process of <code class="literal">spark-worker</code> but created once the
        <code class="literal">monasca-transform</code> process begins processing streams.
        If the process fails on one node only, along with the pyspark process,
        it is likely that the <code class="literal">spark-worker</code> has failed to
        connect to the elected leader of the <code class="literal">spark-master</code>
        service. In this case the <code class="literal">spark-worker</code> service
        should be started on the affected node. If on multiple nodes check the
        <code class="literal">spark-worker</code>, <code class="literal">spark-master</code> and
        <code class="literal">monasca-transform</code> services and logs. If the
        <code class="literal">monasca-transform</code> or <code class="literal">spark</code>
        services have been interrupted this process may not re-appear for up to
        ten minutes (the stream processing interval).
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span>
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name =
org.apache.spark.executor.CoarseGrainedExecutorBackend</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Service process has
        crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart process on affected node. Review logs.
       </p>
       <p>
        Child process of <code class="literal">spark-worker</code> but created once the
        <code class="literal">monasca-transform</code> process begins processing streams.
        If the process fails on one node only, along with the pyspark process,
        it is likely that the <code class="literal">spark-worker</code> has failed to
        connect to the elected leader of the <code class="literal">spark-master</code>
        service. In this case the <code class="literal">spark-worker</code> service
        should be started on the affected node. If on multiple nodes check the
        <code class="literal">spark-worker</code>, <code class="literal">spark-master</code> and
        <code class="literal">monasca-transform</code> services and logs. If the
        <code class="literal">monasca-transform</code> or <code class="literal">spark</code>
        services have been interrupted this process may not re-appear for up to
        ten minutes (the stream processing interval).
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> <code class="literal">process_name =
        monasca-transform</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Service process has
        crashed.
       </p>
      </td><td valign="top">Restart the service on affected node. Review logs.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.5.20.4.5.8.8"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.5.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: MONITORING in Telemetery section</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.8.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-telemetry_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-telemetry_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1" width="99%"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Persister Health Check
        <code class="literal">component = monasca-persister</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The process has crashed
        or a dependency is out.
       </p>
      </td><td valign="top">
       <p>
        If the process has crashed, restart it using the steps below. If a
        dependent service is down, address that issue.
       </p>
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if <code class="literal">monasca-api</code> is running on all nodes with
          this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags monasca-persister</pre></div></li><li class="step "><p>
          Use the monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags persister</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags monasca-persister</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> API Health Check
        <code class="literal">component = monasca-api</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The process has crashed
        or a dependency is out.
       </p>
      </td><td valign="top">
       <p>
        If the process has crashed, restart it using the steps below. If a
        dependent service is down, address that issue.
       </p>
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if <code class="literal">monasca-api</code> is running on all nodes with
          this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags monasca-api</pre></div></li><li class="step "><p>
          Use the monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags monasca-api</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags monasca-api</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: monasca Agent Collection Time</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the elapsed
        time the <code class="literal">monasca-agent</code> takes to collect metrics is
        high.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Heavy load on the box or
        a stuck agent plug-in.
       </p>
      </td><td valign="top">
       <p>
        Address the load issue on the machine. If needed, restart the agent
        using the steps below:
       </p>
       <p>
        Restart the agent on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if <code class="literal">monasca-agent</code> is running on all nodes
          with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</pre></div></li><li class="step "><p>
          Use the monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--limit &lt;hostname&gt;</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">component = kafka</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if Kafka is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags kafka</pre></div></li><li class="step "><p>
          Use the monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags kafka</pre></div></li><li class="step "><p>
          Verify that Kafka is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags kafka</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = monasca-notification</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if <code class="literal">monasca-api</code> is running on all nodes with
          this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags notification</pre></div></li><li class="step "><p>
          Use the monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags notification</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags notification</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name = monasca-agent</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the agent on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if <code class="literal">monasca-agent</code> is running on all nodes
          with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</pre></div></li><li class="step "><p>
          Use the monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--limit &lt;hostname&gt;</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-agent-status.yml</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name = monasca-api</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        &gt;Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if <code class="literal">monasca-api</code> is running on all nodes with
          this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags monasca-api</pre></div></li><li class="step "><p>
          Use the monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags monasca-api</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags monasca-api</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name =
        monasca-persister</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if <code class="literal">monasca-api</code> is running on all nodes with
          this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags monasca-persister</pre></div></li><li class="step "><p>
          Use the monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags persister</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags monasca-persister</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = backtype.storm.daemon.nimbus
component = apache-storm</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Review the logs in the <code class="filename">/var/log/storm</code> directory on
        all storm hosts to find the root cause.
       </p>
       <div id="id-1.5.20.4.5.8.8.2.1.4.9.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
         The logs containing threshold engine logging are on the 2nd and 3rd
         controller nodes.
        </p></div>
       <p>
        Restart <code class="literal">monasca-thresh</code>, if necessary, with these
        steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if <code class="literal">monasca-thresh</code> is running on all nodes
          with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags thresh</pre></div></li><li class="step "><p>
          Use the monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags thresh</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags thresh</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = backtype.storm.daemon.supervisor
component = apache-storm</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Review the logs in the <code class="literal">/var/log/storm</code> directory on
        all storm hosts to find the root cause.
       </p>
       <div id="id-1.5.20.4.5.8.8.2.1.4.10.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
         The logs containing threshold engine logging are on the 2nd and 3rd
         controller nodes.
        </p></div>
       <p>
        Restart monasca-thresh with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Stop the monasca-thresh service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-stop.yml \
--tags thresh</pre></div></li><li class="step "><p>
          Start the monasca-thresh service back up:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags thresh</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags thresh</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = backtype.storm.daemon.worker
component = apache-storm</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Review the logs in the <code class="literal">/var/log/storm</code> directory on
        all storm hosts to find the root cause.
       </p>
       <div id="id-1.5.20.4.5.8.8.2.1.4.11.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
         The logs containing threshold engine logging are on the 2nd and 3rd
         controller nodes.
        </p></div>
       <p>
        Restart <code class="literal">monasca-thresh</code> with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Stop the <code class="literal">monasca-thresh</code> service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-stop.yml \
--tags thresh</pre></div></li><li class="step "><p>
          Start the <code class="literal">monasca-thresh</code> service back up:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags thresh</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags thresh</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal"></code>
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = monasca-thresh
component = apache-storm</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Check if <code class="literal">monasca-thresh</code> is running on all nodes
          with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags thresh</pre></div></li><li class="step "><p>
          Use the monasca start playbook against the affected node to restart
          it:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml \
--tags thresh</pre></div></li><li class="step "><p>
          Verify that it is running on all nodes with this playbook:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-status.yml \
--tags thresh</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The service log
        directory, as indicated by the <code class="literal">path</code> dimension, is
        over the 2.5 GB quota.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
       the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
       level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
       error message, do what is needed to resolve the error. If old log files
       exist, configure log rotate to remove them. You could also choose to
       remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div></div><div class="sect3" id="console-alarmdefinitions"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.1.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Console Alarms</span> <a title="Permalink" class="permalink" href="#console-alarmdefinitions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-console_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-console_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span>console-alarmdefinitions</li></ul></div></div></div></div><p>
  These alarms show under the Console section of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Operations Console.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
      <p>
       <span class="bold"><strong>Name: HTTP Status</strong></span>
      </p>
      <p>
       <span class="bold"><strong>Description:</strong></span>
       <code class="literal">service=ops-console</code>
      </p>
      <p>
       <span class="bold"><strong>Likely cause:</strong></span> The Operations Console is
       unresponsive
      </p>
     </td><td valign="top">
      <p>
       Review logs in <code class="filename">/var/log/ops-console</code> and logs in
       <code class="filename">/var/log/apache2</code>. Restart ops-console by running
       the following commands on the Cloud Lifecycle Manager:
      </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ops-console-stop.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ops-console-start.yml</pre></div>
     </td></tr><tr><td valign="top">
      <p>
       <span class="bold"><strong>Name: Process Check</strong></span>
      </p>
      <p>
       <span class="bold"><strong>Description:</strong></span> Alarms when the specified
       process is not running:
       <code class="literal">process_name=leia-leia_monitor</code>
      </p>
      <p>
       <span class="bold"><strong>Likely cause:</strong></span> Process crashed or
       unresponsive.
      </p>
     </td><td valign="top">
      <p>
       Review logs in <code class="filename">/var/log/ops-console</code>. Restart
       ops-console by running the following commands on the Cloud Lifecycle Manager:
      </p>
<div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ops-console-stop.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ops-console-start.yml</pre></div>
     </td></tr></tbody></table></div></div><div class="sect3" id="system-alarmdefinitions"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.1.1.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">System Alarms</span> <a title="Permalink" class="permalink" href="#system-alarmdefinitions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-system_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-system_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span>system-alarmdefinitions</li></ul></div></div></div></div><p>
  These alarms show under the System section and are set up per
  <code class="literal">hostname</code> and/or <code class="literal">mount_point</code>.
 </p><div class="sect4" id="id-1.5.20.4.5.10.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: SYSTEM</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.10.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-system_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-system_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: CPU Usage</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms on high CPU usage.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Heavy load or runaway
        processes.
       </p>
      </td><td valign="top">Log onto the reporting host and diagnose the heavy CPU usage.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Elasticsearch Low Watermark</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> <code class="literal">component =
        elasticsearch</code> Elasticsearch disk low watermark. Backup
        indices. If high watermark is reached, indices will be deleted. Adjust
        curator_low_watermark_percent, curator_high_watermark_percent, and
        elasticsearch_max_total_indices_size_in_bytes if needed.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Running out of disk
        space for <code class="filename">/var/lib/elasticsearch</code>.
       </p>
      </td><td valign="top">
       <p>
        Free up space by removing indices (backing them up first if desired).
        Alternatively, adjust <code class="literal">curator_low_watermark_percent</code>,
        <code class="literal">curator_high_watermark_percent</code>, and/or
        <code class="literal">elasticsearch_max_total_indices_size_in_bytes</code> if
        needed.
       </p>
       <p>
        For more information about how to back up your centralized logs, see
        <a class="xref" href="#central-log-configure-settings" title="13.2.5. Configuring Centralized Logging">Section 13.2.5, “Configuring Centralized Logging”</a>.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Elasticsearch High Watermark</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> <code class="literal">component =
        elasticsearch</code> Elasticsearch disk high watermark. Attempting
        to delete indices to free disk space. Adjust
        <code class="literal">curator_low_watermark_percent</code>,
        <code class="literal">curator_high_watermark_percent</code>, and
        <code class="literal">elasticsearch_max_total_indices_size_in_bytes</code> if
        needed.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Running out of disk
        space for <code class="filename">/var/lib/elasticsearch</code>
       </p>
      </td><td valign="top">
       <p>
        Verify that disk space was freed up by the curator. If needed, free up
        additional space by removing indices (backing them up first if
        desired). Alternatively, adjust curator_low_watermark_percent,
        curator_high_watermark_percent, and/or
        elasticsearch_max_total_indices_size_in_bytes if needed.
       </p>
       <p>
        For more information about how to back up your centralized logs, see
        <a class="xref" href="#central-log-configure-settings" title="13.2.5. Configuring Centralized Logging">Section 13.2.5, “Configuring Centralized Logging”</a>.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Log Partition Low Watermark</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> The
        <code class="filename">/var/log</code> disk space usage has crossed the low
        watermark. If the high watermark is reached,
        <code class="literal">logrotate</code> will be run to free up disk space. Adjust
        <code class="literal">var_log_low_watermark_percent</code> if needed.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to <code class="literal">DEBUG</code> instead of
        <code class="literal">INFO</code> level. Another reason could be due to a
        repeating error message filling up the log files. Finally, it could be
        due to log rotate not configured properly so old log files are not
        being deleted properly.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Log Partition High Watermark</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> The
        <code class="filename">/var/log</code> volume is running low on disk space.
        Logrotate will be run now to free up space. Adjust
        <code class="literal">var_log_high_watermark_percent</code> if needed.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> This could be due to a
        service set to <code class="literal">DEBUG</code> instead of
        <code class="literal">INFO</code> level. Another reason could be due to a
        repeating error message filling up the log files. Finally, it could be
        due to log rotate not configured properly so old log files are not
        being deleted properly.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Crash Dump Count</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms if it receives any
        metrics with <code class="literal">crash.dump_count</code> &gt; 0
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> When a crash dump is
        generated by kdump, the crash dump file is put into the
        <code class="filename">/var/crash</code> directory by default. Any crash dump
        files in this directory will cause the
        <code class="literal">crash.dump_count</code> metric to show a value greater than
        0.
       </p>
      </td><td valign="top">
       <p>
        Analyze the crash dump file(s) located in
        <code class="filename">/var/crash</code> on the host that generated the alarm to
        try to determine if a service or hardware caused the crash.
       </p>
       <p>
        Move the file to a new location so that a developer can take a look at
        it. Make sure all of the processes are back up after the crash (run the
        <code class="literal">&lt;service&gt;-status.yml</code> playbooks). When the
        <code class="filename">/var/crash</code> directory is empty, the <code class="literal">Crash
        Dump Count</code> alarm should transition back to OK.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Disk Inode Usage</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Nearly out of inodes for
        a partition, as indicated by the <code class="literal">mount_point</code>
        reported.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Many files on the disk.
       </p>
      </td><td valign="top">Investigate cleanup of data or migration to other partitions.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Disk Usage</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> High disk usage, as
        indicated by the <code class="literal">mount_point</code> reported.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Large files on the disk.
       </p>
      </td><td valign="top">
       <p>
        Investigate cleanup of data or migration to other partitions.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Host Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alerts when a host is
        unreachable. <code class="literal">test_type = ping</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Host or network is down.
       </p>
      </td><td valign="top">If a single host, attempt to restart the system. If multiple
      hosts, investigate network issues.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Memory Usage</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> High memory usage.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Overloaded system or
        services with memory leaks.
       </p>
      </td><td valign="top">Log onto the reporting host to investigate high memory users.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Network Errors</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms on a high network
        error rate.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Bad network or cabling.
       </p>
      </td><td valign="top">Take this host out of service until the network can be fixed.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: NTP Time Sync</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the NTP time
        offset is high.
       </p>
      </td><td valign="top">
       <p>
        Log in to the reported host and check if the ntp service is running.
       </p>
       <p>
        If it is running, then use these steps:
       </p>
       <div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
          Stop the service:
         </p><div class="verbatim-wrap"><pre class="screen">service ntpd stop</pre></div></li><li class="listitem "><p>
          Resynchronize the node's time:
         </p><div class="verbatim-wrap"><pre class="screen">/usr/sbin/ntpdate -b  &lt;ntp-server&gt;</pre></div></li><li class="listitem "><p>
          Restart the ntp service:
         </p><div class="verbatim-wrap"><pre class="screen">service ntp start</pre></div></li><li class="listitem "><p>
          Restart rsyslog:
         </p><div class="verbatim-wrap"><pre class="screen">service rsyslog restart</pre></div></li></ol></div>
      </td></tr></tbody></table></div></div></div><div class="sect3" id="other-alarmdefinitions"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.1.1.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Other Services Alarms</span> <a title="Permalink" class="permalink" href="#other-alarmdefinitions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span>other-alarmdefinitions</li></ul></div></div></div></div><p>
  These alarms show under the Other Services section of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  Operations Console.
 </p><div class="sect4" id="id-1.5.20.4.5.11.3"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: APACHE</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.11.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Apache Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms on failure to
        reach the Apache status endpoint.
       </p>
      </td><td valign="top"> </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name = apache2</code>
       </p>
      </td><td valign="top">If the Apache process goes down, connect to the affected node via
      SSH and restart it with this command: <code class="command">sudo systemctl restart
      apache2</code>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Apache Idle Worker Count</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when there are no
        idle workers in the Apache server.
       </p>
      </td><td valign="top"> </td></tr></tbody></table></div></div><div class="sect4" id="id-1.5.20.4.5.11.4"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: BACKUP in Other Services section</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.11.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The service log
        directory, as indicated by the <code class="literal">path</code> dimension, is
        over the 2.5 GB quota.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.5.20.4.5.11.5"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.8.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: HAPROXY in Other Services section</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.11.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name = haproxy</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> HA Proxy is not running
        on this machine.
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Run this playbook on the affected node:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts FND-CLU-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
       <p>
        Review the associated logs.
       </p>
      </td></tr></tbody></table></div></div><div class="sect4" id="id-1.5.20.4.5.11.6"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.8.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: ARDANA-UX-SERVICES in Other Services section</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.11.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1" width="99%"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        HTTP endpoint is down or not reachable.
       </p>
      </td><td valign="top"> </td></tr></tbody></table></div></div><div class="sect4" id="id-1.5.20.4.5.11.7"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.8.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: KEY-MANAGER in Other Services section</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.11.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal"></code>
       </p>
       <div class="verbatim-wrap"><pre class="screen">process_name = barbican-api</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process has crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use the barbican start playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts barbican-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        HTTP endpoint is down or not reachable.
       </p>
<div class="verbatim-wrap"><pre class="screen">component = barbican-api
api_endpoint = public or internal</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The endpoint is not
        responsive, it may be down.
       </p>
      </td><td valign="top">
       <p>
        For the HTTP Status alarms for the public and internal endpoints,
        restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Stop the barbican service:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts barbican-stop.yml \
--limit &lt;hostname&gt;</pre></div></li><li class="step "><p>
          Restart the barbican service back up:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts barbican-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
       <p>
        Examine the logs in <code class="filename">/var/log/barbican/</code> for
        possible error messages.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        HTTP endpoint is down or not reachable.
       </p>
<div class="verbatim-wrap"><pre class="screen">component = barbican-api
monitored_host_type = vip</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The barbican API on the
        admin virtual IP is down.
       </p>
      </td><td valign="top">This alarm is verifying access to the barbican API via the virtual
      IP address (HAProxy). If this check is failing but the other two HTTP
      Status alarms for the key-manager service are not then the issue is
      likely with HAProxy so you should view the alarms for that service. If
      the other two HTTP Status alarms are alerting as well then restart
      barbican using the steps listed.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The service log
        directory, as indicated by the <code class="literal">path</code> dimension, is
        over the 2.5 GB quota.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.5.20.4.5.11.8"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.8.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: MYSQL in Other Services section</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.11.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: MySQL Slow Query Rate</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the slow
        query rate is high.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The system load is too
        high.
       </p>
      </td><td valign="top">This could be an indication of near capacity limits or an exposed
      bad query. First, check overall system load and then investigate MySQL
      details.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> MySQL crashed.
       </p>
      </td><td valign="top">Restart MySQL on the affected node.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.5.20.4.5.11.9"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.8.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: OCTAVIA in Other Services section</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.11.9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running. There are individual alarms for each of these
        processes:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          octavia-worker
         </p></li><li class="listitem "><p>
          octavia-housekeeping
         </p></li><li class="listitem "><p>
          octavia-api
         </p></li><li class="listitem "><p>
          octavia-health-manager
         </p></li></ul></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The process has crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the process on the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use the Octavia start playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts octavia-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        HTTP endpoint is down or not reachable.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The <code class="literal">octavia-api
        </code>process could be down or you could be experiencing an issue
        with either haproxy or another network related issue.
       </p>
      </td><td valign="top">
       <p>
        If the <code class="literal">octavia-api</code> process is down, restart it on
        the affected node using these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Use the Octavia start playbook against the affected node:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts octavia-start.yml \
--limit &lt;hostname&gt;</pre></div></li></ol></div></div>
       <p>
        If it is not the <code class="literal">octavia-process</code> that is the issue,
        then check if there is an issue with <code class="literal">haproxy</code> or
        possibly a network issue and troubleshoot accordingly.
       </p>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The service log
        directory, as indicated by the <code class="literal">path</code> dimension, is
        over the 2.5 GB quota.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.5.20.4.5.11.10"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.8.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: ORCHESTRATION in Other Services section</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.11.10">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running. There are individual alarms for each of these
        processes:
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          heat-api
         </p></li><li class="listitem "><p>
          heat-api-cfn
         </p></li><li class="listitem "><p>
          heat-api-cloudwatch
         </p></li><li class="listitem "><p>
          heat-engine
         </p></li></ul></div>
       <p>
        heat-api process check on each node
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">
       <p>
        Restart the process with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Stop all the heat processes:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts heat-start.yml</pre></div></li><li class="step "><p>
          Start the heat processes back up:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts heat-start.yml</pre></div></li></ol></div></div>
       <p>
        Review the relevant log at the following locations on the affected
        node:
       </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/heat/heat-api.log
/var/log/heat/heat-cfn.log
/var/log/heat/heat-cloudwatch.log
/var/log/heat/heat-engine.log</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        HTTP endpoint is down or not reachable.
       </p>
       <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
          heat-api
         </p></li><li class="listitem "><p>
          heat-api-cfn
         </p></li><li class="listitem "><p>
          heat-api-cloudwatch
         </p></li></ul></div>
      </td><td valign="top">
       <p>
        Restart the heat service with these steps:
       </p>
       <div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
          Log in to the Cloud Lifecycle Manager.
         </p></li><li class="step "><p>
          Stop all the heat processes:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts heat-start.yml</pre></div></li><li class="step "><p>
          Start the heat processes back up:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts heat-start.yml</pre></div></li></ol></div></div>
       <p>
        Review the relevant log at the following locations on the affected
        node:
       </p>
<div class="verbatim-wrap"><pre class="screen">/var/log/heat/heat-api.log
/var/log/heat/heat-cfn.log
/var/log/heat/heat-cloudwatch.log</pre></div>
      </td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The service log
        directory, as indicated by the <code class="literal">path</code> dimension, is
        over the 2.5 GB quota.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.5.20.4.5.11.11"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.8.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: OVSVAPP-SERVICEVM in Other Services section</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.11.11">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1" width="99%"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span>Alarms when the specified
        process is not running:
       </p>
<div class="verbatim-wrap"><pre class="screen">process_name = ovs-vswitchd
process_name = neutron-ovsvapp-agent
process_name = ovsdb-server</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process has crashed.
       </p>
      </td><td valign="top">Restart process on affected node. Review logs.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.5.20.4.5.11.12"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.8.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: RABBITMQ in Other Services section</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.11.12">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1" width="99%"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running:
       </p>
<div class="verbatim-wrap"><pre class="screen">process_name = rabbitmq
process_name = epmd</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process has crashed.
       </p>
      </td><td valign="top">Restart process on affected node. Review logs.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.5.20.4.5.11.13"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.8.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: SPARK in Other Services section</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.11.13">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1" width="99%"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running
       </p>
<div class="verbatim-wrap"><pre class="screen">process_name = org.apache.spark.deploy.master.Master
process_name = org.apache.spark.deploy.worker.Worker</pre></div>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process has crashed.
       </p>
      </td><td valign="top">Restart process on affected node. Review logs.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.5.20.4.5.11.14"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.8.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: WEB-UI in Other Services section</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.11.14">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: HTTP Status</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        HTTP endpoint is down or not reachable.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Apache is not running or
        there is a misconfiguration.
       </p>
      </td><td valign="top">Check that Apache is running; investigate horizon logs.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Service Log Directory Size</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Service log directory
        consuming more disk than its quota.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> The service log
        directory, as indicated by the <code class="literal">path</code> dimension, is
        over the 2.5 GB quota.
       </p>
      </td><td valign="top">Find the service that is consuming too much disk space. Look at
      the logs. If <code class="literal">DEBUG</code> log entries exist, set the logging
      level to <code class="literal">INFO</code>. If the logs are repeatedly logging an
      error message, do what is needed to resolve the error. If old log files
      exist, configure log rotate to remove them. You could also choose to
      remove old log files by hand after backing them up if needed.</td></tr></tbody></table></div></div><div class="sect4" id="id-1.5.20.4.5.11.15"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.1.1.8.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SERVICE: ZOOKEEPER in Other Services section</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.5.11.15">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-other_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-other_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: Process Check</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the specified
        process is not running: <code class="literal">process_name =
        org.apache.zookeeper.server</code>
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Process crashed.
       </p>
      </td><td valign="top">Restart the process on the affected node. Review the associated
      logs.</td></tr><tr><td valign="top">
       <p>
        <span class="bold"><strong>Name: ZooKeeper Latency</strong></span>
       </p>
       <p>
        <span class="bold"><strong>Description:</strong></span> Alarms when the ZooKeeper
        latency is high.
       </p>
       <p>
        <span class="bold"><strong>Likely cause:</strong></span> Heavy system load.
       </p>
      </td><td valign="top">Check the individual system as well as activity across the entire
      service.</td></tr></tbody></table></div></div></div><div class="sect3" id="esx-alarmdefinitions"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.1.1.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">ESX vCenter Plugin Alarms</span> <a title="Permalink" class="permalink" href="#esx-alarmdefinitions">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-esx_alarmdefinitions.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-esx_alarmdefinitions.xml</li><li><span class="ds-label">ID: </span>esx-alarmdefinitions</li></ul></div></div></div></div><p>
  These alarms relate to your ESX cluster, if you are utilizing one.
 </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Alarm Information</th><th>Mitigation Tasks</th></tr></thead><tbody valign="top"><tr><td valign="top">
      <p>
       <span class="bold"><strong>Name: ESX cluster CPU Usage</strong></span>
      </p>
      <p>
       <span class="bold"><strong>Description:</strong></span> Alarms when average of CPU
       usage for a particular cluster exceeds 90% continuously for 3 polling
       cycles.
      </p>
      <p>
       Alarm will have the following dimension:
      </p>
<div class="verbatim-wrap"><pre class="screen">esx_cluster_id=&lt;domain&gt;.&lt;vcenter-id&gt;</pre></div>
      <p>
       <span class="bold"><strong>Likely cause:</strong></span> Virtual machines are
       consuming more than 90% of allocated vCPUs.
      </p>
     </td><td valign="top">
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Reduce the load on highly consuming virtual machines by
         restarting/stopping one or more services.
        </p></li><li class="listitem "><p>
         Add more vCPUs to the host(s) attached to the cluster.
        </p></li></ul></div>
     </td></tr><tr><td valign="top">
      <p>
       <span class="bold"><strong>Name: ESX cluster Disk Usage</strong></span>
      </p>
      <p>
       <span class="bold"><strong>Description:</strong></span>
      </p>
      <div class="itemizedlist " id="ul-ctc-lpy-5x"><ul class="itemizedlist"><li class="listitem "><p>
         Alarms when the total size of the all shared datastores attached to
         the cluster exceeds 90% of their total allocated capacity.
        </p></li></ul></div>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Or in the case of cluster having a single host, the size of non-shared
         datastore exceeds 90% of its allocated capacity.
        </p></li><li class="listitem "><p>
         Alarm will have the following dimension:
        </p><div class="verbatim-wrap"><pre class="screen">esx_cluster_id=&lt;domain&gt;.&lt;vcenter-id&gt;</pre></div></li></ul></div>
      <p>
       <span class="bold"><strong>Likely cause:</strong></span>
      </p>
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Virtual machines occupying the storage.
        </p></li><li class="listitem "><p>
         Large file or image being copied on the datastore(s).
        </p></li></ul></div>
     </td><td valign="top">
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Check the virtual machines that are consuming more disk space. Delete
         unnecessary files.
        </p></li><li class="listitem "><p>
         Delete unnecessary files and images from database(s).
        </p></li><li class="listitem "><p>
         Add storage to the datastore(s).
        </p></li></ul></div>
     </td></tr><tr><td valign="top">
      <p>
       <span class="bold"><strong>Name: ESX cluster Memory Usage</strong></span>
      </p>
      <p>
       <span class="bold"><strong>Description:</strong></span> Alarms when average of RAM
       memory usage for a particular cluster, exceeds 90% continuously for 3
       polling cycles.
      </p>
      <p>
       Alarm will have the following dimension:
      </p>
<div class="verbatim-wrap"><pre class="screen">esx_cluster_id=&lt;domain&gt;.&lt;vcenter-id&gt;</pre></div>
      <p>
       <span class="bold"><strong>Likely cause:</strong></span> Virtual machines are
       consuming more than 90% of their total allocated memory.
      </p>
     </td><td valign="top">
      <div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Reduce the load on the highly consuming virtual machines by restarting
         or stopping one or more services.
        </p></li><li class="listitem "><p>
         Add more memory to the host(s) attached to the cluster.
        </p></li></ul></div>
     </td></tr></tbody></table></div></div></div><div class="sect2" id="topic-ask-vgb-dv"><div class="titlepage"><div><div><h3 class="title"><span class="number">18.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Support Resources</span> <a title="Permalink" class="permalink" href="#topic-ask-vgb-dv">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-contacting_support.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-contacting_support.xml</li><li><span class="ds-label">ID: </span>topic-ask-vgb-dv</li></ul></div></div></div></div><p>
  To solve issues in your cloud, consult the Knowledge Base or contact
  Sales Engineering.
 </p><div class="sect3" id="kb"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Knowledge Base</span> <a title="Permalink" class="permalink" href="#kb">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-contacting_support.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-contacting_support.xml</li><li><span class="ds-label">ID: </span>kb</li></ul></div></div></div></div><p>
   Support information is available at the SUSE Support page <a class="link" href="https://www.suse.com/products/suse-openstack-cloud/" target="_blank">https://www.suse.com/products/suse-openstack-cloud/</a>. This page
   offers access to the Knowledge Base, forums and documentation.
  </p></div><div class="sect3" id="id-1.5.20.4.6.4"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.1.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Contacting SUSE Support</span> <a title="Permalink" class="permalink" href="#id-1.5.20.4.6.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-contacting_support.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-contacting_support.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   The central location for information about accessing and using SUSE
   Technical Support is available at <a class="link" href="https://www.suse.com/support/handbook/" target="_blank">https://www.suse.com/support/handbook/</a>. This page has
   guidelines and links to many online support services, such as support
   account management, incident reporting, issue reporting, feature requests,
   training, consulting.
  </p></div></div></div><div class="sect1" id="troubleshooting-controlplane"><div class="titlepage"><div><div><h2 class="title"><span class="number">18.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Control Plane Troubleshooting</span> <a title="Permalink" class="permalink" href="#troubleshooting-controlplane">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_controlplane.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_controlplane.xml</li><li><span class="ds-label">ID: </span>troubleshooting-controlplane</li></ul></div></div></div></div><p>
  Troubleshooting procedures for control plane services.
 </p><div class="sect2" id="recoverrabbit"><div class="titlepage"><div><div><h3 class="title"><span class="number">18.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Understanding and Recovering RabbitMQ after Failure</span> <a title="Permalink" class="permalink" href="#recoverrabbit">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-recover_rabbit.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-recover_rabbit.xml</li><li><span class="ds-label">ID: </span>recoverrabbit</li></ul></div></div></div></div><p>
  RabbitMQ is the message queue service that runs on each of your controller
  nodes and brokers communication between multiple services in your
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 cloud environment. It is important for cloud operators to
  understand how different troubleshooting scenarios affect RabbitMQ so they
  can minimize downtime in their environments. We are going to discuss multiple
  scenarios and how it affects RabbitMQ. We will also explain how you can
  recover from them if there are issues.
 </p><div class="sect3" id="idg-all-operations-troubleshooting-recover-rabbit-xml-7"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.2.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How upgrades affect RabbitMQ</span> <a title="Permalink" class="permalink" href="#idg-all-operations-troubleshooting-recover-rabbit-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-recover_rabbit.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-recover_rabbit.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-recover-rabbit-xml-7</li></ul></div></div></div></div><p>
   There are two types of upgrades within <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> -- major and minor. The
   effect that the upgrade process has on RabbitMQ depends on these types.
  </p><p>
   A major upgrade is defined by an erlang change or major version upgrade of
   RabbitMQ. A minor upgrade would be an upgrade where RabbitMQ stays within
   the same version, such as v3.4.3 to v.3.4.6.
  </p><p>
   During both types of upgrades there may be minor blips in the authentication
   process of client services as the accounts are recreated.
  </p><p>
   <span class="bold"><strong>RabbitMQ during a major upgrade</strong></span>
  </p><p>
   There will be a RabbitMQ service outage while the upgrade is performed.
  </p><p>
   During the upgrade, high availability consistency is compromised -- all but
   the primary node will go down and will be reset, meaning their database
   copies are deleted. The primary node is not taken down until the last step
   and then it is upgrade. The database of users and permissions is maintained
   during this process. Then the other nodes are brought back into the cluster
   and resynchronized.
  </p><p>
   <span class="bold"><strong>RabbitMQ during a minor upgrade</strong></span>
  </p><p>
   Minor upgrades are performed node by node. This "rolling" process means
   there should be no overall service outage because each node is taken out of
   its cluster in turn, its database is reset, and then it is added back to the
   cluster and resynchronized.
  </p></div><div class="sect3" id="idg-all-operations-troubleshooting-recover-rabbit-xml-8"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.2.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How RabbitMQ is affected by other operational processes</span> <a title="Permalink" class="permalink" href="#idg-all-operations-troubleshooting-recover-rabbit-xml-8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-recover_rabbit.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-recover_rabbit.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-recover-rabbit-xml-8</li></ul></div></div></div></div><p>
   There are operational tasks, such as <a class="xref" href="#stop-restart" title="15.1.1.1. Bringing Down Your Cloud: Services Down Method">Section 15.1.1.1, “Bringing Down Your Cloud: Services Down Method”</a>, where
   you use the <code class="filename">ardana-stop.yml</code> and
   <code class="filename">ardana-start.yml</code> playbooks to gracefully restart your cloud.
   If you use these playbooks, and there are no errors associated with them
   forcing you to troubleshoot further, then RabbitMQ is brought down
   gracefully and brought back up. There is nothing special to note regarding
   RabbitMQ in these normal operational processes.
  </p><p>
   However, there are other scenarios where an understanding of RabbitMQ is
   important when a graceful shutdown did not occur.
  </p><p>
   These examples that follow assume you are using one of the entry-scale
   models where RabbitMQ is hosted on your controller node cluster. If you are
   using a mid-scale model or have a dedicated cluster that RabbitMQ lives on
   you may need to alter the steps accordingly. To determine which nodes
   RabbitMQ is on you can use the <code class="filename">rabbit-status.yml</code> playbook
   from your Cloud Lifecycle Manager.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts rabbitmq-status.yml</pre></div><p>
   <span class="bold"><strong>Your entire control plane cluster goes down</strong></span>
  </p><p>
   If you have a scenario where all of your controller nodes went down, either
   manually or via another process such as a power outage, then an
   understanding of how RabbitMQ should be brought back up is important. Follow
   these steps to recover RabbitMQ on your controller node cluster in these
   cases:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     The order in which the nodes went down is key here. Locate the last node
     to go down as this will be used as the primary node when bringing the
     RabbitMQ cluster back up. You can review the timestamps in the
     <code class="filename">/var/log/rabbitmq</code> log file to determine what the last
     node was.
    </p><div id="id-1.5.20.5.3.4.8.1.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The <code class="literal">primary</code> status of a node is transient, it only applies for the
      duration that this process is running. There is no long-term distinction
      between any of the nodes in your cluster. The primary node is simply
      the one that owns the RabbitMQ configuration database that will be
      synchronized across the cluster.
     </p></div></li><li class="step "><p>
     Run the <code class="filename">ardana-start.yml</code> playbook specifying the primary
     node (aka the last node down determined in the first step):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml -e rabbit_primary_hostname=&lt;hostname&gt;</pre></div><div id="id-1.5.20.5.3.4.8.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The <code class="literal">&lt;hostname&gt;</code> value will be the "shortname" for
      your node, as found in the <code class="filename">/etc/hosts</code> file.
     </p></div></li></ol></div></div><p>
   <span class="bold"><strong>If one of your controller nodes goes down</strong></span>
  </p><p>
   First step here is to determine whether the controller that went down is the
   primary RabbitMQ host or not. The primary host is going to be the first host
   member in the <code class="literal">FND-RMQ</code> group in the file below on your
   Cloud Lifecycle Manager:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>~/scratch/ansible/next/ardana/ansible/hosts/verb_hosts</pre></div><p>
   In this example below, <code class="literal">ardana-cp1-c1-m1-mgmt</code> would be the
   primary:
  </p><div class="verbatim-wrap"><pre class="screen">[FND-RMQ-ccp-cluster1:children]
ardana-cp1-c1-m1-mgmt
ardana-cp1-c1-m2-mgmt
ardana-cp1-c1-m3-mgmt</pre></div><p>
   If your primary RabbitMQ controller node has gone down and you need to bring
   it back up, you can follow these steps. In this playbook you are using the
   <code class="literal">rabbit_primary_hostname</code> parameter to specify the hostname
   for one of the other controller nodes in your environment hosting RabbitMQ,
   which will service as the primary node in the recovery. You will also use
   the <code class="literal">--limit</code> parameter to specify the controller node you
   are attempting to bring back up.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml -e rabbit_primary_hostname=&lt;new_primary_hostname&gt; --limit &lt;hostname_of_node_you_are_bringing_up&gt;</pre></div><p>
   If the node you need to bring back is <span class="bold"><strong>not</strong></span>
   the primary RabbitMQ node then you can just run the
   <code class="filename">ardana-start.yml</code> playbook with the
   <code class="literal">--limit</code> parameter and your node should recover:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit &lt;hostname_of_node_you_are_bringing_up&gt;</pre></div><p>
   <span class="bold"><strong>If you are replacing one or more of your controller
   nodes</strong></span>
  </p><p>
   The same general process noted above is used if you are removing or
   replacing one or more of your controller nodes.
  </p><p>
   If your node needs minor hardware repairs, but does not need to be replaced
   with a new node, you should use the <code class="filename">ardana-stop.yml</code> playbook
   with the <code class="literal">--limit</code> parameter to stop services on that node
   prior to removing it from the cluster.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log into the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Run the <code class="filename">rabbitmq-stop.yml</code> playbook, specifying the
     hostname of the node you are removing, which will remove the node from the
     RabbitMQ cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts rabbitmq-stop.yml --limit &lt;hostname_of_node_you_are_removing&gt;</pre></div></li><li class="step "><p>
     Run the <code class="filename">ardana-stop.yml</code> playbook, again specifying the
     hostname of the node you are removing, which will stop the rest of the
     services and prepare it to be removed:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-stop.yml --limit &lt;hostname_of_node_you_are_removing&gt;</pre></div></li></ol></div></div><p>
   If your node cannot be repaired and needs to be replaced with another
   baremetal node, any references to the replaced node must be removed from the
   RabbitMQ cluster. This is because RabbitMQ associates a cookie with each
   node in the cluster which is derived, in part, by the specific hardware.  So
   it is possible to replace a hard drive in a node. However changing a
   motherboard or replacing the node with another node entirely may cause
   RabbitMQ to stop working. When this happens, the running RabbitMQ cluster
   must be edited from a running RabbitMQ node. The following steps show how to
   do this.
  </p><p>
   In this example, controller 3 is the node being replaced with the following
   steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible</pre></div></li><li class="step "><p>
     SSH to a running RabbitMQ cluster node.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ssh cloud-cp1-rmq-mysql-m1-mgmt</pre></div></li><li class="step "><p>
     Force the cluster to forget the node you are removing (in this example,
     the controller 3 node).
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo rabbitmqctl forget_cluster_node \
rabbit@cloud-cp1-rmq-mysql-m3-mgmt</pre></div></li><li class="step "><p>
     Confirm that the node has been removed.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo rabbitmqctl cluster_status</pre></div></li><li class="step "><p>
     On the replacement node, information and services related to RabbitMQ must be removed.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo systemctl stop rabbitmq-server
<code class="prompt user">ardana &gt; </code>sudo systemctl stop epmd.socket&gt;</pre></div></li><li class="step "><p>
     Verify that the epmd service has stopped (kill it if it is still running).
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ps -eaf | grep epmd.</pre></div></li><li class="step "><p>
     Remove the Mnesia database directory.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo rm -rf /var/lib/rabbitmq/mnesia</pre></div></li><li class="step "><p>
     Restart the RabbitMQ server.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>sudo systemctl start rabbitmq-server</pre></div></li><li class="step "><p>
     On the Cloud Lifecycle Manager, run the <code class="filename">ardana-start.yml</code> playbook.
    </p></li></ol></div></div><p>
   If the node you are removing/replacing is your primary host then when you
   are adding it to your cluster then you will want to ensure that you specify
   a new primary host when doing so, as follows:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml -e rabbit_primary_hostname=&lt;new_primary_hostname&gt; --limit &lt;hostname_of_node_you_are_adding&gt;</pre></div><p>
   If the node you are removing/replacing is not your primary host then you can
   add it as follows:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit &lt;hostname_of_node_you_are_adding&gt;</pre></div><p>
   <span class="bold"><strong>If one of your controller nodes has rebooted or
   temporarily lost power</strong></span>
  </p><p>
   After a single reboot, RabbitMQ will not automatically restart. This is by
   design to protect your RabbitMQ cluster. To restart RabbitMQ, you should
   follow the process below.
  </p><p>
   If the rebooted node was your primary RabbitMQ host, you will specify a
   different primary hostname using one of the other nodes in your cluster:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml -e rabbit_primary_hostname=&lt;new_primary_hostname&gt; --limit &lt;hostname_of_node_that_rebooted&gt;</pre></div><p>
   If the rebooted node was not the primary RabbitMQ host then you can just
   start it back up with this playbook:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit &lt;hostname_of_node_that_rebooted&gt;</pre></div></div><div class="sect3" id="recovery"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.2.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recovering RabbitMQ</span> <a title="Permalink" class="permalink" href="#recovery">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-recover_rabbit.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-recover_rabbit.xml</li><li><span class="ds-label">ID: </span>recovery</li></ul></div></div></div></div><p>
   In this section we will show you how to check the status of RabbitMQ and how
   to do a variety of disaster recovery procedures.
  </p><p>
   <span class="bold"><strong>Verifying the status of RabbitMQ</strong></span>
  </p><p>
   You can verify the status of RabbitMQ on each of your controller nodes by
   using the following steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Run the <code class="filename">rabbitmq-status.yml</code> playbook:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts rabbitmq-status.yml</pre></div></li><li class="step "><p>
     If all is well, you should see an output similar to the following:
    </p><div class="verbatim-wrap"><pre class="screen">PLAY RECAP ********************************************************************
rabbitmq | status | Check RabbitMQ running hosts in cluster ------------- 2.12s
rabbitmq | status | Check RabbitMQ service running ---------------------- 1.69s
rabbitmq | status | Report status of RabbitMQ --------------------------- 0.32s
-------------------------------------------------------------------------------
Total: ------------------------------------------------------------------ 4.36s
ardana-cp1-c1-m1-mgmt  : ok=2    changed=0    unreachable=0    failed=0
ardana-cp1-c1-m2-mgmt  : ok=2    changed=0    unreachable=0    failed=0
ardana-cp1-c1-m3-mgmt  : ok=2    changed=0    unreachable=0    failed=0</pre></div></li></ol></div></div><p>
   If one or more of your controller nodes are having RabbitMQ issues then
   continue reading, looking for the scenario that best matches yours.
  </p><p>
   <span class="bold"><strong>RabbitMQ recovery after a small network
   outage</strong></span>
  </p><p>
   In the case of a transient network outage, the version of RabbitMQ included
   with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> 9 is likely to recover automatically without any further
   action needed. However, if yours does not and the
   <code class="filename">rabbitmq-status.yml</code> playbook is reporting an issue then
   use the scenarios below to resolve your issues.
  </p><p>
   <span class="bold"><strong>All of your controller nodes have gone down and using
   other methods have not brought RabbitMQ back up</strong></span>
  </p><p>
   If your RabbitMQ cluster is irrecoverable and you need rapid service
   recovery because other methods either cannot resolve the issue or you do not
   have time to investigate more nuanced approaches then we provide a disaster
   recovery playbook for you to use. This playbook will tear down and reset any
   RabbitMQ services. This does have an extreme effect on your services. The
   process will ensure that the RabbitMQ cluster is recreated.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Run the RabbitMQ disaster recovery playbook. This generally takes around
     two minutes.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts rabbitmq-disaster-recovery.yml</pre></div></li><li class="step "><p>
     Run the reconfigure playbooks for both cinder (Block Storage) and heat
     (Orchestration), if those services are present in your cloud. These
     services are affected when the fan-out queues are not recovered correctly.
     The reconfigure generally takes around five minutes.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts heat-reconfigure.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts kronos-server-configure.yml</pre></div></li><li class="step "><p>
     If you need to do a safe recovery of all the services in your environment
     then you can use this playbook. This is a more lengthy process as all
     services are inspected.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml</pre></div></li></ol></div></div><p>
   <span class="bold"><strong>One of your controller nodes has gone down and using
   other methods have not brought RabbitMQ back up</strong></span>
  </p><p>
   This disaster recovery procedure has the same caveats as the preceding one,
   but the steps differ.
  </p><p>
   If your primary RabbitMQ controller node has gone down and you need to
   perform a disaster recovery, use this playbook from your Cloud Lifecycle Manager:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts rabbitmq-disaster-recovery.yml -e rabbit_primary_hostname=&lt;new_primary_hostname&gt; --limit &lt;hostname_of_node_that_needs_recovered&gt;</pre></div><p>
   If the controller node is not your primary, you can use this playbook:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts rabbitmq-disaster-recovery.yml --limit &lt;hostname_of_node_that_needs_recovered&gt;</pre></div><p>
   No reconfigure playbooks are needed because all of the fan-out exchanges are
   maintained by the running members of your RabbitMQ cluster.
  </p></div></div></div><div class="sect1" id="ts-compute"><div class="titlepage"><div><div><h2 class="title"><span class="number">18.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting Compute service</span> <a title="Permalink" class="permalink" href="#ts-compute">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_compute.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_compute.xml</li><li><span class="ds-label">ID: </span>ts-compute</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the nova service.
 </p><p>
  nova offers scalable, on-demand, self-service access to compute resources.
  You can use this guide to help with known issues and troubleshooting of nova
  services.
 </p><div class="sect2" id="idg-all-operations-troubleshooting-ts-compute-xml-6"><div class="titlepage"><div><div><h3 class="title"><span class="number">18.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How can I reset the state of a compute instance?</span> <a title="Permalink" class="permalink" href="#idg-all-operations-troubleshooting-ts-compute-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_compute.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_compute.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-ts-compute-xml-6</li></ul></div></div></div></div><p>
   If you have an instance that is stuck in a non-Active state, such as
   <code class="literal">Deleting</code> or <code class="literal">Rebooting</code> and you want to
   reset the state so you can interact with the instance again, there is a way
   to do this.
  </p><p>
   The OSC command-line tool command, <code class="literal">openstack server set
   –state</code>, allows you to reset the state of a server.
  </p><p>
   Here is the content of the help information about the command which shows
   the syntax:
  </p><div class="verbatim-wrap"><pre class="screen">$ openstack help reset-state
        usage: openstack set -state [--active] &lt;server&gt; [&lt;server&gt; ...]

        Reset the state of a server.

        Positional arguments:
        &lt;server&gt;  Name or ID of server(s).

        Optional arguments:
        --active  Request the server be reset to "active" state instead of "error"
        state (the default).</pre></div><p>
   If you had an instance that was stuck in a <code class="literal">Rebooting</code>
   state you would use this command to reset it back to
   <code class="literal">Active</code>:
  </p><div class="verbatim-wrap"><pre class="screen">openstack server set –state --active &lt;instance_id&gt;</pre></div></div><div class="sect2" id="ts-resize"><div class="titlepage"><div><div><h3 class="title"><span class="number">18.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling the migrate or resize functions in nova post-installation when using encryption</span> <a title="Permalink" class="permalink" href="#ts-resize">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_compute.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_compute.xml</li><li><span class="ds-label">ID: </span>ts-resize</li></ul></div></div></div></div><p>
   If you have used encryption for your data when running the configuration
   processor during your cloud deployment and are enabling the nova resize and
   migrate functionality after the initial installation, there is an issue that
   arises if you have made additional configuration changes that required you to
   run the configuration processor before enabling these features.
  </p><p>
   You will only experience an issue if you have enabled encryption. If you
   haven't enabled encryption, then there is no need to follow the procedure
   below. If you are using encryption and you have made a configuration change
   and run the configuration processor after your initial install or upgrade,
   and you have run the <code class="filename">ready-deployment.yml</code> playbook, and
   you want to enable migrate or resize in nova, then the following steps will
   allow you to proceed. Note that the ansible vault key referred to below is
   the encryption key that you have provided to the configuration processor.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Checkout the ansible branch of your local git:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack
git checkout ansible</pre></div></li><li class="step "><p>
     Do a git log, and pick the previous commit:
    </p><div class="verbatim-wrap"><pre class="screen">git log</pre></div><p>
     In this example below, the commit is
     <code class="literal">ac54d619b4fd84b497c7797ec61d989b64b9edb3</code>:
    </p><div class="verbatim-wrap"><pre class="screen">$ git log

              commit 69f95002f9bad0b17f48687e4d97b2a791476c6a
              Merge: 439a85e ac54d61
              Author: git user &lt;user@company.com&gt;
              Date:   Fri May 6 09:08:55 2016 +0000

              Merging promotion of saved output

              commit 439a85e209aeeca3ab54d1a9184efb01604dbbbb
              Author: git user &lt;user@company.com&gt;
              Date:   Fri May 6 09:08:24 2016 +0000

              Saved output from CP run on 1d3976dac4fd7e2e78afad8d23f7b64f9d138778

              commit <span class="bold"><strong>ac54d619b4fd84b497c7797ec61d989b64b9edb3</strong></span>
              Merge: a794083 66ffe07
              Author: git user &lt;user@company.com&gt;
              Date:   Fri May 6 08:32:04 2016 +0000

              Merging promotion of saved output</pre></div></li><li class="step "><p>
     Checkout the commit:
    </p><div class="verbatim-wrap"><pre class="screen">git checkout &lt;commit_ID&gt;</pre></div><p>
     Using the same example above, here is the command:
    </p><div class="verbatim-wrap"><pre class="screen">$ git checkout ac54d619b4fd84b497c7797ec61d989b64b9edb3
              Note: checking out 'ac54d619b4fd84b497c7797ec61d989b64b9edb3'.

              You are in 'detached HEAD' state. You can look around, make experimental
              changes and commit them, and you can discard any commits you make in this
              state without impacting any branches by performing another checkout.

              If you want to create a new branch to retain commits you create, you may
              do so (now or later) by using -b with the checkout command again. Example:

              git checkout -b new_branch_name

              HEAD is now at ac54d61... Merging promotion of saved output</pre></div></li><li class="step "><p>
     Change to the ansible output directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/my_cloud/stage/ansible/group_vars/</pre></div></li><li class="step "><p>
     View the <code class="literal">group_vars</code> file from the ansible vault - it
     will be of the form below, with your compute cluster name being the
     indicator:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;cloud name&gt;-&lt;control plane name&gt;-&lt;compute cluster name&gt;</pre></div><p>
     View this group_vars file from the ansible vault with this command which
     will prompt you for your vault password:
    </p><div class="verbatim-wrap"><pre class="screen">ansible-vault view &lt;group_vars_file&gt;</pre></div></li><li class="step "><p>
     Search the contents of this file for the <code class="literal">nova_ssh_key</code>
     section which will contain both the private and public SSH keys which you
     should then save into a temporary file so you can use it in a later step.
    </p><p>
     Here is an example snippet, with the bold part being what you need to
     save:
    </p><div class="verbatim-wrap"><pre class="screen">NOV_KVM:
                vars:
                <span class="bold"><strong>              nova_ssh_key:
                  private: '-----BEGIN RSA PRIVATE KEY-----
                  MIIEpAIBAAKCAQEAv/hhekzykD2K8HnVNBKZcJWYrVlUyb6gR8cvE6hbh2ISzooA
                  jQc3xgglIwpt5TuwpTY3LL0C4PEHObxy9WwqXTHBZp8jg/02RzD02bEcZ1WT49x7
                  Rj8f5+S1zutHlDv7PwEIMZPAHA8lihfGFG5o+QHUmsUHgjShkWPdHXw1+6mCO9V/
                  eJVZb3nDbiunMOBvyyk364w+fSzes4UDkmCq8joDa5KkpTgQK6xfw5auEosyrh8D
                  zocN/JSdr6xStlT6yY8naWziXr7p/QhG44RPD9SSD7dhkyJh+bdCfoFVGdjmF8yA
                  h5DlcLu9QhbJ/scb7yMP84W4L5GwvuWCCFJTHQIDAQABAoIBAQCCH5O7ecMFoKG4
                  JW0uMdlOJijqf93oLk2oucwgUANSvlivJX4AGj9k/YpmuSAKvS4cnqZBrhDwdpCG
                  Q0XNM7d3mk1VCVPimNWc5gNiOBpftPNdBcuNryYqYq4WBwdq5EmGyGVMbbFPk7jH
                  ZRwAJ2MCPoplKl7PlGtcCMwNu29AGNaxCQEZFmztXcEFdMrfpTh3kuBI536pBlEi
                  Srh23mRILn0nvLXMAHwo94S6bI3JOQSK1DBCwtA52r5YgX0nkZbi2MvHISY1TXBw
                  SiWgzqW8dakzVu9UNif9nTDyaJDpU0kr0/LWtBQNdcpXnDSkHGjjnIm2pJVBC+QJ
                  SM9o8h1lAoGBANjGHtG762+dNPEUUkSNWVwd7tvzW9CZY35iMR0Rlux4PO+OXwNq
                  agldHeUpgG1MPl1ya+rkf0GD62Uf4LHTDgaEkUfiXkYtcJwHbjOnj3EjZLXaYMX2
                  LYBE0bMKUkQCBdYtCvZmo6+dfC2DBEWPEhvWi7zf7o0CJ9260aS4UHJzAoGBAOK1
                  P//K7HBWXvKpY1yV2KSCEBEoiM9NA9+RYcLkNtIy/4rIk9ShLdCJQVWWgDfDTfso
                  sJKc5S0OtOsRcomvv3OIQD1PvZVfZJLKpgKkt20/w7RwfJkYC/jSjQpzgDpZdKRU
                  vRY8P5iryptleyImeqV+Vhf+1kcH8t5VQMUU2XAvAoGATpfeOqqIXMpBlJqKjUI2
                  QNi1bleYVVQXp43QQrrK3mdlqHEU77cYRNbW7OwUHQyEm/rNN7eqj8VVhi99lttv
                  fVt5FPf0uDrnVhq3kNDSh/GOJQTNC1kK/DN3WBOI6hFVrmZcUCO8ewJ9MD8NQG7z
                  4NXzigIiiktayuBd+/u7ZxMCgYEAm6X7KaBlkn8KMypuyIsssU2GwHEG9OSYay9C
                  Ym8S4GAZKGyrakm6zbjefWeV4jMZ3/1AtXg4tCWrutRAwh1CoYyDJlUQAXT79Phi
                  39+8+6nSsJimQunKlmvgX7OK7wSp24U+SPzWYPhZYzVaQ8kNXYAOlezlquDfMxxv
                  GqBE5QsCgYA8K2p/z2kGXCNjdMrEM02reeE2J1Ft8DS/iiXjg35PX7WVIZ31KCBk
                  wgYTWq0Fwo2W/EoJVl2o74qQTHK0Bs+FTnR2nkVF3htEOAW2YXQTTN2rEsHmlQqE
                  A9iGTNwm9hvzbvrWeXtx8Zk/6aYfsXCoxq193KglS40shOCaXzWX0w==
                  -----END RSA PRIVATE KEY-----'
                  public: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC/+GF6TPKQPYrwedU0Epl
                  wlZitWVTJvqBHxy8TqFuHYhLOigCNBzfGCCUjCm3lO7ClNjcsvQLg8Qc5vHL1bCpdMc
                  FmnyOD/TZHMPTZsRxnVZPj3HtGPx/n5LXO60eUO/s/AQgxk8AcDyWKF8YUbmj5Ad
                  SaxQeCNKGRY90dfDX7qYI71X94lVlvecNuK6cw4G/LKTfrjD59LN6zhQOSYKryOgNrkq
                  SlOBArrF/Dlq4SizKuHwPOhw38lJ2vrFK2VPrJjydpbOJevun9CEbjhE8P1JIPt2GTImH5t0
                  J+gVUZ2OYXzICHkOVwu71CFsn+xxvvIw/zhbgvkbC+5YIIUlMd
                  Generated Key for nova User</strong></span>
                NTP_CLI:</pre></div></li><li class="step "><p>
     Switch back to the <code class="literal">site</code> branch by checking it out:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack
git checkout site</pre></div></li><li class="step "><p>
     Navigate to your group_vars directory in this branch:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible/group_vars</pre></div></li><li class="step "><p>
     Edit your compute group_vars file, which will prompt you for your vault
     password:
    </p><div class="verbatim-wrap"><pre class="screen">ansible-vault edit &lt;group_vars_file&gt;
              Vault password:
              Decryption successful</pre></div></li><li class="step "><p>
     Search the contents of this file for the <code class="literal">nova_ssh_key</code>
     section and replace the private and public keys with the contents that you
     had saved in a temporary file in step #7 earlier.
    </p></li><li class="step "><p>
     Remove the temporary file that you created earlier. You are now ready to
     run the deployment. For information about enabling nova resizing and
     migration, see <a class="xref" href="#enabling-the-nova-resize" title="6.4. Enabling the Nova Resize and Migrate Features">Section 6.4, “Enabling the Nova Resize and Migrate Features”</a>.
    </p></li></ol></div></div></div><div class="sect2" id="idg-all-operations-troubleshooting-ts-compute-xml-9"><div class="titlepage"><div><div><h3 class="title"><span class="number">18.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute (ESX)</span> <a title="Permalink" class="permalink" href="#idg-all-operations-troubleshooting-ts-compute-xml-9">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_compute.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_compute.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-ts-compute-xml-9</li></ul></div></div></div></div><p>

   <span class="bold"><strong>Unable to Create Instance Snapshot when Instance is
   Active</strong></span>
  </p><p>
   There is a known issue with VMWare vCenter where if you have a compute
   instance in <code class="literal">Active</code> state you will receive the error below
   when attempting to take a snapshot of it:
  </p><div class="verbatim-wrap"><pre class="screen">An error occurred while saving the snapshot: Failed to quiesce the virtual machine</pre></div><p>
   The workaround for this issue is to stop the instance. Here are steps to
   achieve this using the command line tool:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Stop the instance using the OpenStackClient:
    </p><div class="verbatim-wrap"><pre class="screen">openstack server stop &lt;instance UUID&gt;</pre></div></li><li class="step "><p>
     Take the snapshot of the instance.
    </p></li><li class="step "><p>
     Start the instance back up:
    </p><div class="verbatim-wrap"><pre class="screen">openstack server start &lt;instance UUID&gt;</pre></div></li></ol></div></div></div><div class="sect2" id="idg-all-operations-troubleshooting-ts-compute-db-archive"><div class="titlepage"><div><div><h3 class="title"><span class="number">18.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How to archive deleted instances from the database</span> <a title="Permalink" class="permalink" href="#idg-all-operations-troubleshooting-ts-compute-db-archive">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_compute.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_compute.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-ts-compute-db-archive</li></ul></div></div></div></div><p>
   The nova-reconfigure.yml playbook can take a long time to run if the
   database has a large number of deleted instances.
  </p><p>
   To find the number of rows being used by deleted instances:
  </p><div class="verbatim-wrap"><pre class="screen">sudo mysql nova -e "select count(*) from instances where vm_state='deleted';"</pre></div><p>
   To archive a batch of 1000 deleted instances to shadow tables:
  </p><div class="verbatim-wrap"><pre class="screen">sudo /opt/stack/service/nova-api/venv/bin/nova-manage \
    --config-dir /opt/stack/service/nova-api/etc/nova/ \
    db archive_deleted_rows --verbose --max_rows 1000</pre></div></div></div><div class="sect1" id="neutron-troubleshooting"><div class="titlepage"><div><div><h2 class="title"><span class="number">18.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Service Troubleshooting</span> <a title="Permalink" class="permalink" href="#neutron-troubleshooting">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_troubleshooting.xml</li><li><span class="ds-label">ID: </span>neutron-troubleshooting</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the Networking service.
 </p><div class="sect2" id="id-1.5.20.7.3"><div class="titlepage"><div><div><h3 class="title"><span class="number">18.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">CVR HA - Split-brain result of failover of L3 agent when master comes back up</span> <a title="Permalink" class="permalink" href="#id-1.5.20.7.3">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_troubleshooting.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>This situation is specific to when L3 HA is configured and a network failure occurs to the node hosting the
   currently active l3 agent. L3 HA is intended to provide HA in situations
   where the l3-agent crashes or the node hosting an l3-agent crashes/restarts.
   In the case of a physical networking issue which isolates the active l3
   agent, the stand-by l3-agent takes over but when the physical networking
   issue is resolved, traffic to the VMs is disrupted due to a "split-brain"
   situation in which traffic is split over the two L3 agents. The solution is
   to restart the L3-agent that was originally the master.</p></div><div class="sect2" id="id-1.5.20.7.4"><div class="titlepage"><div><div><h3 class="title"><span class="number">18.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">OVSvApp Loses Connectivity with vCenter</span> <a title="Permalink" class="permalink" href="#id-1.5.20.7.4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_troubleshooting.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>If the OVSvApp loses connectivity with the vCenter cluster, you receive
   the following errors:</p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     The OVSvApp VM will go into ERROR state
    </p></li><li class="step "><p>
     The OVSvApp VM will not get IP address
    </p></li></ol></div></div><p>
   When you see these symptoms:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Restart the OVSvApp agent on the OVSvApp VM.
    </p></li><li class="step "><p>
     Execute the following command to restart the Network (neutron) service:
    </p><div class="verbatim-wrap"><pre class="screen">sudo service neutron-ovsvapp-agent restart</pre></div></li></ol></div></div></div><div class="sect2" id="id-1.5.20.7.5"><div class="titlepage"><div><div><h3 class="title"><span class="number">18.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Fail over a plain CVR router because the node became unavailable:</span> <a title="Permalink" class="permalink" href="#id-1.5.20.7.5">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_troubleshooting.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Get a list of l3 agent UUIDs which can be used in the commands that follow
    </p><div class="verbatim-wrap"><pre class="screen"> openstack network agent list | grep l3</pre></div></li><li class="step "><p>
     Determine the current host
    </p><div class="verbatim-wrap"><pre class="screen"> openstack network agent list –routers &lt;router uuid&gt;</pre></div></li><li class="step "><p>
     Remove the router from the current host
    </p><div class="verbatim-wrap"><pre class="screen">openstack network agent remove router –agent-type l3 &lt;current l3 agent uuid&gt; &lt;router uuid&gt;</pre></div></li><li class="step "><p>
     Add the router to a new host
    </p><div class="verbatim-wrap"><pre class="screen">openstack network agent add router –agent-type l3 &lt;new l3 agent uuid&gt; &lt;router uuid&gt;</pre></div></li></ol></div></div></div><div class="sect2" id="id-1.5.20.7.6"><div class="titlepage"><div><div><h3 class="title"><span class="number">18.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Trouble setting maximum transmission units (MTU)</span> <a title="Permalink" class="permalink" href="#id-1.5.20.7.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_troubleshooting.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p> See <a class="xref" href="#configureMTU" title="10.4.11. Configuring Maximum Transmission Units in neutron">Section 10.4.11, “Configuring Maximum Transmission Units in neutron”</a> for more information. </p></div><div class="sect2" id="id-1.5.20.7.7"><div class="titlepage"><div><div><h3 class="title"><span class="number">18.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Floating IP on allowed_address_pair port with
   DVR-routed networks <code class="literal">allowed_address_pair</code></span> <a title="Permalink" class="permalink" href="#id-1.5.20.7.7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_troubleshooting.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   <span class="bold"><strong>You may notice this issue:</strong></span> If you have an
   <code class="literal">allowed_address_pair</code> associated with multiple virtual
   machine (VM) ports, and if all the VM ports are ACTIVE, then the
   <code class="literal">allowed_address_pair</code> port binding will have the last
   ACTIVE VM's binding host as its bound host.
  </p><p>
   <span class="bold"><strong>In addition, you may notice</strong></span> that if the
   floating IP is assigned to the <code class="literal">allowed_address_pair</code> that
   is bound to multiple VMs that are ACTIVE, then the floating IP will not work
   with DVR routers. This is different from the centralized router behavior
   where it can handle unbound <code class="literal">allowed_address_pair</code> ports
   that are associated with floating IPs.
  </p><p>
   Currently we support <code class="literal">allowed_address_pair</code> ports with DVR
   only if they have floating IPs enabled, and have just one ACTIVE port.
  </p><p>
   Using the CLI, you can follow these steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Create a network to add the host to:
    </p><div class="verbatim-wrap"><pre class="screen">$ openstack network create vrrp-net</pre></div></li><li class="step "><p>
     Attach a subnet to that network with a specified allocation-pool range:
    </p><div class="verbatim-wrap"><pre class="screen">$ openstack subnet create  --name vrrp-subnet --allocation-pool start=10.0.0.2,end=10.0.0.200 vrrp-net 10.0.0.0/24</pre></div></li><li class="step "><p>
     Create a router, uplink the vrrp-subnet to it, and attach the router to an
     upstream network called public:
    </p><div class="verbatim-wrap"><pre class="screen">$ openstack router create router1
$ openstack router add subnet router1 vrrp-subnet
$ openstack router set router1 public</pre></div><p>
     Create a security group called vrrp-sec-group and add ingress rules to
     allow ICMP and TCP port 80 and 22:
    </p><div class="verbatim-wrap"><pre class="screen">$ openstack security group create vrrp-sec-group
$ openstack security group rule create  --protocol icmp vrrp-sec-group
$ openstack security group rule create  --protocol tcp  --port-range-min80 --port-range-max80 vrrp-sec-group
$ openstack security group rule create  --protocol tcp  --port-range-min22 --port-range-max22 vrrp-sec-group</pre></div></li><li class="step "><p>
     Next, boot two instances:
    </p><div class="verbatim-wrap"><pre class="screen">$ openstack server create --num-instances 2 --image ubuntu-12.04 --flavor 1 --nic net-id=24e92ee1-8ae4-4c23-90af-accb3919f4d1 vrrp-node --security_groups vrrp-sec-group</pre></div></li><li class="step "><p>
     When you create two instances, make sure that both the instances are not
     in ACTIVE state before you associate the
     <code class="literal">allowed_address_pair</code>. The instances:
    </p><div class="verbatim-wrap"><pre class="screen">$ openstack server list
+--------------------------------------+-------------------------------------------------+--------+------------+-------------+--------------------------------------------------------+
| ID                                   | Name                                            | Status | Task State | Power State | Networks                                               |
+--------------------------------------+-------------------------------------------------+--------+------------+-------------+--------------------------------------------------------+
| 15b70af7-2628-4906-a877-39753082f84f | vrrp-node-15b70af7-2628-4906-a877-39753082f84f | ACTIVE  | -          | Running     | vrrp-net=10.0.0.3                                      |
| e683e9d1-7eea-48dd-9d3a-a54cf9d9b7d6 | vrrp-node-e683e9d1-7eea-48dd-9d3a-a54cf9d9b7d6 | DOWN    | -          | Running     | vrrp-net=10.0.0.4                                      |
+--------------------------------------+-------------------------------------------------+--------+------------+-------------+--------------------------------------------------------+</pre></div></li><li class="step "><p>
     Create a port in the VRRP IP range that was left out of the ip-allocation
     range:
    </p><div class="verbatim-wrap"><pre class="screen">$ openstack port create --fixed-ip ip_address=10.0.0.201 --security-group vrrp-sec-group vrrp-net
Created a new port:
+-----------------------+-----------------------------------------------------------------------------------+
| Field                 | Value                                                                             |
+-----------------------+-----------------------------------------------------------------------------------+
| admin_state_up        | True                                                                              |
| allowed_address_pairs |                                                                                   |
| device_id             |                                                                                   |
| device_owner          |                                                                                   |
| fixed_ips             | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.201"} |
| id                    | 6239f501-e902-4b02-8d5c-69062896a2dd                                              |
| mac_address           | fa:16:3e:20:67:9f                                                                 |
| name                  |                                                                                   |
| network_id            | 24e92ee1-8ae4-4c23-90af-accb3919f4d1                                              |
| port_security_enabled | True                                                                              |
| security_groups       | 36c8131f-d504-4bcc-b708-f330c9f6b67a                                              |
| status                | DOWN                                                                              |
| tenant_id             | d4e4332d5f8c4a8eab9fcb1345406cb0                                                  |
+-----------------------+-----------------------------------------------------------------------------------+</pre></div></li><li class="step "><p>
     Another thing to cross check after you associate the allowed_address_pair
     port to the VM port, is whether the
     <code class="literal">allowed_address_pair</code> port has inherited the VM's host
     binding:
    </p><div class="verbatim-wrap"><pre class="screen">$ neutron --os-username admin --os-password ZIy9xitH55 --os-tenant-name admin port-show f5a252b2-701f-40e9-a314-59ef9b5ed7de
+-----------------------+--------------------------------------------------------------------------------------------------------+
| Field                 | Value                                                                                                  |
+-----------------------+--------------------------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                                   |
| allowed_address_pairs |                                                                                                        |
| {color:red}binding:host_id{color} | ...-cp1-comp0001-mgmt                                                                      |
| binding:profile       | {}                                                                                                     |
| binding:vif_details   | {"port_filter": true, "ovs_hybrid_plug": true}                                                         |
| binding:vif_type      | ovs                                                                                                    |
| binding:vnic_type     | normal                                                                                                 |
| device_id             |                                                                                                        |
| device_owner          | compute:None                                                                                           |
| dns_assignment        | {"hostname": "host-10-0-0-201", "ip_address": "10.0.0.201", "fqdn": "host-10-0-0-201.openstacklocal."} |
| dns_name              |                                                                                                        |
| extra_dhcp_opts       |                                                                                                        |
| fixed_ips             | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.201"}                      |
| id                    | 6239f501-e902-4b02-8d5c-69062896a2dd                                                                   |
| mac_address           | fa:16:3e:20:67:9f                                                                                      |
| name                  |                                                                                                        |
| network_id            | 24e92ee1-8ae4-4c23-90af-accb3919f4d1                                                                   |
| port_security_enabled | True                                                                                                   |
| security_groups       | 36c8131f-d504-4bcc-b708-f330c9f6b67a                                                                   |
| status                | DOWN                                                                                                   |
| tenant_id             | d4e4332d5f8c4a8eab9fcb1345406cb0                                                                       |
+-----------------------+--------------------------------------------------------------------------------------------------------+</pre></div></li><li class="step "><p>
     Note that you were allocated a port with the IP address 10.0.0.201 as
     requested. Next, associate a floating IP to this port to be able to access
     it publicly:
    </p><div class="verbatim-wrap"><pre class="screen">$ openstack floating ip create --port-id=6239f501-e902-4b02-8d5c-69062896a2dd public
Created a new floatingip:
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| fixed_ip_address    | 10.0.0.201                           |
| floating_ip_address | 10.36.12.139                         |
| floating_network_id | 3696c581-9474-4c57-aaa0-b6c70f2529b0 |
| id                  | a26931de-bc94-4fd8-a8b9-c5d4031667e9 |
| port_id             | 6239f501-e902-4b02-8d5c-69062896a2dd |
| router_id           | 178fde65-e9e7-4d84-a218-b1cc7c7b09c7 |
| tenant_id           | d4e4332d5f8c4a8eab9fcb1345406cb0     |
+---------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     Now update the ports attached to your VRRP instances to include this IP
     address as an allowed-address-pair so they will be able to send traffic
     out using this address. First find the ports attached to these instances:
    </p><div class="verbatim-wrap"><pre class="screen">$ openstack port list -- --network_id=24e92ee1-8ae4-4c23-90af-accb3919f4d1
+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------------+
| id                                   | name | mac_address       | fixed_ips                                                                         |
+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------------+
| 12bf9ea4-4845-4e2c-b511-3b8b1ad7291d |      | fa:16:3e:7a:7b:18 | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.4"}   |
| 14f57a85-35af-4edb-8bec-6f81beb9db88 |      | fa:16:3e:2f:7e:ee | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.2"}   |
| 6239f501-e902-4b02-8d5c-69062896a2dd |      | fa:16:3e:20:67:9f | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.201"} |
| 87094048-3832-472e-a100-7f9b45829da5 |      | fa:16:3e:b3:38:30 | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.1"}   |
| c080dbeb-491e-46e2-ab7e-192e7627d050 |      | fa:16:3e:88:2e:e2 | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.3"}   |
+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------------+</pre></div></li><li class="step "><p>
     Add this address to the ports c080dbeb-491e-46e2-ab7e-192e7627d050 and
     12bf9ea4-4845-4e2c-b511-3b8b1ad7291d which are 10.0.0.3 and 10.0.0.4 (your
     vrrp-node instances):
    </p><div class="verbatim-wrap"><pre class="screen">$ openstack port set  c080dbeb-491e-46e2-ab7e-192e7627d050 --allowed_address_pairs list=truetype=dict ip_address=10.0.0.201
$ openstack port set  12bf9ea4-4845-4e2c-b511-3b8b1ad7291d --allowed_address_pairs list=truetype=dict ip_address=10.0.0.201</pre></div></li><li class="step "><p>
     The allowed-address-pair 10.0.0.201 now shows up on the port:
    </p><div class="verbatim-wrap"><pre class="screen">$ openstack port show 12bf9ea4-4845-4e2c-b511-3b8b1ad7291d
+-----------------------+---------------------------------------------------------------------------------+
| Field                 | Value                                                                           |
+-----------------------+---------------------------------------------------------------------------------+
| admin_state_up        | True                                                                            |
| allowed_address_pairs | {"ip_address": "10.0.0.201", "mac_address": "fa:16:3e:7a:7b:18"}                |
| device_id             | e683e9d1-7eea-48dd-9d3a-a54cf9d9b7d6                                            |
| device_owner          | compute:None                                                                    |
| fixed_ips             | {"subnet_id": "94a0c371-d37c-4796-821e-57c2a8ec65ae", "ip_address": "10.0.0.4"} |
| id                    | 12bf9ea4-4845-4e2c-b511-3b8b1ad7291d                                            |
| mac_address           | fa:16:3e:7a:7b:18                                                               |
| name                  |                                                                                 |
| network_id            | 24e92ee1-8ae4-4c23-90af-accb3919f4d1                                            |
| port_security_enabled | True                                                                            |
| security_groups       | 36c8131f-d504-4bcc-b708-f330c9f6b67a                                            |
| status                | ACTIVE                                                                          |
| tenant_id             | d4e4332d5f8c4a8eab9fcb1345406cb0                                                |</pre></div></li></ol></div></div></div><div class="sect2" id="id-1.5.20.7.8"><div class="titlepage"><div><div><h3 class="title"><span class="number">18.4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">OpenStack traffic that must traverse VXLAN tunnel dropped when using HPE 5930 switch</span> <a title="Permalink" class="permalink" href="#id-1.5.20.7.8">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_troubleshooting.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>Cause: UDP destination port 4789 is conflicting with OpenStack
       VXLAN traffic.</p><p>
   There is a configuration setting you can use in the switch to configure the
   port number the HPN kit will use for its own VXLAN tunnels. Setting this to
   a port number other than the one neutron will use by default (4789) will
   keep the HPN kit from absconding with neutron's VXLAN traffic. Specifically:
  </p><p>
   <span class="bold"><strong>Parameters: </strong></span>
  </p><p>
   port-number: Specifies a UDP port number in the range of 1 to 65535. As a
   best practice, specify a port number in the range of 1024 to 65535 to avoid
   conflict with well-known ports.
  </p><p>
   <span class="bold"><strong>Usage guidelines:</strong></span>
  </p><p>
   You must configure the same destination UDP port number on all VTEPs in a
   VXLAN.
  </p><p>
   Examples
  </p><div class="verbatim-wrap"><pre class="screen"># Set the destination UDP port number to 6666 for VXLAN packets.
&lt;Sysname&gt; system-view
[Sysname] vxlan udp-port 6666</pre></div><p>
   Use vxlan udp-port to configure the destination UDP port number of VXLAN
   packets.   Mandatory for all VXLAN packets to specify a UDP port Default
   The destination UDP port number is 4789 for VXLAN packets.
  </p><p>
   OVS can be configured to use a different port number itself:
  </p><div class="verbatim-wrap"><pre class="screen"># (IntOpt) The port number to utilize if tunnel_types includes 'vxlan'. By
# default, this will make use of the Open vSwitch default value of '4789' if
# not specified.
#
# vxlan_udp_port =
# Example: vxlan_udp_port = 8472
#</pre></div></div><div class="sect2" id="DOCS-3584"><div class="titlepage"><div><div><h3 class="title"><span class="number">18.4.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Issue: PCI-PT virtual machine gets stuck at boot</span> <a title="Permalink" class="permalink" href="#DOCS-3584">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/networking-neutron_troubleshooting.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>networking-neutron_troubleshooting.xml</li><li><span class="ds-label">ID: </span>DOCS-3584</li></ul></div></div></div></div><p>
    If you are using a machine that uses Intel NICs, if the PCI-PT virtual
    machine gets stuck at boot, the boot agent should be disabled.
   </p><p>
    When Intel cards are used for PCI-PT, sometimes the tenant virtual machine
    gets stuck at boot. If this happens, you should download Intel bootutils
    and use it to disable the bootagent.
   </p><p>
    Use the following steps:
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Download <code class="literal">preebot.tar.gz</code> from the
      <a class="link" href="https://downloadcenter.intel.com/download/19186/Intel-Ethernet-Connections-Boot-Utility-Preboot-Images-and-EFI-Drivers" target="_blank">Intel
      website</a>.
     </p></li><li class="step "><p>
      Untar the <code class="literal">preboot.tar.gz</code> file on the compute host
      where the PCI-PT virtual machine is to be hosted.
     </p></li><li class="step "><p>
      Go to path <code class="literal">~/APPS/BootUtil/Linux_x64</code> and then run
      following command:
     </p><div class="verbatim-wrap"><pre class="screen">./bootutil64e -BOOTENABLE disable -all</pre></div></li><li class="step "><p>
      Now boot the PCI-PT virtual machine and it should boot without getting
      stuck.
     </p></li></ol></div></div></div></div><div class="sect1" id="troubleshooting-glance"><div class="titlepage"><div><div><h2 class="title"><span class="number">18.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting the Image (glance) Service</span> <a title="Permalink" class="permalink" href="#troubleshooting-glance">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_image.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_image.xml</li><li><span class="ds-label">ID: </span>troubleshooting-glance</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the glance service. We have
  gathered some of the common issues and troubleshooting steps that will help
  when resolving issues that occur with the glance service.
 </p><div class="sect2" id="image-upload"><div class="titlepage"><div><div><h3 class="title"><span class="number">18.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Images Created in Horizon UI Get Stuck in a Queued State</span> <a title="Permalink" class="permalink" href="#image-upload">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_image.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_image.xml</li><li><span class="ds-label">ID: </span>image-upload</li></ul></div></div></div></div><p>
   When creating a new image in the horizon UI you will see the option for
   <code class="literal">Image Location</code> which allows you to enter a HTTP source to
   use when creating a new image for your cloud. However, this option is
   disabled by default for security reasons. This results in any new images
   created via this method getting stuck in a <code class="literal">Queued</code> state.
  </p><p>
   We cannot guarantee the security of any third party sites you use as image
   sources and the traffic goes over HTTP (non-SSL) traffic.
  </p><p>
   <span class="bold"><strong>Resolution:</strong></span> You will need your cloud
   administrator to enable the HTTP store option in glance for your cloud.
  </p><p>
   Here are the steps to enable this option:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Edit the file below:
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack/ardana/ansible/roles/GLA-API/templates/glance-api.conf.j2</pre></div></li><li class="step "><p>
     Locate the glance store options and add the <code class="literal">http</code> value
     in the <code class="literal">stores</code> field. It will look like this:
    </p><div class="verbatim-wrap"><pre class="screen">[glance_store]
stores = {{ glance_stores }}</pre></div><p>
     Change this to:
    </p><div class="verbatim-wrap"><pre class="screen">[glance_store]
stores = {{ glance_stores }}<span class="bold"><strong>,http</strong></span></pre></div></li><li class="step "><p>
     Commit your configuration to the <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>, as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "adding HTTP option to glance store list"</pre></div></li><li class="step "><p>
     Run the configuration processor with this command:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Use the playbook below to create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the glance service reconfigure playbook which will update these
     settings:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts glance-reconfigure.yml</pre></div></li></ol></div></div></div></div><div class="sect1" id="troubleshooting-storage"><div class="titlepage"><div><div><h2 class="title"><span class="number">18.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Storage Troubleshooting</span> <a title="Permalink" class="permalink" href="#troubleshooting-storage">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_storage.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_storage.xml</li><li><span class="ds-label">ID: </span>troubleshooting-storage</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for swift services.
 </p><div class="sect2" id="troubleshooting-blockstorage"><div class="titlepage"><div><div><h3 class="title"><span class="number">18.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Block Storage Troubleshooting</span> <a title="Permalink" class="permalink" href="#troubleshooting-blockstorage">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_blockstorage.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_blockstorage.xml</li><li><span class="ds-label">ID: </span>troubleshooting-blockstorage</li></ul></div></div></div></div><p>
  The block storage service utilizes <span class="productname">OpenStack</span> cinder and can integrate with
  multiple back-ends including 3Par, SUSE Enterprise Storage, and Ceph. Failures may exist at the cinder API level,
  an operation may fail, or you may see an alarm trigger in the monitoring
  service. These may be caused by configuration problems, network issues, or
  issues with your servers or storage back-ends. The purpose of this page and
  section is to describe how the service works, where to find additional
  information, some of the common problems that come up, and how to address
  them.
 </p><div class="sect3" id="logs"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.6.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Where to find information</span> <a title="Permalink" class="permalink" href="#logs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_blockstorage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_blockstorage.xml</li><li><span class="ds-label">ID: </span>logs</li></ul></div></div></div></div><p>
   When debugging block storage issues it is helpful to understand the
   deployment topology and know where to locate the logs with additional
   information.
  </p><p>
   The cinder service consists of:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     An API service, typically deployed and active on the controller nodes.
    </p></li><li class="listitem "><p>
     A scheduler service, also typically deployed and active on the controller
     nodes.
    </p></li><li class="listitem "><p>
     A volume service, which is deployed on all of the controller nodes but
     only active on one of them.
    </p></li><li class="listitem "><p>
     A backup service, which is deployed on the same controller node as the
     volume service.
    </p></li></ul></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-hos.docs-troubleshooting-cinder_topology.png" target="_blank"><img src="images/media-hos.docs-troubleshooting-cinder_topology.png" width="" /></a></div></div><p>
   You can refer to your configuration files (usually located in
   <code class="literal">~/openstack/my_cloud/definition/</code> on the Cloud Lifecycle Manager) for
   specifics about where your services are located. They will usually be
   located on the controller nodes.
  </p><p>
   cinder uses a MariaDB database and communicates between components by
   consuming messages from a RabbitMQ message service.
  </p><p>
   The cinder API service is layered underneath a HAProxy service and accessed
   using a virtual IP address maintained using keepalived.
  </p><p>
   If any of the cinder components is not running on its intended host then an
   alarm will be raised. Details on how to resolve these alarms can be found on
   our <a class="xref" href="#alarmdefinitions" title="18.1.1. Alarm Resolution Procedures">Section 18.1.1, “Alarm Resolution Procedures”</a> page. You should check the logs for
   the service on the appropriate nodes. All cinder logs are stored in
   <code class="literal">/var/log/cinder/</code> and all log entries above
   <code class="literal">INFO</code> level are also sent to the centralized logging
   service. For details on how to change the logging level of the cinder
   service, see <a class="xref" href="#central-log-configure-services" title="13.2.6. Configuring Settings for Other Services">Section 13.2.6, “Configuring Settings for Other Services”</a>.
  </p><p>
   In order to get the full context of an error you may need to examine the
   full log files on individual nodes. Note that if a component runs on more
   than one node you will need to review the logs on each of the nodes that
   component runs on. Also remember that as logs rotate that the time interval
   you are interested in may be in an older log file.
  </p><p>
   <span class="bold"><strong>Log locations:</strong></span>
  </p><p>
   <code class="literal">/var/log/cinder/cinder-api.log</code> - Check this log if you
   have endpoint or connectivity issues
  </p><p>
   <code class="literal">/var/log/cinder/cinder-scheduler.log</code> - Check this log if
   the system cannot assign your volume to a back-end
  </p><p>
   <code class="literal">/var/log/cinder/cinder-backup.log</code> - Check this log if you
   have backup or restore issues
  </p><p>
   <code class="literal">/var/log/cinder-cinder-volume.log</code> - Check here for
   failures during volume creation
  </p><p>
   <code class="literal">/var/log/nova/nova-compute.log</code> - Check here for failures
   with attaching volumes to compute instances
  </p><p>
   You can also check the logs for the database and/or the RabbitMQ service if
   your cloud exhibits database or messaging errors.
  </p><p>
   If the API servers are up and running but the API is not reachable then
   checking the HAProxy logs on the active keepalived node would be the place
   to look.
  </p><p>
   If you have errors attaching volumes to compute instances using the nova API
   then the logs would be on the compute node associated with the instance. You
   can use the following command to determine which node is hosting the
   instance:
  </p><div class="verbatim-wrap"><pre class="screen">openstack server show &lt;instance_uuid&gt;</pre></div><p>
   Then you can check the logs located at
   <code class="literal">/var/log/nova/nova-compute.log</code> on that compute node.
  </p></div><div class="sect3" id="volume-states"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.6.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Understanding the cinder volume states</span> <a title="Permalink" class="permalink" href="#volume-states">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_blockstorage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_blockstorage.xml</li><li><span class="ds-label">ID: </span>volume-states</li></ul></div></div></div></div><p>
   Once the topology is understood, if the issue with the cinder service
   relates to a specific volume then you should have a good understanding of
   what the various states a volume can be in are. The states are:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     attaching
    </p></li><li class="listitem "><p>
     available
    </p></li><li class="listitem "><p>
     backing-up
    </p></li><li class="listitem "><p>
     creating
    </p></li><li class="listitem "><p>
     deleting
    </p></li><li class="listitem "><p>
     downloading
    </p></li><li class="listitem "><p>
     error
    </p></li><li class="listitem "><p>
     error attaching
    </p></li><li class="listitem "><p>
     error deleting
    </p></li><li class="listitem "><p>
     error detaching
    </p></li><li class="listitem "><p>
     error extending
    </p></li><li class="listitem "><p>
     error restoring
    </p></li><li class="listitem "><p>
     in-use
    </p></li><li class="listitem "><p>
     extending
    </p></li><li class="listitem "><p>
     restoring
    </p></li><li class="listitem "><p>
     restoring backup
    </p></li><li class="listitem "><p>
     retyping
    </p></li><li class="listitem "><p>
     uploading
    </p></li></ul></div><p>
   The common states are <code class="literal">in-use</code> which indicates a volume is
   currently attached to a compute instance and <code class="literal">available</code>
   means the volume is created on a back-end and is free to be attached to an
   instance. All <code class="literal">-ing</code> states are transient and represent a
   transition. If a volume stays in one of those states for too long indicating
   it is stuck, or if it fails and goes into an error state, you should check
   for failures in the logs.
  </p></div><div class="sect3" id="idg-all-operations-troubleshooting-ts-blockstorage-xml-7"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.6.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Initial troubleshooting steps</span> <a title="Permalink" class="permalink" href="#idg-all-operations-troubleshooting-ts-blockstorage-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_blockstorage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_blockstorage.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-ts-blockstorage-xml-7</li></ul></div></div></div></div><p>
   These should be the initial troubleshooting steps you go through.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     If you have noticed an issue with the service, you should check your
     monitoring system for any alarms that may have triggered. See
     <a class="xref" href="#alarmdefinitions" title="18.1.1. Alarm Resolution Procedures">Section 18.1.1, “Alarm Resolution Procedures”</a> for resolution steps for those alarms.
    </p></li><li class="step "><p>
     Check if the cinder API service is active by listing the available volumes
     from the Cloud Lifecycle Manager:
    </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc
openstack volume list</pre></div></li></ol></div></div></div><div class="sect3" id="common-issues"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.6.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Common failures</span> <a title="Permalink" class="permalink" href="#common-issues">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_blockstorage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_blockstorage.xml</li><li><span class="ds-label">ID: </span>common-issues</li></ul></div></div></div></div><p>
   <span class="bold"><strong>Alerts from the cinder service</strong></span>
  </p><p>
   Check for alerts associated with the block storage service, noting that
   these could include alerts related to the server nodes being down, alerts
   related to the messaging and database services, or the HAProxy and
   keepalived services, as well as alerts directly attributed to the block
   storage service.
  </p><p>
   The Operations Console provides a web UI method for checking
   alarms.
  </p><p>
   <span class="bold"><strong>cinder volume service is down</strong></span>
  </p><p>
   The cinder volume service could be down if the server hosting the
   volume service fails. (Running the command <code class="command">openstack volume service
   list</code> will show the state of the volume service.) In this case,
   follow the documented procedure linked below to start the volume service on
   another controller node. See <a class="xref" href="#sec-operation-manage-block-storage" title="8.1.3. Managing cinder Volume and Backup Services">Section 8.1.3, “Managing cinder Volume and Backup Services”</a> for details.
  </p><p>
   <span class="bold"><strong>Creating a cinder bootable volume fails</strong></span>
  </p><p>
   When creating a bootable volume from an image, your cinder volume must be
   larger than the Virtual Size (raw size) of your image or creation will fail
   with an error.
  </p><p>
   When creating your disk model for nodes that will have the cinder volume
   role, make sure that there is sufficient disk space allocated for temporary
   space for image conversion if you will be creating bootable volumes.
   Allocate enough space to the filesystem as would be needed to contain
   the raw size of images to be used for bootable volumes. For example,
   Windows images can be quite large in raw format.
  </p><p>
   By default, cinder uses <code class="literal">/var/lib/cinder</code> for image
   conversion and this will be on the root filesystem unless it is explicitly
   separated. You can ensure there is enough space by ensuring that the root
   file system is sufficiently large, or by creating a logical volume mounted
   at <code class="literal">/var/lib/cinder</code> in the disk model when installing the
   system.
  </p><p>
   If your system is already installed, use these steps to update this:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Edit the configuration item <code class="literal">image_conversion_dir</code> in
     <code class="literal">cinder.conf.j2</code> to point to another location with more
     disk space. Make sure that the new directory location has the same
     ownership and permissions as <code class="literal">/var/lib/cinder</code>
     (owner:cinder group:cinder. mode 0750).
    </p></li><li class="listitem "><p>
     Then run this playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</pre></div></li></ol></div><p>
   <span class="bold"><strong>API-level failures</strong></span>
  </p><p>
   If the API is inaccessible, determine if the API service has halted on
   the controller nodes. If a single instance of <code class="literal">cinder-api</code>
   goes down but other instances remain online on other controllers, load
   balancing would typically automatically direct all traffic to the online
   nodes. The <code class="filename">cinder-status.yml</code> playbook can be used to
   report on the health of the API service from the Cloud Lifecycle Manager:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts cinder-status.yml</pre></div><p>
   Service failures can be diagnosed by reviewing the logs in centralized
   logging or on the individual controller nodes.
  </p><div id="id-1.5.20.9.3.6.17" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    After a controller node is rebooted, you must make sure to run the
    <code class="literal">ardana-start.yml</code> playbook to ensure all the services are
    up and running. For more information, see
    <a class="xref" href="#recover-downed-cluster" title="15.2.3.1. Restarting Controller Nodes After a Reboot">Section 15.2.3.1, “Restarting Controller Nodes After a Reboot”</a>.
   </p></div><p>
   If the API service is returning an error code, look for the error message in
   the API logs on all API nodes. Successful completions would be logged like
   this:
  </p><div class="verbatim-wrap"><pre class="screen">2016-04-25 10:09:51.107 30743 INFO eventlet.wsgi.server [<span class="bold"><strong>req-a14cd6f3-6c7c-4076-adc3-48f8c91448f6</strong></span>
dfb484eb00f94fb39b5d8f5a894cd163 7b61149483ba4eeb8a05efa92ef5b197 - - -] 192.168.186.105 - - [25/Apr/2016
10:09:51] "GET /v2/7b61149483ba4eeb8a05efa92ef5b197/volumes/detail HTTP/1.1" <span class="bold"><strong>200</strong></span> 13915 0.235921</pre></div><p>
   where <code class="literal">200</code> represents HTTP status 200 for a successful
   completion. Look for a line with your status code and then examine all
   entries associated with the request id. The request ID in the successful
   completion is highlighted in bold above.
  </p><p>
   The request may have failed at the scheduler or at the volume or backup
   service and you should also check those logs at the time interval of
   interest, noting that the log file of interest may be on a different node.
  </p><p>
   <span class="bold"><strong>Operations that do not complete</strong></span>
  </p><p>
   If you have started an operation, such as creating or deleting a volume,
   that does not complete, the cinder volume may be stuck in a state. You
   should follow the procedures for detaling with stuck volumes.
  </p><p>
   There are six transitory states that a volume can get stuck in:
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>State</th><th>Description</th></tr></thead><tbody><tr><td>creating</td><td>The cinder volume manager has sent a request
                                                  to a back-end driver to create a volume, but has
                                                  not received confirmation that the volume is
                                                  available.</td></tr><tr><td>attaching</td><td>cinder has received a request from nova to
                                                  make a volume available for attaching to an
                                                  instance but has not received confirmation from
                                                  nova that the attachment is complete.</td></tr><tr><td>detaching</td><td>cinder has received notification from nova
                                                  that it will detach a volume from an instance but
                                                  has not received notification that the detachment
                                                  is complete.</td></tr><tr><td>deleting</td><td>cinder has received a request to delete a
                                                  volume but has not completed the
                                                  operation.</td></tr><tr><td>backing-up</td><td>cinder backup manager has started to back a
                                                  volume up to swift, or some other backup target,
                                                  but has not completed the operation.</td></tr><tr><td>restoring</td><td>cinder backup manager has started to restore
                                                  a volume from swift, or some other backup target,
                                                  but has not completed the operation.</td></tr></tbody></table></div><p>
   At a high level, the steps that you would take to address any of these
   states are similar:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Confirm that the volume is actually stuck, and not just temporarily
     blocked.
    </p></li><li class="listitem "><p>
     Where possible, remove any resources being held by the volume. For
     example, if a volume is stuck detaching it may be necessary to remove
     associated iSCSI or DM devices on the compute node.
    </p></li><li class="listitem "><p>
     Reset the state of the volume to an appropriate state, for example to
     <code class="literal">available</code> or <code class="literal">error</code>.
    </p></li><li class="listitem "><p>
     Do any final cleanup. For example, if you reset the state to
     <code class="literal">error</code> you can then delete the volume.
    </p></li></ol></div><p>
   The next sections will describe specific steps you can take for volumes
   stuck in each of the transitory states.
  </p><p>
   <span class="bold"><strong>Volumes stuck in Creating</strong></span>
  </p><p>
   Broadly speaking, there are two possible scenarios where a volume would get
   stuck in <code class="literal">creating</code>. The <code class="literal">cinder-volume</code>
   service could have thrown an exception while it was attempting to create the
   volume, and failed to handle the exception correctly. Or the volume back-end
   could have failed, or gone offline, after it received the request from
   cinder to create the volume.
  </p><p>
   These two cases are different in that for the second case you will need to
   determine the reason the back-end is offline and restart it. Often, when the
   back-end has been restarted, the volume will move from
   <code class="literal">creating</code> to <code class="literal">available</code> so your issue
   will be resolved.
  </p><p>
   If you can create volumes successfully on the same back-end as the volume
   stuck in <code class="literal">creating</code> then the back-end is not down. So you
   will need to reset the state for the volume and then delete it.
  </p><p>
   To reset the state of a volume you can use the <code class="literal">openstack volume set
   --state</code> command. You can use either the UUID or the volume name of
   the stuck volume.
  </p><p>
   For example, here is a volume list where we have a stuck volume:
  </p><div class="verbatim-wrap"><pre class="screen">$ openstack volume list
+--------------------------------------+-----------+------+------+-------------+------------+
|                  ID                  |   Status  | Name | Size | Volume Type |Attached to |
+--------------------------------------+-----------+------+------+-------------+------------+
| 14b76133-e076-4bd3-b335-fa67e09e51f6 | creating  | vol1 |  1   |      -      |            |
+--------------------------------------+-----------+------+------+-------------+------------+</pre></div><p>
   You can reset the state by using the <code class="literal">openstack volume set --state</code>
   command, like this:
  </p><div class="verbatim-wrap"><pre class="screen">openstack volume set --state --state error 14b76133-e076-4bd3-b335-fa67e09e51f6</pre></div><p>
   Confirm that with another listing:
  </p><div class="verbatim-wrap"><pre class="screen">$ openstack volume list
+--------------------------------------+-----------+------+------+-------------+------------+
|                  ID                  |   Status  | Name | Size | Volume Type |Attached to |
+--------------------------------------+-----------+------+------+-------------+------------+
| 14b76133-e076-4bd3-b335-fa67e09e51f6 | error     | vol1 |  1   |      -      |            |
+--------------------------------------+-----------+------+------+-------------+------------+</pre></div><p>
   You can then delete the volume:
  </p><div class="verbatim-wrap"><pre class="screen">$ openstack volume delete 14b76133-e076-4bd3-b335-fa67e09e51f6
Request to delete volume 14b76133-e076-4bd3-b335-fa67e09e51f6 has been accepted.</pre></div><p>
   <span class="bold"><strong>Volumes stuck in Deleting</strong></span>
  </p><p>
   If a volume is stuck in the deleting state then the request to delete the
   volume may or may not have been sent to and actioned by the back-end. If you
   can identify volumes on the back-end then you can examine the back-end to
   determine whether the volume is still there or not. Then you can decide
   which of the following paths you can take. It may also be useful to
   determine whether the back-end is responding, either by checking for recent
   volume create attempts, or creating and deleting a test volume.
  </p><p>
   The first option is to reset the state of the volume to
   <code class="literal">available</code> and then attempt to delete the volume again.
  </p><p>
   The second option is to reset the state of the volume to
   <code class="literal">error</code> and then delete the volume.
  </p><p>
   If you have reset the volume state to <code class="literal">error</code> then the volume
   may still be consuming storage on the back-end. If that is the case then you
   will need to delete it from the back-end using your back-end's specific tool.
  </p><p>
   <span class="bold"><strong>Volumes stuck in Attaching</strong></span>
  </p><p>
   The most complicated situation to deal with is where a volume is stuck
   either in attaching or detaching, because as well as dealing with the state
   of the volume in cinder and the back-end, you have to deal with exports from
   the back-end, imports to the compute node, and attachments to the compute
   instance.
  </p><p>
   The two options you have here are to make sure that all exports and imports
   are deleted and to reset the state of the volume to
   <code class="literal">available</code> or to make sure all of the exports and imports
   are correct and to reset the state of the volume to
   <code class="literal">in-use</code>.
  </p><p>
   A volume that is in attaching state should never have been made available to
   a compute instance and therefore should not have any data written to it, or
   in any buffers between the compute instance and the volume back-end. In that
   situation, it is often safe to manually tear down the devices exported on
   the back-end and imported on the compute host and then reset the volume state
   to <code class="literal">available</code>.
  </p><p>
   You can use the management features of the back-end you are using to locate
   the compute host to where the volume is being exported.
  </p><p>
   <span class="bold"><strong>Volumes stuck in Detaching</strong></span>
  </p><p>
   The steps in dealing with a volume stuck in <code class="literal">detaching</code>
   state are very similar to those for a volume stuck in
   <code class="literal">attaching</code>. However, there is the added consideration that
   the volume was attached to, and probably servicing, I/O from a compute
   instance. So you must take care to ensure that all buffers are properly
   flushed before detaching the volume.
  </p><p>
   When a volume is stuck in <code class="literal">detaching</code>, the output from a
   <code class="literal">openstack volume list</code> command will include the UUID for the
   instance to which the volume was attached. From that you can identify the
   compute host that is running the instance using the <code class="literal">openstack
   server show</code> command.
  </p><p>
   For example, here are some snippets:
  </p><div class="verbatim-wrap"><pre class="screen">$ openstack volume list
+--------------------------------------+-----------+-----------------------+-----------------+
|                  ID                  |   Status  |       Name            |   Attached to   |
+--------------------------------------+-----------+-----------------------+-----------------+
| 85384325-5505-419a-81bb-546c69064ec2 | detaching |        vol1           | 4bedaa76-78ca-… |
+--------------------------------------+-----------+-----------------------+-----------------+</pre></div><div class="verbatim-wrap"><pre class="screen">$ openstack server show 4bedaa76-78ca-4fe3-806a-3ba57a9af361|grep host
| OS-EXT-SRV-ATTR:host                 | mycloud-cp1-comp0005-mgmt
| OS-EXT-SRV-ATTR:hypervisor_hostname  | mycloud-cp1-comp0005-mgmt
| hostId                               | 61369a349bd6e17611a47adba60da317bd575be9a900ea590c1be816</pre></div><p>
   The first thing to check in this case is whether the instance is still
   importing the volume. Use <code class="command">virsh list</code> and <code class="command">virsh
   dumpxml <em class="replaceable ">ID</em></code> to see the underlying
   condition of the virtual machine. If the XML for the
   instance has a reference to the device, then you should reset the volume
   state to <code class="literal">in-use</code> and attempt the <code class="command">cinder
   detach</code> operation again.
  </p><div class="verbatim-wrap"><pre class="screen">$ openstack volume set --state --state in-use --attach-status attached 85384325-5505-419a-81bb-546c69064ec2</pre></div><p>
   If the volume gets stuck detaching again, there may be a more fundamental
   problem, which is outside the scope of this document and you should contact
   the Support team.
  </p><p>
   If the volume is not referenced in the XML for the instance then you should
   remove any devices on the compute node and back-end and then reset the state
   of the volume to <code class="literal">available</code>.
  </p><div class="verbatim-wrap"><pre class="screen">$ openstack volume set --state --state available --attach-status detached 85384325-5505-419a-81bb-546c69064ec2</pre></div><p>
   You can use the management features of the back-end you are using to locate
   the compute host to where the volume is being exported.
  </p><p>
   <span class="bold"><strong>Volumes stuck in restoring</strong></span>
  </p><p>
   Restoring a cinder volume from backup will be as slow as backing it up. So
   you must confirm that the volume is actually stuck by examining the
   <code class="literal">cinder-backup.log</code>. For example:
  </p><div class="verbatim-wrap"><pre class="screen"># tail -f cinder-backup.log |grep 162de6d5-ba92-4e36-aba4-e37cac41081b
2016-04-27 12:39:14.612 6689 DEBUG swiftclient [req-0c65ec42-8f9d-430a-b0d5-05446bf17e34 - -
2016-04-27 12:39:15.533 6689 DEBUG cinder.backup.chunkeddriver [req-0c65ec42-8f9d-430a-b0d5-
2016-04-27 12:39:15.566 6689 DEBUG requests.packages.urllib3.connectionpool [req-0c65ec42-
2016-04-27 12:39:15.567 6689 DEBUG swiftclient [req-0c65ec42-8f9d-430a-b0d5-05446bf17e34 - - -</pre></div><p>
   If you determine that the volume is genuinely stuck in the
   <code class="literal">restoring</code> state then you must follow the procedure described
   in the detaching section above to remove any volumes that remain exported from
   the back-end and imported on the controller node. Remember that in this case
   the volumes will be imported and mounted on the controller node running
   <code class="literal">cinder-backup</code>. So you do not have to search for the
   correct compute host. Also remember that no instances are involved so you do
   not need to confirm that the volume is not imported to any instances.
  </p></div><div class="sect3" id="debugging-attachment"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.6.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Debugging volume attachment</span> <a title="Permalink" class="permalink" href="#debugging-attachment">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_blockstorage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_blockstorage.xml</li><li><span class="ds-label">ID: </span>debugging-attachment</li></ul></div></div></div></div><p>
   In an error case, it is possible for a cinder volume to fail to complete an
   operation and revert back to its initial state. For example, attaching a
   cinder volume to a nova instance, so you would follow the steps above to
   examine the nova compute logs for the attach request.
  </p></div><div class="sect3" id="errors-creating"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.6.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Errors creating volumes</span> <a title="Permalink" class="permalink" href="#errors-creating">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_blockstorage.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_blockstorage.xml</li><li><span class="ds-label">ID: </span>errors-creating</li></ul></div></div></div></div><p>
   If you are creating a volume and it goes into the <code class="literal">ERROR</code>
   state, a common error to see is <code class="literal">No valid host was found</code>.
   This means that the scheduler could not schedule your volume to a back-end.
   You should check that the volume service is up and running. You can use this
   command:
  </p><div class="verbatim-wrap"><pre class="screen">$ sudo cinder-manage service list
Binary           Host                                 Zone             Status     State Updated At
cinder-scheduler ha-volume-manager                    nova             enabled    :-)   2016-04-25 11:39:30
cinder-volume    ha-volume-manager@ses1               nova             enabled    XXX   2016-04-25 11:27:26
cinder-backup    ha-volume-manager                    nova             enabled    :-)   2016-04-25 11:39:28</pre></div><p>
   In this example, the state of <code class="literal">XXX</code> indicates that the
   service is down.
  </p><p>
   If the service is up, next check that the back-end has sufficient space. You
   can use this command to show the available and total space on each back-end:
  </p><div class="verbatim-wrap"><pre class="screen">openstack volume backend pool list --detail</pre></div><p>
   If your deployment is using volume types, verify that the
   <code class="literal">volume_backend_name</code> in your
   <code class="literal">cinder.conf</code> file matches the
   <code class="literal">volume_backend_name</code> for the volume type you selected.
  </p><p>
   You can verify the back-end name on your volume type by using this command:
  </p><div class="verbatim-wrap"><pre class="screen">openstack volume type list</pre></div><p>
   Then list the details about your volume type. For example:
  </p><div class="verbatim-wrap"><pre class="screen">$ openstack volume type show dfa8ecbd-8b95-49eb-bde7-6520aebacde0
+---------------------------------+--------------------------------------+
| Field                           | Value                                |
+---------------------------------+--------------------------------------+
| description                     | None                                 |
| id                              | dfa8ecbd-8b95-49eb-bde7-6520aebacde0 |
| is_public                       | True                                 |
| name                            | my3par                               |
| os-volume-type-access:is_public | True                                 |
| properties                      | volume_backend_name='3par'           |
+---------------------------------+--------------------------------------+</pre></div></div></div><div class="sect2" id="troubleshooting-objectstorage"><div class="titlepage"><div><div><h3 class="title"><span class="number">18.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">swift Storage Troubleshooting</span> <a title="Permalink" class="permalink" href="#troubleshooting-objectstorage">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_swift.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_swift.xml</li><li><span class="ds-label">ID: </span>troubleshooting-objectstorage</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the swift service. You can use
  these guides to help you identify and resolve basic problems you may
  experience while deploying or using the Object Storage service. It contains
  the following troubleshooting scenarios:
 </p><div class="sect3" id="topic-shs-j2b-kt"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.6.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment Fails With <span class="quote">“<span class="quote ">MSDOS Disks Labels Do Not Support Partition Names</span>”</span></span> <a title="Permalink" class="permalink" href="#topic-shs-j2b-kt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-objectstorage-deploy_fails_with_msdos_disks.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-deploy_fails_with_msdos_disks.xml</li><li><span class="ds-label">ID: </span>topic-shs-j2b-kt</li></ul></div></div></div></div><p>
 Description
</p><p>
 If a disk drive allocated to swift uses the MBR partition table type, the
 deploy process refuses to label and format the drive. This is to prevent
 potential data loss. (For more information, see <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 11 “Modifying Example Configurations for Object Storage using Swift”, Section 11.5 “Allocating Disk Drives for Object Storage”</span>. If you intend to use the disk drive for
 swift, you must convert the MBR partition table to GPT on the drive using
 <code class="literal">/sbin/sgdisk</code>.
</p><div id="id-1.5.20.9.4.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   This process only applies to swift drives. It does not apply to the
   operating system or boot drive.
  </p></div><p>
 Resolution
</p><p>
 You must install <code class="literal">gdisk</code>, before using
 <code class="literal">sgdisk</code>:
</p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
   Run the following command to install <code class="literal">gdisk</code>:
  </p><div class="verbatim-wrap"><pre class="screen">sudo zypper install gdisk</pre></div></li><li class="step "><p>
   Convert to the GPT partition type. Following is an example for
   converting <code class="literal">/dev/sdd</code> to the GPT partition type:
  </p><div class="verbatim-wrap"><pre class="screen">sudo sgdisk -g /dev/sdd</pre></div></li><li class="step "><p>
   Reboot the node to take effect. You may then resume the deployment
   (repeat the playbook that reported the error).
  </p></li></ol></div></div></div><div class="sect3" id="topic-ev4-q2b-kt"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.6.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Examining Planned Ring Changes</span> <a title="Permalink" class="permalink" href="#topic-ev4-q2b-kt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-objectstorage-examine_details_planned_ring_changes_prior_deploy.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-examine_details_planned_ring_changes_prior_deploy.xml</li><li><span class="ds-label">ID: </span>topic-ev4-q2b-kt</li></ul></div></div></div></div><p>
  Before making major changes to your rings, you can see the planned layout of
  swift rings using the following steps:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="step "><p>
    Run the <code class="literal">swift-compare-model-rings.yml</code> playbook:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml --extra-vars "drive_detail=yes"</pre></div></li><li class="step "><p>
    Validate the following in the output:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Drives are being added to all rings in the ring specifications.
     </p></li><li class="listitem "><p>
      Servers are being used as expected (for example, you may have a different
      set of servers for the account/container rings than the object rings.)
     </p></li><li class="listitem "><p>
      The drive size is the expected size.
     </p></li></ul></div></li></ol></div></div></div><div class="sect3" id="sec-input-swift-error"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.6.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Interpreting Swift Input Model Validation Errors</span> <a title="Permalink" class="permalink" href="#sec-input-swift-error">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-objectstorage-interpreting_swift_validate_input_model.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-interpreting_swift_validate_input_model.xml</li><li><span class="ds-label">ID: </span>sec-input-swift-error</li></ul></div></div></div></div><p>
  The following examples provide an error message, description, and resolution.
 </p><div id="id-1.5.20.9.4.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   To resolve an error, you must first modify the input model and re-run the
   configuration processor. (For instructions, see
   <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>.) Then, continue with the deployment.
  </p></div><div class="orderedlist " id="ol-vnh-g2b-kt"><ol class="orderedlist" type="1"><li class="listitem "><p>
    <span class="bold"><strong>Example Message - Model Mismatch: Cannot find drive
    /dev/sdt on padawan-ccp-c1-m2 (192.168.245.3))</strong></span>
   </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /></colgroup><tbody><tr><td><span class="bold"><strong>Description</strong></span>
       </td><td>The disk model used for node <span class="bold"><strong>padawan-ccp-c1-m2</strong></span> has drive
                                    <code class="literal">/dev/sdt</code> listed in the devices list of a
                                device-group where swift is the consumer. However, the
                                    <code class="literal">dev/sdt</code> device does not exist on that
                                node.</td></tr><tr><td><span class="bold"><strong>Resolution</strong></span>
       </td><td>
        <p>
         If a drive or controller is failed on a node, the operating system
         does not see the drive and so the corresponding block device may not
         exist. Sometimes this is transitory and a reboot may resolve the
         problem. The problem may not be with <code class="literal">/dev/sdt</code>, but
         with another drive. For example, if <code class="literal">/dev/sds</code> is
         failed, when you boot the node, the drive that you expect to be called
         <code class="literal">/dev/sdt</code> is actually called
         <code class="literal">/dev/sds</code>.
        </p>
        <p>
         Alternatively, there may not be enough drives installed in the server.
         You can add drives. Another option is to remove
         <code class="literal">/dev/sdt</code> from the appropriate disk model. However,
         this removes the drive for all servers using the disk model.
        </p>
       </td></tr></tbody></table></div></li><li class="listitem "><p>
    <span class="bold"><strong>Example Message - Model Mismatch: Cannot find drive
    /dev/sdd2 on padawan-ccp-c1-m2 (192.168.245.3)</strong></span>
   </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /></colgroup><tbody><tr><td><span class="bold"><strong>Description</strong></span>
       </td><td>The disk model used for node<span class="bold"><strong>
                                    padawan-ccp-c1-m2</strong></span> has drive
                                    <code class="literal">/dev/sdt</code> listed in the devices list of a
                                device-group where swift is the consumer. However, the partition
                                number (2) has been specified in the model. This is not supported -
                                only specify the block device name (for example
                                    <code class="literal">/dev/sdd</code>), not partition names in disk
                                models.</td></tr><tr><td><span class="bold"><strong>Resolution</strong></span>
       </td><td>Remove the partition number from the disk model.</td></tr></tbody></table></div></li><li class="listitem "><p>
    <span class="bold"><strong>Example Message - Cannot find IP address of
    padawan-ccp-c1-m3-swift for ring: account host:
    padawan-ccp-c1-m3-mgmt</strong></span>
   </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /></colgroup><tbody><tr><td><span class="bold"><strong>Description</strong></span>
       </td><td>The service (in this example, swift-account) is running on the
                                node <span class="bold"><strong>padawan-ccp-c1-m3</strong></span>. However,
                                this node does not have a connection to the network designated for
                                the <code class="literal">swift-account</code> service (that is, the SWIFT
                                network).</td></tr><tr><td><span class="bold"><strong>Resolution</strong></span>
       </td><td>Check the input model for which networks are configured for each
                                node type.</td></tr></tbody></table></div></li><li class="listitem "><p>
    <span class="bold"><strong>Example Message - Ring: object-2 has specified
    replication_policy and erasure_coding_policy. Only one may be specified.
    </strong></span>
   </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /></colgroup><tbody><tr><td><span class="bold"><strong>Description</strong></span>
       </td><td>Only either <code class="literal">replication-policy</code> or
                                    <code class="literal">erasure-coding-policy</code> may be used in
                                    <code class="literal">ring-specifications</code>.</td></tr><tr><td><span class="bold"><strong>Resolution</strong></span>
       </td><td>Remove one of the policy types.</td></tr></tbody></table></div></li><li class="listitem "><p>
    <span class="bold"><strong>Example Message - Ring: object-3 is missing a policy
    type (replication-policy or erasure-coding-policy) </strong></span>
   </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col /><col /></colgroup><tbody><tr><td><span class="bold"><strong>Description</strong></span>
       </td><td>There is no <code class="literal">replication-policy</code> or
                                    <code class="literal">erasure-coding-policy</code> section in
                                    <code class="literal">ring-specifications</code> for the object-0
                                ring.</td></tr><tr><td><span class="bold"><strong>Resolution</strong></span>
       </td><td>Add a policy type to the input model file. </td></tr></tbody></table></div></li></ol></div></div><div class="sect3" id="topic-rtc-s3t-mt"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.6.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Identifying the Swift Ring Building Server</span> <a title="Permalink" class="permalink" href="#topic-rtc-s3t-mt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-objectstorage-identify_ring_builder.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-identify_ring_builder.xml</li><li><span class="ds-label">ID: </span>topic-rtc-s3t-mt</li></ul></div></div></div></div><div class="sect4" id="idg-all-operations-troubleshooting-objectstorage-identify-ring-builder-xml-6"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.6.2.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Identify the swift Ring Building server</span> <a title="Permalink" class="permalink" href="#idg-all-operations-troubleshooting-objectstorage-identify-ring-builder-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-objectstorage-identify_ring_builder.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-identify_ring_builder.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-objectstorage-identify-ring-builder-xml-6</li></ul></div></div></div></div><p>
   Perform the following steps to identify the swift ring building server:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Run the following command:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-status.yml --limit SWF-ACC[0]</pre></div></li><li class="step "><p>
     Examine the output of this playbook. The last line underneath the play
     recap will give you the server name which is your swift ring building
     server.
    </p><div class="verbatim-wrap"><pre class="screen">PLAY RECAP ********************************************************************
_SWF_CMN | status | Check systemd service running ----------------------- 1.61s
_SWF_CMN | status | Check systemd service running ----------------------- 1.16s
_SWF_CMN | status | Check systemd service running ----------------------- 1.09s
_SWF_CMN | status | Check systemd service running ----------------------- 0.32s
_SWF_CMN | status | Check systemd service running ----------------------- 0.31s
_SWF_CMN | status | Check systemd service running ----------------------- 0.26s
-------------------------------------------------------------------------------
Total: ------------------------------------------------------------------ 7.88s
<span class="bold"><strong>ardana-cp1-c1-m1-mgmt</strong></span>      : ok=7    changed=0    unreachable=0    failed=0</pre></div><p>
     In the above example, the first swift proxy server is
     <code class="literal">ardana-cp1-c1-m1-mgmt</code>.
    </p></li></ol></div></div><div id="id-1.5.20.9.4.6.2.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    For the purposes of this document, any errors you see in the output of this
    playbook can be ignored if all you are looking for is the server name for
    your swift ring builder server.
   </p></div></div></div><div class="sect3" id="verify-partition-label"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.6.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Verifying a Swift Partition Label</span> <a title="Permalink" class="permalink" href="#verify-partition-label">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-objectstorage-label_on_partition.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-label_on_partition.xml</li><li><span class="ds-label">ID: </span>verify-partition-label</li></ul></div></div></div></div><div id="id-1.5.20.9.4.7.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
   For a system upgrade do NOT clear the label before starting the upgrade.
  </p></div><p>
  This topic describes how to check whether a device has a label on a
  partition.
 </p><div class="sect4" id="label-partition"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.6.2.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Check Partition Label</span> <a title="Permalink" class="permalink" href="#label-partition">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-objectstorage-label_on_partition.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-label_on_partition.xml</li><li><span class="ds-label">ID: </span>label-partition</li></ul></div></div></div></div><p>
   To check whether a device has label on a partition, perform the following
   step:
  </p><div class="procedure "><div class="procedure-contents"><ul class="procedure"><li class="step "><p>
     Log on to the node and use the <code class="literal">parted</code> command:
    </p><div class="verbatim-wrap"><pre class="screen">sudo parted -l</pre></div><p>
     The output lists all of the block devices. Following is an example output
     for <code class="literal">/dev/sdc</code> with a single partition and a label of
     <span class="bold"><strong>c0a8f502h000</strong></span>.
     Because the partition has a label, if you are about to install and deploy
     the system, you must clear this label before starting the deployment. As
     part of the deployment process, the system will label the partition.
    </p><div class="verbatim-wrap"><pre class="screen">.
.
.
Model: QEMU QEMU HARDDISK (scsi)
Disk /dev/sdc: 20.0GB
Sector size (logical/physical): 512B/512B
Partition Table: gpt
Disk Flags:

Number  Start   End     Size    File system  Name           Flags
1       1049kB  20.0GB  20.0GB  xfs          <span class="bold"><strong>c0a8f502h000</strong></span>

.
.
.</pre></div></li></ul></div></div></div></div><div class="sect3" id="verify-the-filesystem-label"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.6.2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Verifying a Swift File System Label</span> <a title="Permalink" class="permalink" href="#verify-the-filesystem-label">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-objectstorage-filesystem_label.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-filesystem_label.xml</li><li><span class="ds-label">ID: </span>verify-the-filesystem-label</li></ul></div></div></div></div><div id="id-1.5.20.9.4.8.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
   For a system upgrade do NOT clear the label before starting the upgrade.
  </p></div><p>
  This topic describes how to check whether a file system in a partition has a
  label.
 </p><p>
  To check whether a file system in a partition has a label, perform the
  following step:
 </p><div class="procedure "><div class="procedure-contents"><ul class="procedure"><li class="step "><p>
    Log on to the server and execute the <code class="literal">xfs_admin</code> command
    (where <code class="literal">/dev/sdc1</code> is the partition where the file system
    is located):
   </p><div class="verbatim-wrap"><pre class="screen">sudo xfs_admin -l /dev/sdc1</pre></div><p>
    The output shows if a file system has a label. For example, this shows a
    label of <span class="bold"><strong>c0a8f502h000</strong></span>:
   </p><div class="verbatim-wrap"><pre class="screen">$ sudo xfs_admin -l /dev/sdc1
label = "<span class="bold"><strong>c0a8f502h000</strong></span>"</pre></div><p>
    If no file system exists, the result is as follows:
   </p><div class="verbatim-wrap"><pre class="screen">$ sudo xfs_admin -l /dev/sde1
xfs_admin: /dev/sde is not a valid XFS file system (unexpected SB magic number 0x00000000)</pre></div><p>
    If you are about to install and deploy the system, you must delete the
    label before starting the deployment. As part of the deployment process,
    the system will label the partition.
   </p></li></ul></div></div></div><div class="sect3" id="topic-gbz-13t-mt"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.6.2.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recovering swift Builder Files</span> <a title="Permalink" class="permalink" href="#topic-gbz-13t-mt">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-objectstorage-recovering_builder_file.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-recovering_builder_file.xml</li><li><span class="ds-label">ID: </span>topic-gbz-13t-mt</li></ul></div></div></div></div><p>
  When you execute the deploy process for a system, a copy of the builder files
  is stored on the following nodes and directories:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    On the swift ring building node, the primary reference copy is stored in
    the
    <code class="literal">/etc/swiftlm/&lt;cloud-name&gt;/&lt;control-plane-name&gt;/builder_dir/</code>
    directory.
   </p></li><li class="listitem "><p>
    On the next node after the swift ring building node, a backup copy is
    stored in the
    <code class="literal">/etc/swiftlm/&lt;cloud-name&gt;/&lt;control-plane-name&gt;/builder_dir/</code>
    directory.
   </p></li><li class="listitem "><p>
    In addition, in the deploy process, the builder files are also copied to
    the <code class="literal">/etc/swiftlm/deploy_dir/&lt;cloud-name&gt;</code> directory
    on every swift node.
   </p></li></ul></div><p>
  If these builder files are found on the primary swift ring building node
  (to identify which node is the primary ring building node, see
  <a class="xref" href="#topic-rtc-s3t-mt" title="18.6.2.4. Identifying the Swift Ring Building Server">Section 18.6.2.4, “Identifying the Swift Ring Building Server”</a>) in the directory
  <code class="literal">/etc/swiftlm/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir</code>,
  then no further recover action is needed. If not, then you need to copy the
  files from an intact swift node onto the primary swift ring building node.
 </p><p>
  If you have no intact <code class="literal">/etc/swiftlm</code> directory on any swift
  node, you may be able to restore from a backup. See
  <a class="xref" href="#recovering-controller-nodes" title="15.2.3.2. Recovering the Control Plane">Section 15.2.3.2, “Recovering the Control Plane”</a>.
 </p><p>
  To restore builder files on the primary ring builder node from a backup stored
  on another member of the ring, use the following process:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Log in to the swift ring building server (To identify the swift ring
    building server, see <a class="xref" href="#topic-rtc-s3t-mt" title="18.6.2.4. Identifying the Swift Ring Building Server">Section 18.6.2.4, “Identifying the Swift Ring Building Server”</a>).
   </p></li><li class="step "><p>
    Create the <code class="literal">/etc/swiftlm/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir</code>
    directory structure with these commands:
   </p><p>
    Replace <em class="replaceable ">CLOUD_NAME</em> with the name of your cloud
    and <em class="replaceable ">CONTROL_PLANE_NAME</em> with the name of your
    control plane.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo mkdir -p /etc/swiftlm/<em class="replaceable ">CLOUD_NAME</em>/<em class="replaceable ">CONTROL_PLANE_NAME</em>/builder_dir/
<code class="prompt user">tux &gt; </code>sudo chown -R ardana.ardana /etc/swiftlm/</pre></div></li><li class="step "><p>
    Log in to a swift node where an intact
    <code class="literal">/etc/swiftlm/deploy_dir</code> directory exists.
   </p></li><li class="step "><p>
    Copy the builder files to the swift ring building node. In the example
    below we use scp to transfer the files, where
    <code class="literal">swpac-c1-m2-mgmt</code> is the node where the files can be
    found, <code class="literal">cloud1</code> is the cloud, and <code class="literal">cp1</code>
    is the control plane name:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>sudo mkdir -p /etc/swiftlm/cloud1/cp1/builder_dir
<code class="prompt user">tux &gt; </code>sudo cd /etc/swiftlm/cloud1/cp1/builder_dir
<code class="prompt user">tux &gt; </code>sudo scp -r ardana@swpac-ccp-c1-m1-mgmt:/etc/swiftlm/cloud1/cp1/builder_dir/* ./
<code class="prompt user">tux &gt; </code>sudo chown -R swift:swift /etc/swiftlm</pre></div><p>
    (Any permissions errors related to files in the <code class="literal">backups</code>
    directory can be ignored.)
   </p></li><li class="step "><p>
    Skip this step if you are rebuilding the entire node. It should only be
    used if swift components are already present and functioning on the server,
    and you are recovering or updating the ring builder files.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="sec-trouble-restart-storeage-deployment"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.6.2.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restarting the Object Storage Deployment</span> <a title="Permalink" class="permalink" href="#sec-trouble-restart-storeage-deployment">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-objectstorage-restart_deploy_from_scratch.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-restart_deploy_from_scratch.xml</li><li><span class="ds-label">ID: </span>sec-trouble-restart-storeage-deployment</li></ul></div></div></div></div><p>
  This page describes the various operational procedures performed by swift.
 </p><div class="sect4" id="restart-swift-depl"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.6.2.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Restart the Swift Object Storage Deployment</span> <a title="Permalink" class="permalink" href="#restart-swift-depl">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-objectstorage-restart_deploy_from_scratch.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-restart_deploy_from_scratch.xml</li><li><span class="ds-label">ID: </span>restart-swift-depl</li></ul></div></div></div></div><p>
   The structure of ring is built in an incremental stages. When you modify a
   ring, the new ring uses the state of the old ring as a basis for the new
   ring. Rings are stored in the builder file. The
   <code class="literal">swiftlm-ring-supervisor</code> stores builder files in the
   <code class="literal">/etc/swiftlm/cloud1/cp1/builder_dir/</code>
   directory on the Ring-Builder node. The builder files are named
   &lt;ring-name&gt; builder. Prior versions of the builder files are stored in
   the
   <code class="literal">/etc/swiftlm/cloud1/cp1/builder_dir/backups</code>
   directory.
  </p><p>
   Generally, you use an existing builder file as the basis for changes to a
   ring. However, at initial deployment, when you create a ring there will be
   no builder file. Instead, the first step in the process is to build a
   builder file. The deploy playbook does this as a part of the deployment
   process. If you have successfully deployed some of the system, the ring
   builder files will exist.
  </p><p>
   If you change your input model (for example, by adding servers) now, the
   process assumes you are <span class="emphasis"><em>modifying</em></span> a ring and behaves
   differently than while creating a ring from scratch. In this case, the ring
   is not balanced. So, if the cloud model contains an error or you decide to
   make substantive changes, it is a best practice to start from scratch and
   build rings using the steps below.
  </p></div><div class="sect4" id="reset-builder-files"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.6.2.8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reset Builder Files</span> <a title="Permalink" class="permalink" href="#reset-builder-files">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-objectstorage-restart_deploy_from_scratch.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-restart_deploy_from_scratch.xml</li><li><span class="ds-label">ID: </span>reset-builder-files</li></ul></div></div></div></div><p>
   You must reset the builder files during the initial deployment process
   (only). This process should be used only when you want to restart a
   deployment from scratch. If you reset the builder files after completing
   your initial deployment, then you are at a risk of losing critical system
   data.
  </p><p>
   Delete the builder files in the
   <code class="filename">/etc/swiftlm/cloud1/cp1/builder-dir/</code>
   directory. For example, for the region0 keystone region (the default single
   region designation), do the following:
  </p><div class="verbatim-wrap"><pre class="screen">sudo rm /etc/swiftlm/cloud1/cp1/builder_dir/*.builder</pre></div><div id="id-1.5.20.9.4.10.4.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    If you have successfully deployed a system and accidentally delete the
    builder files, you can recover to the correct state. For instructions, see
    <a class="xref" href="#topic-gbz-13t-mt" title="18.6.2.7. Recovering swift Builder Files">Section 18.6.2.7, “Recovering swift Builder Files”</a>.
   </p></div></div></div><div class="sect3" id="increase-timeout"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.6.2.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Increasing the Swift Node Timeout Value</span> <a title="Permalink" class="permalink" href="#increase-timeout">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-objectstorage-increase_timeout.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-increase_timeout.xml</li><li><span class="ds-label">ID: </span>increase-timeout</li></ul></div></div></div></div><p>
  On a heavily loaded Object Storage system timeouts may occur when
  transferring data to or from swift, particularly large objects.
 </p><p>
  The following is an example of a timeout message in the log
  (<code class="literal">/var/log/swift/swift.log</code>) on a swift proxy server:
 </p><div class="verbatim-wrap"><pre class="screen">Jan 21 16:55:08 ardana-cp1-swpaco-m1-mgmt proxy-server: ERROR with Object server 10.243.66.202:6000/disk1 re: Trying to write to
/v1/AUTH_1234/testcontainer/largeobject: ChunkWriteTimeout (10s)</pre></div><p>
  If this occurs, it may be necessary to increase the
  <code class="literal">node_timeout</code> parameter in the
  <code class="literal">proxy-server.conf</code> configuration file.
 </p><p>
  The <code class="literal">node_timeout</code> parameter in the swift
  <code class="literal">proxy-server.conf</code> file is the maximum amount of time the
  proxy server will wait for a response from the account, container, or object
  server. The default value is 10 seconds.
 </p><p>
  In order to modify the timeout you can use these steps:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Log in to the Cloud Lifecycle Manager.
   </p></li><li class="step "><p>
    Edit the
    <code class="literal">~/openstack/my_cloud/config/swift/proxy-server.conf.j2</code> file
    and add a line specifying the <code class="literal">node_timeout</code> into the
    <code class="literal">[app:proxy-server]</code> section of the file.
   </p><p>
    Example, in bold, increasing the timeout to 30 seconds:
   </p><div class="verbatim-wrap"><pre class="screen">[app:proxy-server]
use = egg:swift#proxy
.
.
<span class="bold"><strong>node_timeout = 30</strong></span></pre></div></li><li class="step "><p>
    Commit your configuration to the <span class="intraxref">Book “<em class="citetitle ">Deployment Guide using Cloud Lifecycle Manager</em>”, Chapter 22 “Using Git for Configuration Management”</span>, as follows:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
    Run the configuration processor:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
    Use the playbook below to create a deployment directory:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
    Change to the deployment directory and run the swift reconfigure playbook:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect3" id="swift-filesystem-ts"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.6.2.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting Swift File System Usage Issues</span> <a title="Permalink" class="permalink" href="#swift-filesystem-ts">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-objectstorage-filesystem_usage_nowipe.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-filesystem_usage_nowipe.xml</li><li><span class="ds-label">ID: </span>swift-filesystem-ts</li></ul></div></div></div></div><p>
  If you have recycled your environment to do a re-installation and you haven't
  run the <code class="literal">wipe_disks.yml</code> playbook in the process, you may
  experience an issue where your file system usage continues to grow
  exponentially even though you are not adding any files to your swift system.
  This is likely occurring because the quarantined directory is getting filled
  up. You can find this directory at
  <code class="literal">/srv/node/disk0/quarantined</code>.
 </p><p>
  You can resolve this issue by following these steps:
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    SSH to each of your swift nodes and stop the replication processes on each
    of them. The following commands must be executed on each of your swift
    nodes. Make note of the time that you performed this action as you will
    reference it in step three.
   </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl stop swift-account-replicator
sudo systemctl stop swift-container-replicator
sudo systemctl stop swift-object-replicator</pre></div></li><li class="step "><p>
    Examine the <code class="literal">/var/log/swift/swift.log</code> file for events
    that indicate when the auditor processes have started and completed audit
    cycles. For more details, see <a class="xref" href="#swift-filesystem-ts" title="18.6.2.10. Troubleshooting Swift File System Usage Issues">Section 18.6.2.10, “Troubleshooting Swift File System Usage Issues”</a>.
   </p></li><li class="step "><p>
    Wait until you see that the auditor processes have finished two complete
    cycles since the time you stopped the replication processes (from step
    one). You must check every swift node, which on a lightly loaded system
    that was recently installed this should take less than two hours.
   </p></li><li class="step "><p>
    At this point you should notice that your quarantined directory has stopped
    growing. You may now delete the files in that directory on each of your
    nodes.
   </p></li><li class="step "><p>
    Restart the replication processes using the swift start playbook:
   </p><ol type="a" class="substeps "><li class="step "><p>
      Log in to the Cloud Lifecycle Manager.
     </p></li><li class="step "><p>
      Run the swift start playbook:
     </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-start.yml</pre></div></li></ol></li></ol></div></div><div class="sect4" id="swift-log"><div class="titlepage"><div><div><h5 class="title"><span class="number">18.6.2.10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Examining the swift Log for Audit Event Cycles</span> <a title="Permalink" class="permalink" href="#swift-log">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-objectstorage-filesystem_usage_nowipe.xml" title="Edit the source file for this section">Edit source</a></h5><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-objectstorage-filesystem_usage_nowipe.xml</li><li><span class="ds-label">ID: </span>swift-log</li></ul></div></div></div></div><p>
   Below is an example of the <code class="literal">object-server</code> start and end
   cycle details. They were taken by using the following command on a swift
   node:
  </p><div class="verbatim-wrap"><pre class="screen">sudo grep object-auditor /var/log/swift/swift.log|grep ALL</pre></div><p>
   Example output:
  </p><div class="verbatim-wrap"><pre class="screen">$ sudo grep object-auditor /var/log/swift/swift.log|grep ALL
...
Apr  1 13:31:18 padawan-ccp-c1-m1-mgmt object-auditor: Begin object audit "forever" mode (ALL)
Apr  1 13:31:18 padawan-ccp-c1-m1-mgmt object-auditor: Object audit (ALL). Since Fri Apr  1 13:31:18 2016: Locally: 0 passed, 0 quarantined, 0 errors files/sec: 0.00 , bytes/sec: 0.00, Total time: 0.00, Auditing time: 0.00, Rate: 0.00
Apr  1 13:51:32 padawan-ccp-c1-m1-mgmt object-auditor: Object audit (ALL) "forever" mode completed: 1213.78s. Total quarantined: 0, Total errors: 0, Total files/sec: 7.02, Total bytes/sec: 9999722.38, Auditing time: 1213.07, Rate: 1.00</pre></div><p>
   In this example, the auditor started at <code class="literal">13:31</code> and ended
   at <code class="literal">13:51</code>.
  </p><p>
   In this next example, the <code class="literal">account-auditor</code> and
   <code class="literal">container-auditor</code> use similar message structure, so we
   only show the container auditor. You can substitute
   <code class="literal">account</code> for <code class="literal">container</code> as well:
  </p><div class="verbatim-wrap"><pre class="screen">$ sudo grep container-auditor /var/log/swift/swift.log
...
Apr  1 14:07:00 padawan-ccp-c1-m1-mgmt container-auditor: Begin container audit pass.
Apr  1 14:07:00 padawan-ccp-c1-m1-mgmt container-auditor: Since Fri Apr  1 13:07:00 2016: Container audits: 42 passed audit, 0 failed audit
Apr  1 14:37:00 padawan-ccp-c1-m1-mgmt container-auditor: Container audit pass completed: 0.10s</pre></div><p>
   In the example, the container auditor started a cycle at
   <code class="literal">14:07</code> and the cycle finished at <code class="literal">14:37</code>.
  </p></div></div></div></div><div class="sect1" id="monitoring-logging-usage-reporting"><div class="titlepage"><div><div><h2 class="title"><span class="number">18.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring, Logging, and Usage Reporting Troubleshooting</span> <a title="Permalink" class="permalink" href="#monitoring-logging-usage-reporting">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_telemetry.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_telemetry.xml</li><li><span class="ds-label">ID: </span>monitoring-logging-usage-reporting</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the Monitoring, Logging, and
  Usage Reporting services.
 </p><div class="sect2" id="sec-central-log-troubleshoot"><div class="titlepage"><div><div><h3 class="title"><span class="number">18.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting Centralized Logging</span> <a title="Permalink" class="permalink" href="#sec-central-log-troubleshoot">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_troubleshoot.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_troubleshoot.xml</li><li><span class="ds-label">ID: </span>sec-central-log-troubleshoot</li></ul></div></div></div></div><p>
  This section contains the following scenarios:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="#sec-central-log-review-logs" title="18.7.1.1. Reviewing Log Files">Section 18.7.1.1, “Reviewing Log Files”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-central-log-monitor" title="18.7.1.2. Monitoring Centralized Logging">Section 18.7.1.2, “Monitoring Centralized Logging”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-central-log-log-collection" title="18.7.1.3. Situations In Which Logs Might Not Be Collected">Section 18.7.1.3, “Situations In Which Logs Might Not Be Collected”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-central-log-error-kibana" title="18.7.1.4. Error When Creating a Kibana Visualization">Section 18.7.1.4, “Error When Creating a Kibana Visualization”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-central-log-store-log" title="18.7.1.5. After Deploying Logging-API, Logs Are Not Centrally Stored">Section 18.7.1.5, “After Deploying Logging-API, Logs Are Not Centrally Stored”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec-central-log-slow-log" title="18.7.1.6. Re-enabling Slow Logging">Section 18.7.1.6, “Re-enabling Slow Logging”</a>
   </p></li></ul></div><div class="sect3" id="sec-central-log-review-logs"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.7.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reviewing Log Files</span> <a title="Permalink" class="permalink" href="#sec-central-log-review-logs">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_troubleshoot.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_troubleshoot.xml</li><li><span class="ds-label">ID: </span>sec-central-log-review-logs</li></ul></div></div></div></div><p>
   You can troubleshoot service-specific issues by reviewing the logs. After
   logging into Kibana, follow these steps to load the logs for viewing:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Navigate to the <span class="bold"><strong>Settings</strong></span> menu to
     configure an index pattern to search for.
    </p></li><li class="listitem "><p>
     In the <span class="bold"><strong>Index name or pattern</strong></span> field, you
     can enter <code class="literal">logstash-*</code> to query all Elasticsearch
     indices.
    </p></li><li class="listitem "><p>
     Click the green <span class="bold"><strong>Create</strong></span> button to create
     and load the index.
    </p></li><li class="listitem "><p>
     Navigate to the <span class="bold"><strong>Discover</strong></span> menu to load the
     index and make it available to search.
    </p></li></ol></div><div id="id-1.5.20.10.3.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    If you want to search specific Elasticsearch indices, you can run the
    following command from the control plane to get a full list of available
    indices:
   </p><div class="verbatim-wrap"><pre class="screen">curl localhost:9200/_cat/indices?v</pre></div></div><p>
   Once the logs load you can change the timeframe from the dropdown in the
   upper-righthand corner of the Kibana window. You have the following options
   to choose from:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>Quick</strong></span> - a variety of time frame choices
     will be available here
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Relative</strong></span> - allows you to select a start
     time relative to the current time to show this range
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>Absolute</strong></span> - allows you to select a date
     range to query
    </p></li></ul></div><p>
   When searching there are common fields you will want to use, such as:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="bold"><strong>type</strong></span> - this will include the service
     name, such as <code class="literal">keystone</code> or <code class="literal">ceilometer</code>
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>host </strong></span>- you can specify a specific host to
     search for in the logs
    </p></li><li class="listitem "><p>
     <span class="bold"><strong>file</strong></span> - you can specify a specific log
     file to search
    </p></li></ul></div><p>
   For more details on using Kibana and Elasticsearch to query logs, see
   <a class="link" href="https://www.elastic.co/guide/en/kibana/3.0/working-with-queries-and-filters.html" target="_blank">https://www.elastic.co/guide/en/kibana/3.0/working-with-queries-and-filters.html</a>
  </p></div><div class="sect3" id="sec-central-log-monitor"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.7.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring Centralized Logging</span> <a title="Permalink" class="permalink" href="#sec-central-log-monitor">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_troubleshoot.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_troubleshoot.xml</li><li><span class="ds-label">ID: </span>sec-central-log-monitor</li></ul></div></div></div></div><p>
   To help keep ahead of potential logging issues and resolve issues before
   they affect logging, you may want to monitor the Centralized Logging Alarms.
  </p><p>
   <span class="bold"><strong>To monitor logging alarms:</strong></span>
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to Operations Console.
    </p></li><li class="listitem "><p>
     From the menu button in the upper left corner, navigate to the
     <span class="bold"><strong>Alarm Definitions</strong></span> page.
    </p></li><li class="listitem "><p>
     Find the alarm definitions that are applied to the various hosts. See the
     <a class="xref" href="#alarmdefinitions" title="18.1.1. Alarm Resolution Procedures">Section 18.1.1, “Alarm Resolution Procedures”</a> for the Centralized Logging Alarm
     Definitions.
    </p></li><li class="listitem "><p>
     Navigate to the <span class="bold"><strong>Alarms</strong></span> page
    </p></li><li class="listitem "><p>
     Find the alarm definitions applied to the various hosts. These should
     match the alarm definitions in the <a class="xref" href="#alarmdefinitions" title="18.1.1. Alarm Resolution Procedures">Section 18.1.1, “Alarm Resolution Procedures”</a>.
    </p></li><li class="listitem "><p>
     See if the alarm is green (good) or is in a bad state. If any are in a bad
     state, see the possible actions to perform in the
     <a class="xref" href="#alarmdefinitions" title="18.1.1. Alarm Resolution Procedures">Section 18.1.1, “Alarm Resolution Procedures”</a>.
    </p></li></ol></div><p>
   You can use this filtering technique in the "Alarms" page to look for the
   following:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     To look for processes that may be down, filter for
     <span class="bold"><strong>"Process"</strong></span> then make sure the process are
     up:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Elasticsearch
      </p></li><li class="listitem "><p>
       Logstash
      </p></li><li class="listitem "><p>
       Beaver
      </p></li><li class="listitem "><p>
       Apache (Kafka)
      </p></li><li class="listitem "><p>
       Kibana
      </p></li><li class="listitem "><p>
       monasca
      </p></li></ul></div></li><li class="listitem "><p>
     To look for sufficient disk space, filter for
     <span class="bold"><strong>"Disk"</strong></span>
    </p></li><li class="listitem "><p>
     To look for sufficient RAM memory, filter for
     <span class="bold"><strong>"Memory"</strong></span>
    </p></li></ol></div></div><div class="sect3" id="sec-central-log-log-collection"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.7.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Situations In Which Logs Might Not Be Collected</span> <a title="Permalink" class="permalink" href="#sec-central-log-log-collection">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_troubleshoot.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_troubleshoot.xml</li><li><span class="ds-label">ID: </span>sec-central-log-log-collection</li></ul></div></div></div></div><p>
   Centralized logging might not collect log data under the following
   circumstances:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     If the Beaver service is not running on one or more of the nodes
     (controller or compute), logs from these nodes will not be collected.
    </p></li></ul></div></div><div class="sect3" id="sec-central-log-error-kibana"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.7.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Error When Creating a Kibana Visualization</span> <a title="Permalink" class="permalink" href="#sec-central-log-error-kibana">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_troubleshoot.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_troubleshoot.xml</li><li><span class="ds-label">ID: </span>sec-central-log-error-kibana</li></ul></div></div></div></div><p>
   When creating a visualization in Kibana you may get an error similiar to
   this:
  </p><div class="verbatim-wrap"><pre class="screen">"logstash-*" index pattern does not contain any of the following field types: number</pre></div><p>
   To resolve this issue:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Log in to Kibana.
    </p></li><li class="listitem "><p>
     Navigate to the <code class="literal">Settings</code> page.
    </p></li><li class="listitem "><p>
     In the left panel, select the <code class="literal">logstash-*</code> index.
    </p></li><li class="listitem "><p>
     Click the <span class="bold"><strong>Refresh</strong></span> button. You may see a
     mapping conflict warning after refreshing the index.
    </p></li><li class="listitem "><p>
     Re-create the visualization.
    </p></li></ol></div></div><div class="sect3" id="sec-central-log-store-log"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.7.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">After Deploying Logging-API, Logs Are Not Centrally Stored</span> <a title="Permalink" class="permalink" href="#sec-central-log-store-log">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_troubleshoot.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_troubleshoot.xml</li><li><span class="ds-label">ID: </span>sec-central-log-store-log</li></ul></div></div></div></div><p>
   If you are using the Logging-API and logs are not being centrally stored,
   use the following checklist to troubleshoot Logging-API.
  </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>☐</th><th>Item</th></tr></thead><tbody><tr><td> </td><td>
       <p>
        Ensure monasca is running.
       </p>
      </td></tr><tr><td> </td><td>
       <p>
        Check any alarms monasca has triggered.
       </p>
      </td></tr><tr><td> </td><td>
       <p>
        Check to see if the Logging-API (monasca-log-api) process alarm has
        triggered.
       </p>
      </td></tr><tr><td> </td><td>
       <p>
        Run an Ansible playbook to get status of the Cloud Lifecycle Manager:
       </p>
<div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts ardana-status.yml</pre></div>
      </td></tr><tr><td> </td><td>
       <p>
        Troubleshoot all specific tasks that have failed on the Cloud Lifecycle Manager.
       </p>
      </td></tr><tr><td> </td><td>Ensure that the Logging-API daemon is up.</td></tr><tr><td> </td><td>
       <p>
        Run an Ansible playbook to try and bring the Logging-API daemon up:
       </p>
<div class="verbatim-wrap"><pre class="screen">ansible-playbook –I hosts/verb_hosts logging-start.yml</pre></div>
      </td></tr><tr><td> </td><td>
       <p>
        If you get errors trying to bring up the daemon, resolve them.
       </p>
      </td></tr><tr><td> </td><td>
       <p>
        Verify the Logging-API configuration settings are correct in the
        configuration file:
       </p>
<div class="verbatim-wrap"><pre class="screen">roles/kronos-api/templates/kronos-apache2.conf.j2</pre></div>
      </td></tr></tbody></table></div><p>
   The following is a sample Logging-API configuration file:
  </p><div class="verbatim-wrap"><pre class="screen">{#
# (c) Copyright 2015-2016 Hewlett Packard Enterprise Development LP
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
#}
Listen {{ kronos_api_host }}:{{ kronos_api_port }}
&lt;VirtualHost *:{{ kronos_api_port }}&gt;
    WSGIDaemonProcess log-api processes=4 threads=4 socket-timeout=300  user={{ kronos_user }} group={{ kronos_group }} python-path=/opt/stack/service/kronos/venv:/opt/stack/service/kronos/venv/bin/../lib/python2.7/site-packages/ display-name=monasca-log-api
    WSGIProcessGroup log-api
    WSGIApplicationGroup log-api
    WSGIScriptAlias / {{ kronos_wsgi_dir }}/app.wsgi
    ErrorLog /var/log/kronos/wsgi.log
    LogLevel info
    CustomLog /var/log/kronos/wsgi-access.log combined

    &lt;Directory /opt/stack/service/kronos/venv/bin/../lib/python2.7/site-packages/monasca_log_api&gt;
      Options Indexes FollowSymLinks MultiViews
      Require all granted
      AllowOverride None
      Order allow,deny
      allow from all
      LimitRequestBody 102400
    &lt;/Directory&gt;

    SetEnv no-gzip 1
&lt;/VirtualHost&gt;</pre></div></div><div class="sect3" id="sec-central-log-slow-log"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.7.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Re-enabling Slow Logging</span> <a title="Permalink" class="permalink" href="#sec-central-log-slow-log">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-central_log_troubleshoot.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-central_log_troubleshoot.xml</li><li><span class="ds-label">ID: </span>sec-central-log-slow-log</li></ul></div></div></div></div><p>
   MariaDB slow logging was enabled by default in earlier versions. Slow
   logging logs slow MariaDB queries to
   <code class="filename">/var/log/mysql/mysql-slow.log</code> on
   FND-MDB hosts.
  </p><p>
   As it is possible for temporary tokens to be logged to the slow log, we have
   disabled slow log in this version for security reasons.
  </p><p>
   To re-enable slow logging follow the following procedure:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Login to the Cloud Lifecycle Manager and set a mariadb service configurable to
     enable slow logging.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/my_cloud</pre></div><div class="orderedlist "><ol class="orderedlist" type="a"><li class="listitem "><p>
       Check slow_query_log is currently disabled with a value of 0:
      </p><div class="verbatim-wrap"><pre class="screen">grep slow ./config/percona/my.cfg.j2
slow_query_log          = 0
slow_query_log_file     = /var/log/mysql/mysql-slow.log</pre></div></li><li class="listitem "><p>
       Enable slow logging in the server configurable template file and confirm
       the new value:
      </p><div class="verbatim-wrap"><pre class="screen">sed -e 's/slow_query_log = 0/slow_query_log = 1/' -i ./config/percona/my.cfg.j2
grep slow ./config/percona/my.cfg.j2
slow_query_log          = 1
slow_query_log_file     = /var/log/mysql/mysql-slow.log</pre></div></li><li class="listitem "><p>
       Commit the changes:
      </p><div class="verbatim-wrap"><pre class="screen">git add -A
git commit -m "Enable Slow Logging"</pre></div></li></ol></div></li><li class="listitem "><p>
     Run the configuration procesor.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible/
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="listitem "><p>
     You will be prompted for an encryption key, and also asked if you want to
     change the encryption key to a new value, and it must be a different key.
     You can turn off encryption by typing the following:
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost config-processor-run.yml -e encrypt="" -e rekey=""</pre></div></li><li class="listitem "><p>
     Create a deployment directory.
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="listitem "><p>
     Reconfigure Percona (note this will restart your mysqld server on your
     cluster hosts).
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts percona-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect2" id="topic1976"><div class="titlepage"><div><div><h3 class="title"><span class="number">18.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Usage Reporting Troubleshooting</span> <a title="Permalink" class="permalink" href="#topic1976">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_metering.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_metering.xml</li><li><span class="ds-label">ID: </span>topic1976</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the ceilometer service.
 </p><p>
  This page describes troubleshooting scenarios for ceilometer.
 </p><div class="sect3" id="idg-all-operations-troubleshooting-ts-metering-xml-6"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.7.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Logging</span> <a title="Permalink" class="permalink" href="#idg-all-operations-troubleshooting-ts-metering-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_metering.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_metering.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-ts-metering-xml-6</li></ul></div></div></div></div><p>
   Logs for the various running components in the Overcloud Controllers can be
   found at <span class="emphasis"><em>/var/log/ceilometer.log</em></span>
  </p><p>
   The Upstart for the services also logs data at
   <span class="bold"><strong>/var/log/upstart</strong></span>
  </p></div><div class="sect3" id="idg-all-operations-troubleshooting-ts-metering-xml-7"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.7.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Modifying</span> <a title="Permalink" class="permalink" href="#idg-all-operations-troubleshooting-ts-metering-xml-7">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_metering.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_metering.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-ts-metering-xml-7</li></ul></div></div></div></div><p>
   Change the level of debugging in ceilometer by editing the
   <span class="bold"><strong>ceilometer.conf</strong></span>
   file located at
   <span class="bold"><strong>/etc/ceilometer/ceilometer.conf</strong></span>.
   To log the maximum amount of information, change the
   <span class="bold"><strong>level</strong></span>
   entry to <span class="bold"><strong>DEBUG</strong></span>.
  </p><p>
   <span class="bold"><strong>Note</strong></span>: When the logging level for a service
   is changed, that service must be re-started before the change will take
   effect.
  </p><p>
   This is an excerpt of the <span class="bold"><strong>ceilometer.conf</strong></span>
   configuration file showing where to make changes:
  </p><div class="verbatim-wrap"><pre class="screen">[loggers]
 keys: root

[handlers]
 keys: watchedfile, logstash

[formatters]
 keys: context, logstash

[logger_root]
 qualname: root
 handlers: watchedfile, logstash
 level: NOTSET</pre></div></div><div class="sect3" id="qerrors"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.7.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Messaging/Queuing Errors</span> <a title="Permalink" class="permalink" href="#qerrors">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_metering.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_metering.xml</li><li><span class="ds-label">ID: </span>qerrors</li></ul></div></div></div></div><p>
   ceilometer relies on a message bus for passing data between the various
   components. In high-availability scenarios, RabbitMQ servers are used for
   this purpose. If these servers are not available, the ceilometer log will
   record errors during "Connecting to AMQP" attempts.
  </p><p>
   These errors may indicate that the RabbitMQ messaging nodes are not running
   as expected and/or the RPC publishing pipeline is stale. When these errors
   occur, re-start the instances.
  </p><p>
   Example error:
  </p><div class="verbatim-wrap"><pre class="screen">Error: unable to connect to node 'rabbit@xxxx-rabbitmq0000': nodedown</pre></div><p>
   Use the RabbitMQ CLI to re-start the instances and then the host.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     Restart the downed cluster node.
    </p><div class="verbatim-wrap"><pre class="screen">sudo invoke-rc.d rabbitmq-server start</pre></div></li><li class="listitem "><p>
     Restart the RabbitMQ host
    </p><div class="verbatim-wrap"><pre class="screen">sudo rabbitmqctl start_app</pre></div></li></ol></div></div></div></div><div class="sect1" id="troubleshooting-orchestration"><div class="titlepage"><div><div><h2 class="title"><span class="number">18.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Orchestration Troubleshooting</span> <a title="Permalink" class="permalink" href="#troubleshooting-orchestration">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_orchestration.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_orchestration.xml</li><li><span class="ds-label">ID: </span>troubleshooting-orchestration</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the Orchestration services.
  Troubleshooting scenarios with resolutions for the Orchestration services.
 </p><div class="sect2" id="troubleshootingheat"><div class="titlepage"><div><div><h3 class="title"><span class="number">18.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Heat Troubleshooting</span> <a title="Permalink" class="permalink" href="#troubleshootingheat">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_heat.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_heat.xml</li><li><span class="ds-label">ID: </span>troubleshootingheat</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the heat service.
 </p><div class="sect3" id="rpc-timeout-heat"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.8.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">RPC timeout on Heat Stack Creation</span> <a title="Permalink" class="permalink" href="#rpc-timeout-heat">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_heat.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_heat.xml</li><li><span class="ds-label">ID: </span>rpc-timeout-heat</li></ul></div></div></div></div><p>
   If you exerience a remote procedure call (RPC) timeout failure when
   attempting <code class="literal">heat stack-create</code>, you can work around the
   issue by increasing the timeout value and purging records of deleted stacks
   from the database.  To do so, follow the steps below. An example of the
   error is:
  </p><div class="verbatim-wrap"><pre class="screen">MessagingTimeout: resources.XXX-LCP-Pair01.resources[0]: Timed out waiting for a reply to message ID e861c4e0d9d74f2ea77d3ec1984c5cb6</pre></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Increase the timeout value.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/my_cloud/config/heat</pre></div></li><li class="step "><p>
     Make changes to heat config files. In <code class="filename">heat.conf.j2</code>, add this timeout value:
    </p><div class="verbatim-wrap"><pre class="screen">rpc_response_timeout=300</pre></div><p>
     Commit your changes:
    </p><div class="verbatim-wrap"><pre class="screen">git commit -a -m "some message"</pre></div></li><li class="step "><p>
     Move to the <code class="filename">ansible</code> directory and run the following
     playbooks:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Change to the <code class="filename">scratch</code> directory and run
     <code class="literal">heat-reconfigure</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts heat-reconfigure.yml</pre></div></li><li class="step "><p>
     Purge records of deleted stacks from the database. First delete all stacks
     that are in failed state. Then execute the following
    </p><div class="verbatim-wrap"><pre class="screen">sudo /opt/stack/venv/heat-20151116T000451Z/bin/python2
/opt/stack/service/heat-engine/venv/bin/heat-manage
--config-file /opt/stack/service/heat-engine-20151116T000451Z/etc/heat/heat.conf
--config-file /opt/stack/service/heat-engine-20151116T000451Z/etc/heat/engine.conf purge_deleted 0</pre></div></li></ol></div></div></div><div class="sect3" id="hrat-stack-create-errors"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.8.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">General Heat Stack Creation Errors</span> <a title="Permalink" class="permalink" href="#hrat-stack-create-errors">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_heat.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_heat.xml</li><li><span class="ds-label">ID: </span>hrat-stack-create-errors</li></ul></div></div></div></div><p>
   Generally in heat, when a timeout occurs it means that the underlying
   resource service such as nova, neutron, or cinder fails to complete the
   required action. No matter what error this underlying service reports, heat
   simply reports it back. So in the case of time-out in <code class="literal">heat stack
   create</code>, look at the logs of the underlying services, most
   importantly the nova service, to understand the reason for the timeout.
  </p></div><div class="sect3" id="heat-stack-create-failure"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.8.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Multiple Heat Stack Create Failure</span> <a title="Permalink" class="permalink" href="#heat-stack-create-failure">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_heat.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_heat.xml</li><li><span class="ds-label">ID: </span>heat-stack-create-failure</li></ul></div></div></div></div><p>
   The monasca AlarmDefinition resource,
   <code class="literal">OS::monasca::AlarmDefinition</code> used for heat autoscaling,
   consists of an optional property <span class="bold"><strong>name</strong></span> for
   defining the alarm name. In case this optional property being specified in
   the heat template, this name must be unique in the same project of the
   system. Otherwise, multiple heat stack create using this heat template will
   fail with the following conflict:
  </p><div class="verbatim-wrap"><pre class="screen">| cpu_alarm_low  | 5fe0151b-5c6a-4a54-bd64-67405336a740 | HTTPConflict: resources.cpu_alarm_low: An alarm definition already exists for project / tenant: 835d6aeeb36249b88903b25ed3d2e55a named: CPU utilization less than 15 percent  | CREATE_FAILED  | 2016-07-29T10:28:47 |</pre></div><p>
   This is due to the fact that the monasca registers the alarm definition name
   using this name property when it is defined in the heat template. This name
   must be unique.
  </p><p>
   To avoid this problem, if you want to define an alarm name using this
   property in the template, you must be sure this name is unique within a
   project in the system. Otherwise, you can leave this optional property
   undefined in your template. In this case, the system will create an unique
   alarm name automatically during heat stack create.
  </p></div><div class="sect3" id="id-1.5.20.11.3.6"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.8.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Unable to Retrieve QOS Policies</span> <a title="Permalink" class="permalink" href="#id-1.5.20.11.3.6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_heat.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_heat.xml</li><li><span class="ds-label">ID: </span><span class="ds-message">no ID found</span></li></ul></div></div></div></div><p>
   Launching the Orchestration Template Generator may trigger the message:
   <code class="literal">Unable to retrieve resources Qos Policies</code>. This is a
   known <a class="link" href="https://storyboard.openstack.org/#!/story/2003523" target="_blank">upstream
   bug</a>. This information message can be ignored.
  </p></div></div><div class="sect2" id="ts-magnum"><div class="titlepage"><div><div><h3 class="title"><span class="number">18.8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting Magnum Service</span> <a title="Permalink" class="permalink" href="#ts-magnum">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_magnum.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_magnum.xml</li><li><span class="ds-label">ID: </span>ts-magnum</li></ul></div></div></div></div><p>
  Troubleshooting scenarios with resolutions for the Magnum service. Magnum
  Service provides container orchestration engines such as Docker Swarm,
  Kubernetes, and Apache Mesos available as first class resources. You can use
  this guide to help with known issues and troubleshooting of Magnum services.
 </p><div class="sect3" id="idg-all-operations-troubleshooting-ts-magnum-xml-6"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.8.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Magnum cluster fails to create</span> <a title="Permalink" class="permalink" href="#idg-all-operations-troubleshooting-ts-magnum-xml-6">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_magnum.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_magnum.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-ts-magnum-xml-6</li></ul></div></div></div></div><p>
   Typically, small size clusters need about 3-5 minutes to stand up. If
   cluster stand up takes longer, you may proceed with troubleshooting, not
   waiting for status to turn to CREATE_FAILED after timing out.
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
      Use <code class="literal">heat resource-list <span class="emphasis"><em>STACK-ID</em></span></code> to
      identify which heat stack resource is stuck in
      <span class="bold"><strong>CREATE_IN_PROGRESS</strong></span>.
    </p><div id="id-1.5.20.11.4.3.3.1.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The main heat stack has nested stacks, one for kubemaster(s) and one
      for kubeminion(s). These stacks are visible as resources of type
      <span class="emphasis"><em>OS::heat::ResourceGroup</em></span> (in parent stack) and
      <span class="emphasis"><em>file:///...</em></span> in nested stack. If any resource
      remains in <span class="emphasis"><em>CREATE_IN_PROGRESS</em></span> state within the
      nested stack, the overall state of the resource will be
      <span class="emphasis"><em>CREATE_IN_PROGRESS</em></span>.
     </p></div><div class="verbatim-wrap"><pre class="screen">$ heat resource-list -n2 22385a42-9e15-49d9-a382-f28acef36810
+-------------------------------+--------------------------------------+--------------------------------------+--------------------+----------------------+------------------------------------------------------------------+
| resource_name                 | physical_resource_id                 | resource_type                        | resource_status    | updated_time         | stack_name                                                       |
+-------------------------------+--------------------------------------+--------------------------------------+--------------------+----------------------+------------------------------------------------------------------+
| api_address_floating_switch   | 06b2cc0d-77f9-4633-8d96-f51e2db1faf3 | Magnum::FloatingIPAddressSwitcher    | CREATE_COMPLETE    | 2017-04-10T21:25:10Z | my-cluster-z4aquda2mgpv                                          |
. . .

| fixed_subnet                  | d782bdf2-1324-49db-83a8-6a3e04f48bb9 | OS::neutron::Subnet                  | CREATE_COMPLETE    | 2017-04-10T21:25:11Z | my-cluster-z4aquda2mgpv                                          |
| kube_masters                  | f0d000aa-d7b1-441a-a32b-17125552d3e0 | OS::heat::ResourceGroup              | CREATE_IN_PROGRESS | 2017-04-10T21:25:10Z | my-cluster-z4aquda2mgpv                                          |
| 0                             | b1ff8e2c-23dc-490e-ac7e-14e9f419cfb6 | file:///opt/s...ates/kubemaster.yaml | CREATE_IN_PROGRESS | 2017-04-10T21:25:41Z | my-cluster-z4aquda2mgpv-kube_masters-utyggcbucbhb                |
| kube_master                   | 4d96510e-c202-4c62-8157-c0e3dddff6d5 | OS::nova::Server                     | CREATE_IN_PROGRESS | 2017-04-10T21:25:48Z | my-cluster-z4aquda2mgpv-kube_masters-utyggcbucbhb-0-saafd5k7l7im |
. . .</pre></div></li><li class="listitem "><p>
     If stack creation failed on some native OpenStack resource, like
     <span class="bold"><strong>OS::nova::Server</strong></span> or
     <span class="bold"><strong>OS::neutron::Router</strong></span>, proceed with
     respective service troubleshooting. This type of error usually does not
     cause time out, and cluster turns into status
     <span class="bold"><strong>CREATE_FAILED</strong></span> quickly. The underlying
     reason of the failure, reported by heat, can be checked via the
     <code class="literal">magnum cluster-show</code> command.
    </p></li><li class="listitem "><p>
     If stack creation stopped on resource of type OS::heat::WaitCondition,
     heat is not receiving notification from cluster VM about bootstrap
     sequence completion. Locate corresponding resource of type
     <span class="bold"><strong>OS::nova::Server</strong></span> and use its
     <span class="bold"><strong>physical_resource_id</strong></span> to get information
     about the VM (which should be in status
     <span class="bold"><strong>CREATE_COMPLETE</strong></span>)
    </p><div class="verbatim-wrap"><pre class="screen">$ openstack server show 4d96510e-c202-4c62-8157-c0e3dddff6d5
+--------------------------------------+---------------------------------------------------------------------------------------------------------------+
| Property                             | Value                                                                                                         |
+--------------------------------------+---------------------------------------------------------------------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                                                                                        |
| OS-EXT-AZ:availability_zone          | nova                                                                                                          |
| OS-EXT-SRV-ATTR:host                 | comp1                                                                                                         |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | comp1                                                                                                         |
| OS-EXT-SRV-ATTR:instance_name        | instance-00000025                                                                                             |
| OS-EXT-STS:power_state               | 1                                                                                                             |
| OS-EXT-STS:task_state                | -                                                                                                             |
| OS-EXT-STS:vm_state                  | active                                                                                                        |
| OS-SRV-USG:launched_at               | 2017-04-10T22:10:40.000000                                                                                    |
| OS-SRV-USG:terminated_at             | -                                                                                                             |
| accessIPv4                           |                                                                                                               |
| accessIPv6                           |                                                                                                               |
| config_drive                         |                                                                                                               |
| created                              | 2017-04-10T22:09:53Z                                                                                          |
| flavor                               | m1.small (2)                                                                                                  |
| hostId                               | eb101a0293a9c4c3a2d79cee4297ab6969e0f4ddd105f4d207df67d2                                                      |
| id                                   | 4d96510e-c202-4c62-8157-c0e3dddff6d5                                                                          |
| image                                | fedora-atomic-26-20170723.0.x86_64 (4277115a-f254-46c0-9fb0-fffc45d2fd38)                                     |
| key_name                             | testkey                                                                                                       |
| metadata                             | {}                                                                                                            |
| name                                 | my-zaqshggwge-0-sqhpyez4dig7-kube_master-wc4vv7ta42r6                                                         |
| os-extended-volumes:volumes_attached | [{"id": "24012ce2-43dd-42b7-818f-12967cb4eb81"}]                                                              |
| private network                      | 10.0.0.14, 172.31.0.6                                                                                         |
| progress                             | 0                                                                                                             |
| security_groups                      | my-cluster-z7ttt2jvmyqf-secgroup_base-gzcpzsiqkhxx, my-cluster-z7ttt2jvmyqf-secgroup_kube_master-27mzhmkjiv5v |
| status                               | ACTIVE                                                                                                        |
| tenant_id                            | 2f5b83ab49d54aaea4b39f5082301d09                                                                              |
| updated                              | 2017-04-10T22:10:40Z                                                                                          |
| user_id                              | 7eba6d32db154d4790e1d3877f6056fb                                                                              |
+--------------------------------------+---------------------------------------------------------------------------------------------------------------+</pre></div></li><li class="listitem "><p>
     Use the floating IP of the master VM to log into first master node. Use
     the appropriate username below for your VM type. Passwords should not be
     required as the VMs should have public ssh key installed.
    </p><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th align="center">VM Type</th><th align="center">Username</th></tr></thead><tbody><tr><td>Kubernetes or Swarm on Fedora Atomic</td><td align="center">fedora</td></tr><tr><td>Kubernetes on CoreOS</td><td align="center">core</td></tr><tr><td>Mesos on Ubuntu</td><td align="center">ubuntu</td></tr></tbody></table></div></li><li class="listitem "><p>
     Useful dianostic commands
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Kubernetes cluster on Fedora Atomic
      </p><div class="verbatim-wrap"><pre class="screen">sudo journalctl --system
sudo journalctl -u cloud-init.service
sudo journalctl -u etcd.service
sudo journalctl -u docker.service
sudo journalctl -u kube-apiserver.service
sudo journalctl -u kubelet.service
sudo journalctl -u wc-notify.service</pre></div></li><li class="listitem "><p>
       Kubernetes cluster on CoreOS
      </p><div class="verbatim-wrap"><pre class="screen">sudo journalctl --system
sudo journalctl -u oem-cloudinit.service
sudo journalctl -u etcd2.service
sudo journalctl -u containerd.service
sudo journalctl -u flanneld.service
sudo journalctl -u docker.service
sudo journalctl -u kubelet.service
sudo journalctl -u wc-notify.service</pre></div></li><li class="listitem "><p>
       Swarm cluster on Fedora Atomic
      </p><div class="verbatim-wrap"><pre class="screen">sudo journalctl --system
sudo journalctl -u cloud-init.service
sudo journalctl -u docker.service
sudo journalctl -u swarm-manager.service
sudo journalctl -u wc-notify.service</pre></div></li><li class="listitem "><p>
       Mesos cluster on Ubuntu
      </p><div class="verbatim-wrap"><pre class="screen">sudo less /var/log/syslog
sudo less /var/log/cloud-init.log
sudo less /var/log/cloud-init-output.log
sudo less /var/log/os-collect-config.log
sudo less /var/log/marathon.log
sudo less /var/log/mesos-master.log</pre></div></li></ul></div></li></ol></div></div></div></div><div class="sect1" id="troubleshooting-tools"><div class="titlepage"><div><div><h2 class="title"><span class="number">18.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting Tools</span> <a title="Permalink" class="permalink" href="#troubleshooting-tools">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-ts_tools.xml" title="Edit the source file for this section">Edit source</a></h2><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-ts_tools.xml</li><li><span class="ds-label">ID: </span>troubleshooting-tools</li></ul></div></div></div></div><p>
  Tools to assist with troubleshooting issues in your cloud. Additional
  troubleshooting information is available at <a class="xref" href="#general-troubleshooting" title="18.1. General Troubleshooting">Section 18.1, “General Troubleshooting”</a>.
 </p><div class="sect2" id="idg-all-operations-troubleshooting-troubleshooting-sosreport-xml-1"><div class="titlepage"><div><div><h3 class="title"><span class="number">18.9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Retrieving the SOS Report</span> <a title="Permalink" class="permalink" href="#idg-all-operations-troubleshooting-troubleshooting-sosreport-xml-1">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-troubleshooting_sosreport.xml" title="Edit the source file for this section">Edit source</a></h3><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-troubleshooting_sosreport.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-troubleshooting-sosreport-xml-1</li></ul></div></div></div></div><p>
  The SOS report provides debug level information about your environment to
  assist in troubleshooting issues. When troubleshooting and debugging issues
  in your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> environment you can run an ansible playbook that will
  provide you with a full debug report, referred to as a SOS report. These
  reports can be sent to the support team when seeking assistance.
 </p><div class="sect3" id="idg-all-operations-troubleshooting-troubleshooting-sosreport-xml-4"><div class="titlepage"><div><div><h4 class="title"><span class="number">18.9.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Retrieving the SOS Report</span> <a title="Permalink" class="permalink" href="#idg-all-operations-troubleshooting-troubleshooting-sosreport-xml-4">#</a><a class="report-bug" target="_blank" href="https://github.com/SUSE-Cloud/doc-cloud/blob/maintenance/cloud_9/xml/operations-troubleshooting-troubleshooting_sosreport.xml" title="Edit the source file for this section">Edit source</a></h4><div class="doc-status"><ul><li><span class="ds-label">File Name: </span>operations-troubleshooting-troubleshooting_sosreport.xml</li><li><span class="ds-label">ID: </span>idg-all-operations-troubleshooting-troubleshooting-sosreport-xml-4</li></ul></div></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Run the SOS report ansible playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts sosreport-run.yml</pre></div></li><li class="step "><p>
     Retrieve the SOS report tarballs, which will be in the following
     directories on your Cloud Lifecycle Manager:
    </p><div class="verbatim-wrap"><pre class="screen">/tmp
/tmp/sosreport-report-archives/</pre></div></li><li class="step "><p>
     You can then use these reports to troubleshoot issues further or provide
     to the support team when you reach out to them.
    </p></li></ol></div></div><div id="id-1.5.20.12.3.3.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
    The SOS Report may contain sensitive information because service
    configuration file data is included in the report. Please remove any
    sensitive information before sending the SOSReport tarball externally.
   </p></div></div></div></div></div></div></div><div class="page-bottom"><div class="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span class="_share-fb bottom-button">Facebook</span><span class="spacer"> • </span><span class="_share-in bottom-button">LinkedIn</span><span class="spacer"> • </span><span class="_share-tw bottom-button">Twitter</span><span class="spacer"> • </span><span class="_share-mail bottom-button">E-Mail</span></span></div><div class="print"><span class="_print-button bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2021 
        SUSE</p><ul><li><a href="https://jobs.suse.com/" target="_top">Careers</a></li><li><a href="https://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="https://www.suse.com/company/about/" target="_top">About</a></li><li><a href="https://www.suse.com/contact/" target="_top">Contact Us</a></li></ul></div></div></body></html>